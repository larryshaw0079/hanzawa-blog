<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.0"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Hanzawa の 部屋</title><meta property="og:type" content="blog"><meta property="og:title" content="Hanzawa の 部屋"><meta property="og:url" content="http://qfxiao.me/"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:locale" content="zh_CN"><meta property="article:author" content="Hanzawa"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="http://qfxiao.me/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://qfxiao.me"},"headline":"Hanzawa の 部屋","image":["http://qfxiao.me/img/og_image.png"],"author":{"@type":"Person","name":"Hanzawa"},"description":null}</script><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Hanzawa の 部屋" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/"><img class="thumbnail" src="http://qfxiao.me/img/image-20200301230011495.png" alt="Discovering Physical Concepts with Neural Networks"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-03-01T14:55:02.000Z" title="2020-03-01T14:55:02.000Z">2020-03-01</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Misc/">Misc</a></span><span class="level-item">13 分钟 读完 (大约 2019 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/">Discovering Physical Concepts with Neural Networks</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>如题目所示，本文的目的是利用神经网络来发掘物理概念。其思路是从实验数据学到表示，然后用学到的表示来回答物理问题，由此物理概念可以从学到的表示来提取出。作者进行了4个实验：</p>
<ol>
<li>在阻尼振动实验中，模型学到了相关的物理参数；</li>
<li>在角动量守恒实验中，模型预测了质点的运动；</li>
<li>给定量子系统的观测数据，模型正确的识别出了量子状态的自由度；</li>
<li>给定从地球观测的太阳和火星的位置时间序列数据，模型发现了日心说模型。</li>
</ol>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><p>作者在附录中对神经网络的基础知识进行了介绍，这里不再赘述，只截取了一些相对前沿的内容。</p>
<img src="http://qfxiao.me/img/image-20200301230151518.png" style="zoom:67%;" />

<h2 id="Variational-Autoencoders"><a href="#Variational-Autoencoders" class="headerlink" title="Variational Autoencoders"></a>Variational Autoencoders</h2><p>本文用到的模型基础是VAE：</p>
<img src="http://qfxiao.me/img/image-20200301230200451.png" style="zoom:67%;" />

<h3 id="Representation-Learning"><a href="#Representation-Learning" class="headerlink" title="Representation Learning"></a>Representation Learning</h3><p><em>Representation learning</em>的主要目标是将数据映射到一个隐向量 (encoder)，为了保证隐向量包含了所有相关信息， 那么应该能够从隐向量还原原数据 (decoder)。传统的Autoencoder是这个思想的最简单实现，而VAE则将AE和<em>Variational Inference</em>结合了起来，是一种经典的生成式模型。现在很多研究关注<em>Disentangled Representation Learning</em>，也就是说我们希望模型能够无监督地学习数据，从中学到有意义的表示。</p>
<h3 id="boldsymbol-beta-VAE"><a href="#boldsymbol-beta-VAE" class="headerlink" title="$\boldsymbol \beta$-VAE"></a>$\boldsymbol \beta$-VAE</h3><p>$\beta$-VAE是一种特殊的VAE，也是一个经典的<em>Disentangled Representation Learning</em>模型，它和VAE主要的区别是对KL散度一项加上了权重$\beta$进行调节：<br>$$<br>C_\beta(x)=-\left[\mathbb{E}_{z\sim p_\phi(z|x)}\log p_\theta(x|z)\right] + \beta D_\text{KL}\left[p_\phi(z|x)\parallel h(z)\right]<br>$$<br>如果假设$p_\phi(z|x)=\mathcal{N}(\mu,\sigma)$，那么损失函数可以进行简化：<br>$$<br>C_\beta(x)=\parallel \hat{x} - x \parallel^2_2-\frac{\beta}{2}\left(\sum\limits_i\log(\sigma_i^2)-\mu_i^2-\sigma_i^2\right)+C<br>$$</p>
<h1 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h1><h2 id="Network-Structure-SciNet"><a href="#Network-Structure-SciNet" class="headerlink" title="Network Structure: SciNet"></a>Network Structure: <em>SciNet</em></h2><p>模仿物理学家建模物理问题的过程，作者提出了<em>SciNet</em>，如下图所示：</p>
<img src="http://qfxiao.me/img/image-20200301225708559.png" style="zoom:67%;" />

<p>物理学家在建模物理问题的时候，往往是从一些实验数据出发，根据物理常识提取更加精练的表示，然后用学到的表示来回答物理问题。</p>
<p>对于单纯的输入输出问题，<em>SciNet</em>可以看作是一个映射，$F:\mathcal{O}\times\mathcal{Q}\rightarrow\mathcal{A}$。$\mathcal{O}$是可能的实验数据集合，$\mathcal{Q}$是可能的问题集合，$\mathcal{A}$是可能的答案集合。可以将其分为两个步骤：编码过程$E:\mathcal{O}\rightarrow\mathcal{R}$从实验数据学到表示，解码过程$D:\mathcal{R}\times \mathcal{Q}\rightarrow \mathcal{A}$根据给定的问题从表示来回答问题。由此，$F(o,q)=D(E(o),q)$。在实现方面，<em>SciNet</em>采用的是全连接网络。</p>
<h2 id="Training-and-Testing-SciNet"><a href="#Training-and-Testing-SciNet" class="headerlink" title="Training and Testing SciNet"></a>Training and Testing <em>SciNet</em></h2><p>用来训练的数据形式为$(o,q,a_{cor}(o,q))$，观测$o$和问题$q$分别从观测集$\mathcal{O}$和问题集$\mathcal{Q}$选出，$a_{cor}(o,q)$为对应的正确答案。在训练过程中，我们希望准确度尽量高，并且学到<em>minimal uncorrelated representations</em>。为此，作者采用<em>disentangling variational autoencoder</em>作为模型。</p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>在文中，作者进行了4个实验来验证模型的有效性。</p>
<h2 id="Damped-Pendulum"><a href="#Damped-Pendulum" class="headerlink" title="Damped Pendulum"></a>Damped Pendulum</h2><p>阻尼振动实验：</p>
<ul>
<li><p>任务：预测一维阻尼振动在不同时间的位置。</p>
</li>
<li><p>物理模型：$-kx-b\dot{x}=m\ddot{x}$，$k$为弹性模量，$b$为阻尼系数，通解为$x(t)=A_0e^{-\frac{b}{2m}t}\cos(\omega t+\delta_0), \space \omega=\sqrt{\frac{k}{m}}\sqrt{1-\frac{b^2}{4mk}}$</p>
</li>
<li><p>观测数据：位置时间序列数据$o=[x(t_i)]_{i\in{1,\cdots,50}}\in\mathbb{R}^{50}$，时间间隔相等，质量$m=1\text{kg}$，振幅$A_0=1\text{m}$，相位$\delta_0=0$，弹性模量$k\in[5,10]\text{kg}/\text{s}^2$，阻尼系数$b\in[0.5,1]\text{kg}/\text{s}$。</p>
</li>
<li><p>问题：预测$q=t_\text{pred}\in\mathbb{R}$</p>
</li>
</ul>
<p><img src="http://qfxiao.me/img/image-20200301225805576.png" alt=""></p>
<p>隐变量大小设置为3，结果如下图所示：</p>
<img src="http://qfxiao.me/img/image-20200301225838353.png" style="zoom:67%;" />

<p>(b)中的三幅图分别是学到的三个隐变量和我们感兴趣的参数$k$和$b$的关系图。第一幅图中变量$1$与$b$几乎完全线性相关，与$k$基于线性无关，变量$2$只和$k$相关。变量$3$几乎为一个常数，故不提供额外的信息。由此作者认为<em>SciNet</em>学到了我们关心的两个参数的知识。</p>
<h2 id="Conservation-of-Angular-Momentum"><a href="#Conservation-of-Angular-Momentum" class="headerlink" title="Conservation of Angular Momentum"></a>Conservation of Angular Momentum</h2><p>角动量守恒实验：</p>
<ul>
<li>任务：预测一个由长度为$r$的绳子捆绑着的旋转质点在位置$(0,r)$经一个自由质点撞击后的位置</li>
<li>物理模型：给定撞击之前的角动量，自由质点撞击之后的速度，旋转质点在撞击之后在时间$t_\text{pred}^\prime$的位置可以由角动量守恒定律给出：</li>
</ul>
<p>$$<br>J=m_\text{rot}r^2\omega-rm_\text{free}(\mathbf{v}_\text{free})_x=m_\text{rot}r^2\omega^\prime-rm_\text{free}(\mathbf{v}^\prime_\text{free})_x=J^\prime<br>$$</p>
<ul>
<li>观测数据：在撞击之前两个质点的位置数据$o=[(t_i^\text{rot},q_\text{rot}(t_i^\text{rot})),(t_i^\text{free},q_\text{free}(t_i^\text{free}))]_{i\in{1,\cdots,5}}$，质量为固定值，半径$r$也为固定值。数据添加高斯噪声。</li>
<li>问题：预测撞击之后自由质点在时间$t_\text{pred}^\prime$的位置</li>
</ul>
<p><img src="http://qfxiao.me/img/image-20200301225858626.png" alt=""></p>
<p>实验室意图如下：</p>
<img src="http://qfxiao.me/img/image-20200301225917614.png" style="zoom:67%;" />

<p>实验结果表明<em>SciNet</em>能够正确预测质点撞击之后的位置，同时对噪音鲁棒。根据(b)，隐变量和角动量存在线性相关关系，作者认为<em>SciNet</em>学到了守恒的动量这一概念。</p>
<h2 id="Representation-of-Qubits"><a href="#Representation-of-Qubits" class="headerlink" title="Representation of Qubits"></a>Representation of Qubits</h2><p>量子比特实验：</p>
<ul>
<li>任务：预测在$n=1,2$的纯$n$量子位状态$\psi\in\mathbb{C}^{2^n}$下任何二进制投影测量$\omega\in\mathbb{C}^{2^n}$的测量概率。</li>
<li>物理模型：在执行测量$\omega\in\mathbb{C}^{2^n}$的状态$\psi\in\mathbb{C}^{2^n}$下测量0的概率$p(\omega,\psi)$由$p(\omega,\psi)=|\left&lt;\omega,\psi\right&gt;|^2$给定</li>
<li>观测数据：状态$\psi: o=[p(\alpha_i,\psi)]<em>{i\in{i,\cdots,n_1}}$的操作参数化：表示一组固定的随机二元射影测量值$\mathcal{M}_1={\alpha_1,\cdots,\alpha</em>{n_1}}$（一个量子位$n_1 = 10$，两个量子位$n_1 = 30$）</li>
<li>问题：对于固定的一组随机二元射影测量$\mathcal{M}<em>2={\beta_1,\cdots,\beta</em>{n_2}}$，测量$\omega:q=[p(\beta_i,\omega)]_{i\in{1,\cdots,n_2}}$的Operational参数化（一个量子位$n_2 = 10$，两个量子位$n_2 = 30$）</li>
</ul>
<p><img src="http://qfxiao.me/img/image-20200301225929696.png" alt=""></p>
<p>实验结果如下：</p>
<img src="http://qfxiao.me/img/image-20200301225958663.png" style="zoom:67%;" />

<p>通过实验发现，<em>SciNet</em>可以在不提供先验物理知识的条件下确定表述状态$\psi$最小的参数数量。同时，<em>SciNet</em>还能分辨<em>tomographically complete</em>和<em>tomographically incomplete</em>。</p>
<h2 id="Heliocentric-Model-of-the-Solar-System"><a href="#Heliocentric-Model-of-the-Solar-System" class="headerlink" title="Heliocentric Model of the Solar System"></a>Heliocentric Model of the Solar System</h2><p>日心说模型：</p>
<ul>
<li>问题：在给定初始条件下预测相对与地球的太阳和火星的角度$\theta_M(t)$和$\theta_S(t)$</li>
<li>物理模型：地球和火星围绕太阳以一定角速度做近似圆周运动</li>
<li>观测数据：给定初始角度，随机选择周周期的哥白尼的观测数据</li>
</ul>
<p><img src="http://qfxiao.me/img/image-20200301230035141.png" alt=""></p>
<p>模型的实现稍有变化，如下图所示：</p>
<img src="http://qfxiao.me/img/image-20200301230011495.png" style="zoom:67%;" />

<p>这样，对于不同时间都对应一个隐变量$r(t_i)$，而且隐变量是时间依赖的，对于一个隐变量$r(t_i)$有一个解码器来输出答案。</p>
<img src="http://qfxiao.me/img/image-20200301230101709.png" style="zoom:67%;" />

<p>实验结果表示，<em>SciNet</em>不仅正确预测了太阳和火星相对地球的角度，同时隐变量揭示了火星和地球相对太阳的角度。</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/"><img class="thumbnail" src="http://qfxiao.me/img/image-20200109102830713.png" alt="Transfer Anomaly Detection by Inferring Latent Domain Representations"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-02-27T12:02:18.000Z" title="2020-02-27T12:02:18.000Z">2020-02-27</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span><span class="level-item">12 分钟 读完 (大约 1731 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/">Transfer Anomaly Detection by Inferring Latent Domain Representations</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者提出了一种利用迁移学习提升target domain异常检测性能的算法。文中指出现有的基于迁移学习的异常检测算法需要对每个 target domain 进行单独训练，这样做会带来很大的计算开销。本文通过<em>latent domain vectors</em>来实现无需重新训练的异常检测。<em>latent domain vectors</em>是domain的一种隐含表示，通过该domain中的正常样本得到。在本文中，<em>anomaly score function</em>通过Auto-encoder得到。</p>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>令$\mathbf{X}<em>d^+:={\mathbf{x}^+</em>{dn}}^{N^+<em>d}</em>{n=1}$为第$d$个domain的异常样本集，$\mathbf{x}_{dn}^+\in\mathbb{R}^M$为其中第$n$个样本的$M$维特征向量，$N^+_d$为第$d$个domain异常样本的数量。</p>
<p>类似的，令$\mathbf{X}<em>d^-:={\mathbf{x}^-</em>{dn}}^{N^-<em>d}</em>{n=1}$为第$d$个domain的正常样本集。我们假设对于每个domain都有$N^+_d\ll N^-_d$，且特征向量维度都为$M$。</p>
<p>假设在 source domain $D_S$都有正常样本和异常样本，记为${\mathbf{X}^+<em>d\cup\mathbf{X}_d^-}^{D_S}</em>{d=1}$，在 target domain $D_T$只有正常样本${\mathbf{X}<em>d^-}^{D_S+D_T}</em>{d=D_S+1}$。我们的目标是得到一个对于 target domain 合适的 domain-specific 的异常打分函数。</p>
<img src="http://qfxiao.me/img/image-20200109102606099.png" style="zoom:67%;" />

<h2 id="Domain-specific-Anomaly-Score-Function"><a href="#Domain-specific-Anomaly-Score-Function" class="headerlink" title="Domain-specific Anomaly Score Function"></a>Domain-specific Anomaly Score Function</h2><p>我们基于Auto-encoder定义异常打分函数。对于每个domain，我们假设存在一个$K$维的隐变量$\mathbf{z}<em>d\in\mathbb{R}^K$。对于第$d$个 domain，异常打分函数定义如下：<br>$$<br>s_\theta(\mathbf{x}</em>{dn}|\mathbf{z}<em>d):=\parallel\mathbf{x}</em>{dn}-G_{\theta_G}(F_{\theta_F}(\mathbf{x}_{dn},\mathbf{z}_d))\parallel^2<br>$$<br>其中参数$\theta:=(\theta_G,\theta_F)$在所有 domain 之间共享。</p>
<h2 id="Models-for-Latent-Domain-Vectors"><a href="#Models-for-Latent-Domain-Vectors" class="headerlink" title="Models for Latent Domain Vectors"></a>Models for Latent Domain Vectors</h2><p>隐变量$\mathbf{z}_d$是无法观测到的，只能通过数据来估计。首先$\mathbf{z}_d$在$\mathbf{X}_d^-$条件下的条件分布假设为高斯分布：</p>
<p>$$<br>q_\theta(\mathbf{z}<em>d|\mathbf{X}_d^-):=\mathcal{N}(\mathbf{z}_d|\mu_\phi(\mathbf{X}_d^-),\text{diag}(\sigma_\phi^2(\mathbf{X}_d^-)))<br>$$<br>其中均值$\mu_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K$和方差$\sigma^2_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K</em>+$由神经网络建模，且在所有 domain 之间共享。在$\mathbf{X}_d^-$给定的时候，我们便能够推断出该 domain 对应的隐变量，</p>
<p>$q_\phi$的输入为正常样本的集合，故神经网络需要满足<em>permutation invariant</em>。$\tau(\mathbf{X}<em>d^-)=\rho(\sum</em>{n=1}^{N_d^-}\eta(\mathbf{x}_{dn}^-))$，其中$\tau(\mathbf{X}_d^-)$表示$\mu_\phi(\mathbf{X_d^-})$或$\ln\sigma_\phi^2(\mathbf{X}_d^-)$，$\rho$和$\eta$为神经网络，</p>
<h2 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h2><p>目标函数由anomaly score函数和隐变量组成。第$d$个domain在对应的隐变量$\mathbf{z}_d$条件下的目标函数为：</p>
<p>$$<br>L_d(\theta|\mathbf{z}<em>d):=\frac{1}{N_d^-}\sum\limits</em>{n=1}^{N_d^-}s_\theta(\mathbf{x}<em>{dn}^-|\mathbf{z}_d)-\frac{\lambda}{N_d^-N_d^+}\sum\limits</em>{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}<em>{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}</em>{dn}^-|\mathbf{z}_d))<br>$$</p>
<p>其中$\lambda\geq 0$为超参数，$f$为sigmoid函数。公式的第一项表示第$d$个domain正常样本对应的<em>anomaly score</em>。第二项为可微分的AUC。异常样本的<em>anomaly score</em>应当大于正常样本，所以对任何$\mathbf x_{dm}^+\in\mathbf X_d^+, \mathbf x_{dn}^-\in\mathbf X_d^-$有$s_\theta(\mathbf x_{dm}^+|\mathbf z_d)&gt;s_\theta(\mathbf x_{dn}^-|\mathbf z_d)$。第二项$\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}<em>{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}</em>{dn}^-|\mathbf{z}<em>d))$的取值范围是$[0,1]$，当所有的$s_\theta(\mathbf{x}</em>{dm}^+|\mathbf{z}<em>d)\gg s_\theta(\mathbf{x}</em>{dm}^-|\mathbf{z}<em>d)$时该项为1，当所有的$s_\theta(\mathbf{x}</em>{dm}^+|\mathbf{z}<em>d)\ll s_\theta(\mathbf{x}</em>{dm}^-|\mathbf{z}<em>d)$时该项为0，所以最小化该项的相反数相当于鼓励$s_\theta(\mathbf{x}</em>{dm}^+|\mathbf{z}<em>d)\gg s_\theta(\mathbf{x}</em>{dm}^-|\mathbf{z}_d)$。</p>
<p>因为隐变量$\mathbf z_d$包含不确定性，我们应该在目标函数里考虑这一点：<br>$$<br>\mathcal{L}<em>d(\theta,\phi):=\mathbb{E}</em>{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))<br>$$</p>
<p>第一项是$L_d(\theta|\mathbf z_d)$关于$q_\phi(\mathbf z_d|\mathbf X_d^-)$的期望，第二项是$q_\phi(\mathbf z_d|\mathbf X_d^-)$和$p(\mathbf z_d):=\mathcal{N}(\boldsymbol 0,\boldsymbol I)$的KL散度。第一项可以用<em>monte carlo</em>估计$\mathbb{E}<em>{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]\approx\frac{1}{L}\sum</em>{\ell=1}^L L_d(\theta|\mathbf z_d^{(\ell)})$，除此之外还需要用到<em>reparametrization trick</em>。</p>
<p>对于第$d$个target domain，因为没有异常样本（假设），所以$L_d(\theta|\mathbf{z}<em>d):=\frac{1}{N_d^-}\sum\limits</em>{n=1}^{N_d^-}s_\theta(\mathbf{x}<em>{dn}^-|\mathbf{z}_d)$，有：<br>$$<br>\mathcal{L}_d(\theta,\phi):=\mathbb{E}</em>{q_\phi(\mathbf{z}<em>d|\mathbf{X}_d^-)}\left[\frac{1}{N_d^-}\sum\limits</em>{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z}_d))<br>$$</p>
<p>所以总的损失函数为各domain对应的损失函数之和$\mathcal{L}(\theta,\phi):=\sum_{d=1}^{D_S+D_T}\alpha_d\mathcal{L}_d(\theta,\phi)$。</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>训练好之后，domain-specific的<em>anomaly score</em>可以由下式计算出：</p>
<p>$$<br>s(\mathbf{x}<em>{d^\prime}):=\int s</em>{\theta_<em>}(\mathbf{x_{d^\prime}}|\mathbf{z}<em>{d^\prime})q</em>{\phi_</em>}(\mathbf{z}<em>{d^\prime}|\mathbf{X}</em>{d^\prime}^-)\mathrm{d}\mathbf{z}<em>{d^\prime}\approx\frac{1}{L}\sum\limits</em>{\ell=1}^L s_{\theta_*}(\mathbf{x}<em>{d^\prime}|\mathbf{z}</em>{d^\prime}^{(\ell)})<br>$$</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>实验包含五个数据集，第一个是合成数据集。如下图(a)所示，围绕$(0,0)$有$8$个圈，每个圈包含了一个内圈作为异常样本，第$7$个圈被选为target domain，其余的为source domain。第二个是MNIST-r，是加入旋转的MNIST，包含6个domain，其中数字“4”被选为异常样本，其余为正常。第三个为Anuran Calls，包含5个domain。第四个是Landmine，主要用在多任务学习中。第五个是IoT，网络流量数据，包含8个domain。</p>
<img src="http://qfxiao.me/img/image-20200109102643644.png" style="zoom:50%;" />

<h2 id="Comparison-Methods"><a href="#Comparison-Methods" class="headerlink" title="Comparison Methods"></a>Comparison Methods</h2><p>对比的baseline包括NN（普通多层神经网络），NNAUC（加入可微分AUC作为损失函数），AE（普通Autoencoer），AEAUC（加入可微分AUC的AE），OSVM（单类支持向量机），CCSA，TOSVM和OTL。</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>4个真实数据集的结果如下：</p>
<img src="http://qfxiao.me/img/image-20200109102713247.png" style="zoom:50%;" />

<img src="http://qfxiao.me/img/image-20200109102730126.png" style="zoom:50%;" />

<img src="http://qfxiao.me/img/image-20200109102742222.png" style="zoom:50%;" />

<img src="http://qfxiao.me/img/image-20200109102753827.png" style="zoom:50%;" />

<p>表5为考虑隐变量不确定性的ablation study。将原来的公式$\mathcal{L}<em>d(\theta,\phi):=\mathbb{E}</em>{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))$中$q_\phi(\mathbf z_d|\mathbf X_d^-)$用迪利克雷分布$q_\phi(\mathbf z_d|\mathbf X_d^-)=\delta(\mathbf z_d-\mu_\phi(\mathbf X_d^-))$代替并且去掉KL散度。</p>
<img src="http://qfxiao.me/img/image-20200109102804386.png" style="zoom: 50%;" />

<p>表6展示了不同异常比例对效果的影响。</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/"><img class="thumbnail" src="http://qfxiao.me/img/image-20200113104953902.png" alt="Deep Anomaly Detection with Deviation Networks"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-02-24T02:45:08.000Z" title="2020-02-24T02:45:08.000Z">2020-02-24</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span><span class="level-item">15 分钟 读完 (大约 2216 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/">Deep Anomaly Detection with Deviation Networks</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文关注<code>Deep Anomaly Detection</code>，也就是用深度学习的方法来进行异常检测。文中提到现有的<code>Deep Anomaly Detection</code>存在两个弊端：一个是采用深度学习方法来进行特征学习，然后通过下游任务得到<code>Anomaly Score</code>，相比文中End-to-End的<code>Anomaly Score</code>学习，存在优化不充分的风险；另一个是现有的方法主要是无监督学习，无法利用已知的信息（如少量标签）。为此，本文提出了一种端到端的异常检测框架，来解决上述问题。</p>
<p>本文的主要贡献如下：</p>
<ul>
<li>提出了一种端到端的异常检测框架，直接学习<code>Anomaly Score</code>并且可以利用已知信息；</li>
<li>基于提出的框架，文中提出了一种实例方法 (DevNet)。</li>
</ul>
<img src="http://qfxiao.me/img/image-20200113104938784.png" style="zoom:67%;" />

<h1 id="Proposed-Model"><a href="#Proposed-Model" class="headerlink" title="Proposed Model"></a>Proposed Model</h1><h2 id="End-To-End-Anomaly-Score-Learning"><a href="#End-To-End-Anomaly-Score-Learning" class="headerlink" title="End-To-End Anomaly Score Learning"></a>End-To-End Anomaly Score Learning</h2><h3 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h3><p>为了区别于传统的两阶段异常检测（先学习特征表示，然后在学到的特征上定义一个<code>anomaly measure</code>来得到<code>anomaly score</code>），作者对端到端的异常检测问题重新进行形式化。</p>
<p>给定$N+K$个样本$\mathcal{X}={\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N,\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}}$，其中$\boldsymbol x_i\in\mathbb{R}^D$，无标签样本集$\mathcal{U}={\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N}$，有标签样本集$\mathcal{K}={\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}}$，且$K\ll N$。异常检测的目标是学习一个<code>anomaly scoring function</code>$\phi:\mathcal{X}\mapsto\mathbb{R}$使得$\phi(\boldsymbol x_i)&gt;\phi(\boldsymbol x_j)$，其中$\boldsymbol x_i$为异常样本，$\boldsymbol x_j$为正常样本。</p>
<h3 id="The-Proposed-Framework"><a href="#The-Proposed-Framework" class="headerlink" title="The Proposed Framework"></a>The Proposed Framework</h3><p>为了解决这个问题，文中提出了一种通用异常检测框架，模型框架如下图所示：</p>
<p>模型框架如下图所示：</p>
<img src="http://qfxiao.me/img/image-20200113104953902.png" style="zoom:50%;" />

<p>主要包含三个部分：</p>
<ol>
<li><em>anomaly scoring network</em>. 图中左边的部分，一个函数$\phi$，输入样本$\mathbf{x}$，输出<code>anomaly score</code></li>
<li><em>reference score generator</em>. 图中右边的部分。只有一个<em>anomaly scoring network</em>并不能进行训练，需要训练的目标。为此加入<em>reference score generator</em>，输入为随机选择的$l$个正常样本，输出<code>reference score</code>（这$l$个正常样本<code>anomaly score</code>的均值，记为$\mu_\mathcal{R}$）</li>
<li><em>deviation loss</em>. $\phi(\mathbf{x})$，$\mu_\mathcal{R}$及对应的标准差$\sigma_\mathcal{R}$作为<code>deviation loss</code>函数的输入。因为$\mu_\mathcal{R}$和$\sigma_\mathcal{R}$对应正常样本集的均值和方差，那么异常样本的<code>anomaly score</code>应该和$\mu_\mathcal{R}$差别比较大，而正常样本则应该接近$\mu_\mathcal{R}$。</li>
</ol>
<h2 id="Deviation-Networks"><a href="#Deviation-Networks" class="headerlink" title="Deviation Networks"></a>Deviation Networks</h2><p>下面是上述三个部件的具体实现。</p>
<h3 id="End-To-End-Anomaly-Scoring-Network"><a href="#End-To-End-Anomaly-Scoring-Network" class="headerlink" title="End-To-End Anomaly Scoring Network"></a>End-To-End Anomaly Scoring Network</h3><p>记$\mathcal{Q}\in\mathbb{R}^M$为中间表示空间，<code>anomaly scoring network</code>$\phi(\cdot;\Theta):\mathcal{X}\mapsto\mathbb{R}$可以定义为数据表示学习$\psi(\cdot;\Theta_t):\mathcal{X}\mapsto\mathcal{Q}$和异常分数学习$\eta(\cdot;\Theta_s):\mathcal{Q}\mapsto\mathbb{R}$两阶段的组合，其中$\Theta={\Theta_t,\Theta_s}$。</p>
<p>$\psi(\cdot;\Theta_t)$可以用一个$H$层神经网络来实现：<br>$$<br>\mathrm{q}=\psi(\mathbf{x};\Theta_t)<br>$$<br>其中$\mathbf{x}\in\mathcal{X}$，$\mathrm{q}\in\mathcal{Q}$。</p>
<p>$\eta(\cdot;\Theta_s)$可以用一个单层的神经网络来实现：<br>$$<br>\eta(\mathrm q;\Theta_s)=\sum\limits_{i=1}^M w_i^oq_i+w_{M+1}^o<br>$$<br>其中$\mathrm q\in\mathcal Q$，$\Theta_s={\mathbf{w}^o}$。</p>
<p>所以有：<br>$$<br>\phi(\mathbf{x};\Theta)=\eta(\psi(\mathbf{x};\Theta_t);\Theta_s)<br>$$</p>
<h3 id="Gaussian-Prior-based-Reference-Scores"><a href="#Gaussian-Prior-based-Reference-Scores" class="headerlink" title="Gaussian Prior-based Reference Scores"></a>Gaussian Prior-based Reference Scores</h3><p>有两种方法来获得$\mu_\mathcal{R}$，一种是data-driven，一种是prior-driven。如果是data-driven的话则采用另一个神经网络，文中表示为了更好的解释性和计算效率，所以采用的是prior-driven。<br>$$<br>\begin{align}<br>r_1,r_2,\cdots,r_l\sim \mathcal{N}(\mu,\sigma^2),\<br>\mu_\mathcal{R}=\frac{1}{l}\sum\limits_{i=1}^l r_i<br>\end{align}<br>$$<br>在文中，采用的prior是标准高斯分布。</p>
<h2 id="Z-Score-Based-Deviation-Loss"><a href="#Z-Score-Based-Deviation-Loss" class="headerlink" title="Z-Score Based Deviation Loss"></a>Z-Score Based Deviation Loss</h2><p><em>anomaly scoring network</em>的优化目标可以定义为Z-Score的方式：<br>$$<br>dev(\boldsymbol x)=\frac{\phi(\boldsymbol x;\Theta)-\mu_{\mathcal{R}}}{\sigma_{\mathcal{R}}}<br>$$<br>$dev(\boldsymbol x)$可以看作是样本偏离标准的程度，而我们肯定希望异常样本偏离标准越大，正常样本越接近标准。文中采用的损失函数是<code>Contrastive Loss</code>：<br>$$<br>L(\phi(\boldsymbol x;\Theta),\mu_\mathcal{R},\sigma_\mathcal{R})=(1-y)|dev(\boldsymbol x)| + y \max(0, a - dev(\boldsymbol x))<br>$$<br><code>Contrastive Loss</code>的直观解释可以看下图：</p>
<img src="http://qfxiao.me/img/contrastive_2020_2_24.png" style="zoom:50%;" />

<p>对于负例（正常），优化过程将他们尽量向原点靠近，对于正例（异常），优化过程将他们拉向边界。</p>
<h2 id="The-DevNet-Algorithm"><a href="#The-DevNet-Algorithm" class="headerlink" title="The DevNet Algorithm"></a>The DevNet Algorithm</h2><p><code>DevNet</code>的算法流程图如下：</p>
<p><img src="http://qfxiao.me/img/image-20200113105040134.png" alt=""></p>
<h2 id="Interpretability-of-Anomaly-Scores"><a href="#Interpretability-of-Anomaly-Scores" class="headerlink" title="Interpretability of Anomaly Scores"></a>Interpretability of Anomaly Scores</h2><p>因为<em>reference score generator</em>选择的是确定的高斯分布，于是可以用概率论给出一些解释性。作者给出了一个结论，</p>
<blockquote>
<p><strong>PROPOSITION</strong>： 设$\boldsymbol x\in\mathcal{X}$，$z_p$为$\mathcal{N}(\mu,\sigma^2)$的分位数，那么$\phi(\boldsymbol x)$在区间$\mu\pm z_p\sigma$的概率为$2(1-p)$。</p>
</blockquote>
<p>例如，假设$p=0.95$，那么$z_{0.95}=1.96$，表示异常分数高于1.96的样本将以0.95的置信度为异常。</p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>实验用到了9个数据集，4个Baseline (REPEN，DSVDD，FSNET，iForest)，以及ROC和PR曲线两种评测标准。</p>
<h2 id="Effectiveness-in-Real-world-Data-Sets"><a href="#Effectiveness-in-Real-world-Data-Sets" class="headerlink" title="Effectiveness in Real-world Data Sets"></a>Effectiveness in Real-world Data Sets</h2><h3 id="Experiment-Settings"><a href="#Experiment-Settings" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p>这一个实验主要是为了验证算法在真实场景下的效果，即大量无标签数据和极少量标签数据。训练集包含两部分，一部分是无标签数据$\mathcal{U}$,包含$2%$的异常样本，另一部分是有标签数据$\mathcal{K}$，由随机采样$0.005%-1%$的训练数据和$0.08%-6%$的异常样本组成。</p>
<h3 id="Findings"><a href="#Findings" class="headerlink" title="Findings"></a>Findings</h3><p>实验结果如下表所示：</p>
<p><img src="http://qfxiao.me/img/image-20200113110000432.png" alt=""></p>
<p>从结果上看来，本文提出的方法在所有数据集上都比Baseline好，说明<code>DevNet</code>端到端直接优化<code>Anomaly Score</code>的方式是有效的。</p>
<h2 id="Data-Efficiency"><a href="#Data-Efficiency" class="headerlink" title="Data Efficiency"></a>Data Efficiency</h2><h3 id="Experiment-Settings-1"><a href="#Experiment-Settings-1" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p>这一个实验主要是为了探究基于深度的异常检测方法的<em>data efficiency</em>。和上一个实验一样，无标签数据集包含$2%$的异常，而有标签的异常数量从$5$到$120$不等。本实验试图回答以下两个问题：</p>
<ul>
<li><code>DevNet</code>的<em>data efficiency</em>如何？</li>
<li>基于深度的方法在多大程度上能够利用标签信息？</li>
</ul>
<h3 id="Findings-1"><a href="#Findings-1" class="headerlink" title="Findings"></a>Findings</h3><p>在几个基于深度的Baseline中，<code>DevNet</code>的效果是最好的，同时在有标签异常非常有限的情况下，<code>DevNet</code>也能很好的利用标签信息，达到更好的效果。</p>
<p><img src="http://qfxiao.me/img/image-20200113110017195.png" alt=""></p>
<h2 id="Robustness-w-r-t-Anomaly-Contamination"><a href="#Robustness-w-r-t-Anomaly-Contamination" class="headerlink" title="Robustness w.r.t. Anomaly Contamination"></a>Robustness w.r.t. Anomaly Contamination</h2><h3 id="Experiment-Settings-2"><a href="#Experiment-Settings-2" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p>在第一个实验中，无标签数据集$\mathcal{U}$包含的是固定的异常比例$2%$，而在这个实验中，作者测试了从$0%$到$20%$之间不同异常比例来测试算法的鲁棒性（即使$\mathcal{U}$中包含异常，由于没有标签，在训练的时候仍然假设都为正常来进行训练）。本实验试图回答以下问题：</p>
<ul>
<li>基于深度的异常检测方法的鲁棒性如何？</li>
<li>当训练集中异常污染的比例较高的时候基于深度的方法能否打败无监督的方法？</li>
</ul>
<h3 id="Findings-2"><a href="#Findings-2" class="headerlink" title="Findings"></a>Findings</h3><p>下图为实验结果：</p>
<p><img src="http://qfxiao.me/img/image-20200113110035878.png" alt=""></p>
<p>从结果上来看，<code>DevNet</code>比其他基于深度的方法鲁棒性更好，同时在高异常污染的情况下仍然比纯无监督方法效果要好。</p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>本实验设置了<code>DevNet</code>的三个变体（默认的<code>DevNet-Def</code>为单层隐层加上一个输出层）来进行消融实验，分别是：</p>
<ul>
<li><code>DevNet-Rep</code>，去掉了<em>anomaly scoring network</em>网络的输出层，对应<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>；</li>
<li><code>DevNet-Linear</code>，去掉了网络中的非线性层，对应<em>learning of non-linear features</em>；</li>
<li><code>DevNet-3HL</code>，隐层数量为3层。</li>
</ul>
<p>对比结果如下：</p>
<p><img src="http://qfxiao.me/img/image-20200113110048598.png" alt=""></p>
<p>通过实验可以发现，<code>DevNet-Rep</code>说明了<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>的有效性，而<code>DevNet-Linear</code>说明了<em>learning of non-linear features</em>的重要性。<code>DevNet-3HL</code>说明了加深网络并不总能带来性能的提升。</p>
<h2 id="Scalability-Test"><a href="#Scalability-Test" class="headerlink" title="Scalability Test"></a>Scalability Test</h2><p>这一个实验使用合成的数据来测试算法对大规模数据的处理能力，分别从<em>Data Size</em>和<em>Data Dimensionality</em>两方面来测试。结果如下：</p>
<p><img src="http://qfxiao.me/img/image-20200113110113423.png" alt=""></p>
<p>可以看出，<code>DevNet</code>对<em>Data Size</em>并不敏感，同时，面对高维数据，<code>DevNet</code>也没有表现出劣势。</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"><img class="thumbnail" src="http://qfxiao.me/img/image-20200131113557592.png" alt="Geant4 安装教程与调试环境配置"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-01-31T03:25:59.000Z" title="2020-01-31T03:25:59.000Z">2020-01-31</time><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></span><span class="level-item">5 分钟 读完 (大约 713 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/">Geant4 安装教程与调试环境配置</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Geant4安装的教程很多，版本都很旧了，这里写一个新版本（10.6）基于Ubuntu的安装教程，并且开启CLion IDE调试。</p>
<h1 id="Step-1-Download-Packages"><a href="#Step-1-Download-Packages" class="headerlink" title="Step 1: Download Packages"></a>Step 1: Download Packages</h1><p>首先进入官网(<a href="http://geant4.web.cern.ch/support/download">http://geant4.web.cern.ch/support/download</a>)下载源代码（推荐tar.gz格式）及数据文件，解压。新建一个文件夹专门用来放<code>Geant4</code>相关文件，新建data，source，build文件夹，将Geant4的文件复制进来并按如下结构组织：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build</span><br><span class="line">├── data</span><br><span class="line">│   ├── G4ABLA3.1</span><br><span class="line">│   ├── G4EMLOW7.9</span><br><span class="line">│   ├── G4ENSDFSTATE2.2</span><br><span class="line">│   ├── G4INCL1.0</span><br><span class="line">│   ├── G4NDL4.6</span><br><span class="line">│   ├── G4PARTICLEXS2.1</span><br><span class="line">│   ├── G4PII1.3</span><br><span class="line">│   ├── G4SAIDDATA2.0</span><br><span class="line">│   ├── G4TENDL1.3.2</span><br><span class="line">│   ├── PhotonEvaporation5.5</span><br><span class="line">│   ├── RadioactiveDecay5.4</span><br><span class="line">│   └── RealSurface2.1.1</span><br><span class="line">└── <span class="built_in">source</span></span><br><span class="line">    └── geant4.10.06</span><br></pre></td></tr></table></figure>



<p><img src="http://qfxiao.me/img/image-20200131113618716.png" alt=""></p>
<p><img src="http://qfxiao.me/img/image-20200131113628061.png" alt=""></p>
<h1 id="Step-2-Install-Dependencies"><a href="#Step-2-Install-Dependencies" class="headerlink" title="Step 2: Install Dependencies"></a>Step 2: Install Dependencies</h1><p>安装编译所需环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential cmake</span><br></pre></td></tr></table></figure>

<p>安装相关依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libgl1-mesa-dev libglu1-mesa-dev libxt-dev libxmu-dev libxi-dev zlib1g-dev libgl2ps-dev libexpat1-dev libxerces-c-dev</span><br></pre></td></tr></table></figure>



<p>如果要用到QT需要单独安装QT。</p>
<h1 id="Step-3-Compile"><a href="#Step-3-Compile" class="headerlink" title="Step 3: Compile"></a>Step 3: Compile</h1><p>进入build文件夹，用cmake命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake ../<span class="built_in">source</span>/geant4.10.06/ -DCMAKE_BUILD_TYPE=DEBUG -DGEANT4_USE_GDML=ON -DGEANT4_USE_OPENGL_X11=ON -DGEANT4_USE_RAYTRACER_X11=ON -DGEANT4_BUILD_MULTITHREADED=ON</span><br></pre></td></tr></table></figure>

<p>其中<code>../source/geant4.10.06/</code>替换成换成（如果版本不一样）你自己的Geant4源代码所在目录，需要QT则加上<code>-DGEANT4_USE_QT=ON</code>。如果不需要调试则把<code>-DCMAKE_BUILD_TYPE=DEBUG</code>改成<code>-DCMAKE_BUILD_TYPE=RELEASE</code>。<code>-DGEANT4_BUILD_MULTITHREADED=ON</code>是多线程，视情况开启。</p>
<p>完成之后开始编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -jX</span><br></pre></td></tr></table></figure>

<p><code>-jX</code>为多线程编译，如<code>-j8</code>。</p>
<p>编译完成之后进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<h1 id="Step-4-Configure"><a href="#Step-4-Configure" class="headerlink" title="Step 4: Configure"></a>Step 4: Configure</h1><p>安装的默认路径在<code>/usr/local/share/Geant4-10.6.0</code>，将下载的数据文件复制到该文件夹：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp -r ./data/ /usr/<span class="built_in">local</span>/share/Geant4-10.6.0/</span><br></pre></td></tr></table></figure>

<p>之后，在<code>~/.bashrc</code>里添加<code>/usr/local/share/Geant4-10.6.0/geant4make/geant4make.sh</code>，如果你的版本和我的不一样，相应修改即可。</p>
<h1 id="Step-5-CLion-Configuration"><a href="#Step-5-CLion-Configuration" class="headerlink" title="Step 5: CLion Configuration"></a>Step 5: CLion Configuration</h1><p>最后我们来配置CLion环境，配好之后可以在IDE中编写<code>Geant4</code>代码，还可以断点调试，非常方便。安装CLion的过程这里省略，打开一个<code>Geant4</code>自带的例子或者自己新建一个项目，打开<code>Edit Configurations</code>。</p>
<p><img src="http://qfxiao.me/img/image-20200131121415836.png" alt=""></p>
<p>随便打开一个终端，输入一下命令获取环境变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env | grep G4</span><br></pre></td></tr></table></figure>

<p>在<code>Environment variables</code>填入刚才获取的环境变量（复制之后按一下粘贴就可以了），然后把<code>Working directory</code>设置成当前文件夹。</p>
<p><img src="http://qfxiao.me/img/image-20200131121655508.png" alt=""></p>
<p><img src="http://qfxiao.me/img/image-20200131121807181.png" alt=""></p>
<p>现在就大功告成了！</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/"><img class="thumbnail" src="http://qfxiao.me/img/image-20200109102204802.png" alt="Complementary Set Variational Autoencoder for Supervised Anomaly Detection"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-01-09T02:15:03.000Z" title="2020-01-09T02:15:03.000Z">2020-01-09</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span><span class="level-item">10 分钟 读完 (大约 1470 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/">Complementary Set Variational Autoencoder for Supervised Anomaly Detection</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对于异常检测问题，异常的模式是多种多样的。有监督模型能够较好地处理训练集中出现过的模式，无监督模型能够处理训练集中未出现过的模式，但对于训练集中出现过的异常模型并没有学习。本文提出了一种既能学习训练集中出现过的异常模式，同时能处理未出现过的异常模式的方法。</p>
<h1 id="Proposed-Model"><a href="#Proposed-Model" class="headerlink" title="Proposed Model"></a>Proposed Model</h1><h2 id="Conventional-VAE"><a href="#Conventional-VAE" class="headerlink" title="Conventional VAE"></a>Conventional VAE</h2><p>首先回顾一下原始的VAE。</p>
<p>原始VAE中的损失函数为：<br>$$<br>\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})=\mathbb{E}<em>{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}\parallel p(\boldsymbol{z}))]<br>$$<br>原文中作者证明了$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\leq\log p(\boldsymbol{x};\boldsymbol{\theta})$，所以$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})$可以看作是数据分布$p(\boldsymbol{x})$对数似然的一个下界。$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})$又被称为证据下界 (ELBO)。$\mathbb{E}</em>{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]$中的期望一般用蒙特卡洛来进行估计：<br>$$<br>\begin{align}<br>\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq&amp; \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})\parallel p(\boldsymbol{z})],\<br>\boldsymbol{z}^{(l)}&amp;\sim q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}), \space l\in{1,2,\cdots,L}<br>\end{align}<br>$$<br>对于隐变量$\boldsymbol{z}$，一般假设先验服从标准高斯分布，后验服从均值为$\mu$，方差为$\sigma^2$的高斯分布，故KL散度能直接写出解析式：<br>$$<br>\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-C(-\frac{1}{2}-\log\sigma+\frac{1}{2}\sigma^2+\frac{1}{2}\mu^2)<br>$$<br>使用VAE来做异常检测通常是在正常数据上进行训练，在检测阶段，如果是异常样本，那么VAE不能很好地重构它，这样会导致较大的重构误差。</p>
<h2 id="Prior-Distribution-for-Anomalies"><a href="#Prior-Distribution-for-Anomalies" class="headerlink" title="Prior Distribution for Anomalies"></a>Prior Distribution for Anomalies</h2><p><img src="http://qfxiao.me/img/image-20200109102204802.png" alt=""></p>
<p>在原始VAE异常检测中，无论输入样本$\boldsymbol{x}$是否异常，VAE都会使对应编码的后验$p(\boldsymbol{z}|\boldsymbol{x})$服从高斯分布，且施加标准高斯分布的约束。在本文中，作者对异常和正常样本对应的隐变量的先验分布做了不同假设。首先，正常先验依然是标准高斯分布，记为$p_n(\boldsymbol{z})$。而对于异常先验，作者认为异常即为“不正常”，和正常是补集的关系。作者在文中定义异常先验分布$p_a(\boldsymbol{z})$为：<br>$$<br>p_a(\boldsymbol{z})=\frac{1}{Y^\prime}(\max\limits_{\boldsymbol{z}^\prime}p_n(\boldsymbol{z}^\prime)-p_n(\boldsymbol{z}))<br>$$</p>
<p>其中$Y^\prime$为使$p_a(\boldsymbol{z})$成为一个概率分布的调节因子。实际上，$Y^\prime$往往会成为无限大，因为$p(\boldsymbol z)$在整个定义域上都有定义。为了解决这个问题，作者加入了$p_w(\boldsymbol z)$，一个在每个维度都足够宽的辅助分布：</p>
<p>$$<br>p_a(\boldsymbol z)=\frac{1}{Y}p_w(\boldsymbol z)\left(\max\limits_{\boldsymbol z^\prime}p_n(\boldsymbol z^\prime)-p_n(\boldsymbol z)\right)<br>$$</p>
<p>其中$Y$为有限的常数。在文中$p_n(\boldsymbol z)$和$p_w(\boldsymbol z)$都为高斯分布，那么$p_a(\boldsymbol z)$的具体形式为：</p>
<p>$$<br>p_a(\boldsymbol z)=\frac{1}{Y}\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2){\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)-\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol 1)}<br>$$</p>
<p>其中：</p>
<p>$$<br>\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)=\frac{1}{\sqrt{2\pi}}<br>$$</p>
<p>$$<br>Y=\int_{-\infty}^{\infty}p_a(\boldsymbol z)\mathrm{d}\boldsymbol z=\frac{1}{\sqrt{2\pi}}\left{1-\frac{1}{\boldsymbol s^2+1}\right}<br>$$</p>
<p>$\boldsymbol s^2$为超参数，控制分布的宽度。用文中的先验替换VAE原始的KL散度，可写为：</p>
<p>$$<br>\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]=\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\frac{\mathcal N(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)}{\frac{1}{Y}\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\left{\frac{1}{2\pi}-\mathcal N(\boldsymbol z;\boldsymbol0,\boldsymbol 1)\right}}\mathrm{d}\boldsymbol z<br>$$</p>
<p>展开后：</p>
<p>$$<br>\begin{align}<br>\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;=<br>\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)\mathrm{d}\boldsymbol z\<br>&amp;+\log Y\<br>&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\mathrm{d}\boldsymbol z\<br>&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\left{\frac{1}{\sqrt{2\pi}}-\mathcal{N}(\boldsymbol z;\boldsymbol 0, \boldsymbol 1)\right}\mathrm{d}\boldsymbol z<br>\end{align}<br>$$</p>
<p>使用泰勒展开，$\log (x+\frac{1}{2\pi})\simeq-\log 2\pi+2\pi x$，KL散度可以用下式估计：</p>
<p>$$<br>\begin{align}<br>\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;\simeq\sqrt{\frac{2\pi}{\boldsymbol\sigma^2+1}}\exp\left(\frac{-\boldsymbol\mu^2}{2(\boldsymbol\sigma^2+1)}\right)\<br>&amp;+\frac{\boldsymbol\mu^2+\boldsymbol\sigma^2}{2\boldsymbol s^2}-\log\boldsymbol\sigma+\log\boldsymbol s+\log\left(\sqrt{\boldsymbol s^2+1}-1\right)\<br>&amp;-\frac{\log(\boldsymbol s^2+1)}{2}+\frac{\log(2\pi)-1}{2}<br>\end{align}<br>$$</p>
<p>下图为一维时$p_n(\boldsymbol z)$和$p_a(\boldsymbol z)$的示例：</p>
<p><img src="http://qfxiao.me/img/image-20200109102255322.png" alt=""></p>
<h3 id="Implementation-of-proposed-method"><a href="#Implementation-of-proposed-method" class="headerlink" title="Implementation of proposed method"></a>Implementation of proposed method</h3><p>文中使用编码器输出的分布$\mathcal{N}(\boldsymbol z;\boldsymbol \mu, \boldsymbol \sigma^2)$与标准正态分布之间的KL散度来作为异常分数。在每一轮的训练过程中，加入一轮使用Anomaly Prior的训练。</p>
<p><img src="http://qfxiao.me/img/image-20200109102309048.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><p>作者设计了两个Task：</p>
<ol>
<li>Task 1. $N$ vs. $\bar{N}$. 将手写数字中的一个作为已知异常，其他作为正常，并加入均匀分布作为未知的异常。</li>
<li>Task 2. 手写数字被分为3组：已知异常，正常，未知异常。</li>
</ol>
<p>细节如下表所示：</p>
<p><img src="http://qfxiao.me/img/image-20200109102402499.png" alt=""></p>
<p>在实现上，使用Adam优化器，<code>batch_size</code>为100，<code>epochs</code>为200。<code>Encoder</code>和<code>Decoder</code>都由三层感知机组成，超参数$s^2$设置为400。评测标准使用AUC (area under the receiver characteristic curve)。</p>
<p>下表为实验结果：</p>
<p><img src="http://qfxiao.me/img/image-20200109102343271.png" alt=""></p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/01/08/Time-Series-Anomaly-Detection-Paper-List/"><img class="thumbnail" src="http://qfxiao.me/img/1571569717718.png" alt="Time Series Anomaly Detection Paper List"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-01-08T02:27:58.000Z" title="2020-01-08T02:27:58.000Z">2020-01-08</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span><span class="level-item">19 分钟 读完 (大约 2792 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/08/Time-Series-Anomaly-Detection-Paper-List/">Time Series Anomaly Detection Paper List</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>时间序列异常检测在很多领域例如运维、金融、交通都扮演者重要的角色，其定义如下：</p>
<blockquote>
<p>给定时间序列$X={\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_n}\in\mathbb{R}^{m\times n}$，异常检测的任务是输出异常标签$y={y_1,y_2,\cdots,y_n}\in\mathbb{R}^n$，其中$y_t=1$代表$\mathbf{x}_t$为异常，$y_t=0$代表$\mathbf{x}_t$正常</p>
</blockquote>
<p>本文罗列了一些时间序列异常检测领域值得读的一些文章，Model一章主要按时间序列、通用和图的异常检测分类，Related主要是一些非异常检测、但是能够解决异常检测研究中的一些问题的文章。</p>
<p><img src="http://qfxiao.me/img/1571569717718.png" alt=""></p>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Time-Series"><a href="#Time-Series" class="headerlink" title="Time Series"></a>Time Series</h2><h3 id="Statistical"><a href="#Statistical" class="headerlink" title="Statistical"></a>Statistical</h3><p>主要包括偏向统计方法的模型：</p>
<table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Temporal Anomaly Detection: Calibrating the Surprise</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1705.10085">📃Paper</a></td>
</tr>
<tr>
<td>Non-Parametric Outliers Detection in Multiple Time Series A Case Study: Power Grid Data Analysis</td>
<td></td>
<td></td>
<td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16315">📃Paper</a></td>
</tr>
<tr>
<td>Anomaly Detection in Streams with Extreme Value Theory</td>
<td>KDD17</td>
<td>提出了以Extreme Value Theory为基础的时序异常检测和参数选择方法</td>
<td><a href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-in-streams-with-extreme-value-theory">📃Paper</a></td>
</tr>
<tr>
<td>Semi-Markov Switching Vector Autoregressive Model-Based Anomaly Detection in Aviation Systems</td>
<td></td>
<td></td>
<td><a href="https://www.kdd.org/kdd2016/subtopic/view/semi-markov-switching-vector-autoregressive-model-based-anomaly-detection-i">📃Paper</a></td>
</tr>
<tr>
<td>Stochastic Online Anomaly Analysis for Streaming Time Series</td>
<td></td>
<td></td>
<td><a href="https://www.ijcai.org/proceedings/2017/445">📃Paper</a></td>
</tr>
<tr>
<td>Unsupervised Real-time Anomaly Detection for Streaming Data</td>
<td></td>
<td></td>
<td><a href="https://www.sciencedirect.com/science/article/pii/S0925231217309864">📃Paper</a></td>
</tr>
<tr>
<td>Automatic Anomaly Detection in the Cloud Via Statistical Learning</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/abs/1704.07706">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="Classic-Machine-Learning"><a href="#Classic-Machine-Learning" class="headerlink" title="Classic Machine Learning"></a>Classic Machine Learning</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Semi-supervised Anomaly Detection with an Application to Water Analytics</td>
<td></td>
<td></td>
<td><a href="https://people.cs.kuleuven.be/~vincent.vercruyssen/publications/2018/ICDM_conference_manuscript.pdf">📃Paper</a></td>
</tr>
<tr>
<td>DILOF: Effective and Memory Efficient Local Outlier Detection in Data Streams</td>
<td></td>
<td></td>
<td><a href="https://www.kdd.org/kdd2018/accepted-papers/view/dsilof-effective-and-memory-efficient-local-outlier-detection-in-data-strea">📃Paper</a></td>
</tr>
<tr>
<td>Robust Random Cut Forest Based Anomaly Detection On Streams</td>
<td>ICML16</td>
<td>孤立森林的改进版本来做时序异常检测</td>
<td><a href="http://proceedings.mlr.press/v48/guha16.pdf">📃Paper</a> <a href="https://github.com/kLabUM/rrcf">📥Code</a></td>
</tr>
<tr>
<td>Anomaly Detection for an E-commerce Pricing System</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1902.09566">📃Paper</a></td>
</tr>
<tr>
<td>An Adaptive Approach for Anomaly Detector Selection and Fine-Tuning in Time Series</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1907.07843">📃Paper</a></td>
</tr>
<tr>
<td>Opprentice: Towards Practical and Automatic Anomaly Detection Through Machine Learning</td>
<td>IMC15</td>
<td>以随机森林为基础的时序异常检测</td>
<td><a href="http://netman.cs.tsinghua.edu.cn/wp-content/uploads/2015/11/liu_imc15_Opprentice.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1602.07109">📃Paper</a></td>
</tr>
<tr>
<td>A Self-Learning and Online Algorithm for Time SeriesAnomaly Detection, with Application in CPU Manufacturing</td>
<td></td>
<td></td>
<td><a href="https://dl.acm.org/citation.cfm?id=2983344">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</td>
<td>WWW18</td>
<td>普通VAE</td>
<td><a href="https://arxiv.org/pdf/1802.03903">📃Paper</a> <a href="https://github.com/haowen-xu/donut">📥Code</a> <a href="http://qfxiao.me/2019/09/22/Unsupervised-Anomaly-Detection-via-Variational-Auto-Encoder-for-Seasonal-KPIs-in-Web-Applications/">✍Blog</a></td>
</tr>
<tr>
<td>Robust and Unsupervised KPI Anomaly Detection Based on Conditional Variational Autoencoder</td>
<td>IPCCC18</td>
<td>条件VAE</td>
<td><a href="https://ieeexplore.ieee.org/document/8710885">📃Paper</a> <a href="https://github.com/yantijin/Buzz">📥Code</a></td>
</tr>
<tr>
<td>Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE</td>
<td>INFOCOM19</td>
<td>VAE加对抗训练</td>
<td><a href="https://ieeexplore.ieee.org/abstract/document/8737430/">📃Paper</a></td>
</tr>
<tr>
<td>Multidimensional Time Series Anomaly Detection: A GRU-based Gaussian Mixture Variational Autoencoder Approach</td>
<td></td>
<td></td>
<td><a href="http://proceedings.mlr.press/v95/guo18a/guo18a.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</td>
<td>KDD19</td>
<td>多变量基于VAE和RNN的时序异常检测</td>
<td><a href="https://www.kdd.org/kdd2019/accepted-papers/view/robust-anomaly-detection-for-multivariate-time-series-through-stochastic-re">📃Paper</a> <a href="https://github.com/smallcowbaby/OmniAnomaly">📥Code</a></td>
</tr>
<tr>
<td>A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-based Variational Autoencoder</td>
<td></td>
<td></td>
<td><a href="http://ieeexplore.ieee.org/document/8279425/">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1802.04431">📃Paper</a></td>
</tr>
<tr>
<td>BINet: Multivariate Business Process Anomaly Detection Using Deep Learning</td>
<td></td>
<td></td>
<td><a href="https://link.springer.com/chapter/10.1007/978-3-319-98648-7_16">📃Paper</a></td>
</tr>
<tr>
<td>Outlier Detection for Time Series with Recurrent Autoencoder Ensembles</td>
<td></td>
<td></td>
<td><a href="https://www.ijcai.org/proceedings/2019/0378.pdf">📃Paper</a></td>
</tr>
<tr>
<td>LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1607.00148">📃Paper</a></td>
</tr>
<tr>
<td>Detecting Anomalies in Space using Multivariate Convolutional LSTM with Mixtures of Probabilistic PCA</td>
<td>KDD19</td>
<td></td>
<td><a href="https://dl.acm.org/doi/10.1145/3292500.3330776">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1809.04758">📃Paper</a> <a href="http://larryshaw0079.coding.me/blog/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/">📥Blog</a></td>
</tr>
<tr>
<td>MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks</td>
<td>ICANN19</td>
<td>普通GAN做时序异常检测，没有编码结构</td>
<td><a href="https://arxiv.org/pdf/1901.04997">📃Paper</a> <a href="https://github.com/LiDan456/MAD-GANs">📥Code</a></td>
</tr>
<tr>
<td>BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series</td>
<td>IJCAI19</td>
<td>ECG异常检测，没有隐变量约束</td>
<td><a href="https://www.ijcai.org/proceedings/2019/616">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="Miscellaneous"><a href="#Miscellaneous" class="headerlink" title="Miscellaneous"></a>Miscellaneous</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1811.08055">📃Paper</a></td>
</tr>
<tr>
<td>ALSR: An Adaptive Label Screening and Relearning Approach for Interval-Oriented Anomaly Detection</td>
<td>Expert Systems With Applications</td>
<td>有监督KPI异常检测，使用两阶段训练来提升性能</td>
<td><a href="https://www.sciencedirect.com/science/article/pii/S0957417419304282">📃Paper</a> <a href="http://larryshaw0079.coding.me/blog/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/">✍Blog</a></td>
</tr>
<tr>
<td>Time-Series Anomaly Detection Service at Microsoft</td>
<td>KDD19</td>
<td>将视觉异常检测中的谱残差应用到了时序异常检测</td>
<td><a href="https://arxiv.org/pdf/1906.03821">📃Paper</a> <a href="https://github.com/microsoft/anomalydetector">📥Code</a> <a href="http://larryshaw0079.coding.me/blog/2019/09/22/Time-Series-Anomaly-Detection-Service-at-Microsoft/">✍Blog</a></td>
</tr>
<tr>
<td>Outlier Detection for Multidimensional Time Series using Deep Neural Networks</td>
<td></td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/8411269">📃Paper</a></td>
</tr>
</tbody></table>
<h2 id="General"><a href="#General" class="headerlink" title="General"></a>General</h2><h3 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h3><ul>
<li><em>Deep Learning for Anomaly Detection: A Survey</em> <a href="https://arxiv.org/abs/1901.03407">[Paper]</a></li>
</ul>
<h3 id="Classic-Machine-Learning-1"><a href="#Classic-Machine-Learning-1" class="headerlink" title="Classic Machine Learning"></a>Classic Machine Learning</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>LOF: Identifying Density-Based Local Outliers</td>
<td></td>
<td></td>
<td><a href="https://dl.acm.org/citation.cfm?id=335388">📃Paper</a></td>
</tr>
<tr>
<td>Isolation Forest</td>
<td></td>
<td></td>
<td><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Extended Isolation Forest</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1811.02141">📃Paper</a></td>
</tr>
<tr>
<td>Hidden Markov Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="http://proceedings.mlr.press/v37/goernitz15.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Linear-Time Outlier Detection via Sensitivity</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1605.00519">📃Paper</a></td>
</tr>
<tr>
<td>Reverse Nearest Neighbors in Unsupervised Distance-Based Outlier Detection</td>
<td></td>
<td></td>
<td><a href="http://ieeexplore.ieee.org/document/6948273">📃Paper</a></td>
</tr>
<tr>
<td>Theoretical Foundations and Algorithms for Outlier Ensembles</td>
<td></td>
<td></td>
<td><a href="https://www.kdd.org/exploration_files/Article4.pdf">📃Paper</a></td>
</tr>
<tr>
<td>R1SVM: A Randomised Nonlinear Approach to Large-Scale Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9343">📃Paper</a></td>
</tr>
<tr>
<td>Random Gradient Descent Tree: A Combinatorial Approach for SVM with Outliers</td>
<td></td>
<td></td>
<td><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9477">📃Paper</a></td>
</tr>
<tr>
<td>Sparse Gaussian Markov Random Field Mixtures for Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/7837932/">📃Paper</a></td>
</tr>
<tr>
<td>Sequential Ensemble Learning for Outlier Detection: A Bias-Variance Perspective</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1609.05528">📃Paper</a></td>
</tr>
<tr>
<td>Sparse Modeling-Based Sequential Ensemble Learning for Effective Outlier Detection in High-Dimensional Numeric Data</td>
<td></td>
<td></td>
<td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16648">📃Paper</a></td>
</tr>
<tr>
<td>Contextual Spatial Outlier Detection with Metric Learning</td>
<td></td>
<td></td>
<td><a href="https://www.kdd.org/kdd2017/papers/view/contextual-spatial-outlier-detection-with-metric-learning">📃Paper</a></td>
</tr>
<tr>
<td>Human-Assisted Online Anomaly Detection with Normal Outlier Retraining</td>
<td></td>
<td></td>
<td><a href="https://www.andrew.cmu.edu/user/lakoglu/odd/accepted_papers/ODD_v50_paper_11.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Efficient Anomaly Detection via Matrix Sketching</td>
<td></td>
<td></td>
<td><a href="https://papers.nips.cc/paper/8030-efficient-anomaly-detection-via-matrix-sketching.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Dual-Regularized Multi-View Outlier Detection</td>
<td></td>
<td></td>
<td><a href="https://www.ijcai.org/Proceedings/15/Papers/572.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Deep Anomaly Detection Using Geometric Transformations</td>
<td></td>
<td></td>
<td><a href="https://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models</td>
<td></td>
<td></td>
<td><a href="https://papers.nips.cc/paper/6456-multi-view-anomaly-detection-via-robust-probabilistic-latent-variable-models.pdf">📃Paper</a></td>
</tr>
<tr>
<td>Partial Multi-View Outlier Detection Based on Collective Learning</td>
<td></td>
<td></td>
<td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17166">📃Paper</a></td>
</tr>
<tr>
<td>Multi-View Anomaly Detection: Neighborhood in Locality Matters</td>
<td></td>
<td></td>
<td><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4418">📃Paper</a></td>
</tr>
<tr>
<td>Latent Discriminant Subspace Representations for Multi-View Outlier Detection</td>
<td></td>
<td></td>
<td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17401">📃Paper</a></td>
</tr>
<tr>
<td>Anomaly Detection with Partially Observed Anomalies</td>
<td></td>
<td></td>
<td><a href="https://dl.acm.org/citation.cfm?id=3184558.3186580">📃Paper</a></td>
</tr>
<tr>
<td>One-Class Active Learning for Outlier Detection with Multiple Subspaces</td>
<td></td>
<td></td>
<td><a href="https://dl.acm.org/citation.cfm?id=3357873">📃Paper</a></td>
</tr>
<tr>
<td>Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection</td>
<td>NIPS19</td>
<td></td>
<td><a href="http://papers.nips.cc/paper/9274-statistical-analysis-of-nearest-neighbor-methods-for-anomaly-detection">📃Paper</a></td>
</tr>
<tr>
<td>SNIPER: Few-shot Learning for Anomaly Detection to Minimize False-negative Rate with Ensured True-positive Rate</td>
<td>ICASSP19</td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/8683667">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="AE-VAE"><a href="#AE-VAE" class="headerlink" title="AE/VAE"></a>AE/VAE</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Estimation of Dimensions Contributing to Detected Anomalies with Variational Autoencoders</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1811.04576">📃Paper</a></td>
</tr>
<tr>
<td>Anomaly Detection with Robust Deep Autoencoders</td>
<td></td>
<td></td>
<td><a href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-with-robust-deep-auto-encoders">📃Paper</a></td>
</tr>
<tr>
<td>Complementary Set Variational Autoencoder for Supervised Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/8462181">📃Paper</a></td>
</tr>
<tr>
<td>A Two-class Hyper-spherical Autoencoder for Supervised Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/8683790">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="GAN-1"><a href="#GAN-1" class="headerlink" title="GAN"></a>GAN</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Adversarially Learned Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1812.02288">📃Paper</a></td>
</tr>
<tr>
<td>AMAD: Adversarial Multiscale Anomaly Detection on High-Dimensional and Time-Evolving Categorical Data</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1907.06582">📃Paper</a></td>
</tr>
<tr>
<td>Adversarially Learned One-Class Classifier for Novelty Detection</td>
<td></td>
<td></td>
<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.pdf">📃Paper</a> <a href="https://github.com/khalooei/ALOCC-CVPR2018">📥Code</a></td>
</tr>
<tr>
<td>Generative Probabilistic Novelty Detection with Adversarial Autoencoders</td>
<td></td>
<td></td>
<td><a href="https://papers.nips.cc/paper/7915-generative-probabilistic-novelty-detection-with-adversarial-autoencoders.pdf">📃Paper</a> <a href="https://github.com/podgorskiy/GPND">📥Code</a></td>
</tr>
<tr>
<td>OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1903.08550">📃Paper</a></td>
</tr>
<tr>
<td>Fence GAN: Towards Better Anomaly Detection</td>
<td>Arxiv</td>
<td></td>
<td><a href="https://arxiv.org/abs/1904.01209">📃Paper</a></td>
</tr>
<tr>
<td>Anomaly Detection via Minimum Likelihood Generative Adversarial Networks</td>
<td>Arxiv</td>
<td></td>
<td><a href="https://arxiv.org/abs/1808.00200">📃Paper</a></td>
</tr>
<tr>
<td>DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN</td>
<td>ICDM18</td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/8594955">📃Paper</a></td>
</tr>
<tr>
<td>Learning Competitive and Discriminative Reconstructions for Anomaly Detection</td>
<td>AAAI19</td>
<td></td>
<td><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4451">📃Paper</a></td>
</tr>
</tbody></table>
<h3 id="Miscellaneous-1"><a href="#Miscellaneous-1" class="headerlink" title="Miscellaneous"></a>Miscellaneous</h3><table>
<thead>
<tr>
<th><strong>Title</strong></th>
<th><strong>Conf/Journal</strong></th>
<th><strong>Description</strong></th>
<th><strong>Links</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Deep Structured Energy Based Models for Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1605.07717">📃Paper</a></td>
</tr>
<tr>
<td>Anomaly Detection using One-Class Neural Networks</td>
<td></td>
<td></td>
<td><a href="https://arxiv.org/pdf/1802.06360">📃Paper</a> <a href="https://github.com/raghavchalapathy/oc-nn">📥Code</a></td>
</tr>
<tr>
<td>High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning</td>
<td></td>
<td></td>
<td><a href="https://www.sciencedirect.com/science/article/pii/S0031320316300267">📃Paper</a></td>
</tr>
<tr>
<td>Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection</td>
<td></td>
<td></td>
<td><a href="https://openreview.net/pdf?id=BJJLHbb0-">📃Paper</a></td>
</tr>
<tr>
<td>Deep Anomaly Detection with Outlier Exposure</td>
<td></td>
<td></td>
<td><a href="https://openreview.net/forum?id=HyxCxhRcY7">📃Paper</a></td>
</tr>
<tr>
<td>Are Generative Deep Models for Novelty Detection Truly Better?</td>
<td>KDD18 Workshop</td>
<td></td>
<td><a href="https://arxiv.org/abs/1807.05027">📃Paper</a></td>
</tr>
<tr>
<td>Probabilistic-Mismatch Anomaly Detection: Do one’s Medications Match with the Diagnoses?</td>
<td>ICDM16</td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/7837890">📃Paper</a></td>
</tr>
<tr>
<td>Deep Anomaly Detection with Deviation Networks</td>
<td>KDD19</td>
<td></td>
<td><a href="https://dl.acm.org/doi/10.1145/3292500.3330871">📃Paper</a></td>
</tr>
<tr>
<td>Weakly-supervised Deep Anomaly Detection with Pairwise Relation Learning</td>
<td>AAAI20</td>
<td></td>
<td><a href="https://arxiv.org/abs/1910.13601">📃Paper</a></td>
</tr>
<tr>
<td>Transfer Anomaly Detection by Inferring Latent Domain Representations</td>
<td>NIPS19</td>
<td></td>
<td><a href="http://papers.nips.cc/paper/8517-transfer-anomaly-detection-by-inferring-latent-domain-representations">📃Paper</a></td>
</tr>
<tr>
<td>Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models</td>
<td>NIPS16</td>
<td></td>
<td><a href="http://papers.nips.cc/paper/6456-multi-view-anomaly-detection-via-robust-probabilistic-latent-variable-models">📃Paper</a></td>
</tr>
<tr>
<td>Continual Learning for Anomaly Detection with Variational Autoencoder</td>
<td>ICASSP19</td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/8682702">📃Paper</a></td>
</tr>
<tr>
<td>AdaFlow: Domain-adaptive Density Estimator with Application to Anomaly Detection and Unpaired Cross-domain Translation</td>
<td>ICASSP19</td>
<td></td>
<td><a href="https://ieeexplore.ieee.org/document/8683072">📃Paper</a></td>
</tr>
</tbody></table>
<h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><ul>
<li><em>AddGraph: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN</em> <a href="https://www.ijcai.org/proceedings/2019/614">[Paper]</a></li>
<li><em>Outlier Detection in Graph Streams</em> <a href="https://ieeexplore.ieee.org/document/5767885">[Paper]</a></li>
<li><em>NetWalk: A Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Networks</em> <a href="https://www.kdd.org/kdd2018/accepted-papers/view/netwalk-a-flexible-deep-embedding-approach-for-anomaly-detection-in-dynamic">[Paper]</a></li>
<li><em>Anomaly Detection in Dynamic Networks using Multi-view Time-Series Hypersphere Learning</em> <a href="https://dl.acm.org/citation.cfm?id=3132964">[Paper]</a></li>
</ul>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><p>这一部分主要是一些非异常检测文章，但是可以用来解决异常检测中的问题的相关文章。</p>
<h2 id="Infrastructure"><a href="#Infrastructure" class="headerlink" title="Infrastructure"></a>Infrastructure</h2><h3 id="GAN-2"><a href="#GAN-2" class="headerlink" title="GAN"></a>GAN</h3><ul>
<li><em>Generative Adversarial Networks</em> <a href="https://arxiv.org/abs/1406.2661">[Paper]</a></li>
</ul>
<h4 id="VAE-amp-GAN-Combination"><a href="#VAE-amp-GAN-Combination" class="headerlink" title="VAE &amp; GAN Combination"></a>VAE &amp; GAN Combination</h4><ul>
<li><em>Variational Approaches for Auto-Encoding Generative Adversarial Networks</em> <a href="https://arxiv.org/abs/1706.04987">[Paper]</a></li>
<li><em>Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks</em> <a href="https://arxiv.org/abs/1701.04722">[Paper]</a></li>
<li><em>On Unifying Deep Generative Models</em> <a href="https://arxiv.org/abs/1706.00550">[Paper]</a></li>
</ul>
<h4 id="Bidirectional-GANs"><a href="#Bidirectional-GANs" class="headerlink" title="Bidirectional GANs"></a>Bidirectional GANs</h4><ul>
<li><em>Adversarial Feature Learning</em> [[Paper]](Adversarial Feature Learning)</li>
<li><em>Adversarially Learned Inference</em> <a href="https://arxiv.org/abs/1606.00704">[Paper]</a></li>
<li><em>It Takes (Only) Two: Adversarial Generator-Encoder Networks</em> <a href="https://arxiv.org/abs/1704.02304">[Paper]</a></li>
</ul>
<h3 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h3><ul>
<li><em>Auto-Encoding Variational Bayes</em> <a href="https://arxiv.org/abs/1312.6114">[Paper]</a></li>
</ul>
<h2 id="Class-Imbalance"><a href="#Class-Imbalance" class="headerlink" title="Class Imbalance"></a>Class Imbalance</h2><ul>
<li><em>Focal Loss for Dense Object Detection</em> <a href="https://arxiv.org/abs/1708.02002">[Paper]</a></li>
<li><em>Gradient Harmonized Single-stage Detector</em> <a href="https://arxiv.org/abs/1811.05181">[Paper]</a></li>
</ul>
<h2 id="Stochastic-Temporal-Modeling"><a href="#Stochastic-Temporal-Modeling" class="headerlink" title="Stochastic Temporal Modeling"></a>Stochastic Temporal Modeling</h2><ul>
<li><em>Sequential Neural Models with Stochastic Layers</em> <a href="https://arxiv.org/abs/1605.07571">[Paper]</a></li>
<li><em>A Recurrent Latent Variable Model for Sequential Data</em> <a href="https://papers.nips.cc/paper/5653-a-recurrent-latent-variable-model-for-sequential-data">[Paper]</a></li>
<li><em>Deep State Space Models for Time Series Forecasting</em> <a href="https://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting">[Paper]</a></li>
<li><em>Bayesian Recurrent Neural Networks</em> <a href="https://arxiv.org/abs/1704.02798">[Paper]</a></li>
</ul>
<h2 id="Detection-without-Closed-form-Likelihood"><a href="#Detection-without-Closed-form-Likelihood" class="headerlink" title="Detection without Closed-form Likelihood"></a>Detection without Closed-form Likelihood</h2><h3 id="Reconstruction"><a href="#Reconstruction" class="headerlink" title="Reconstruction"></a>Reconstruction</h3><ul>
<li><em>Adversarially Learned One-Class Classifier for Novelty Detection</em> <a href="https://arxiv.org/abs/1802.09088">[Paper]</a></li>
<li><em>Generative Adversarial Network Based Novelty Detection Using Minimized Reconstruction Error</em> <a href="https://link.springer.com/article/10.1631/FITEE.1700786">[Paper]</a></li>
<li><em>Learning Discriminative Reconstructions for Unsupervised Outlier Removal</em> <a href="https://ieeexplore.ieee.org/document/7410534">[Paper]</a></li>
</ul>
<h3 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h3><ul>
<li><em>A Lipschitz-constrained Anomaly Discriminator Framework</em> <a href="https://arxiv.org/abs/1905.10710">[Paper]</a></li>
</ul>
<h3 id="Out-of-distribution"><a href="#Out-of-distribution" class="headerlink" title="Out-of-distribution"></a>Out-of-distribution</h3><ul>
<li><em>Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</em> <a href="https://arxiv.org/abs/1706.02690">[Paper]</a></li>
<li><em>Learning Confidence for Out-of-Distribution Detection in Neural Networks</em> <a href="https://arxiv.org/abs/1802.04865">[Paper]</a></li>
<li><em>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</em> <a href="https://arxiv.org/abs/1610.02136">[Paper]</a></li>
<li><em>A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</em> <a href="https://arxiv.org/abs/1807.03888">[Paper]</a></li>
</ul>
<h2 id="GANs-for-Incomplete-Data"><a href="#GANs-for-Incomplete-Data" class="headerlink" title="GANs for Incomplete Data"></a>GANs for Incomplete Data</h2><ul>
<li><em>AmbientGAN: Generative Models From Lossy Measurements</em> <a href="https://openreview.net/forum?id=Hy7fDog0b">[Paper]</a></li>
<li><em>MisGAN: Learning from Incomplete Data with Generative Adversarial Networks</em> <a href="https://openreview.net/forum?id=S1lDV3RcKm">[Paper]</a></li>
</ul>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p>详见<a href="http://qfxiao.me/2020/02/03/Datasets-for-Time-Series-Anomaly-Detection/。">http://qfxiao.me/2020/02/03/Datasets-for-Time-Series-Anomaly-Detection/。</a></p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2020/01/06/Unsupervised-Anomaly-Detection-for-Intricate-KPIs-via-Adversarial-Training-of-VAE/"><img class="thumbnail" src="http://qfxiao.me/img/image-20200106141915707.png" alt="Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2020-01-06T05:09:32.000Z" title="2020-01-06T05:09:32.000Z">2020-01-06</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span><span class="level-item">18 分钟 读完 (大约 2770 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/06/Unsupervised-Anomaly-Detection-for-Intricate-KPIs-via-Adversarial-Training-of-VAE/">Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/8737430/">论文📃</a></p>
<p><a href="https://github.com/yantijin/Buzz">代码📥</a></p>
<p>本文介绍了一种利用对抗训练来进行时间序列异常检测的方法<em>Buzz</em>。作者认为在现实中复杂的KPI数据大量存在，这种数据通常带有非高斯分布的噪声，同时数据分布复杂，导致一般的生成式模型无法对数据进行很好的建模，所以作者提出了基于对抗训练的模型。在文中，作者的创新点主要有三个：</p>
<ol>
<li>为了处理复杂数据，将数据空间分为多个子空间，在每个子空间上进行距离的度量；</li>
<li>采用<em>Wasserstein</em>距离度量模型建模的分布和真实分布之间的距离；</li>
<li>建立了基于对抗训练的<em>Buzz</em>的损失函数和VAE之间的关系。</li>
</ol>
<p><img src="http://qfxiao.me/img/image-20200106141915707.png" alt=""></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><p>对于任意时间$t$，给定历史观察值$x_{t-T+1},\cdots,x_t$，确定异常是否发生(记为$y_t=1$)。通常来收异常检测算法给出的是发生异常的可能性，如$p(y_t=1|x_{t-T+1},\cdots,x_t)$。</p>
<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><table>
<thead>
<tr>
<th><strong>Model</strong></th>
<th><strong>Latent</strong></th>
<th><strong>Data</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong><em>Auto-encoder (AE)</em></strong></td>
<td><code>None</code></td>
<td><code>L1Loss</code></td>
</tr>
<tr>
<td><strong><em>Variational Auto-encoder (VAE)</em></strong></td>
<td><code>KL Divergence</code></td>
<td><code>Log Likelihood</code></td>
</tr>
<tr>
<td><strong><em>Adversarial Auto-encoder (AAE)</em></strong></td>
<td><code>Discriminator</code></td>
<td><code>L1Loss</code></td>
</tr>
<tr>
<td><strong><em>Wasserstein Auto-encoder (WAE)</em></strong></td>
<td><code>MaxMeanDiscrepancy</code> or <code>Discriminator</code></td>
<td><code>L1Loss</code></td>
</tr>
<tr>
<td><strong><em>AlphaGAN</em></strong></td>
<td><code>Discriminator</code></td>
<td><code>Discriminator</code>+<code>L1Loss</code></td>
</tr>
</tbody></table>
<h2 id="GAN-and-WGAN-GP"><a href="#GAN-and-WGAN-GP" class="headerlink" title="GAN and WGAN-GP"></a>GAN and WGAN-GP</h2><p>原始GAN等价于优化：<br>$$<br>\mathbb{E}<em>{x\sim P_r}\log{\frac{P_r(x)}{\frac{1}{2}\left[P_r(x)+P_g(x)\right]}}+\mathbb{E}</em>{x\sim P_g}\log{\frac{P_g(x)}{\frac{1}{2}\left[P_r(x)+P_g(x)\right]}}<br>$$<br>根据KL散度和JS散度的定义：<br>$$<br>\text{KL}(P_1\parallel P_2)=\mathbb{E}_{x\sim P_1}\log{\frac{P_1}{P_2}}<br>$$</p>
<p>$$<br>\text{JS}(P_1\parallel P_2)=\frac{1}{2}\text{KL}(P_1\parallel \frac{P_1+P_2}{2})+\frac{1}{2}\text{KL}(P_2\parallel \frac{P_1+P_2}{2})<br>$$</p>
<p>可重写为：<br>$$<br>2\text{JS}(P_r\parallel P_g)-2\log 2<br>$$<br>当$P_r$与$P_g$的支撑集（support）是高维空间中的低维流形（manifold）时，$P_r$与$P_g$重叠部分测度（measure）为0的概率为1。</p>
<ul>
<li>支撑集（support）其实就是函数的非零部分子集，比如<code>ReLU</code>函数的支撑集就是[公式]，一个概率分布的支撑集就是所有概率密度非零部分的集合。</li>
<li>流形（manifold）是高维空间中曲线、曲面概念的拓广，我们可以在低维上直观理解这个概念，比如我们说三维空间中的一个曲面是一个二维流形，因为它的本质维度（intrinsic dimension）只有2，一个点在这个二维流形上移动只有两个方向的自由度。同理，三维空间或者二维空间中的一条曲线都是一个一维流形。</li>
<li>测度（measure）是高维空间中长度、面积、体积概念的拓广，可以理解为“超体积”。</li>
</ul>
<p><em>Wasserstein</em>距离定义如下：<br>$$<br>W(P_r,P_g)=\inf\limits_{\gamma\sim\prod(P_r,P_g)}\mathbb{E}<em>{(x,y)\sim \gamma}\left[\parallel x-y\parallel\right]<br>$$<br>下确界$\inf$没法直接求解，不过根据相关定理其等价于：<br>$$<br>W(P_r,P_g)=\frac{1}{K}\sup\limits</em>{\parallel f\parallel_L\leq K}\mathbb{E}<em>{x\sim P_r}[f(x)]-\mathbb{E}</em>{x\sim P_g}[f(x)]<br>$$<br><em>Lipschitz</em>连续是指存在一个常数$K\geq 0$使得定义域内任意两个元素$x_1$和$x_2$都满足：<br>$$<br>|f(x_1)-f(x_2)|\leq K|x_1-x_2|<br>$$<br>WAN的损失函数：<br>$$<br>\mathcal{L}=\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}<em>g}\left[D({\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits</em>{\mathbf{x}\sim\mathbb{P}<em>r}\left[D(\mathbf{x})\right]<br>$$<br>WGAN-GP的损失函数为：<br>$$<br>\mathcal{L}=\mathop{\mathbb{E}}\limits</em>{\tilde{\mathbf{x}}\sim\mathbb{P}<em>g}\left[D(\tilde{\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits</em>{\mathbf{x}\sim\mathbb{P}<em>r}\left[D(\mathbf{x})\right] + \lambda\mathop{\mathbb{E}}\limits</em>{\hat{\mathbf{x}}\sim\mathbb{P}<em>{\hat{\mathbf{x}}}}\left[(\parallel\nabla</em>{\hat{\mathbf{x}}}D(\hat{\mathbf{x}})\parallel_2-1)^2\right]<br>$$</p>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><p>下图为<em>Buzz</em>的总体流程：</p>
<p><img src="http://qfxiao.me/img/image-20200106141958376.png" alt=""></p>
<p>数据会首先进行一些预处理，之后进行训练。在检测阶段会根据异常分数来判定异常。</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>在文中，最关键的两个创新点分别是<em>Wasserstein</em>距离和对数据分布进行分区的方法。</p>
<ul>
<li><p>在使用距离度量方面， 因为<em>Wasserstein</em>在WGAN中取得了很好的效果，是一种鲁棒的距离度量，所以作者在文中采用了<em>Wasserstein</em>距离来衡量生成的分布和真实的分布之间的距离，并由此引入了对抗训练；</p>
</li>
<li><p>在分区方法方面，作者认为原始数据过于复杂，所以将数据空间$\mathcal{X}$进行划分，然后在每个子空间上使用<em>Wasserstein</em>度量距离，而总体的距离由每个分区的距离的期望求得。</p>
</li>
</ul>
<p>作者还发现，当划分地越来越细时，总体距离接近于特定形式的VAE的重构误差项。</p>
<p><img src="http://qfxiao.me/img/image-20200106142008894.png" alt=""></p>
<h2 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h2><p>下图为模型的网络结构：</p>
<p><img src="http://qfxiao.me/img/image-20200106142018827.png" alt=""></p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h3><p>先定义一些符号：</p>
<ul>
<li>$b$和$s$分别为Batch的大小和邻居的大小，数据集按$s$进行切分，然后随机打乱，每个Batch包含$b$个$s$，之后$s/=2,b*=2$；</li>
<li>$\mathcal{W}={w_1,w_2,\cdots,w_b}$为一个Batch，且满足每个$w_i$是$s$的倍数；</li>
<li>$w\in\mathcal{W}$的邻域集(neighborhood set)为一个时间上的partition，记为${w,w+1,\cdots,w+s-1}$</li>
<li>$\mathbf{x}^{(w)},\mathbf{x}^{(w+1)},\cdots,\mathbf{x}^{(w+s-1)}$为在空间$\mathcal{X}$上的一个partition，记为$S_w$，其中$\mathbf{x}^{(w)}$表示以$w$为结尾的时间窗口。</li>
</ul>
<p><em>Buzz</em>的损失函数和WGAN-GP类似，但做了一些改进，由下面四部分组成，下面分别解释。</p>
<p>第一个是每一个partition的$\mathbf{z}$后验的KL散度：<br>$$<br>\mathcal{K} = \frac{1}{bs}\sum\limits_{w\in\mathcal{W}}\sum\limits_{i=1}^{s-1}\text{KL}\left[q_\phi(\mathbf{z}|\mathbf{x})\parallel\mathcal{N}(\mathbf{0},\mathbf{1})\right]<br>$$</p>
<p>第二个在训练时是一个常数：<br>$$<br>Z(\lambda) = \frac{\Gamma(W)}{\Gamma(\frac{W}{2})}2\pi^{\frac{W}{2}}\lambda^{-W}<br>$$</p>
<p>其中$\Gamma$是<em>Gamma</em>函数。</p>
<p>第三个是<em>Wasserstein</em>距离：<br>$$<br>\mathcal{T}(F,w)=\frac{1}{bs}\sum\limits_{i=1}^{s-1}\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x}^{w+i})}\left[F(\mathbf{x}^{(w+i)})-F(G(\mathbf{z}))\right]<br>$$</p>
<p>第四个是<em>Gradient Penalty</em>：</p>
<p>$$<br>\mathcal{R}(F,w)=\frac{1}{bs}\sum\limits_{i=1}^{s-1}\mathbb{E}<em>{q_\phi(\mathbf{z}|\mathbf{x}^{(w+i)})}\left[\mathbb{E}</em>{\varepsilon\sim[0,1]}(\parallel\nabla_{\hat{\mathbf{x}}}(\hat{\mathbf{x}})\parallel-\mathbf{1})^2\right]<br>$$</p>
<p>其中$\hat{\mathbf{x}}=\varepsilon \mathbf{x}^{w+i}+(1-\varepsilon)G(\mathbf{z})$为生成数据与真实数据的插值。</p>
<blockquote>
<p>原始的WGAN-GP的损失函数为：<br>$$<br>L=\mathop{\mathbb{E}}\limits_{\tilde{\mathbf{x}}\sim\mathbb{P}<em>g}\left[D(\tilde{\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits</em>{\mathbf{x}\sim\mathbb{P}<em>r}\left[D(\mathbf{x})\right] + \lambda\mathop{\mathbb{E}}\limits</em>{\hat{\mathbf{x}}\sim\mathbb{P}<em>{\hat{\mathbf{x}}}}\left[(\parallel\nabla</em>{\hat{\mathbf{x}}}D(\hat{\mathbf{x}})\parallel_2-1)^2\right]<br>$$<br>其中$\mathbb{P}<em>g$为生成器的分布，$\mathbb{P}_r$为真实分布，$\mathbb{P}</em>{\hat{\mathbf{x}}}$为真实数据和生成数据插值得到的分布。</p>
</blockquote>
<p>$$<br>\hat{\mathcal{L}}<em>{Buzz}=-\lambda\sup\limits_F\left[\sum\limits</em>{w\in\mathcal{W}}(\left|\mathcal{T}(F,w)\right|-\eta\mathcal{R}(F,w))\right]-\mathcal{K}-\log Z(\lambda)<br>$$</p>
<h3 id="Training-Procedure"><a href="#Training-Procedure" class="headerlink" title="Training Procedure"></a>Training Procedure</h3><p><em>Buzz</em>的训练过程与WGAN-GP类似，</p>
<h2 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a>Detection</h2><p>文中假设解码器的输出服从如下分布：</p>
<p>$$<br>p_\theta(\mathbf{x}|\mathbf{z})=\frac{1}{Z(\lambda)}\exp{-\lambda\parallel\mathbf{x}-G(\mathbf{z})\parallel}<br>$$</p>
<p>作者定义异常分数为：<br>$$<br>\mathcal{S}=\log p_\theta(\mathbf{x})-\log p_\theta(\bar{\mathbf{x}})<br>$$<br>其中$\bar{\mathbf{x}}$为经过MCMC填充后的样本。</p>
<p>异常分数也可以展开为：<br>$$<br>\log\frac{1}{L}\sum\limits_{l=1}^L\left[\frac{p_\theta(\mathbf{x}|\mathbf{z^{(l)}})p_\theta(\mathbf{z}^{(l)})}{q_\phi(\mathbf{z}^{(l)}|\bar{\mathbf{x}})}\right]-\log\frac{1}{L}\sum\limits_{l=1}^L\left[\frac{p_\theta(\bar{\mathbf{x}}|\mathbf{z}^{(l)})p_\theta(\mathbf{z}^{(l)})}{q_\phi(\mathbf{z}^{(l)}|\bar{\mathbf{x}})}\right]<br>$$</p>
<p>最终算法流程图为：</p>
<p><img src="http://qfxiao.me/img/image-20200106142053342.png" alt=""></p>
<h2 id="Theoretical-Analysis"><a href="#Theoretical-Analysis" class="headerlink" title="Theoretical Analysis"></a>Theoretical Analysis</h2><p>在理论分析中，作者主要是想建立$\mathcal{L}<em>{Buzz}$和VAE的损失函数$\mathcal{L}</em>{vae}$之间的联系，损失函数$\mathcal{\hat{L}}_{Buzz}$为：</p>
<p>$$<br>\hat{\mathcal{L}}<em>{Buzz}=-\lambda\sup\limits_F\left[\sum\limits</em>{w\in\mathcal{W}}(\left|\mathcal{T}(F,w)\right|-\eta\mathcal{R}(F,w))\right]-\mathcal{K}-\log Z(\lambda)<br>$$</p>
<p>为了便于分析，去掉<em>Gradient Penalty</em>的部分，公式可简化为：</p>
<p>$$<br>\mathcal{L}<em>{Buzz}=-\lambda\mathbb{E}</em>{p(w)}W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]-\mathcal{K}-\log Z(\lambda)<br>$$</p>
<p>实际上$Z(\lambda)=\mathfrak{S}_W\Gamma(W)\lambda^{-W}$，其中$\mathfrak{S}_W$为$W$维单位球的表面积。</p>
<blockquote>
<p>$n$维空间单位球表面积公式：<br>$$<br>\omega_n=\frac{2\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}<br>$$</p>
</blockquote>
<p>而$W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]$为<em>Wasserstein</em>距离：<br>$$<br>W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]=\sup\limits_{Lip(f)\leq 1}\left{\int_\mathcal{X}f(\mathbf{x})p(\mathbf{x}|w)\mathrm{d}\mathbf{x}-\int_\mathcal{X}f(\mathbf{y})p(\mathbf{y}|w)\mathrm{d}\mathbf{y}\right}<br>$$</p>
<h3 id="Lemma-1"><a href="#Lemma-1" class="headerlink" title="Lemma 1"></a>Lemma 1</h3><p>通过设定具体形式的后验分布，VAE的损失函数可以写为：</p>
<blockquote>
<p>设$\mathbf{x}$的后验分布$p(\mathbf{x}|\mathbf{z})=\frac{1}{Z(\lambda)}\exp{-\lambda\parallel\mathbf{x}-G(\mathbf{z})\parallel}$，那么VAE的损失函数为：<br>$$<br>\mathcal{L}<em>{vae}=\lambda\mathbb{E}</em>{p(w)}\left[\mathbb{E}<em>{p(\mathbf{x}|w)}\mathbb{E}</em>{p_G(\mathbf{y}|\mathbf{x})}-\parallel\mathbf{x}-\mathbf{y}\parallel\right]-\mathcal{K}-\log{Z(\lambda)}<br>$$</p>
</blockquote>
<p>后验分布实际上是一个Laplace分布：</p>
<blockquote>
<p><strong>Laplace Distribution</strong>:<br>$$<br>f(x|\theta,\lambda)=\frac{1}{2\lambda}\exp{\left(-\frac{|x-\theta|}{\lambda}\right)}<br>$$</p>
</blockquote>
<p>可以直接把后验分布带入VAE的损失函数就得到了。</p>
<h3 id="Lemma-2"><a href="#Lemma-2" class="headerlink" title="Lemma 2"></a>Lemma 2</h3><p>$S_w$定义为数据空间$\mathcal{X}$的一个partition，而$S={(\mathbf{x}_1,\mathbf{x}_2)|\exist w, \mathbf{x}_1\in S_w,\mathbf{x}_2\in S_w}$。</p>
<blockquote>
<p>当$G,\phi,\lambda$固定时，$S\downarrow$有$\mathcal{L}_{Buzz}\downarrow$</p>
</blockquote>
<h3 id="Lemma-3"><a href="#Lemma-3" class="headerlink" title="Lemma 3"></a>Lemma 3</h3><blockquote>
<p>$\max\mathcal{L}<em>{Buzz}\geq\max{\mathcal{L}</em>{vae}}$，同时，当$S\downarrow\text{diag}{\mathcal{X}}$时$\max\mathcal{L}<em>{Buzz}\downarrow\max\mathcal{L}</em>{vae}$</p>
</blockquote>
<p><img src="http://qfxiao.me/img/image-20200106183833975.png" alt=""></p>
<h3 id="Lemma-4"><a href="#Lemma-4" class="headerlink" title="Lemma 4"></a>Lemma 4</h3><blockquote>
<p>令$p^\prime_G(\mathbf{y}|\mathbf{x})$表示$\mathbb{E}<em>{q</em>{\phi^\prime}}\left[p_G(\mathbf{y}|\mathbf{z})\right]$。如果$(G,\phi,\lambda)$是一个解，那么存在$(G,\phi^\prime,\lambda)$使得：<br>$$<br>\mathbb{E}<em>{p(\mathbf{x}|w)}\mathbb{E}</em>{p_G^\prime(\mathbf{y}|\mathbf{x})}\parallel\mathbf{x}-\mathbf{y}\parallel=W^1\left[P(\mathbf{x}|w)\parallel P_G(\mathbf{y}|w)\right]<br>$$<br>此时$\mathcal{L}<em>{Buzz}-\mathcal{L}</em>{vae}^\prime=\mathcal{K}^\prime-\mathcal{K}$，其中$\mathcal{L}^\prime,\mathcal{K}^\prime$分别为$(G,\phi^\prime,\lambda)$时的$\mathcal{L}$和$\mathcal{K}$。</p>
</blockquote>
<p>$$<br>\mathcal{L}^\dagger_{Buzz}=\mathbb{E}<em>{p(\mathbf{x})}\left[\mathbb{E}</em>{q_{\phi^\prime}(\mathbf{z}|\mathbf{x})}\log_{p_\theta}(\mathbf{x}|\mathbf{z})\right]-\min\limits_{\bar{\phi}\sim\phi^\prime}\bar{\mathcal{K}}<br>$$</p>
<h3 id="Lemma-5"><a href="#Lemma-5" class="headerlink" title="Lemma 5"></a>Lemma 5</h3><p>这里主要是想证明</p>
<blockquote>
<p>对于固定的$w$，令：<br>$$<br>\mathcal{F}={f|Lip(f)\leq 1}, \space \mathcal{F}^<em>=\left{f|<em>{S_w}\bigg|Lip(f|</em>{S_w})\leq 1\right}<br>$$<br>有$\sup_{f\in\mathcal{F}}\mathcal{T}(f)=\sup_{f|_{S_w}\in\mathcal{F}^</em>}\mathcal{T}^*\left(f|_{S_w}\right)$。</p>
</blockquote>
<h3 id="Theorem-6"><a href="#Theorem-6" class="headerlink" title="Theorem 6"></a>Theorem 6</h3><blockquote>
<p>$\mathcal{L}<em>{Buzz}$的对偶形式为：<br>$$<br>\mathcal{L}</em>{Buzz}=-\lambda\sup\limits_{Lip(F;S)\leq 1}\mathbb{E}_{p(w)}\mathcal{T}^*(F)-\mathcal{K}-\log Z(\lambda)<br>$$</p>
</blockquote>
<p>近似的$\mathcal{L}<em>{Buzz}$的对偶形式为：<br>$$<br>\bar{\mathcal{L}}</em>{Buzz}=-\lambda\sup\limits_{Lip(F;S)\leq 1}\mathbb{E}_{p(w)}\mathcal{T}(F)-\mathcal{K}-\log Z(\lambda)<br>$$</p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p><img src="http://qfxiao.me/img/image-20200106142131500.png" alt=""></p>
<p><img src="http://qfxiao.me/img/image-20200106142155978.png" alt=""></p>
<p><img src="http://qfxiao.me/img/image-20200106142212110.png" alt=""></p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2019/11/02/Variational-Approaches-for-Auto-Encoding-Generative-Adversarial-Networks/"><img class="thumbnail" src="http://qfxiao.me/img/image-20191102233101258.png" alt="Variational Approaches for Auto-Encoding Generative Adversarial Networks"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-11-02T15:28:53.000Z" title="2019-11-02T15:28:53.000Z">2019-11-02</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/GAN/">GAN</a></span><span class="level-item">18 分钟 读完 (大约 2774 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/11/02/Variational-Approaches-for-Auto-Encoding-Generative-Adversarial-Networks/">Variational Approaches for Auto-Encoding Generative Adversarial Networks</a></h1><div class="content"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文揭示了对抗生成网络（Generative Adversarial Networks, GAN）和变分自编码器（Variational Auto-encoders, VAE）之间的联系，并据此提出了一种将两者结合的新模型。文中主要是将不可解的似然函数和未知的后验分布用一个非确定的分布（Immplicit Distribution）替代，并加入判别器来使得该分布逼近真实的分布。通过这个方法，作者将VAE中的损失函数进行了替换，变成了GAN中的“生成-判别”模式。</p>
<p><a href="https://arxiv.org/abs/1706.04987">原文</a></p>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><p>本文有如下贡献：</p>
<ul>
<li>本文提出变分推断（Variational Inference）也能通过对非确定分布的估计应用在GAN中；</li>
<li>基于似然的模型（Likelihood-based Models）和非似然模型（Likelihood-free Models）能够通过对抗学习结合起来；</li>
<li>作者根据文中提出的新观点修改了VAE的损失函数，将其称之为Auto-encoding GAN ($\alpha$-GAN)，并提出了对应的实用的改进；</li>
<li>本文与众多State-of-Art模型进行了对比</li>
</ul>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="Overcoming-Intractability-in-Generative-Models"><a href="#Overcoming-Intractability-in-Generative-Models" class="headerlink" title="Overcoming Intractability in Generative Models"></a>Overcoming Intractability in Generative Models</h2><h3 id="Latent-Variable-Models"><a href="#Latent-Variable-Models" class="headerlink" title="Latent Variable Models"></a>Latent Variable Models</h3><p>隐变量模型通过隐变量的形式描述了数据的产生过程。最简单的形式是假设隐变量$\mathbf{z}$服从一个先验分布$\mathbf{z}\sim p(\mathbf{z})$，而数据$\mathbf{x}$从条件分布$p(\mathbf{x}|\mathbf{z})$抽样产生。通常来说描述$p(\mathbf{x}|\mathbf{z})$的模型称为生成器$\mathcal{G}_\theta(\mathbf{z})$，带有可优化的参数$\theta$，而$\mathbf{z}$通常假设为正态分布$\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$。</p>
<p>文中区分了两种隐变量模型，一种是<em>Implicit Latent Variable Models</em>，一种是<em>Prescribed Latent Variable Models</em>。文中的描述不太清楚，个人认为两者的区别是前者图模型中的$\mathbf{x}$不是一个随机变量，在优化的时候需要用一个刻画生成的$\mathbf{x}$和真实的$\mathbf{x}$的差别的函数$\delta(\mathbf{x}-\mathcal{G}_\theta(\mathbf{z}))$，而后者图模型中$\mathbf{x}$是一个随机变量，这样可以写出似然函数用极大似然估计。</p>
<p>无论是GAN还是VAE都需要通过边缘分布$p_\theta(\mathbf{x})$来刻画建模的好坏，比如说根据$p_\theta(\mathbf{x})$与真实分布$p^<em>_\theta(\mathbf{x})$之间的KL散度$\text{KL}\left[p_\theta(\mathbf{x})\parallel p_\theta^</em>(\mathbf{x})\right]$。但通常情况下$p_\theta(\mathbf{x})$都是不可解的，而GAN和VAE通过不同的途径解决了这个问题。</p>
<h3 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h3><p>GAN没有直接计算$p_\theta(\mathbf{x})$，而是使用了一个判别器来判别样本是从$p_\theta(\mathbf{x})$还是$p_\theta^<em>(\mathbf{x})$采样得到的，如果判别器无法进行区分，那我们认为此时$p_\theta(\mathbf{x})\approx p_\theta^</em>(\mathbf{x})$。</p>
<p>令随机变量$y\in{0,1}$，$y=1$表示样本$\mathbf{x}$来自真实分布，$y=0$表示样本$\mathbf{x}$来自生成分布，而判别器的输出为$\mathbf{x}$来自真实分布的概率$\mathcal{D}<em>\phi(\mathbf{x})=p(y=1|\mathbf{x})$。GAN通过对来自真实分布和生成分布的样本求二元交叉熵来作为判别器损失函数：<br>$$<br>\textbf{Discriminator Loss: }\mathbb{E}</em>{p^*(\mathbf{x})}[-\log\mathcal{D}<em>\phi(\mathbf{x})]+\mathbb{E}</em>{p_\theta(\mathbf{x})}[-\log(1-\mathcal{D}_\phi(\mathbf{x}))]<br>$$</p>
<p>生成器将最大化判别器对生成样本判定为真的概率作为损失函数，同时还有一个等价的但在实践中表现更好的替代版本：</p>
<p>$$<br>\textbf{Generator Loss: }\mathbb{E}<em>{p_\theta(\mathbf{x})}[\log(1-\mathcal{D}_\phi(\mathbf{x}))];\textbf{ Alternative Loss: }\mathbf{E}</em>{p_\theta(\mathbf{x})}[-\log\mathcal{D}_\phi(\mathbf{x})]<br>$$</p>
<h3 id="The-Density-Ratio-Trick"><a href="#The-Density-Ratio-Trick" class="headerlink" title="The Density Ratio Trick"></a>The Density Ratio Trick</h3><p>令$p^<em>(\mathbf{x})=p(\mathbf{x}|y=1)$，$p_\theta(\mathbf{x})=p(\mathbf{x}|y=0)$。定义*Density Ratio</em> $r_\phi(\mathbf{x})$为真实分布和生成分布之间的比例：</p>
<p>$$<br>r_\phi(\mathbf{x})=\frac{p^*(\mathbf{x})}{p_\theta(\mathbf{x})}=\frac{p(\mathbf{x}|y=1)}{p(\mathbf{x}|y=0)}=\frac{p(y=1|\mathbf{x})}{p(y=0|\mathbf{x})}=\frac{\mathcal{D}_\phi(\mathbf{x})}{1-\mathcal{D}_\phi(\mathbf{x})}<br>$$</p>
<p>上式表明了<em>Density Ratio*的计算可以仅通过从两个分布上采样得到的样本加上一个二分类器$\mathcal{D}_\phi(\mathbf{x})$实现（假设$p(y=0)=p(y=1)$）。更深入的说，对于不可解的分布$p_\theta^</em>(\mathbf{x})$，我们可以通过计算<em>Density Ratio*来了解我们近似的分布$p_\theta(\mathbf{x})$和真实的$p_\theta^</em>(\mathbf{x})$之间的相对性。而且我们只需要能够在两个分布上进行采样，并且训练一个判别器即可。因为判别器是一个普通的分类器，所以大量的主流分类器都可以使用。</p>
<h3 id="Variational-Inference"><a href="#Variational-Inference" class="headerlink" title="Variational Inference"></a>Variational Inference</h3><p>现在来看VAE，另一种解决不可解分布的方法是近似。<em>Variational Inference</em>通过引入一个变分分布$q_\eta(\mathbf{z}|\mathbf{x})$推出了不可解的$\mathbf{x}$的对数似然的下界（常被称为证据下界ELBO）：</p>
<p>$$<br>\log p_\theta(\mathbf{x})=\log\int p_\theta(\mathbb{x}|\mathbb{z})p(\mathbf{z})\text{d}\mathbf{z}\geq \mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log p_\theta(\mathbf{x}|\mathbf{z})\right]-\text{KL}\left[q_\eta(\mathbf{z}|\mathbf{x})\parallel p(\mathbf{z})\right]=\mathcal{F}(\boldsymbol{\theta}, \boldsymbol{\eta})<br>$$</p>
<p>VAE是<em>Variational Inference</em>的一种实现，变分分布通过一个神经网络进行建模，并且建立起了完整的可优化的模型。</p>
<h3 id="Synthetic-Likelihood"><a href="#Synthetic-Likelihood" class="headerlink" title="Synthetic Likelihood"></a>Synthetic Likelihood</h3><p>当似然函数未知（GAN中没有显式的似然函数，而VAE中有）的时候，<em>Variational Inference*便无法直接使用。对于没有显式的似然函数的情况，以VAE的ELBO的第一项为例，假设$p_\theta(\mathbf{x}|\mathbf{z})$分布的具体形式未知，我们只有从$p_\theta(\mathbf{x}|\mathbf{z})$采样得到的样本，如何计算$\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]$呢？一个方法是乘以$p_\theta^</em>(\mathbf{x})$再除以$p_\theta^*(\mathbf{x})$：</p>
<p>$$<br>\mathbb{E}<em>{q_\eta(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]=\mathbb{E}</em>{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{p_\theta(\mathbf{x}|\mathbf{z})}{p^<em>(\mathbf{x})}\right]+\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p^</em>(\mathbf{x})]<br>$$</p>
<p>公式(5)中的第一项包括了合成似然$R(\theta)=\frac{p_\theta(\mathbf{x}|\mathbf{z})}{p^*(\mathbf{x})}$，优化$R(\theta)$相当于优化$\log p_\theta(\mathbf{x}|\mathbf{z})$。第二项与生成网络的参数$\theta$无关，所以在优化的时候可以忽略。</p>
<h2 id="A-Fusion-of-Variational-and-Adversarial-Learning"><a href="#A-Fusion-of-Variational-and-Adversarial-Learning" class="headerlink" title="A Fusion of Variational and Adversarial Learning"></a>A Fusion of Variational and Adversarial Learning</h2><p>GAN和VAE分别从不同的角度解决了生成模型的推断问题，我们下面从VAE出发，考虑将两者结合起来。</p>
<h3 id="Implicit-Variational-Distributions"><a href="#Implicit-Variational-Distributions" class="headerlink" title="Implicit Variational Distributions"></a>Implicit Variational Distributions</h3><p>变分推断<strong>Variational Inference</strong>的主要任务就是确定$q_\eta(\mathbf{z}|\mathbf{x})$，通常的做法如<strong>Mean-field Variational Inference</strong>会假设一个简单的分布，如高斯分布。在本文中不对$q_\eta(\mathbf{z}|\mathbf{x})$的形式作假设，仅假设其为一个隐含的分布。运用上文提到的<em>Density Ratio Trick</em>，我们可以将VAE损失函数中的第二项改写为：</p>
<p>$$<br>-\text{KL}[q_\eta(\mathbf{z}|\mathbf{x})\parallel p(\mathbf{z})]=\mathbb{E}<em>{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{p(\mathbf{z})}{q_\eta(\mathbf{z}|\mathbf{x})}\right]\approx\mathbb{E}</em>{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}{1-\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}\right]<br>$$</p>
<p>文中引入了一个隐变量分类器（Latent Classifier）$\mathcal{C}_{\boldsymbol{\omega}}(\mathbf{z})$，用来判别$\mathbf{z}$是从编码网络还是从标准高斯分布中采样得到的（猜测这样做的好处是不用再对$\mathbf{z}$的后验做高斯分布的假设了，也不需要在变分网络输出形成的高斯分布上采样得到$\mathbf{z}$了，这样重参数技巧也省了）。具体实现上，期望可以用蒙特卡洛方法（采样多次取均值）进行计算。</p>
<h3 id="Likelihood-Choice"><a href="#Likelihood-Choice" class="headerlink" title="Likelihood Choice"></a>Likelihood Choice</h3><p>对于VAE损失函数第一项，对应生成网络，我们可以选择对$p(\mathbf{x}|\mathbf{z})$分布的具体形式做假设， 这样对应<em>Likelihood-based<em>的情况。文中选择的是</em>Zero-mean Laplace Distribution</em> $p_\theta(\mathbf{x}|\mathbf{z})\propto\exp(-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1)$（不就是$L_1$ Loss吗？？？）。</p>
<p>对于<em>Likelihood-free</em>的情况，可以继续使用上面提到的<em>Density Ratio Trick</em>，这时需要加一个一个判别器。</p>
<p>$$<br>\mathbb{E}<em>{q_\eta(\mathbf{z}|\mathbf{x})}\left[-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1\right]\space\space\text{  or  }\space\space\mathbb{E}</em>{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}{1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}\right]<br>$$</p>
<p>对于两种选择，前者对应VAE，好处是不会出现模式崩溃的情况，后者对应GAN，容易出现模式崩溃的情况，但是可以使用对抗学习的方式（这是优点？？？），本文选择两种都用（我全都要.jpg）。</p>
<h3 id="Hybrid-Loss-Functions"><a href="#Hybrid-Loss-Functions" class="headerlink" title="Hybrid Loss Functions"></a>Hybrid Loss Functions</h3><p>将前面的讨论结合起来，最后的损失函数就是：</p>
<p>$$<br>\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\eta})=\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1+\log\frac{\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}{1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}+\log\frac{\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}{1-\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}\right]<br>$$</p>
<p>最后模型包含四个网络：生成网络$p_\theta(\mathbf{x}|\mathbf{z})$、推断网络$q_\eta(\mathbf{z}|\mathbf{x})$以及两个判别器$\mathcal{C}_{\boldsymbol{\omega}}$和$\mathcal{D}_\phi$，作者将其命名为$\alpha$-GAN。</p>
<p>算法流程如下：</p>
<p><img src="http://qfxiao.me/img/image-20191106220923748.png" alt=""></p>
<h3 id="Improved-Techniques"><a href="#Improved-Techniques" class="headerlink" title="Improved Techniques"></a>Improved Techniques</h3><p>作者为了改进模型的稳定性和效率，将生成器的Loss中的$-\log(1-\mathcal{D}_\phi)$修改为了$\log\mathcal{D}_\phi-\log(1-\mathcal{D}_\phi)$，并声称这样能提供非饱和（Non-saturating）的梯度：</p>
<p>$$<br>\textbf{Generator Loss: } \mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1-\log\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))+\log(1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z})))\right]<br>$$</p>
<p>作者认为在生成器损失函数中加入$\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1$能够在一定程度防止模式崩溃。</p>
<p>除此之外，作者发现将真实样本（原文是The Samples）作为生成的样本输入到判别器中能够提升性能。作者给出的解释是根据Jensen不等式：$\log p_\theta(\mathbf{x})=\log\int p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})\text{d}\mathbf{z}\geq \mathbb{E}_{p(\mathbf{z})}[\log p_\theta(\mathbf{x}|\mathbf{z})]$，</p>
<p>[TODO]</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>[TODO]</p>
<p><img src="http://qfxiao.me/img/image-20191102233101258.png" alt=""></p>
<p><img src="http://qfxiao.me/img/image-20191102233151987.png" alt=""></p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>[TODO]</p>
<h2 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h2><p>本文使用了几种不同的评测生成模型的方法：</p>
<ul>
<li>*<em>Inception Score: *</em></li>
<li>*<em>Multi-scale Structural Similarity (MS-SSIM): *</em></li>
<li>*<em>Independent Wasserstein Critic: *</em></li>
</ul>
<h2 id="Results-on-ColorMNIST"><a href="#Results-on-ColorMNIST" class="headerlink" title="Results on ColorMNIST"></a>Results on ColorMNIST</h2><p><img src="http://qfxiao.me/img/image-20191102233220700.png" alt=""></p>
<p><img src="http://qfxiao.me/img/image-20191102233244847.png" alt=""></p>
<h2 id="Results-on-CelebA"><a href="#Results-on-CelebA" class="headerlink" title="Results on CelebA"></a>Results on CelebA</h2><p><img src="http://qfxiao.me/img/image-20191102233303345.png" alt=""></p>
<h2 id="Results-on-CIFAR-10"><a href="#Results-on-CIFAR-10" class="headerlink" title="Results on CIFAR-10"></a>Results on CIFAR-10</h2><p><img src="http://qfxiao.me/img/image-20191102233338213.png" alt=""></p>
<p><img src="http://qfxiao.me/img/image-20191102233350494.png" alt=""></p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/"><img class="thumbnail" src="http://qfxiao.me/img/image-20191029114054583.png" alt="Anomaly Detection in Streams with Extreme Value Theory"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-10-29T03:36:22.000Z" title="2019-10-29T03:36:22.000Z">2019-10-29</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span><span class="level-item">18 分钟 读完 (大约 2650 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/">Anomaly Detection in Streams with Extreme Value Theory</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文基于<strong>Extreme Value Theory</strong>提出了一种不需要手动设置阈值也不需要对数据分布作任何假设的时间序列异常检测方法。除此之外，本方法可以用在通用的自动阈值选择的场合中。</p>
<p><a href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-in-streams-with-extreme-value-theory">原文</a></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>在很多情况下我们需要进行阈值的选择。阈值的选择可以通过实验的方法或者对数据分布进行假设的方法来得到，不过这样做通常不准确。借助<strong>Extreme Value Theory</strong>我们可以在不需要对原始数据的分布作很强的假设的情况下，推断我们想要的极端事件的分布（在异常检测中就是异常值）。</p>
<p>下面给出一些数学符号，$X$为随机变量，$F$为累积分布函数，即$F(x)=\mathbb{P}(X\leq x)$。记$F$的“末尾”分布$\bar{F}(x)=1-F(x)=\mathbb{P}(X&gt;x)$。对于一个随机变量$X$和给定的概率$q$，记$z_q$为在$1-q$水平的分位数，即$z_q$为满足$\mathbb{P}(X\leq z_q)\geq 1-q$最小的值。</p>
<h2 id="Extreme-Value-Distributions"><a href="#Extreme-Value-Distributions" class="headerlink" title="Extreme Value Distributions"></a>Extreme Value Distributions</h2><p><strong>Extreme Value Theory</strong>主要是为了找出极端事件发生的规律，有学者证明，在很弱的条件下，所有极端事件都服从一个特定的分布，而不管原始分布如何。具体形式如下：</p>
<p>$$<br>G_\gamma:x\mapsto \exp(-(1+\gamma x)^{-\frac{1}{\gamma}}), \space\space\space\space\space\gamma\in\mathbb{R}, \space\space\space\space\space 1+\gamma x&gt;0<br>$$</p>
<p>其中$\gamma$称为<strong>Extreme Value Index</strong>，由原始分布决定。</p>
<p>更严谨的说法是Fisher-Tippett-Gnedenko定理（极值理论第一定理）：</p>
<blockquote>
<p>*<em>THEOREM: *</em>(Fisher-Tippett-Gnedenko). 令$X_1,X_2,\cdots,X_n,\cdots$为独立同分布的随机变量序列，$M_n=\max {X_1,\cdots,X_n}$。如果实数对序列$(a_n,b_n)$存在且满足$a_n&gt;0$和$\lim\limits_{n\rightarrow \infty}P\left(\frac{M_n-b_n}{a_n}\leq x\right)=F(x)$，其中$F$为非退化分布函数，那么$F$属于Gumbel、Fréchet或Weibull分布族（或总称Generalized Extreme Value Distribution）中的一种。</p>
</blockquote>
<p>这是一个反直觉的结论，但是想到当事件发生变得极端时，即$\mathbb{P}(X&gt;x)\rightarrow 0$，$\bar{F}(x)=\mathbb{P}(X&gt;x)$分布的形状其实并没有很多种选择。Table 1展示了几种不同分布对应的$\gamma$：</p>
<p><img src="http://qfxiao.me/img/image-20191029162343219.png" alt=""></p>
<p>Figure 1展示了几种不同$\gamma$情况下的“末尾”分布：</p>
<p><img src="http://qfxiao.me/img/image-20191029113903386.png" alt=""></p>
<h2 id="Power-of-EVT"><a href="#Power-of-EVT" class="headerlink" title="Power of EVT"></a>Power of EVT</h2><p>根据<strong>Extreme Value Theory</strong>，我们可以在原始分布未知的情况下计算极端事件的概率。但是$\bar{G}_\gamma$分布中参数$\gamma$是未知的，我们需要一种高效的方法来进行估计。<strong>The Peaks-Over-Threshold</strong> (POT) 方法是本文介绍的一种方法。</p>
<p><img src="http://qfxiao.me/img/image-20191029162456947.png" alt=""></p>
<h2 id="Peaks-Over-Threshold-Approach"><a href="#Peaks-Over-Threshold-Approach" class="headerlink" title="Peaks-Over-Threshold Approach"></a>Peaks-Over-Threshold Approach</h2><p>POT方法依赖于Pickands-Balkema-De Haan定理（极值理论第二定理），维基百科版：</p>
<blockquote>
<p>考虑一个未知分布$F$和随机变量$X$，我们的目标是估计$X$在超过确定阈值$u$下的条件分布$F_u$，定义为：<br>$$<br>F_u(y)=P(X-u\leq y|X&gt;u)=\frac{F(u+y)-F(u)}{1-F(u)}<br>$$<br>其中$0\leq y\leq x_F-u$，$x_F$为$F$的右端点。$F_u$描述了超过特征阈值$u$的分布，称为<strong>Conditional Excess Distribution Function</strong>。</p>
<p>*<em>STATEMENT: *</em>(Pickands-Balkema-De Haan). 设$(X_1,X_2,\cdots)$为独立同分布随机变量序列，$F_u$为相应的Conditional Excess Distribution Function。对于一大类的$F$和很大的$u$，$F_u$能够很好的被Generalized Pareto Distribution所拟合：<br>$$<br>F_u(y)\rightarrow G_{k,\sigma}(y),\space\space \text{as } u\rightarrow \infty<br>$$<br>其中：<br>$$<br>G_{k,\sigma}(y)=<br>\begin{cases}<br>1-(1+ky/\sigma)^{-1/k}, &amp;\text{if }k\neq 0\<br>1-e^{-y/\sigma}, &amp;\text{if }k=0<br>\end{cases}<br>$$<br>当$k\geq 0$时$\sigma&gt;0, y\geq 0$，$k&lt;0$时$0\leq y\leq -\sigma/k$。</p>
</blockquote>
<p>论文中给出的定理如下：</p>
<blockquote>
<p>*<em>THEOREM: *</em>(Pickands-Balkema-De Haan). 累积概率密度函数$F\in\mathcal{D}<em>\gamma$当且仅当函数$\sigma$存在时，对所有$x\in\mathbb{R}$在$1+\gamma x&gt;0$的条件下有：<br>$$<br>\frac{\bar{F}(t+\sigma(t)x)}{\bar{F}(t)}\mathop{\rightarrow}\limits</em>{t\rightarrow\tau}(1+\gamma x)^{-\frac{1}{\gamma}}<br>$$</p>
</blockquote>
<p>上式可以写成如下形式：<br>$$<br>\bar{F}<em>t(x)=\mathbb{P}(X-t&gt;x|X&gt;t)\mathop{\sim}\limits</em>{t\rightarrow\tau}\left(1+\frac{\gamma x}{\sigma(t)}\right)^{-\frac{1}{\gamma}}<br>$$<br>该式表明$X$超过阈值$t$的概率（写为$X-t$）服从<strong>Generalized Pareto Distribution</strong> (GPD)，参数为$\gamma$和$\sigma$。POT主要是拟合GPD而不是EVT分布。</p>
<p>如果我们要估计参数$\hat{\gamma}$和$\hat{\sigma}$，分位数可以通过下式计算得到：<br>$$<br>z_q\simeq t+\frac{\hat{\sigma}}{\hat{\gamma}}\left(\left(\frac{qn}{N_t}\right)^{-\hat{\gamma}}-1\right)<br>$$</p>
<p>其中$t$是一个“很高”的阈值，$q$是给定的概率值，$n$是所有观测样本的数量，$N_t$是peaks的数量，即$X_i&gt;t$的数量。为了进行高效的参数估计，文中使用了极大似然估计。</p>
<h2 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h2><p>设$X_1,\cdots,X_n$为独立同分布的随机变量，概率密度函数记为$f_\theta$，$\theta$为分布中的参数，那么似然函数可以写为：</p>
<p>$$<br>\mathcal{L}(X_1,\cdots,X_n;\theta)=\prod\limits_{i=1}^n f_\theta(X_i)<br>$$</p>
<p>在极大似然估计中，我们需要找到合适的参数使得似然函数最大化。在我们的问题中，似然函数如下：<br>$$<br>\log\mathcal{L}(\gamma,\sigma)=-N_t\log\sigma-\left(1+\frac{1}{\gamma}\right)\sum\limits_{i=1}^{N_t}\log\left(1+\frac{\gamma}{\sigma}Y_i\right)<br>$$<br>其中$Y_i&gt;0$表示$X_i$超过阈值$t$的部分。</p>
<p>文中使用了<strong>Grimshaw’s Trick</strong>来将含两个参数的优化问题转换为只含一个参数的优化问题。记$\ell(\gamma,\sigma)=\log\mathcal{L}(\gamma,\sigma)$，对于所有极值来说有$\nabla \ell(\gamma, \sigma)=0$。Grimshaw’s Trick表明对于满足$\nabla \ell(\gamma, \sigma)=0$的一对$(\gamma^<em>,\sigma^</em>)$，$x^<em>=\frac{\gamma^</em>}{\sigma^<em>}$为等式$u(X)v(X)=1$的解，其中：<br>$$<br>\begin{align}<br>u(x)&amp;=\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\frac{1}{1+xY_i}\<br>v(x)&amp;=1+\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\log(1+xY_i)<br>\end{align}<br>$$<br>在找到满足该等式的解$x^</em>$后，我们可以得到$\gamma^<em>=v(x^</em>)-1$和$\sigma^<em>=\gamma^</em>/x^*$，于是问题就变成了如何寻找方程的所有根。</p>
<p>因为$\log$的存在，所以有$1+xY_i&gt;0$。而$Y_i$是正数，所以$x^*$的范围一定在$\left(-\frac{1}{Y^M},+\infty\right)$，其中$Y^M=\max Y_i$。</p>
<p>Grimshaw（作者参考的一篇<a href="https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485040">论文</a>）还给出了一个上界：<br>$$<br>x^*_{\text{max}}=2\frac{\bar{Y}-Y^m}{(Y^m)^2}<br>$$<br>其中$Y^m=\min Y_i$，$\bar{Y}$为$Y_i$的均值。详细的优化方法会在下文讨论。</p>
<p>背景部分到此结束，接下来的部分就是作者提出的新方法。</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>Extreme Value Theory给出了在对原始分布未知的情况下估计使得$\mathbb{P}(X&gt;z_q)&lt;q$的$z_q$的方法。</p>
<p>本文据此提出了时间序列流的异常检测方法。首先根据已知的观测值$X_1,\cdots,X_n$得到阈值$z_q$，然后根据数据的特性运用两种不同方法来更新$z_q$。对于平稳时间序列，使用SPOT；对于非平稳时间序列，使用DSPOT。</p>
<h2 id="Initialization-Step"><a href="#Initialization-Step" class="headerlink" title="Initialization Step"></a>Initialization Step</h2><p>在进行异常检测之前，需要根据已有的观测数据进行$z_q$的估计。给定$n$个观测值$X_1,\cdots,X_n$和一个固定的概率值$q$，我们的目标是估计阈值$z_q$使得$\mathbb{P}(X&gt;z_q)&lt;q$。其主要流程是首先设定一个较大的阈值$t$，然后通过拟合GPD分布来计算$z_q$。过程如下图所示：</p>
<p><img src="http://qfxiao.me/img/image-20191029114030518.png" alt=""></p>
<p>算法流程如下所示：</p>
<p><img src="http://qfxiao.me/img/image-20191029114001461.png" alt=""></p>
<p>$Y_t$代表大于$t$的观测值的集合，GPD分布的拟合使用了前文提到的Grimshaw’s Trick。</p>
<h2 id="Finding-Anomalies-in-a-Stream"><a href="#Finding-Anomalies-in-a-Stream" class="headerlink" title="Finding Anomalies in a Stream"></a>Finding Anomalies in a Stream</h2><p>通过Initialization Step使用POT算法得到的$z_q$，我们定义其为”Normality Bound”，用于后面的检测。在后面的步骤中，我们会根据新得到的观测值来更新$z_q$。</p>
<h3 id="Stationary-Case"><a href="#Stationary-Case" class="headerlink" title="Stationary Case"></a>Stationary Case</h3><p>我们首先来讨论时间序列没有时间依赖性的情况（$X_1,\cdots,X_n$之间独立同分布）。通过POT算法对所有观测值得到$z_q$之后，Streaming POT (SPOT) 算法会检查$X_n$之后的值（数据流场景，$X_1,\cdots,X_n$是历史数据，还会有新的数据进来），如果大于$z_q$，则将$X_i$加入异常点集合中；如果大于$t$但小于$z_q$，则将$X_i$加入观测值集合中，更新$z_q$；其他情况我们$X_i$是正常情况。算法流程图如下：</p>
<p><img src="http://qfxiao.me/img/image-20191029114020571.png" alt=""></p>
<h3 id="Drifting-Case"><a href="#Drifting-Case" class="headerlink" title="Drifting Case"></a>Drifting Case</h3><p>SPOT算法只适用于平稳分布的情况，但在现实生活中这样的假设过强了。于是作者提出了能处理时间依赖性的Streaming POT with Drift (DSPOT) 算法。</p>
<p><img src="http://qfxiao.me/img/image-20191029114054583.png" alt=""></p>
<p>在DSPOT中，我们不使用$X_i$的绝对值，而是用相对值$X^\prime_i=X_i-M_i$，其中$M_i$是$i$时刻的局部特征，如Figure 4所示。最简单的实现是使用局部均值，即$M_i=(1/d)\cdot\sum\limits_{k=1}^d X_{i-k}^<em>$，$X_{i-1}^</em>,\cdots,X_{i-d}^*$是长度为$d$的窗口。我们假设$X^\prime_i$服从平稳分布的假设。</p>
<p>算法流程图如下所示：</p>
<p><img src="http://qfxiao.me/img/image-20191029114113201.png" alt=""></p>
<h2 id="Numerical-Optimization"><a href="#Numerical-Optimization" class="headerlink" title="Numerical Optimization"></a>Numerical Optimization</h2><p>现在剩下的问题就是优化了，前文已经提到对GPD的拟合已经被优化成一个参数的优化问题，下面将会详细讨论优化算法。</p>
<h3 id="Reduction-of-the-Optimal-Parameters-Search"><a href="#Reduction-of-the-Optimal-Parameters-Search" class="headerlink" title="Reduction of the Optimal Parameters Search"></a>Reduction of the Optimal Parameters Search</h3><p>前文已经得到了一个初步的$x^<em>$的Bound，即$x^</em>&gt;-\frac{1}{Y^M}$和$x^*\leq 2\frac{\bar{Y}-Y^m}{(Y^m)^2}$，下面将给出一个更严格的Bound。</p>
<blockquote>
<p><em><em>PROPOSITION: *</em>如果$x^</em>$是$u(x)v(x)=1$的解，那么：<br>$$<br>x^<em>\leq 0 \text{ or } x^</em>\geq 2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m}<br>$$</p>
</blockquote>
<p>证明见论文原文。</p>
<p>这样$x^<em>$的范围就进一步缩小了，于是有$u(x)v(X)=1$的解$x^</em>$在以下范围之内：<br>$$<br>\left(-\frac{1}{Y^M},0\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]<br>$$</p>
<h3 id="How-Can-We-Maximize-the-Likelihood-Function"><a href="#How-Can-We-Maximize-the-Likelihood-Function" class="headerlink" title="How Can We Maximize the Likelihood Function?"></a>How Can We Maximize the Likelihood Function?</h3><p>接下来是优化的具体实现问题。文中首先设定了一个很小的值$\epsilon&gt;0\space(\sim 10^{-8})$，然后在下面的范围内寻找函数$w:x\mapsto u(x)v(x)-1$的根：<br>$$<br>\left[-\frac{1}{Y^M}+\epsilon,-\epsilon\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]<br>$$<br>作者没有使用现有的寻找函数根的算法，而是转换为如下优化问题：<br>$$<br>\min\limits_{x_1,\cdots,x_k\in I}\sum\limits_{i=1}^k w(x_k)^2<br>$$<br>其中$I$就是$x^*$的Bound。该问题是一个很典型的优化问题，可以被很多成熟的算法所解决。</p>
<h3 id="Initial-Threshold"><a href="#Initial-Threshold" class="headerlink" title="Initial Threshold"></a>Initial Threshold</h3><p>在算法的Initialization Step，需要事先设定一个阈值$t$，如果设定的太大，那么$Y_t$的数量就会很少。作者给出的建议是保证$t&lt;z_q$，即$t$对应的概率值应该小于$1-q$。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>在实验部分，作者在合成数据和真实数据上试验了SPOT算法和DSPOT算法的有效性。</p>
<h2 id="D-SPOT-Reliability"><a href="#D-SPOT-Reliability" class="headerlink" title="(D)SPOT Reliability"></a>(D)SPOT Reliability</h2><p>作者首先在合成数据上验证SPOT的有效性。具体做法是使用高斯分布生成数据（高斯分布的分位数能够直接计算），然后将SPOT得出的$z_q$和理论值进行对比。误差定义如下：<br>$$<br>\text{error rate}=\left|\frac{z^{\text{SPOT}}-z^{\text{th}}}{z^{\text{th}}}\right|<br>$$<br>下图是采用不同数量观测值的结果：</p>
<p><img src="http://qfxiao.me/img/image-20191029114230661.png" alt=""></p>
<h2 id="Finding-Anomalies-with-SPOT"><a href="#Finding-Anomalies-with-SPOT" class="headerlink" title="Finding Anomalies with SPOT"></a>Finding Anomalies with SPOT</h2><p>在这一节作者在真实数据集上进行了实验以验证SPOT算法的有效性，结果如下图：</p>
<p><img src="http://qfxiao.me/img/image-20191029114255259.png" alt=""></p>
<p>在文中作者说算法的True Positive达到了$86%$，False Positive小于$4%$。</p>
<p><img src="http://qfxiao.me/img/image-20191029114316348.png" alt=""></p>
<h2 id="Finding-Anomalies-with-DSPOT"><a href="#Finding-Anomalies-with-DSPOT" class="headerlink" title="Finding Anomalies with DSPOT"></a>Finding Anomalies with DSPOT</h2><p>在这一节作者使用DSPOT在真实数据集上进行了实验。窗口大小$d=450$，预设的风险概率值$q=10^{-3}$。结果如下图所示：</p>
<p><img src="http://qfxiao.me/img/image-20191029114340936.png" alt=""></p>
<p>在图中可以看出在$8000$ Minutes之后上界显著提高，作者分析了原因，认为是因为超过阈值$t$的点$Y_t$的存储是全局的，在前$8000$ Minutes算法存储了很多较高的$Y_t$值，而在$8000$ Minutes之后，真实数据的趋势开始下降，但算法仍是根据全局的$Y_t$来进行$z_q$的计算（这一段没有特别明白）。作者给出的修正方法是只保存固定数量的Peaks。</p>
<p>下图是作者在股票数据上得到的实验结果：</p>
<p><img src="http://qfxiao.me/img/image-20191029114356004.png" alt=""></p>
<h2 id="Performances"><a href="#Performances" class="headerlink" title="Performances"></a>Performances</h2><p>作者还验证了算法的时间效率。</p>
<p><img src="http://qfxiao.me/img/image-20191029114419226.png" alt=""></p>
<p>表中T代表的是每个Iteration的时间，M代表的是Peaks的比例，”bi-“前缀代表的是同时计算上界和下界。</p>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2019/10/22/An-Introduction-to-Variational-Autoencoders/"><img class="thumbnail" src="http://qfxiao.me/img/autoencoder23849248011.png" alt="An Introduction to Variational Autoencoders"></a></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" dateTime="2019-10-22T07:14:26.000Z" title="2019-10-22T07:14:26.000Z">2019-10-22</time><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Tutorial/">Tutorial</a></span><span class="level-item">22 分钟 读完 (大约 3277 个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/22/An-Introduction-to-Variational-Autoencoders/">An Introduction to Variational Autoencoders</a></h1><div class="content"><h1 id="Deep-Generative-Models"><a href="#Deep-Generative-Models" class="headerlink" title="Deep Generative Models"></a>Deep Generative Models</h1><p>生成模型是指一系列用于随机生成可观测数据的模型。假设在一个高维空间$\mathcal{X}$中，存在一个随机向量$\mathbf{X}$服从一个未知的分布$p_r(x),x\in \mathcal{X}$。生成模型就是根据一些可观测的样本$x^{(1)},x^{(2)},\cdots,x^{(N)}$来学习一个参数化的模型$p_\theta(x)$来近似未知分布$p_r(x)$。</p>
<p>生成模型主要用于密度估计和样本生成。</p>
<hr>
<p>密度估计即给定一组数据$\mathcal{D}={x^{(i)}},1\leq i\leq N$，假设他们都是从相同的概率密度函数$p_r(x)$独立产生的。密度估计就是根据数据集$\mathcal{D}$来估计其概率密度函数$p_r(x)$。</p>
<p>如果将生成模型用于监督学习，那么就是输出标签的条件概率分布$p(y|x)$，根据贝叶斯公式：</p>
<p>$$p(y|x)=\frac{p(x,y)}{\sum_y p(x,y)}$$</p>
<p>问题就变为了联合概率$p(x,y)$的密度估计问题。</p>
<hr>
<p>样本生成即根据给定的概率分布$p_\theta(x)$生成一些服从这个分布的样本，即采样。在含隐变量的生成模型中，生成$x$的过程一般包含两步：</p>
<ol>
<li>根据隐变量的分布$p_\theta(z)$采样得到$z$；</li>
<li>根据条件分布$p_\theta(x|z;\theta)$进行采样得到$x$。</li>
</ol>
<p>所以在生成模型中的重点是估计条件分布$p(x|z;\theta)$。</p>
<h1 id="Parameter-Estimation-for-Hidden-Variable-with-EM-Algorithm"><a href="#Parameter-Estimation-for-Hidden-Variable-with-EM-Algorithm" class="headerlink" title="Parameter Estimation for Hidden Variable with EM Algorithm"></a>Parameter Estimation for Hidden Variable with EM Algorithm</h1><p>如果图模型中存在隐变量，就需要使用EM算法进行参数估计。</p>
<p>在一个包含隐变量的图模型中，令$\mathbf{X}$为可观测变量集合，$\mathbf{Z}$为隐变量集合，则一个样本$x$的边际似然函数为：</p>
<p>$$p(x;\theta)=\sum_z p(x,z;\theta)$$</p>
<p>给定包含$N$个训练样本的训练集$\mathcal{D}={x^{(n)}},1\leq i\leq N$，则训练集的对数边际似然为：</p>
<p>$$\begin{align}\mathcal{L}(\mathcal{D};\theta)&amp;=\frac{1}{N}\sum_{n=1}^N \log p(x^{(n)};\theta)\&amp;=\frac{1}{N}\sum_{n=1}^N \log \sum_z p(x^{(n)},z;\theta)\end{align}$$</p>
<hr>
<p>这时，只要最大化整个训练集的对数边际似然$\mathcal{L}(\mathcal{D};\theta)$，即可估计出最优的参数$\theta^*$。不过在计算梯度的时候，需要在对数函数内部进行求和或积分计算。为了更好的计算$\log p(x;\theta)$，我们引入一个额外的变分函数$q(z)$，$q(z)$为定义在隐变量$z$上的分布。样本$x$的对数边际似然函数为：</p>
<p>$$\begin{align}\log p(x;\theta)&amp;=\log \sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\&amp;\geq\sum_z q(z)\log \frac{p(x,z;\theta)}{q(z)}\&amp;\triangleq ELBO(q,x;\theta)\end{align}$$</p>
<p>其中$ELBO(q,x;\theta)$为对数边际似然函数$\log p(x;\theta)$的下界，称为证据下界。公式中使用了Jensen不等式(即对于凹函数$g$，有$g(\mathbb{E}[x])\geq\mathbb{E}[g(X)]$)。在这里，$\frac{p(x,z;\theta)}{q(z)}$可视为$q(z)$的函数，记为$f(q(z))$，那么$f(q(z))$的期望即$\mathbb{E}[f(q(z))]=\sum_z q(z)f(q(z))=\sum_z q(z)\frac{p(x,z;\theta)}{q(z)}$。而根据Jensen不等式，有$g(\mathbb{E}[f(q(z))])\geq\mathbb{E}[g(f(q(z)))]\Leftrightarrow g(\sum_z q(z)\frac{p(x,z;\theta)}{q(z)})\geq \sum_z q(z)g(\frac{p(x,z;\theta)}{q(z)})$，在这里$g$就是对数函数。</p>
<hr>
<p>根据Jensen不等式取等的条件：$\frac{p(x,z;\theta)}{q(z)}=c$，$c$为常数，有：</p>
<p>$$\begin{align}\sum_z p(x,z;\theta)&amp;=c\sum_z q(z)\\Leftrightarrow\sum_z p(x,z;\theta)&amp;=c\cdot1\end{align}$$</p>
<p>因此：</p>
<p>$$\begin{align}q(z)&amp;=\frac{p(x,z;\theta)}{\sum_z p(x,z;\theta)}\&amp;=\frac{p(x,z;\theta)}{p(x;\theta)}\&amp;=p(z|x;\theta)\end{align}$$</p>
<p>所以，当且仅当$q(z)=p(z|x;\theta)$时，$\log p(x;\theta)=ELBO(q,x;\theta)$。</p>
<hr>
<p>于是最大化对数边际似然函数$\log p(x;\theta)$的过程可以分解为两个步骤：</p>
<ol>
<li>先找到近似分布$q(z)$使得$\log p(x;\theta)=ELBO(q,x;\theta)$；</li>
<li>再寻找参数$\theta$最大化$ELBO(q,x;\theta)$。</li>
</ol>
<p>这就是期望最大化(Expectation-Maximum,EM)算法。</p>
<hr>
<p>EM算法通过迭代的方法，不断重复直到收敛到某个局部最优解。在第$t$步更新时，E步和M步分别为：</p>
<ol>
<li><p>E步：固定参数$\theta_t$，找到一个分布使$ELBO(q,x;\theta_t)$最大，即等于$\log p(x;\theta_t)$：$q_{t+1}(z)=\text{arg}_q \max ELBO(q,x;\theta_t)$；</p>
</li>
<li><p>M步：固定$q_{t+1}(z)$，找到一组参数使得证据下界最大，即：$\theta_{t+1}=\text{arg}<em>\theta\max ELBO(q</em>{t+1},x;\theta)$。</p>
</li>
</ol>
<hr>
<p>对数边际似然也可以通过信息论的视角来进行分解：</p>
<p>$$\begin{align}\log p(x;\theta)&amp;=\sum_z q(z)\log p(x;\theta)\&amp;=\sum_z q(z)(\log p(x,z;\theta)-\log p(z|x;\theta))\&amp;=\sum_z q(z)\log\frac{p(x,z;\theta)}{q(z)}-\sum_z q(z)\log\frac{p(z|x;\theta)}{q(z)}\&amp;=ELBO(q,x;\theta)+D_{KL}(q(z)\parallel p(z|x;\theta))\end{align}$$</p>
<p>其中$D_{KL}(q(z)\parallel p(z|x;\theta))$</p>
<h1 id="Generative-Model-with-Hidden-Variable"><a href="#Generative-Model-with-Hidden-Variable" class="headerlink" title="Generative Model with Hidden Variable"></a>Generative Model with Hidden Variable</h1><p>假设一个生成模型包含不可观测的隐变量，其中可观测变量$x$为一个高维空间中的随机向量，而不可观测的隐变量$z$为一个相对低维空间中的随机向量。</p>
<p>这个生成模型的联合概率密度函数可以表达为：</p>
<p>$$p(x,z;\theta)=p(x|z;\theta)p(z;\theta)$$</p>
<p>其中$p(z;\theta)$为隐变量$z$的先验概率分布；$p(x|z;\theta)$为已知$z$条件下$x$的概率分布。通常情况下，我们可以假设$p(z;\theta)$和$p(x|z;\theta)$服从某种带参的分布族，其形式已知，而参数可以通过最大似然来进行估计。</p>
<p>给定一个样本$x$，其对数边际似然$\log p(x;\theta)$可以分解为：</p>
<p>$$\log p(x;\theta)=ELBO(q,x;\theta,\phi)+D_{KL}(q(z;\phi)\parallel p(z|x;\theta))$$</p>
<p>其中$q(z;\phi)$为额外引入的变分密度函数，$ELBO(q,x;\theta,\phi)$为证据下界：</p>
<p>$$ELBO(q,x;\theta,\phi)=\mathbb{E}_{z\sim q(z;\phi)}[\log{\frac{p(x,z;\theta)}{q(z;\phi)}}]$$</p>
<p>最大化$\log p(x;\theta)$可以用EM算法来求解：</p>
<ul>
<li><strong>E-step:</strong> 寻找一个密度函数$q(z;\phi)$使其等于或接近于后验密度函数$p(z|x;\theta)$;</li>
<li><strong>M-step:</strong> 保持$q(z;\phi)$固定，寻找$\theta$来最大化$ELBO(q,x;\theta,\phi)$。</li>
</ul>
<p>在EM算法的每次迭代中，理论上最优的$q(z;\phi)$为隐变量的后验概率密度函数$p(z|x;\theta)$：</p>
<p>$$p(z|x;\theta)=\frac{p(x|z;\theta)p(z;\theta)}{\int_z p(x|z;\theta)p(z;\theta)\text{d}z}$$</p>
<p>后验密度函数$p(z|x;\theta)$的计算是一个统计推断的问题，在一般情况下$p(x|z;\theta)$也比较难以计算。</p>
<h1 id="Variational-Autoencoder"><a href="#Variational-Autoencoder" class="headerlink" title="Variational Autoencoder"></a>Variational Autoencoder</h1><p>变分自编码器(Variational Autoencoder, VAE)的主要思想是利用神经网络来分别建模两个复杂的条件概率密度函数：</p>
<ol>
<li><p>用神经网络来产生变分分布$q(z;\phi)$，称为推断网络。推断网络的输入为$x$，输出为变分分布$q(z|x;\phi)$；</p>
</li>
<li><p>用神经网络来产生概率分布$p(x|z;\theta)$，称为生成网络。生成网络的输入为$z$，输出为概率分布$p(x|z;\theta)$。</p>
<p><img src="http://qfxiao.me/img/autoencoder23849248011.png" alt=""></p>
</li>
</ol>
<p>VAE的图模型如下图所示：</p>
<p><img src="http://qfxiao.me/img/1565532060281.png" alt=""></p>
<h2 id="Variational-Network"><a href="#Variational-Network" class="headerlink" title="Variational Network"></a>Variational Network</h2><p>假设$q(z|x;\phi)$是服从对角化协方差的高斯分布：</p>
<p>$$q(z|x;\phi)=\mathcal{N}(z;\mu_I,\sigma^2_I I)$$</p>
<p>其中$\mu_I$和$\sigma_I^2$是高斯分布的均值和方差，可以通过推断网络$f_I(x;\phi)$来预测：</p>
<p>$$<br>\left[\begin{matrix}\mu_I\\sigma_I\end{matrix}\right]=f_I(x;\phi)<br>$$<br>推断网络$f_I(x;\phi)$可以是一般的全连接网络或卷积网络，比如一个两层的神经网络：</p>
<p>$$\begin{align}h&amp;=\sigma(W^{(1)}x+b^{(1)})\\mu_I&amp;=W^{(2)}h+b^{(2)}\\sigma_I&amp;=\text{softplus}(W^{(3)}h+b^{(3)})\end{align}$$</p>
<p>其中所有网络参数${W^{(1)},W^{(2)},W^{(3)},b^{(1)},b^{(2)},b^{(3)}}$即对应了变分参数$\phi$。</p>
<hr>
<p>推断网络的目标是使得$q(z|x;\phi)$来尽可能接近真实的后验$p(z|x;\theta)$，需要找到变分参数$\phi^*$来最小化两个分布的KL散度：</p>
<p>$$\phi^*=\text{arg}<em>\phi\min{D</em>{KL}(q(z|x;\phi)\parallel p(z|x;\theta))}$$</p>
<p>由于$p(z|x;\theta)$未知，故KL散度无法直接计算，不过由于$D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))=\log p(x;\theta)-ELBO(q,x;\theta,\phi)$，所以可以直接最大化证据下界，有：</p>
<p>$$\phi^*=\text{arg}_\phi\max{ELBO(q,x;\theta,\phi)}$$</p>
<h2 id="Generative-Network"><a href="#Generative-Network" class="headerlink" title="Generative Network"></a>Generative Network</h2><p>生成模型的联合分布可以分解为两部分：隐变量$z$的先验分布$p(z;\theta)$和条件概率分布$p(x|z;\theta)$。为简单起见，一般假设隐变量$z$的先验分布为标准正态分布$\mathcal{N}(z|0,I)$，隐变量每一维之间都是独立的。条件概率分布$p(x|z;\theta)$可以通过生成网络来建模，我们同样用参数化的分布族来表示条件概率分布$p(x|z;\theta)$，这些分布族的函数可以用生成网络计算得到。根据变量$x$的类型不同，可以假设$p(x|z;\theta)$服从不同的分布族。如果$x\in{0,1}^d$是$d$维的二值向量，可以假设$\log p(x|z;\theta)$服从多变量的伯努利分布，即：</p>
<p>$$\begin{align}p(x|z;\theta)&amp;=\prod\limits_{i=1}^d p(x_i|z;\theta)\&amp;=\prod\limits_{i=1}^d \gamma_i^{x_i}(1-\gamma_i)^{(1-x_i)}\end{align}$$</p>
<p>如果$x\in\mathbb{R}^d$是$d$维的连续向量，可以假设$p(x|z;\theta)$服从对角化协方差的高斯分布，即：</p>
<p>$$p(x|z;\theta)=\mathcal{N}(x;\mu_G,\sigma_G^2 I)$$</p>
<hr>
<p>生成网络的目标是找到一组$\theta^*$最大化证据下界$ELBO(q,x;\theta,\phi)$：</p>
<p>$$\theta^*=\text{arg}_\theta\max ELBO(q,x;\theta,\phi)$$</p>
<h2 id="Model-Combination"><a href="#Model-Combination" class="headerlink" title="Model Combination"></a>Model Combination</h2><p>推断网络和生成网络的目标都是最大化证据下界因此总的目标函数为：</p>
<p>$$\begin{align}\max_{\theta,\phi}ELBO(q,x;\theta,\phi)&amp;=\max_{\theta,\phi}\mathbb{E}<em>{z\sim q(z;\phi)}[\log\frac{p(x|z;\theta)p(z;\theta)}{q(z;\theta)}]\&amp;=\max</em>{\theta,\phi}\mathbb{E}<em>{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]-D</em>{KL}(q(z|x;\phi)\parallel p(z;\theta))\end{align}$$</p>
<p>其中先验分布$p(z;\theta)=\mathcal{N}(z|0,I)$。</p>
<p>公式中$\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]$一般通过采样的方式进行计算，最后取平均值。</p>
<h2 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h2><p>给定数据集$\mathcal{D}$，包含$N$个从未知数据分布中抽取的独立同分布样本$x^{(1)},x^{(2)},\cdots,x^{(N)}$。变分自编码器的目标函数为：</p>
<p>$$\mathcal{J}(\phi,\theta|\mathcal{D})=\sum\limits_{n=1}^N(\frac{1}{M}\sum\limits_{m=1}^M\log p(x^{(n)}|z^{(n,m)};\theta)-D_{KL}(q(z|x^{(n)};\phi)\parallel\mathcal{N}(z;0,I)))$$</p>
<p>如果采用随机梯度下降法，每次从数据集中采一个样本$x$，然后根据$q(z|x;\phi)$采一个隐变量$z$，则目标函数变为：</p>
<p>$$\mathcal{J}(\phi,\theta|x)=\log p(x|z;\theta)-D_{KL}(q(z|x;\phi)\parallel\mathcal{N}(z;0,I))$$</p>
<p>假设$q(z|x;\phi)$是正态分布，KL散度可直接算出：</p>
<p>$$D_{KL}(\mathcal{N}(\mu_1,\Sigma_1)\parallel\mathcal(\mu_2,\Sigma_2))\=\frac{1}{2}(\text{tr}(\sigma_I^2 I)+\mu_I^T\mu_I-d-\log(|\sigma_I^2 I|))$$</p>
<hr>
<p>再参数化是将一个参数为$u$的函数$f(u)$，通过一个函数$u=g(v)$，转换为参数为$v$的函数$\hat{f}(v)=f(g(v))$。在变分自编码器中，一个问题是如何求随机变量$z$关于$\phi$的导数。但由于是采样的方式，无法直接刻画$z$和$\phi$之间的函数关系，因此也无法计算导数。</p>
<p>如果$z\sim q(z|x;\phi)$的随机性独立于参数$\phi$，我们可以通过再参数化的方法来计算导数。假设$q(z|x;\phi)$为正态分布$\mathcal{N}(\mu_I,\sigma^2_I I)$，其中$\mu_I$和$\sigma_I$是推断网络$f_I(x;\phi)$的输出。我们可以通过下面的方式采样$z$：</p>
<p>$$z=\mu_I+\sigma_I\odot \varepsilon$$</p>
<p>其中$\varepsilon\sim\mathcal{N}(0,I)$。这样$z$和$\mu_I,\sigma_I$的关系从采样关系变为函数关系。</p>
<hr>
<p>如果进一步假设$p(x|z;\theta)$服从高斯分布$\mathcal{N}(x|\mu_G,I)$，其中$\mu_G=f_G(z;\theta)$是生成网络的输出，则目标函数可以简化为：</p>
<p>$$\mathcal{J}(\phi,\theta|x)=-\parallel x-\mu_G\parallel^2+D_{KL}(\mathcal{N}(\mu_I,\sigma_I)\parallel\mathcal{N}(0,I))$$</p>
<p>其中第一项可以近似看作是输入$x$的重构正确性，第二项可以看作是正则化项。</p>
</div></article></div><nav class="pagination is-centered mt-4" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">上一页</a></div><div class="pagination-next"><a href="/page/2/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block line-height-inherit">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">12</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Research/"><span class="level-start"><span class="level-item">Research</span></span><span class="level-end"><span class="level-item tag">16</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Research/Anomaly-Detection/"><span class="level-start"><span class="level-item">Anomaly Detection</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Research/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Research/Misc/"><span class="level-start"><span class="level-item">Misc</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Research/RNN/"><span class="level-start"><span class="level-item">RNN</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Research/Time-Series-Imputation/"><span class="level-start"><span class="level-item">Time Series Imputation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Research/Tutorial/"><span class="level-start"><span class="level-item">Tutorial</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/Technical-Notes/"><span class="level-start"><span class="level-item">Technical Notes</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/Technical-Notes/Misc/"><span class="level-start"><span class="level-item">Misc</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><a class="media-left" href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/"><p class="image is-64x64"><img class="thumbnail" src="http://qfxiao.me/img/image-20200301230011495.png" alt="Discovering Physical Concepts with Neural Networks"></p></a><div class="media-content size-small"><p><time dateTime="2020-03-01T14:55:02.000Z">2020-03-01</time></p><p class="title is-6"><a class="link-muted" href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/">Discovering Physical Concepts with Neural Networks</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Research/">Research</a> / <a class="link-muted" href="/categories/Research/Misc/">Misc</a></p></div></article><article class="media"><a class="media-left" href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/"><p class="image is-64x64"><img class="thumbnail" src="http://qfxiao.me/img/image-20200109102830713.png" alt="Transfer Anomaly Detection by Inferring Latent Domain Representations"></p></a><div class="media-content size-small"><p><time dateTime="2020-02-27T12:02:18.000Z">2020-02-27</time></p><p class="title is-6"><a class="link-muted" href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/">Transfer Anomaly Detection by Inferring Latent Domain Representations</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Research/">Research</a> / <a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></p></div></article><article class="media"><a class="media-left" href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/"><p class="image is-64x64"><img class="thumbnail" src="http://qfxiao.me/img/image-20200113104953902.png" alt="Deep Anomaly Detection with Deviation Networks"></p></a><div class="media-content size-small"><p><time dateTime="2020-02-24T02:45:08.000Z">2020-02-24</time></p><p class="title is-6"><a class="link-muted" href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/">Deep Anomaly Detection with Deviation Networks</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Research/">Research</a> / <a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></p></div></article><article class="media"><a class="media-left" href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"><p class="image is-64x64"><img class="thumbnail" src="http://qfxiao.me/img/image-20200131113557592.png" alt="Geant4 安装教程与调试环境配置"></p></a><div class="media-content size-small"><p><time dateTime="2020-01-31T03:25:59.000Z">2020-01-31</time></p><p class="title is-6"><a class="link-muted" href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/">Geant4 安装教程与调试环境配置</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a> / <a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></p></div></article><article class="media"><a class="media-left" href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/"><p class="image is-64x64"><img class="thumbnail" src="http://qfxiao.me/img/image-20200109102204802.png" alt="Complementary Set Variational Autoencoder for Supervised Anomaly Detection"></p></a><div class="media-content size-small"><p><time dateTime="2020-01-09T02:15:03.000Z">2020-01-09</time></p><p class="title is-6"><a class="link-muted" href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/">Complementary Set Variational Autoencoder for Supervised Anomaly Detection</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Research/">Research</a> / <a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/03/"><span class="level-start"><span class="level-item">三月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/02/"><span class="level-start"><span class="level-item">二月 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/01/"><span class="level-start"><span class="level-item">一月 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/11/"><span class="level-start"><span class="level-item">十一月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Anomaly-Detection/"><span class="tag">Anomaly Detection</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag is-grey-lightest">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Flow-based-Model/"><span class="tag">Flow-based Model</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag is-grey-lightest">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RNN/"><span class="tag">RNN</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spectral/"><span class="tag">Spectral</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Statistics/"><span class="tag">Statistics</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Time-Series/"><span class="tag">Time Series</span><span class="tag is-grey-lightest">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transfer-Learning/"><span class="tag">Transfer Learning</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VAE/"><span class="tag">VAE</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Variational-Inference/"><span class="tag">Variational Inference</span><span class="tag is-grey-lightest">2</span></a></div></div></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button is-primary" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Hanzawa の 部屋" height="28"></a><p class="size-small"><span>&copy; 2020 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            site: {
                url: 'http://qfxiao.me',
                external_link: {"enable":true,"exclude":[]}
            },
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><script>window.addEventListener("load", function () {
            MathJax.Hub.Config({
                'HTML-CSS': {
                    matchFontHeight: false
                },
                SVG: {
                    matchFontHeight: false
                },
                CommonHTML: {
                    matchFontHeight: false
                },
                tex2jax: {
                    inlineMath: [
                        ['$','$'],
                        ['\\(','\\)']
                    ]
                }
            });
        });</script><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>