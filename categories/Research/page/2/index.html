<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>分类: Research - Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="website"><meta property="og:title" content="Hanzawa の 部屋"><meta property="og:url" content="http://hanzawa.me/"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://hanzawa.me/img/og_image.png"><meta property="article:author" content="Hanzawa"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://hanzawa.me"},"headline":"Hanzawa の 部屋","image":["http://hanzawa.me/img/og_image.png"],"author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":null}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">Research</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-05-06T03:05:37.000Z" title="2020-5-6 11:05:37 ├F10: AM┤">2020-05-06</time>发表</span><span class="level-item"><time dateTime="2020-06-25T08:15:23.573Z" title="2020-6-25 4:15:23 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/06/Learning-Representations-of-Ultrahigh-dimensional-Data-for-Random-Distance-based-Outlier-Detection/">Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>本文提出了一种针对高维数据异常检测的表示学习方法。文中提出了<strong>RAMODO</strong>框架，一种基于排序的结合表示学习和异常检测的无监督框架。除此之外，基于<strong>RAMODO</strong>，文中还提出了基于此框架的模型<strong>REPEN</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.04808">Paper📰</a></p>
<h1 id="proposed-method">Proposed Method</h1>
<h2 id="the-proposed-framework-ramodo">The Proposed Framework: <strong>RAMODO</strong></h2>
<h3 id="problem-statement">Problem Statement</h3>
<p>我们的目的是为高维数据学习低维表示，同时在学到的低维表示中能够更好地进行异常检测。设有数据集<span class="math inline">\(\mathcal{X}=\{\mathbf x_1,\mathbf x_2,\cdots, \mathbf x_N\}\)</span> (<span class="math inline">\(\mathbf x_i\in \mathbb{R}^D\)</span>) 和一个基于随机距离的异常检测器<span class="math inline">\(\phi:\mathcal{X}\mapsto \mathbb{R}\)</span>，我们的目标是学习一个表示函数<span class="math inline">\(f:\mathcal{X}\mapsto\mathbb{R}^M (M\ll D)\)</span>使得对于所有异常样本<span class="math inline">\(\mathbf x_i\)</span>和正常样本<span class="math inline">\(\mathbf x_j\)</span>都有<span class="math inline">\(\phi(f(\mathbf x_i))&gt;\phi(f(\mathbf x_j))\)</span>。</p>
<h3 id="ranking-model-based-representation-learning-framework">Ranking Model-based Representation Learning Framework</h3>
<p><strong>RAMODO</strong>基于<em>pairwise ranking model</em>。第一步是通过一定的预处理算法（原文中称为<em>outlier thresholding</em>）将数据划分为inlier候选集和outlier候选集；第二步通过随机从inlier候选集采样<span class="math inline">\(n\)</span>个样本生成query set <span class="math inline">\((\mathbf x_i,\cdots,\mathbf x_{i+n-1})\)</span>，从inlier候选集采样一个样本生成<em>positive example</em> <span class="math inline">\((\mathbf x^+)\)</span>，从outlier候选集采样一个样本生成<em>negative example</em> <span class="math inline">\((\mathbf x^-)\)</span>，将三者组合生成 <em>metatriplet</em> <span class="math inline">\(T=(&lt;\mathbf x_i,\cdots,\mathbf x_{i+n-1}&gt;,\mathbf x^+,\mathbf x^-)\)</span>；第三步通过神经网络<span class="math inline">\(f\)</span>学习表示；第四步通过<em>outlier score-based ranking loss</em> <span class="math inline">\(L(\phi(f(\mathbf x^+)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;),\phi(f(\mathbf x^-)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;))\)</span>来进行优化，其中<span class="math inline">\(\phi(\cdot|\cdot)\)</span>为基于距离的异常检测器。</p>
<p><img src="https://i.loli.net/2020/06/25/4I7fx5ZjBhueUDz.png" /></p>
<h2 id="a-ramodo-instance-repen">A <strong>RAMODO</strong> Instance: <strong>REPEN</strong></h2>
<p><strong>REPEN</strong>为<strong>RAMODO</strong>的实例模型，使用Sp作为异常检测器。</p>
<h3 id="outlier-thresholding-using-state-of-the-art-detectors-and-cantellis-inequality">Outlier Thresholding Using State-of-the-art Detectors and Cantelli's Inequality</h3>
<p>第一步使用Sp作为基础获得初始anomaly score：</p>
<blockquote>
<p><strong>Definition 1</strong> (<em>Sp-based Outlier Scoring</em>). 给定样本<span class="math inline">\(x_i\)</span>，Sp 以以下方式定义该样本的异常程度： <span class="math display">\[
r_i=\frac{1}{m}\sum\limits_{j=1}^m nn\_dist(\mathbf x_i|\mathcal{S}_j)
\]</span> 其中<span class="math inline">\(\mathcal S_j\subset \mathcal X\)</span>为数据集随机采样的子集，<span class="math inline">\(m\)</span>为集成大小，<span class="math inline">\(nn_dist(\cdot|\cdot)\)</span>为<span class="math inline">\(\mathcal S_j\)</span>中最近邻居的距离。</p>
</blockquote>
<p>接着通过<em>Cantelli's Inequality</em>来定义<em>Pseudo Outlier</em>：</p>
<blockquote>
<p><strong>Definition 2 </strong>(<em>Cantelli's Inequality-based Outlier Thresholding</em>). 给定异常分数向量<span class="math inline">\(\mathbf r\in\mathbb R^N\)</span>，更高异常分数代表更高的可能性为异常，设<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\delta^2\)</span>分别为均值和方差，<em>Outlier</em>候选集由以下方式确定： <span class="math display">\[
\mathcal{O}=\{\mathbf x_i|r_i \geq \mu + \alpha\delta\}, \space\forall \mathbf x_i\in\mathcal X, \space r_i\in\mathbf r
\]</span> 其中<span class="math inline">\(\alpha\geq 0\)</span>为自定义的阈值。</p>
</blockquote>
<p><em>Inlier</em>候选集<span class="math inline">\(\mathcal I=\mathcal X\backslash \mathcal O\)</span>。</p>
<h3 id="triplet-sampling-based-on-outlier-scores">Triplet Sampling Based on Outlier Scores</h3>
<p>首先，从<span class="math inline">\(\mathcal I\)</span>采样一定数量的样本组成<em>query set</em>，每个样本被采样的概率与其对应的异常分数有关：</p>
<p><span class="math display">\[
p(\mathbf x_i)=\frac{\mathbb Z-r_i}{\sum_{t=1}^{|\mathcal I|}[\mathbb Z-r_t]}
\]</span></p>
<p>其中<span class="math inline">\(\mathbb Z=\sum_{t=1}^{|\mathcal I|}r_t\)</span>。</p>
<p>之后从<em>inlier set</em>中均匀随机采样一个<em>positive sample</em> <span class="math inline">\(\mathbf x^+\)</span>。最后从<em>outlier set</em>中根据以下概率采样一个<em>negative sample</em> <span class="math inline">\(\mathbf x^-\)</span>： <span class="math display">\[
p(\mathbf x_j)=\frac{r_j}{\sum_{t=1}^{|\mathcal O|}r_t}
\]</span></p>
<h3 id="a-shallow-data-representation">A Shallow Data Representation</h3>
<p>单层神经网络用来获得浅层的表示：</p>
<blockquote>
<p><strong>Definition 3 </strong>(<em>Single-layer Fully-connected Representations</em>) 给定输入<span class="math inline">\(x\)</span>， <span class="math display">\[
f_\Theta(\mathbf x)=\{\psi(\mathbf w_1^\top\mathbf x),\psi(\mathbf w_2^\top\mathbf x),\cdots,\psi(\mathbf w_M^\top\mathbf x)\}
\]</span> 其中<span class="math inline">\(\psi(\cdot)\)</span>为激活函数，<span class="math inline">\(\mathbf w\)</span>为权重矩阵。</p>
</blockquote>
<h3 id="ranking-loss-using-random-nearest-neighbor-distance-based-outlier-scores">Ranking Loss Using Random Nearest Neighbor Distance-based Outlier Scores</h3>
<p>设<span class="math inline">\(\mathcal{Q}=&lt;f_\Theta(\mathbf x_i),\cdots,f_\Theta(\mathbf x_{i+n-1})&gt;\)</span>为<em>query set</em>，给定样本<span class="math inline">\(\mathbf x\)</span>，<strong>REPEN</strong>根据最近邻距离定义了<span class="math inline">\(f_\Theta(\mathbf x)\)</span>的异常程度： <span class="math display">\[
\phi(f_\Theta(\mathbf x)|\mathcal{Q})=nn\_dist(f_\Theta(\mathbf x)|\mathcal Q)
\]</span> 因此，给定三元组<span class="math inline">\(T=(\mathcal Q,f_\Theta(\mathbf x^+),f_\Theta(\mathbf x^-))\)</span>，我们的目标是学得表示<span class="math inline">\(f(\cdot)\)</span>使得： <span class="math display">\[
nn\_dist(f_\Theta(\mathbf x^+)|\mathcal Q)&lt;nn\_dist(f_\Theta(\mathbf x^-)|\mathcal Q)
\]</span> 损失函数： <span class="math display">\[
J(\Theta;T)=L(\phi(f_\Theta(\mathbf x^+)|\mathcal Q),\phi(f_\Theta(\mathbf x^-)|\mathcal Q))=\\\max\{0, c+nn\_dist(f_\Theta(\mathbf x^+)|\mathcal Q)-nn\_dist(f_\Theta(\mathbf x^-)|\mathcal Q)\}
\]</span> 其中<span class="math inline">\(c\)</span>为边界参数。给定一系列三元组，最终优化目标如下： <span class="math display">\[
\mathop{\text{arg min}}\limits_{\Theta}\frac{1}{|\mathcal{T}|}\sum\limits_{i=1}^{|\mathcal T|}J(\Theta;T_i)
\]</span></p>
<h3 id="the-algorithm-and-its-time-complexity">The Algorithm and Its Time Complexity</h3>
<p><img src="https://i.loli.net/2020/06/25/eYtKHBJ7szCgjNa.png" /></p>
<h3 id="leveraging-a-few-labeled-outliers-to-improve-triplet-sampling">Leveraging A Few Labeled Outliers to Improve Triplet Sampling</h3>
<h1 id="experiments">Experiments</h1>
<h2 id="datasets">Datasets</h2>
<ul>
<li>AD：网络广告检测</li>
<li>LC：肺癌疾病监测</li>
<li>p53：异常蛋白质活动检测</li>
<li>R8：文本分类</li>
<li>News20：文本分类</li>
<li>URL：异常网址检测</li>
<li>Webspam：Pascal Large Scale LearningChallenge</li>
</ul>
<h2 id="effectiveness-in-real-world-data-with-thousands-to-millions-of-features">Effectiveness in Real-world Data with Thousands to Millions of Features</h2>
<p>作者分别使用原始特征和<em>REPEN</em>学到的特征进行异常检测，IMP代表性能提升比例，SU代表加速比例。</p>
<p><img src="https://i.loli.net/2020/06/25/mvUiE1NzyTwOgV8.png" /></p>
<h2 id="comparing-to-state-of-the-art-representation-learning-competitors">Comparing to State-of-the-art Representation Learning Competitors</h2>
<ul>
<li><strong>AE: </strong>自编码器</li>
<li><strong>HLLE: </strong> <em>Hessian Locally Linear Embedding</em></li>
<li><strong>SRP: </strong> <em>Sparse Random Projection</em></li>
<li><strong>CoP: </strong> <em>Coherent Pursuit</em></li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/yQumCRNrHAheJ34.png" /></p>
<h2 id="the-capability-of-leveraging-labeled-outliers-as-prior-knowledge">The Capability of Leveraging Labeled Outliers as Prior Knowledge</h2>
<p><img src="https://i.loli.net/2020/06/25/NOLfKQd2u1JMPtp.png" /></p>
<h2 id="sensitivity-test-w.r.t.-the-representation-dimension">Sensitivity Test w.r.t. the Representation Dimension</h2>
<p><img src="https://i.loli.net/2020/06/25/BoGjY5SEu6vrK3X.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/Rlx7Df9Hvsjp2Eg.png" /></p>
<p>文中提到了对于R8、URL、News20这三个数据集在维度<span class="math inline">\(M=1\)</span>的时候表现和其他维度一样好，作者给出的解释是在这几个数据集中异常部分是线性可分的，所以1维就足够了，另一个解释是优化问题。</p>
<h2 id="scalability-test">Scalability Test</h2>
<p><img src="https://i.loli.net/2020/06/25/1JfUclWyFYgLdNp.png" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-03-29T16:17:24.000Z" title="2020-3-30 12:17:24 ├F10: AM┤">2020-03-30</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:33:35.419Z" title="2020-6-25 1:33:35 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/30/Deep-Weakly-supervised-Anomaly-Detection/">Deep Weakly-supervised Anomaly Detection</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>在文献中，因为标注成本的昂贵，无监督方法占据了异常检测的主要位置。然而，在现实生活中，我们可能会有少量标签，如何利用这部分标签信息就成为了一个问题，作者将其称之为<em>anomaly-informed modeling</em>。作者提出了两点挑战：</p>
<ol type="1">
<li>少量标签可能无法提供所有类型异常的信息；</li>
<li>大部分无标签数据为正常样本，但其中包含少部分异常（污染）。</li>
</ol>
<p>作者提出了基于pairwise relation learning的方法来解决这些问题。文章的主要贡献如下：</p>
<ol type="1">
<li>提出了一种基于pairing-based data augmentation和ordinal regression来进行弱监督异常检测的框架</li>
<li>基于该框架提出了PReNet，一种基于双流ordinal regression的网络</li>
<li>从理论和实践角度分析了方法的有效性</li>
<li>在40个真实数据集上进行了完善的实验</li>
</ol>
<h1 id="proposed-method">Proposed Method</h1>
<h2 id="learning-anomaly-scores-by-predicting-pairwise-relation">Learning Anomaly Scores by Predicting Pairwise Relation</h2>
<h3 id="problem-formulation">Problem Formulation</h3>
<p>给定数据集<span class="math inline">\(\mathcal{X}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N,\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，包含两部分，一部分是五标签数据<span class="math inline">\(\mathcal{U}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N\}\)</span>，另一部分是有标签异常数据<span class="math inline">\(\mathcal{A}=\{\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，其中<span class="math inline">\(K\ll N\)</span>。我们的任务目标是学习一个打分函数<span class="math inline">\(\phi:\mathcal{X}\mapsto \mathbb{R}\)</span>，使得对任任意异常样本的打分高于任意正常样本。</p>
<p>在这个Formulation里，作者将关系学习和异常打分统一了起来。首先，输入的数据集不再是原始样本，而是样本对。样本对包含三种：<em>anomaly-anomaly</em>，<em>anomaly-unlabeled</em>，<em>unlabeled-unlabeled</em>，记为<span class="math inline">\(C_{\{\mathbf{a},\mathbf{a}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>。每一个样本对包含一个标签<span class="math inline">\(y\)</span>，表示该pair对应的异常分数，整个输入数据集<span class="math inline">\(\mathcal{P}=\{\{\mathbf{x}_i,\mathbf{x}_j,y_{ij}\}|\mathbf{x}_i,\mathbf{x}_j\in\mathcal{X} \space\text{and}\space y_{ij}\in\mathbb{N}\}\)</span>。因为有<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}&gt;y_{\{\mathbf a,\mathbf u\}}&gt;y_{\{\mathbf u,\mathbf u\}}\)</span>，所以对关系的学习也是对异常打分的学习。</p>
<h3 id="the-instantiated-model-prenet">The Instantiated Model: PReNET</h3>
<p>下图为模型示意图，<strong>Data Augmentation</strong>模块负责产生pair数据，<strong>End-to-End Anomaly Score Learner <span class="math inline">\(\phi\)</span></strong> 模块负责关系学习（异常打分）。</p>
<p><img src="https://i.loli.net/2020/06/25/6ZF3w9v1Lux5t4Q.png" style="zoom:67%;" /></p>
<h4 id="data-argumentation-by-pairing">Data Argumentation by Pairing</h4>
<p>数据的产生分为两步：</p>
<ol type="1">
<li>从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>上随机采样，组成pair；</li>
<li>对每个pair打上次序(ordinal class feature) 标签<span class="math inline">\(\mathbf{y}\)</span>。</li>
</ol>
<p>部分<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>和<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>可能包含异常污染，因为在<span class="math inline">\(\mathcal{U}\)</span>中可能会有未标记的异常样本。</p>
<h4 id="end-to-end-anomaly-score-learner">End-to-End Anomaly Score Learner</h4>
<p>令<span class="math inline">\(\mathcal{Z}\in\mathbb{R}^M\)</span>为中间表示空间，那么<strong>Score Learner</strong>可以拆解为特征学习<span class="math inline">\(\psi(\cdot;\Theta_r):\mathcal{X}\mapsto \mathcal{Z}\)</span>和打分函数<span class="math inline">\(\eta((\cdot,\cdot);\Theta_s):(\mathcal{Z},\mathcal{Z})\mapsto\mathbb{R}\)</span>两部分，两部分都由神经网络组成。</p>
<h4 id="ordinal-regression">Ordinal Regression</h4>
<p>损失函数定义为： <span class="math display">\[
L\left(\phi((\mathbf x_i,\mathbf x_j);\Theta),y_{ij}\right)=|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|
\]</span> 采用绝对值而不是均方误差的原因是为了减少异常污染的影响。默认<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}=8\)</span>，<span class="math inline">\(y_{\{\mathbf a,\mathbf u\}}=4\)</span>，<span class="math inline">\(y_{\{\mathbf u,\mathbf u\}}=0\)</span>。最后的优化函数可以写为： <span class="math display">\[
\mathop{\text{argmin}}\limits_{\Theta}\frac{1}{|\mathcal{B}|}\sum\limits_{\{\mathbf x_i,\mathbf x_j, y_{ij}\}\in\mathcal{B}}|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|+\lambda R(\Theta)
\]</span> <span class="math inline">\(\mathcal{B}\)</span>为一个batch，<span class="math inline">\(R(\Theta)\)</span>为正则项。</p>
<h3 id="anomaly-detection-using-prenet">Anomaly Detection Using PReNet</h3>
<h4 id="training-stage">Training Stage</h4>
<p>训练流程如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/oR6uTL3c7HMpwD4.png" style="zoom: 80%;" /></p>
<p>为了保证训练样本类别的平衡，<span class="math inline">\(\frac{|\mathcal{B}|}{2}\)</span>的样本采样自<span class="math inline">\(C_{\{\mathbf u,\mathbf u\}}\)</span>，采样自<span class="math inline">\(C_{\{\mathbf a,\mathbf u\}}\)</span>和<span class="math display">\[C_{\{\mathbf a,\mathbf a\}}\]</span>的样本都占<span class="math inline">\(\frac{|\mathcal{B}|}{4}\)</span>。</p>
<h4 id="anomaly-scoring-stage">Anomaly Scoring Stage</h4>
<p>在测试阶段，给定测试样本<span class="math inline">\(\mathbf{x}_k\)</span>，先分别从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>采样，然后定义以下<em>anomaly score</em>： <span class="math display">\[
s_{\mathbf{x}_k}=\frac{1}{2E}\left[\sum\limits_{i=1}^E\phi((\mathbf a_i,\mathbf x_k);\Theta^*)+\sum\limits_{j=1}^E\phi((\mathbf x_k,\mathbf u_j);\Theta^*)\right]
\]</span> <span class="math inline">\(\mathbf a_i\)</span>和<span class="math inline">\(\mathbf u_j\)</span>为随机采样得到的异常样本和正常样本，采样大小<span class="math inline">\(E\)</span>默认为30。</p>
<h1 id="experiments">Experiments</h1>
<p>实验部分主要是回答以下四个问题：</p>
<ol type="1">
<li>在有限的标签异常情况下，PReNet能否有效地检测已知和未知的异常；</li>
<li>在不同数量标签异常的情况下，PReNet的表现如何；</li>
<li>PReNet对异常污染的鲁棒性如何；</li>
<li>PReNet不同组件的重要性如何。</li>
</ol>
<h2 id="datasets">Datasets</h2>
<p>实验一共用到了40个数据集，其中12个用来评测算法检测已知的异常的能力（如Table 2所示），28个用来评测算法检测未知的异常的能力（如Table 3所示）。</p>
<h2 id="competing-methods-and-parameter-settings">Competing Methods and Parameter Settings</h2>
<p>用到的baseline有以下几个：</p>
<ul>
<li>DevNet：同一作者在KDD2019提出的异常检测框架</li>
<li>Deep support vector data description (DSVDD)：深度支持向量数据描述</li>
<li>Prototypical network： few-shot classification中的一种模型</li>
<li>iForest：孤立森林</li>
</ul>
<h2 id="performance-evaluation-metrics">Performance Evaluation Metrics</h2>
<p>用到的Metrics为AUC-ROC和AUC-PR。</p>
<h2 id="detection-of-known-anomalies">Detection of Known Anomalies</h2>
<p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）是固定的，下表为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/BfhVE9z8DWipAM6.png" style="zoom: 80%;" /></p>
<h2 id="detection-of-unknown-anomalies">Detection of Unknown Anomalies</h2>
<p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）同样是固定的，下表为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/9GM8XTYiSLUn2Ar.png" style="zoom:80%;" /></p>
<p><img src="https://i.loli.net/2020/06/25/4RxrGZWLqHNnXco.png" style="zoom:80%;" /></p>
<h2 id="availability-of-known-anomalies">Availability of Known Anomalies</h2>
<p>本实验主要是研究不同数量标注异常样本的条件下，算法的性能如何。异常污染的比例固定（2%），标注异常的数量从15到120变化。实验结果如下：</p>
<p><img src="https://i.loli.net/2020/06/25/x4Hf3lU5JO71bvo.png" style="zoom:80%;" /></p>
<h2 id="further-analysis-of-prenet">Further Analysis of PReNet</h2>
<h3 id="tolerance-to-anomaly-contamination-in-unlabeled-data">Tolerance to Anomaly Contamination in Unlabeled Data</h3>
<p>本实验主要研究不同异常污染比例下，算法的性能，即探究算法对异常污染的鲁棒性。标注异常样本的数量恒定（60），异常污染比例在<span class="math inline">\(\{0\%,2\%,5\%,10\%\}\)</span>中变化。实验结果如下所示：</p>
<p><img src="https://i.loli.net/2020/06/25/TGYCJUs8LlmreNV.png" style="zoom:80%;" /></p>
<h3 id="ablation-study">Ablation Study</h3>
<p>这一节是消融实验，分别设置了四个变体：</p>
<ul>
<li><strong>BOR: </strong>损失函数替换成了二值回归<em>Binary Ordinal Regression</em>；</li>
<li><strong>OSNet: </strong>将双流结构简化为单流；</li>
<li><strong>LDM: </strong>将网络中的隐藏层去除；</li>
<li><strong>A2H: </strong>加入了额外的隐藏层，并且加入了<span class="math inline">\(\ell_2\)</span>-norm防止过拟合。</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/oR7qlZWfepFT8jK.png" style="zoom:80%;" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-03-01T14:55:02.000Z" title="2020-3-1 10:55:02 ├F10: PM┤">2020-03-01</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:35:20.215Z" title="2020-6-25 1:35:20 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/">Discovering Physical Concepts with Neural Networks</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>如题目所示，本文的目的是利用神经网络来发掘物理概念。其思路是从实验数据学到表示，然后用学到的表示来回答物理问题，由此物理概念可以从学到的表示来提取出。作者进行了4个实验：</p>
<ol type="1">
<li>在阻尼振动实验中，模型学到了相关的物理参数；</li>
<li>在角动量守恒实验中，模型预测了质点的运动；</li>
<li>给定量子系统的观测数据，模型正确的识别出了量子状态的自由度；</li>
<li>给定从地球观测的太阳和火星的位置时间序列数据，模型发现了日心说模型。</li>
</ol>
<h1 id="preliminaries">Preliminaries</h1>
<p>作者在附录中对神经网络的基础知识进行了介绍，这里不再赘述，只截取了一些相对前沿的内容。</p>
<p><img src="https://i.loli.net/2020/06/25/yh5Wj9AQmd6nsFC.png" style="zoom:67%;" /></p>
<h2 id="variational-autoencoders">Variational Autoencoders</h2>
<p>本文用到的模型基础是VAE：</p>
<p><img src="https://i.loli.net/2020/06/25/zCnYjVEZHdbqAD3.png" style="zoom:67%;" /></p>
<h3 id="representation-learning">Representation Learning</h3>
<p><em>Representation learning</em>的主要目标是将数据映射到一个隐向量 (encoder)，为了保证隐向量包含了所有相关信息， 那么应该能够从隐向量还原原数据 (decoder)。传统的Autoencoder是这个思想的最简单实现，而VAE则将AE和<em>Variational Inference</em>结合了起来，是一种经典的生成式模型。现在很多研究关注<em>Disentangled Representation Learning</em>，也就是说我们希望模型能够无监督地学习数据，从中学到有意义的表示。</p>
<h3 id="boldsymbol-beta-vae"><span class="math inline">\(\boldsymbol \beta\)</span>-VAE</h3>
<p><span class="math inline">\(\beta\)</span>-VAE是一种特殊的VAE，也是一个经典的<em>Disentangled Representation Learning</em>模型，它和VAE主要的区别是对KL散度一项加上了权重<span class="math inline">\(\beta\)</span>进行调节： <span class="math display">\[
C_\beta(x)=-\left[\mathbb{E}_{z\sim p_\phi(z|x)}\log p_\theta(x|z)\right] + \beta D_\text{KL}\left[p_\phi(z|x)\parallel h(z)\right]
\]</span> 如果假设<span class="math inline">\(p_\phi(z|x)=\mathcal{N}(\mu,\sigma)\)</span>，那么损失函数可以进行简化： <span class="math display">\[
C_\beta(x)=\parallel \hat{x} - x \parallel^2_2-\frac{\beta}{2}\left(\sum\limits_i\log(\sigma_i^2)-\mu_i^2-\sigma_i^2\right)+C
\]</span></p>
<h1 id="network-structure">Network Structure</h1>
<h2 id="network-structure-scinet">Network Structure: <em>SciNet</em></h2>
<p>模仿物理学家建模物理问题的过程，作者提出了<em>SciNet</em>，如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/uWd1lOUFxXgQJ7f.png" style="zoom:67%;" /></p>
<p>物理学家在建模物理问题的时候，往往是从一些实验数据出发，根据物理常识提取更加精练的表示，然后用学到的表示来回答物理问题。</p>
<p>对于单纯的输入输出问题，<em>SciNet</em>可以看作是一个映射，<span class="math inline">\(F:\mathcal{O}\times\mathcal{Q}\rightarrow\mathcal{A}\)</span>。<span class="math inline">\(\mathcal{O}\)</span>是可能的实验数据集合，<span class="math inline">\(\mathcal{Q}\)</span>是可能的问题集合，<span class="math inline">\(\mathcal{A}\)</span>是可能的答案集合。可以将其分为两个步骤：编码过程<span class="math inline">\(E:\mathcal{O}\rightarrow\mathcal{R}\)</span>从实验数据学到表示，解码过程<span class="math inline">\(D:\mathcal{R}\times \mathcal{Q}\rightarrow \mathcal{A}\)</span>根据给定的问题从表示来回答问题。由此，<span class="math inline">\(F(o,q)=D(E(o),q)\)</span>。在实现方面，<em>SciNet</em>采用的是全连接网络。</p>
<h2 id="training-and-testing-scinet">Training and Testing <em>SciNet</em></h2>
<p>用来训练的数据形式为<span class="math inline">\((o,q,a_{cor}(o,q))\)</span>，观测<span class="math inline">\(o\)</span>和问题<span class="math inline">\(q\)</span>分别从观测集<span class="math inline">\(\mathcal{O}\)</span>和问题集<span class="math inline">\(\mathcal{Q}\)</span>选出，<span class="math inline">\(a_{cor}(o,q)\)</span>为对应的正确答案。在训练过程中，我们希望准确度尽量高，并且学到<em>minimal uncorrelated representations</em>。为此，作者采用<em>disentangling variational autoencoder</em>作为模型。</p>
<h1 id="results">Results</h1>
<p>在文中，作者进行了4个实验来验证模型的有效性。</p>
<h2 id="damped-pendulum">Damped Pendulum</h2>
<p>阻尼振动实验：</p>
<ul>
<li><p>任务：预测一维阻尼振动在不同时间的位置。</p></li>
<li><p>物理模型：<span class="math inline">\(-kx-b\dot{x}=m\ddot{x}\)</span>，<span class="math inline">\(k\)</span>为弹性模量，<span class="math inline">\(b\)</span>为阻尼系数，通解为<span class="math inline">\(x(t)=A_0e^{-\frac{b}{2m}t}\cos(\omega t+\delta_0), \space \omega=\sqrt{\frac{k}{m}}\sqrt{1-\frac{b^2}{4mk}}\)</span></p></li>
<li><p>观测数据：位置时间序列数据<span class="math inline">\(o=[x(t_i)]_{i\in\{1,\cdots,50\}}\in\mathbb{R}^{50}\)</span>，时间间隔相等，质量<span class="math inline">\(m=1\text{kg}\)</span>，振幅<span class="math inline">\(A_0=1\text{m}\)</span>，相位<span class="math inline">\(\delta_0=0\)</span>，弹性模量<span class="math inline">\(k\in[5,10]\text{kg}/\text{s}^2\)</span>，阻尼系数<span class="math inline">\(b\in[0.5,1]\text{kg}/\text{s}\)</span>。</p></li>
<li><p>问题：预测<span class="math inline">\(q=t_\text{pred}\in\mathbb{R}\)</span></p></li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/yWGzxo4eFKmABul.png" /></p>
<p>隐变量大小设置为3，结果如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/Q4PKa3pm2htekqd.png" style="zoom:67%;" /></p>
<p>(b)中的三幅图分别是学到的三个隐变量和我们感兴趣的参数<span class="math inline">\(k\)</span>和<span class="math inline">\(b\)</span>的关系图。第一幅图中变量<span class="math inline">\(1\)</span>与<span class="math inline">\(b\)</span>几乎完全线性相关，与<span class="math inline">\(k\)</span>基于线性无关，变量<span class="math inline">\(2\)</span>只和<span class="math inline">\(k\)</span>相关。变量<span class="math inline">\(3\)</span>几乎为一个常数，故不提供额外的信息。由此作者认为<em>SciNet</em>学到了我们关心的两个参数的知识。</p>
<h2 id="conservation-of-angular-momentum">Conservation of Angular Momentum</h2>
<p>角动量守恒实验：</p>
<ul>
<li>任务：预测一个由长度为<span class="math inline">\(r\)</span>的绳子捆绑着的旋转质点在位置<span class="math inline">\((0,r)\)</span>经一个自由质点撞击后的位置</li>
<li>物理模型：给定撞击之前的角动量，自由质点撞击之后的速度，旋转质点在撞击之后在时间<span class="math inline">\(t_\text{pred}^\prime\)</span>的位置可以由角动量守恒定律给出：</li>
</ul>
<p><span class="math display">\[
J=m_\text{rot}r^2\omega-rm_\text{free}(\mathbf{v}_\text{free})_x=m_\text{rot}r^2\omega^\prime-rm_\text{free}(\mathbf{v}^\prime_\text{free})_x=J^\prime
\]</span></p>
<ul>
<li>观测数据：在撞击之前两个质点的位置数据<span class="math inline">\(o=[(t_i^\text{rot},q_\text{rot}(t_i^\text{rot})),(t_i^\text{free},q_\text{free}(t_i^\text{free}))]_{i\in\{1,\cdots,5\}}\)</span>，质量为固定值，半径<span class="math inline">\(r\)</span>也为固定值。数据添加高斯噪声。</li>
<li>问题：预测撞击之后自由质点在时间<span class="math inline">\(t_\text{pred}^\prime\)</span>的位置</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/SKfJLxl1QmuzFt9.png" /></p>
<p>实验室意图如下：</p>
<p><img src="https://i.loli.net/2020/06/25/qimk9ZYBe7UPs3z.png" style="zoom:67%;" /></p>
<p>实验结果表明<em>SciNet</em>能够正确预测质点撞击之后的位置，同时对噪音鲁棒。根据(b)，隐变量和角动量存在线性相关关系，作者认为<em>SciNet</em>学到了守恒的动量这一概念。</p>
<h2 id="representation-of-qubits">Representation of Qubits</h2>
<p>量子比特实验：</p>
<ul>
<li>任务：预测在<span class="math inline">\(n=1,2\)</span>的纯<span class="math inline">\(n\)</span>量子位状态<span class="math inline">\(\psi\in\mathbb{C}^{2^n}\)</span>下任何二进制投影测量<span class="math inline">\(\omega\in\mathbb{C}^{2^n}\)</span>的测量概率。</li>
<li>物理模型：在执行测量<span class="math inline">\(\omega\in\mathbb{C}^{2^n}\)</span>的状态<span class="math inline">\(\psi\in\mathbb{C}^{2^n}\)</span>下测量0的概率<span class="math inline">\(p(\omega,\psi)\)</span>由<span class="math inline">\(p(\omega,\psi)=|\left&lt;\omega,\psi\right&gt;|^2\)</span>给定</li>
<li>观测数据：状态<span class="math inline">\(\psi: o=[p(\alpha_i,\psi)]_{i\in\{i,\cdots,n_1\}}\)</span>的操作参数化：表示一组固定的随机二元射影测量值<span class="math inline">\(\mathcal{M}_1=\{\alpha_1,\cdots,\alpha_{n_1}\}\)</span>（一个量子位<span class="math inline">\(n_1 = 10\)</span>，两个量子位<span class="math inline">\(n_1 = 30\)</span>）</li>
<li>问题：对于固定的一组随机二元射影测量<span class="math inline">\(\mathcal{M}_2=\{\beta_1,\cdots,\beta_{n_2}\}\)</span>，测量<span class="math inline">\(\omega:q=[p(\beta_i,\omega)]_{i\in\{1,\cdots,n_2\}}\)</span>的Operational参数化（一个量子位<span class="math inline">\(n_2 = 10\)</span>，两个量子位<span class="math inline">\(n_2 = 30\)</span>）</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/8lY1LBsQCZUwX2I.png" /></p>
<p>实验结果如下：</p>
<p><img src="https://i.loli.net/2020/06/25/ZTRKfzb63Jrvk5C.png" style="zoom:67%;" /></p>
<p>通过实验发现，<em>SciNet</em>可以在不提供先验物理知识的条件下确定表述状态<span class="math inline">\(\psi\)</span>最小的参数数量。同时，<em>SciNet</em>还能分辨<em>tomographically complete</em>和<em>tomographically incomplete</em>。</p>
<h2 id="heliocentric-model-of-the-solar-system">Heliocentric Model of the Solar System</h2>
<p>日心说模型：</p>
<ul>
<li>问题：在给定初始条件下预测相对与地球的太阳和火星的角度<span class="math inline">\(\theta_M(t)\)</span>和<span class="math inline">\(\theta_S(t)\)</span></li>
<li>物理模型：地球和火星围绕太阳以一定角速度做近似圆周运动</li>
<li>观测数据：给定初始角度，随机选择周周期的哥白尼的观测数据</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/vXl3Ae4RpzibrmY.png" /></p>
<p>模型的实现稍有变化，如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/XnsYqcRS6izZGEm.png" style="zoom:67%;" /></p>
<p>这样，对于不同时间都对应一个隐变量<span class="math inline">\(r(t_i)\)</span>，而且隐变量是时间依赖的，对于一个隐变量<span class="math inline">\(r(t_i)\)</span>有一个解码器来输出答案。</p>
<p><img src="https://i.loli.net/2020/06/25/az3UmkchyFWevP7.png" style="zoom:67%;" /></p>
<p>实验结果表示，<em>SciNet</em>不仅正确预测了太阳和火星相对地球的角度，同时隐变量揭示了火星和地球相对太阳的角度。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-02-27T12:02:18.000Z" title="2020-2-27 8:02:18 ├F10: PM┤">2020-02-27</time>发表</span><span class="level-item"><time dateTime="2020-06-25T09:01:35.786Z" title="2020-6-25 5:01:35 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/">Transfer Anomaly Detection by Inferring Latent Domain Representations</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>作者提出了一种利用迁移学习提升target domain异常检测性能的算法。文中指出现有的基于迁移学习的异常检测算法需要对每个 target domain 进行单独训练，这样做会带来很大的计算开销。本文通过<em>latent domain vectors</em>来实现无需重新训练的异常检测。<em>latent domain vectors</em>是domain的一种隐含表示，通过该domain中的正常样本得到。在本文中，<em>anomaly score function</em>通过Auto-encoder得到。</p>
<h1 id="proposed-method">Proposed Method</h1>
<h2 id="task">Task</h2>
<p>令<span class="math inline">\(\mathbf{X}_d^+:=\{\mathbf{x}^+_{dn}\}^{N^+_d}_{n=1}\)</span>为第<span class="math inline">\(d\)</span>个domain的异常样本集，<span class="math inline">\(\mathbf{x}_{dn}^+\in\mathbb{R}^M\)</span>为其中第<span class="math inline">\(n\)</span>个样本的<span class="math inline">\(M\)</span>维特征向量，<span class="math inline">\(N^+_d\)</span>为第<span class="math inline">\(d\)</span>个domain异常样本的数量。</p>
<p>类似的，令<span class="math inline">\(\mathbf{X}_d^-:=\{\mathbf{x}^-_{dn}\}^{N^-_d}_{n=1}\)</span>为第<span class="math inline">\(d\)</span>个domain的正常样本集。我们假设对于每个domain都有<span class="math inline">\(N^+_d\ll N^-_d\)</span>，且特征向量维度都为<span class="math inline">\(M\)</span>。</p>
<p>假设在 source domain <span class="math inline">\(D_S\)</span>都有正常样本和异常样本，记为<span class="math inline">\(\{\mathbf{X}^+_d\cup\mathbf{X}_d^-\}^{D_S}_{d=1}\)</span>，在 target domain <span class="math inline">\(D_T\)</span>只有正常样本<span class="math inline">\(\{\mathbf{X}_d^-\}^{D_S+D_T}_{d=D_S+1}\)</span>。我们的目标是得到一个对于 target domain 合适的 domain-specific 的异常打分函数。</p>
<p><img src="https://i.loli.net/2020/06/25/KW2YgScfVZN7Fjz.png" style="zoom:67%;" /></p>
<h2 id="domain-specific-anomaly-score-function">Domain-specific Anomaly Score Function</h2>
<p>我们基于Auto-encoder定义异常打分函数。对于每个domain，我们假设存在一个<span class="math inline">\(K\)</span>维的隐变量<span class="math inline">\(\mathbf{z}_d\in\mathbb{R}^K\)</span>。对于第<span class="math inline">\(d\)</span>个 domain，异常打分函数定义如下： <span class="math display">\[
s_\theta(\mathbf{x}_{dn}|\mathbf{z}_d):=\parallel\mathbf{x}_{dn}-G_{\theta_G}(F_{\theta_F}(\mathbf{x}_{dn},\mathbf{z}_d))\parallel^2
\]</span> 其中参数<span class="math inline">\(\theta:=(\theta_G,\theta_F)\)</span>在所有 domain 之间共享。</p>
<h2 id="models-for-latent-domain-vectors">Models for Latent Domain Vectors</h2>
<p>隐变量<span class="math inline">\(\mathbf{z}_d\)</span>是无法观测到的，只能通过数据来估计。首先<span class="math inline">\(\mathbf{z}_d\)</span>在<span class="math inline">\(\mathbf{X}_d^-\)</span>条件下的条件分布假设为高斯分布：</p>
<p><span class="math display">\[
q_\theta(\mathbf{z}_d|\mathbf{X}_d^-):=\mathcal{N}(\mathbf{z}_d|\mu_\phi(\mathbf{X}_d^-),\text{diag}(\sigma_\phi^2(\mathbf{X}_d^-)))
\]</span> 其中均值<span class="math inline">\(\mu_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K\)</span>和方差<span class="math inline">\(\sigma^2_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K_+\)</span>由神经网络建模，且在所有 domain 之间共享。在<span class="math inline">\(\mathbf{X}_d^-\)</span>给定的时候，我们便能够推断出该 domain 对应的隐变量，</p>
<p><span class="math inline">\(q_\phi\)</span>的输入为正常样本的集合，故神经网络需要满足<em>permutation invariant</em>。<span class="math inline">\(\tau(\mathbf{X}_d^-)=\rho(\sum_{n=1}^{N_d^-}\eta(\mathbf{x}_{dn}^-))\)</span>，其中<span class="math inline">\(\tau(\mathbf{X}_d^-)\)</span>表示<span class="math inline">\(\mu_\phi(\mathbf{X_d^-})\)</span>或<span class="math inline">\(\ln\sigma_\phi^2(\mathbf{X}_d^-)\)</span>，<span class="math inline">\(\rho\)</span>和<span class="math inline">\(\eta\)</span>为神经网络，</p>
<h2 id="objective-function">Objective Function</h2>
<p>目标函数由anomaly score函数和隐变量组成。第<span class="math inline">\(d\)</span>个domain在对应的隐变量<span class="math inline">\(\mathbf{z}_d\)</span>条件下的目标函数为：</p>
<p><span class="math display">\[
L_d(\theta|\mathbf{z}_d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)-\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d))
\]</span></p>
<p>其中<span class="math inline">\(\lambda\geq 0\)</span>为超参数，<span class="math inline">\(f\)</span>为sigmoid函数。公式的第一项表示第<span class="math inline">\(d\)</span>个domain正常样本对应的<em>anomaly score</em>。第二项为可微分的AUC。异常样本的<em>anomaly score</em>应当大于正常样本，所以对任何<span class="math inline">\(\mathbf x_{dm}^+\in\mathbf X_d^+, \mathbf x_{dn}^-\in\mathbf X_d^-\)</span>有<span class="math inline">\(s_\theta(\mathbf x_{dm}^+|\mathbf z_d)&gt;s_\theta(\mathbf x_{dn}^-|\mathbf z_d)\)</span>。第二项<span class="math inline">\(\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d))\)</span>的取值范围是<span class="math inline">\([0,1]\)</span>，当所有的<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>时该项为1，当所有的<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\ll s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>时该项为0，所以最小化该项的相反数相当于鼓励<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>。</p>
<p>因为隐变量<span class="math inline">\(\mathbf z_d\)</span>包含不确定性，我们应该在目标函数里考虑这一点： <span class="math display">\[
\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))
\]</span></p>
<p>第一项是<span class="math inline">\(L_d(\theta|\mathbf z_d)\)</span>关于<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>的期望，第二项是<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>和<span class="math inline">\(p(\mathbf z_d):=\mathcal{N}(\boldsymbol 0,\boldsymbol I)\)</span>的KL散度。第一项可以用<em>monte carlo</em>估计<span class="math inline">\(\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]\approx\frac{1}{L}\sum_{\ell=1}^L L_d(\theta|\mathbf z_d^{(\ell)})\)</span>，除此之外还需要用到<em>reparametrization trick</em>。</p>
<p>对于第<span class="math inline">\(d\)</span>个target domain，因为没有异常样本（假设），所以<span class="math inline">\(L_d(\theta|\mathbf{z}_d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\)</span>，有： <span class="math display">\[
\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z}_d))
\]</span></p>
<p>所以总的损失函数为各domain对应的损失函数之和<span class="math inline">\(\mathcal{L}(\theta,\phi):=\sum_{d=1}^{D_S+D_T}\alpha_d\mathcal{L}_d(\theta,\phi)\)</span>。</p>
<h2 id="inference">Inference</h2>
<p>训练好之后，domain-specific的<em>anomaly score</em>可以由下式计算出：</p>
<p><span class="math display">\[
s(\mathbf{x}_{d^\prime}):=\int s_{\theta_*}(\mathbf{x_{d^\prime}}|\mathbf{z}_{d^\prime})q_{\phi_*}(\mathbf{z}_{d^\prime}|\mathbf{X}_{d^\prime}^-)\mathrm{d}\mathbf{z}_{d^\prime}\approx\frac{1}{L}\sum\limits_{\ell=1}^L s_{\theta_*}(\mathbf{x}_{d^\prime}|\mathbf{z}_{d^\prime}^{(\ell)})
\]</span></p>
<h1 id="experiments">Experiments</h1>
<h2 id="data">Data</h2>
<p>实验包含五个数据集，第一个是合成数据集。如下图(a)所示，围绕<span class="math inline">\((0,0)\)</span>有<span class="math inline">\(8\)</span>个圈，每个圈包含了一个内圈作为异常样本，第<span class="math inline">\(7\)</span>个圈被选为target domain，其余的为source domain。第二个是MNIST-r，是加入旋转的MNIST，包含6个domain，其中数字“4”被选为异常样本，其余为正常。第三个为Anuran Calls，包含5个domain。第四个是Landmine，主要用在多任务学习中。第五个是IoT，网络流量数据，包含8个domain。</p>
<p><img src="https://i.loli.net/2020/06/25/6WLAfMwJPuN5Ov9.png" style="zoom:50%;" /></p>
<h2 id="comparison-methods">Comparison Methods</h2>
<p>对比的baseline包括NN（普通多层神经网络），NNAUC（加入可微分AUC作为损失函数），AE（普通Autoencoer），AEAUC（加入可微分AUC的AE），OSVM（单类支持向量机），CCSA，TOSVM和OTL。</p>
<h2 id="results">Results</h2>
<p>4个真实数据集的结果如下：</p>
<p><img src="https://i.loli.net/2020/06/25/nfkwTVexRNqyFMY.png" style="zoom:50%;" /></p>
<p><img src="https://i.loli.net/2020/06/25/QaMskTZALyeiFI1.png" style="zoom:50%;" /></p>
<p><img src="https://i.loli.net/2020/06/25/F7VTyeHMz8mK2uJ.png" style="zoom:50%;" /></p>
<p><img src="https://i.loli.net/2020/06/25/B32UmgXcwGhZYk1.png" style="zoom:50%;" /></p>
<p>表5为考虑隐变量不确定性的ablation study。将原来的公式<span class="math inline">\(\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))\)</span>中<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>用迪利克雷分布<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)=\delta(\mathbf z_d-\mu_\phi(\mathbf X_d^-))\)</span>代替并且去掉KL散度。</p>
<p><img src="https://i.loli.net/2020/06/25/yUHcTBzixsMlY7f.png" style="zoom: 50%;" /></p>
<p>表6展示了不同异常比例对效果的影响。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-02-24T02:45:08.000Z" title="2020-2-24 10:45:08 ├F10: AM┤">2020-02-24</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:30:55.514Z" title="2020-6-25 1:30:55 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/">Deep Anomaly Detection with Deviation Networks</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>本文关注<code>Deep Anomaly Detection</code>，也就是用深度学习的方法来进行异常检测。文中提到现有的<code>Deep Anomaly Detection</code>存在两个弊端：一个是采用深度学习方法来进行特征学习，然后通过下游任务得到<code>Anomaly Score</code>，相比文中End-to-End的<code>Anomaly Score</code>学习，存在优化不充分的风险；另一个是现有的方法主要是无监督学习，无法利用已知的信息（如少量标签）。为此，本文提出了一种端到端的异常检测框架，来解决上述问题。</p>
<p>本文的主要贡献如下：</p>
<ul>
<li>提出了一种端到端的异常检测框架，直接学习<code>Anomaly Score</code>并且可以利用已知信息；</li>
<li>基于提出的框架，文中提出了一种实例方法 (DevNet)。</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/XT7fqQRWEOuocgy.png" style="zoom:67%;" /></p>
<h1 id="proposed-model">Proposed Model</h1>
<h2 id="end-to-end-anomaly-score-learning">End-To-End Anomaly Score Learning</h2>
<h3 id="problem-statement">Problem Statement</h3>
<p>为了区别于传统的两阶段异常检测（先学习特征表示，然后在学到的特征上定义一个<code>anomaly measure</code>来得到<code>anomaly score</code>），作者对端到端的异常检测问题重新进行形式化。</p>
<p>给定<span class="math inline">\(N+K\)</span>个样本<span class="math inline">\(\mathcal{X}=\{\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N,\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}\}\)</span>，其中<span class="math inline">\(\boldsymbol x_i\in\mathbb{R}^D\)</span>，无标签样本集<span class="math inline">\(\mathcal{U}=\{\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N\}\)</span>，有标签样本集<span class="math inline">\(\mathcal{K}=\{\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}\}\)</span>，且<span class="math inline">\(K\ll N\)</span>。异常检测的目标是学习一个<code>anomaly scoring function</code><span class="math inline">\(\phi:\mathcal{X}\mapsto\mathbb{R}\)</span>使得<span class="math inline">\(\phi(\boldsymbol x_i)&gt;\phi(\boldsymbol x_j)\)</span>，其中<span class="math inline">\(\boldsymbol x_i\)</span>为异常样本，<span class="math inline">\(\boldsymbol x_j\)</span>为正常样本。</p>
<h3 id="the-proposed-framework">The Proposed Framework</h3>
<p>为了解决这个问题，文中提出了一种通用异常检测框架，模型框架如下图所示：</p>
<p>模型框架如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/ZuE1mb2Ytv6Jdl7.png" style="zoom:50%;" /></p>
<p>主要包含三个部分：</p>
<ol type="1">
<li><em>anomaly scoring network</em>. 图中左边的部分，一个函数<span class="math inline">\(\phi\)</span>，输入样本<span class="math inline">\(\mathbf{x}\)</span>，输出<code>anomaly score</code></li>
<li><em>reference score generator</em>. 图中右边的部分。只有一个<em>anomaly scoring network</em>并不能进行训练，需要训练的目标。为此加入<em>reference score generator</em>，输入为随机选择的<span class="math inline">\(l\)</span>个正常样本，输出<code>reference score</code>（这<span class="math inline">\(l\)</span>个正常样本<code>anomaly score</code>的均值，记为<span class="math inline">\(\mu_\mathcal{R}\)</span>）</li>
<li><em>deviation loss</em>. <span class="math inline">\(\phi(\mathbf{x})\)</span>，<span class="math inline">\(\mu_\mathcal{R}\)</span>及对应的标准差<span class="math inline">\(\sigma_\mathcal{R}\)</span>作为<code>deviation loss</code>函数的输入。因为<span class="math inline">\(\mu_\mathcal{R}\)</span>和<span class="math inline">\(\sigma_\mathcal{R}\)</span>对应正常样本集的均值和方差，那么异常样本的<code>anomaly score</code>应该和<span class="math inline">\(\mu_\mathcal{R}\)</span>差别比较大，而正常样本则应该接近<span class="math inline">\(\mu_\mathcal{R}\)</span>。</li>
</ol>
<h2 id="deviation-networks">Deviation Networks</h2>
<p>下面是上述三个部件的具体实现。</p>
<h3 id="end-to-end-anomaly-scoring-network">End-To-End Anomaly Scoring Network</h3>
<p>记<span class="math inline">\(\mathcal{Q}\in\mathbb{R}^M\)</span>为中间表示空间，<code>anomaly scoring network</code><span class="math inline">\(\phi(\cdot;\Theta):\mathcal{X}\mapsto\mathbb{R}\)</span>可以定义为数据表示学习<span class="math inline">\(\psi(\cdot;\Theta_t):\mathcal{X}\mapsto\mathcal{Q}\)</span>和异常分数学习<span class="math inline">\(\eta(\cdot;\Theta_s):\mathcal{Q}\mapsto\mathbb{R}\)</span>两阶段的组合，其中<span class="math inline">\(\Theta=\{\Theta_t,\Theta_s\}\)</span>。</p>
<p><span class="math inline">\(\psi(\cdot;\Theta_t)\)</span>可以用一个<span class="math inline">\(H\)</span>层神经网络来实现： <span class="math display">\[
\mathrm{q}=\psi(\mathbf{x};\Theta_t)
\]</span> 其中<span class="math inline">\(\mathbf{x}\in\mathcal{X}\)</span>，<span class="math inline">\(\mathrm{q}\in\mathcal{Q}\)</span>。</p>
<p><span class="math inline">\(\eta(\cdot;\Theta_s)\)</span>可以用一个单层的神经网络来实现： <span class="math display">\[
\eta(\mathrm q;\Theta_s)=\sum\limits_{i=1}^M w_i^oq_i+w_{M+1}^o
\]</span> 其中<span class="math inline">\(\mathrm q\in\mathcal Q\)</span>，<span class="math inline">\(\Theta_s=\{\mathbf{w}^o\}\)</span>。</p>
<p>所以有： <span class="math display">\[
\phi(\mathbf{x};\Theta)=\eta(\psi(\mathbf{x};\Theta_t);\Theta_s)
\]</span></p>
<h3 id="gaussian-prior-based-reference-scores">Gaussian Prior-based Reference Scores</h3>
<p>有两种方法来获得<span class="math inline">\(\mu_\mathcal{R}\)</span>，一种是data-driven，一种是prior-driven。如果是data-driven的话则采用另一个神经网络，文中表示为了更好的解释性和计算效率，所以采用的是prior-driven。 <span class="math display">\[
\begin{align}
r_1,r_2,\cdots,r_l\sim \mathcal{N}(\mu,\sigma^2),\\
\mu_\mathcal{R}=\frac{1}{l}\sum\limits_{i=1}^l r_i
\end{align}
\]</span> 在文中，采用的prior是标准高斯分布。</p>
<h2 id="z-score-based-deviation-loss">Z-Score Based Deviation Loss</h2>
<p><em>anomaly scoring network</em>的优化目标可以定义为Z-Score的方式： <span class="math display">\[
dev(\boldsymbol x)=\frac{\phi(\boldsymbol x;\Theta)-\mu_{\mathcal{R}}}{\sigma_{\mathcal{R}}}
\]</span> <span class="math inline">\(dev(\boldsymbol x)\)</span>可以看作是样本偏离标准的程度，而我们肯定希望异常样本偏离标准越大，正常样本越接近标准。文中采用的损失函数是<code>Contrastive Loss</code>： <span class="math display">\[
L(\phi(\boldsymbol x;\Theta),\mu_\mathcal{R},\sigma_\mathcal{R})=(1-y)|dev(\boldsymbol x)| + y \max(0, a - dev(\boldsymbol x))
\]</span> <code>Contrastive Loss</code>的直观解释可以看下图：</p>
<p><img src="https://i.loli.net/2020/06/25/aPbSipCsk2JwNcD.png" style="zoom: 33%;" /></p>
<p>对于负例（正常），优化过程将他们尽量向原点靠近，对于正例（异常），优化过程将他们拉向边界。</p>
<h2 id="the-devnet-algorithm">The DevNet Algorithm</h2>
<p><code>DevNet</code>的算法流程图如下：</p>
<p><img src="https://i.loli.net/2020/06/25/km9H5DoNRbOQ784.png" style="zoom:67%;" /></p>
<h2 id="interpretability-of-anomaly-scores">Interpretability of Anomaly Scores</h2>
<p>因为<em>reference score generator</em>选择的是确定的高斯分布，于是可以用概率论给出一些解释性。作者给出了一个结论，</p>
<blockquote>
<p><strong>PROPOSITION</strong>： 设<span class="math inline">\(\boldsymbol x\in\mathcal{X}\)</span>，<span class="math inline">\(z_p\)</span>为<span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>的分位数，那么<span class="math inline">\(\phi(\boldsymbol x)\)</span>在区间<span class="math inline">\(\mu\pm z_p\sigma\)</span>的概率为<span class="math inline">\(2(1-p)\)</span>。</p>
</blockquote>
<p>例如，假设<span class="math inline">\(p=0.95\)</span>，那么<span class="math inline">\(z_{0.95}=1.96\)</span>，表示异常分数高于1.96的样本将以0.95的置信度为异常。</p>
<h1 id="experiment">Experiment</h1>
<p>实验用到了9个数据集，4个Baseline (REPEN，DSVDD，FSNET，iForest)，以及ROC和PR曲线两种评测标准。</p>
<h2 id="effectiveness-in-real-world-data-sets">Effectiveness in Real-world Data Sets</h2>
<h3 id="experiment-settings">Experiment Settings</h3>
<p>这一个实验主要是为了验证算法在真实场景下的效果，即大量无标签数据和极少量标签数据。训练集包含两部分，一部分是无标签数据<span class="math inline">\(\mathcal{U}\)</span>,包含<span class="math inline">\(2\%\)</span>的异常样本，另一部分是有标签数据<span class="math inline">\(\mathcal{K}\)</span>，由随机采样<span class="math inline">\(0.005\%-1\%\)</span>的训练数据和<span class="math inline">\(0.08\%-6\%\)</span>的异常样本组成。</p>
<h3 id="findings">Findings</h3>
<p>实验结果如下表所示：</p>
<p><img src="https://i.loli.net/2020/06/25/DKqxJROngML8IS2.png" style="zoom: 50%;" /></p>
<p>从结果上看来，本文提出的方法在所有数据集上都比Baseline好，说明<code>DevNet</code>端到端直接优化<code>Anomaly Score</code>的方式是有效的。</p>
<h2 id="data-efficiency">Data Efficiency</h2>
<h3 id="experiment-settings-1">Experiment Settings</h3>
<p>这一个实验主要是为了探究基于深度的异常检测方法的<em>data efficiency</em>。和上一个实验一样，无标签数据集包含<span class="math inline">\(2\%\)</span>的异常，而有标签的异常数量从<span class="math inline">\(5\)</span>到<span class="math inline">\(120\)</span>不等。本实验试图回答以下两个问题：</p>
<ul>
<li><code>DevNet</code>的<em>data efficiency</em>如何？</li>
<li>基于深度的方法在多大程度上能够利用标签信息？</li>
</ul>
<h3 id="findings-1">Findings</h3>
<p>在几个基于深度的Baseline中，<code>DevNet</code>的效果是最好的，同时在有标签异常非常有限的情况下，<code>DevNet</code>也能很好的利用标签信息，达到更好的效果。</p>
<p><img src="https://i.loli.net/2020/06/25/iIWGBPosKCuxbRF.png" style="zoom:67%;" /></p>
<h2 id="robustness-w.r.t.-anomaly-contamination">Robustness w.r.t. Anomaly Contamination</h2>
<h3 id="experiment-settings-2">Experiment Settings</h3>
<p>在第一个实验中，无标签数据集<span class="math inline">\(\mathcal{U}\)</span>包含的是固定的异常比例<span class="math inline">\(2\%\)</span>，而在这个实验中，作者测试了从<span class="math inline">\(0\%\)</span>到<span class="math inline">\(20\%\)</span>之间不同异常比例来测试算法的鲁棒性（即使<span class="math inline">\(\mathcal{U}\)</span>中包含异常，由于没有标签，在训练的时候仍然假设都为正常来进行训练）。本实验试图回答以下问题：</p>
<ul>
<li>基于深度的异常检测方法的鲁棒性如何？</li>
<li>当训练集中异常污染的比例较高的时候基于深度的方法能否打败无监督的方法？</li>
</ul>
<h3 id="findings-2">Findings</h3>
<p>下图为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/JCnIjLOc84RFP2V.png" style="zoom:67%;" /></p>
<p>从结果上来看，<code>DevNet</code>比其他基于深度的方法鲁棒性更好，同时在高异常污染的情况下仍然比纯无监督方法效果要好。</p>
<h2 id="ablation-study">Ablation Study</h2>
<p>本实验设置了<code>DevNet</code>的三个变体（默认的<code>DevNet-Def</code>为单层隐层加上一个输出层）来进行消融实验，分别是：</p>
<ul>
<li><code>DevNet-Rep</code>，去掉了<em>anomaly scoring network</em>网络的输出层，对应<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>；</li>
<li><code>DevNet-Linear</code>，去掉了网络中的非线性层，对应<em>learning of non-linear features</em>；</li>
<li><code>DevNet-3HL</code>，隐层数量为3层。</li>
</ul>
<p>对比结果如下：</p>
<p><img src="https://i.loli.net/2020/06/25/5LcyAwGMB8gb2UP.png" style="zoom:67%;" /></p>
<p>通过实验可以发现，<code>DevNet-Rep</code>说明了<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>的有效性，而<code>DevNet-Linear</code>说明了<em>learning of non-linear features</em>的重要性。<code>DevNet-3HL</code>说明了加深网络并不总能带来性能的提升。</p>
<h2 id="scalability-test">Scalability Test</h2>
<p>这一个实验使用合成的数据来测试算法对大规模数据的处理能力，分别从<em>Data Size</em>和<em>Data Dimensionality</em>两方面来测试。结果如下：</p>
<p><img src="https://i.loli.net/2020/06/25/5gbPdJkB47e3FsV.png" style="zoom:67%;" /></p>
<p>可以看出，<code>DevNet</code>对<em>Data Size</em>并不敏感，同时，面对高维数据，<code>DevNet</code>也没有表现出劣势。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-01-09T02:15:03.000Z" title="2020-1-9 10:15:03 ├F10: AM┤">2020-01-09</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:25:53.385Z" title="2020-6-25 1:25:53 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/">Complementary Set Variational Autoencoder for Supervised Anomaly Detection</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>对于异常检测问题，异常的模式是多种多样的。有监督模型能够较好地处理训练集中出现过的模式，无监督模型能够处理训练集中未出现过的模式，但对于训练集中出现过的异常模型并没有学习。本文提出了一种既能学习训练集中出现过的异常模式，同时能处理未出现过的异常模式的方法。</p>
<h1 id="proposed-model">Proposed Model</h1>
<h2 id="conventional-vae">Conventional VAE</h2>
<p>首先回顾一下原始的VAE。</p>
<p>原始VAE中的损失函数为： <span class="math display">\[
\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})=\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}\parallel p(\boldsymbol{z}))]
\]</span> 原文中作者证明了<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\leq\log p(\boldsymbol{x};\boldsymbol{\theta})\)</span>，所以<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\)</span>可以看作是数据分布<span class="math inline">\(p(\boldsymbol{x})\)</span>对数似然的一个下界。<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\)</span>又被称为证据下界 (ELBO)。<span class="math inline">\(\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]\)</span>中的期望一般用蒙特卡洛来进行估计： <span class="math display">\[
\begin{align}
\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq&amp; \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})\parallel p(\boldsymbol{z})],\\
\boldsymbol{z}^{(l)}&amp;\sim q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}), \space l\in\{1,2,\cdots,L\}
\end{align}
\]</span> 对于隐变量<span class="math inline">\(\boldsymbol{z}\)</span>，一般假设先验服从标准高斯分布，后验服从均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^2\)</span>的高斯分布，故KL散度能直接写出解析式： <span class="math display">\[
\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-C(-\frac{1}{2}-\log\sigma+\frac{1}{2}\sigma^2+\frac{1}{2}\mu^2)
\]</span> 使用VAE来做异常检测通常是在正常数据上进行训练，在检测阶段，如果是异常样本，那么VAE不能很好地重构它，这样会导致较大的重构误差。</p>
<h2 id="prior-distribution-for-anomalies">Prior Distribution for Anomalies</h2>
<p><img src="https://i.loli.net/2020/06/25/vrxAzRVCtaE3oLc.png" style="zoom:67%;" /></p>
<p>在原始VAE异常检测中，无论输入样本<span class="math inline">\(\boldsymbol{x}\)</span>是否异常，VAE都会使对应编码的后验<span class="math inline">\(p(\boldsymbol{z}|\boldsymbol{x})\)</span>服从高斯分布，且施加标准高斯分布的约束。在本文中，作者对异常和正常样本对应的隐变量的先验分布做了不同假设。首先，正常先验依然是标准高斯分布，记为<span class="math inline">\(p_n(\boldsymbol{z})\)</span>。而对于异常先验，作者认为异常即为“不正常”，和正常是补集的关系。作者在文中定义异常先验分布<span class="math inline">\(p_a(\boldsymbol{z})\)</span>为： <span class="math display">\[
p_a(\boldsymbol{z})=\frac{1}{Y^\prime}(\max\limits_{\boldsymbol{z}^\prime}p_n(\boldsymbol{z}^\prime)-p_n(\boldsymbol{z}))
\]</span></p>
<p>其中<span class="math inline">\(Y^\prime\)</span>为使<span class="math inline">\(p_a(\boldsymbol{z})\)</span>成为一个概率分布的调节因子。实际上，<span class="math inline">\(Y^\prime\)</span>往往会成为无限大，因为<span class="math inline">\(p(\boldsymbol z)\)</span>在整个定义域上都有定义。为了解决这个问题，作者加入了<span class="math inline">\(p_w(\boldsymbol z)\)</span>，一个在每个维度都足够宽的辅助分布：</p>
<p><span class="math display">\[
p_a(\boldsymbol z)=\frac{1}{Y}p_w(\boldsymbol z)\left(\max\limits_{\boldsymbol z^\prime}p_n(\boldsymbol z^\prime)-p_n(\boldsymbol z)\right)
\]</span></p>
<p>其中<span class="math inline">\(Y\)</span>为有限的常数。在文中<span class="math inline">\(p_n(\boldsymbol z)\)</span>和<span class="math inline">\(p_w(\boldsymbol z)\)</span>都为高斯分布，那么<span class="math inline">\(p_a(\boldsymbol z)\)</span>的具体形式为：</p>
<p><span class="math display">\[
p_a(\boldsymbol z)=\frac{1}{Y}\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\{\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)-\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol 1)\}
\]</span></p>
<p>其中：</p>
<p><span class="math display">\[
\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)=\frac{1}{\sqrt{2\pi}}
\]</span></p>
<p><span class="math display">\[
Y=\int_{-\infty}^{\infty}p_a(\boldsymbol z)\mathrm{d}\boldsymbol z=\frac{1}{\sqrt{2\pi}}\left\{1-\frac{1}{\boldsymbol s^2+1}\right\}
\]</span></p>
<p><span class="math inline">\(\boldsymbol s^2\)</span>为超参数，控制分布的宽度。用文中的先验替换VAE原始的KL散度，可写为：</p>
<p><span class="math display">\[
\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]=\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\frac{\mathcal N(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)}{\frac{1}{Y}\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\left\{\frac{1}{2\pi}-\mathcal N(\boldsymbol z;\boldsymbol0,\boldsymbol 1)\right\}}\mathrm{d}\boldsymbol z
\]</span></p>
<p>展开后：</p>
<p><span class="math display">\[
\begin{align}
\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;=
\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)\mathrm{d}\boldsymbol z\\
&amp;+\log Y\\
&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\mathrm{d}\boldsymbol z\\
&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\left\{\frac{1}{\sqrt{2\pi}}-\mathcal{N}(\boldsymbol z;\boldsymbol 0, \boldsymbol 1)\right\}\mathrm{d}\boldsymbol z
\end{align}
\]</span></p>
<p>使用泰勒展开，<span class="math inline">\(\log (x+\frac{1}{2\pi})\simeq-\log 2\pi+2\pi x\)</span>，KL散度可以用下式估计：</p>
<p><span class="math display">\[
\begin{align}
\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;\simeq\sqrt{\frac{2\pi}{\boldsymbol\sigma^2+1}}\exp\left(\frac{-\boldsymbol\mu^2}{2(\boldsymbol\sigma^2+1)}\right)\\
&amp;+\frac{\boldsymbol\mu^2+\boldsymbol\sigma^2}{2\boldsymbol s^2}-\log\boldsymbol\sigma+\log\boldsymbol s+\log\left(\sqrt{\boldsymbol s^2+1}-1\right)\\
&amp;-\frac{\log(\boldsymbol s^2+1)}{2}+\frac{\log(2\pi)-1}{2}
\end{align}
\]</span></p>
<p>下图为一维时<span class="math inline">\(p_n(\boldsymbol z)\)</span>和<span class="math inline">\(p_a(\boldsymbol z)\)</span>的示例：</p>
<p><img src="https://i.loli.net/2020/06/25/QHXo24cKj9uRwzW.png" style="zoom:67%;" /></p>
<h3 id="implementation-of-proposed-method">Implementation of proposed method</h3>
<p>文中使用编码器输出的分布<span class="math inline">\(\mathcal{N}(\boldsymbol z;\boldsymbol \mu, \boldsymbol \sigma^2)\)</span>与标准正态分布之间的KL散度来作为异常分数。在每一轮的训练过程中，加入一轮使用Anomaly Prior的训练。</p>
<p><img src="https://i.loli.net/2020/06/25/wrmzADXtsyuJ6EZ.png" style="zoom:67%;" /></p>
<h1 id="experiments">Experiments</h1>
<h2 id="mnist">MNIST</h2>
<p>作者设计了两个Task：</p>
<ol type="1">
<li>Task 1. <span class="math inline">\(N\)</span> vs. <span class="math inline">\(\bar{N}\)</span>. 将手写数字中的一个作为已知异常，其他作为正常，并加入均匀分布作为未知的异常。</li>
<li>Task 2. 手写数字被分为3组：已知异常，正常，未知异常。</li>
</ol>
<p>细节如下表所示：</p>
<p><img src="https://i.loli.net/2020/06/25/ifcIxr9zOpEhksA.png" style="zoom:67%;" /></p>
<p>在实现上，使用Adam优化器，<code>batch_size</code>为100，<code>epochs</code>为200。<code>Encoder</code>和<code>Decoder</code>都由三层感知机组成，超参数<span class="math inline">\(s^2\)</span>设置为400。评测标准使用AUC (area under the receiver characteristic curve)。</p>
<p>下表为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/YTpO98y1ZPmAK3g.png" style="zoom:67%;" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-29T03:36:22.000Z" title="2019-10-29 11:36:22 ├F10: AM┤">2019-10-29</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:20:44.374Z" title="2020-6-25 1:20:44 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/">Anomaly Detection in Streams with Extreme Value Theory</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>本文基于<strong>Extreme Value Theory</strong>提出了一种不需要手动设置阈值也不需要对数据分布作任何假设的时间序列异常检测方法。除此之外，本方法可以用在通用的自动阈值选择的场合中。</p>
<p><a target="_blank" rel="noopener" href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-in-streams-with-extreme-value-theory">原文</a></p>
<h1 id="background">Background</h1>
<p>在很多情况下我们需要进行阈值的选择。阈值的选择可以通过实验的方法或者对数据分布进行假设的方法来得到，不过这样做通常不准确。借助<strong>Extreme Value Theory</strong>我们可以在不需要对原始数据的分布作很强的假设的情况下，推断我们想要的极端事件的分布（在异常检测中就是异常值）。</p>
<p>下面给出一些数学符号，<span class="math inline">\(X\)</span>为随机变量，<span class="math inline">\(F\)</span>为累积分布函数，即<span class="math inline">\(F(x)=\mathbb{P}(X\leq x)\)</span>。记<span class="math inline">\(F\)</span>的“末尾”分布<span class="math inline">\(\bar{F}(x)=1-F(x)=\mathbb{P}(X&gt;x)\)</span>。对于一个随机变量<span class="math inline">\(X\)</span>和给定的概率<span class="math inline">\(q\)</span>，记<span class="math inline">\(z_q\)</span>为在<span class="math inline">\(1-q\)</span>水平的分位数，即<span class="math inline">\(z_q\)</span>为满足<span class="math inline">\(\mathbb{P}(X\leq z_q)\geq 1-q\)</span>最小的值。</p>
<h2 id="extreme-value-distributions">Extreme Value Distributions</h2>
<p><strong>Extreme Value Theory</strong>主要是为了找出极端事件发生的规律，有学者证明，在很弱的条件下，所有极端事件都服从一个特定的分布，而不管原始分布如何。具体形式如下：</p>
<p><span class="math display">\[
G_\gamma:x\mapsto \exp(-(1+\gamma x)^{-\frac{1}{\gamma}}), \space\space\space\space\space\gamma\in\mathbb{R}, \space\space\space\space\space 1+\gamma x&gt;0
\]</span></p>
<p>其中<span class="math inline">\(\gamma\)</span>称为<strong>Extreme Value Index</strong>，由原始分布决定。</p>
<p>更严谨的说法是Fisher-Tippett-Gnedenko定理（极值理论第一定理）：</p>
<blockquote>
<p><strong>THEOREM: </strong>(Fisher-Tippett-Gnedenko). 令<span class="math inline">\(X_1,X_2,\cdots,X_n,\cdots\)</span>为独立同分布的随机变量序列，<span class="math inline">\(M_n=\max \{X_1,\cdots,X_n\}\)</span>。如果实数对序列<span class="math inline">\((a_n,b_n)\)</span>存在且满足<span class="math inline">\(a_n&gt;0\)</span>和<span class="math inline">\(\lim\limits_{n\rightarrow \infty}P\left(\frac{M_n-b_n}{a_n}\leq x\right)=F(x)\)</span>，其中<span class="math inline">\(F\)</span>为非退化分布函数，那么<span class="math inline">\(F\)</span>属于Gumbel、Fréchet或Weibull分布族（或总称Generalized Extreme Value Distribution）中的一种。</p>
</blockquote>
<p>这是一个反直觉的结论，但是想到当事件发生变得极端时，即<span class="math inline">\(\mathbb{P}(X&gt;x)\rightarrow 0\)</span>，<span class="math inline">\(\bar{F}(x)=\mathbb{P}(X&gt;x)\)</span>分布的形状其实并没有很多种选择。Table 1展示了几种不同分布对应的<span class="math inline">\(\gamma\)</span>：</p>
<p><img src="https://i.loli.net/2020/06/24/jyhoWZGc2gFTrJv.png" /></p>
<p>Figure 1展示了几种不同<span class="math inline">\(\gamma\)</span>情况下的“末尾”分布：</p>
<p><img src="https://i.loli.net/2020/06/24/4rmZL1AMcBJ2Vzq.png" /></p>
<h2 id="power-of-evt">Power of EVT</h2>
<p>根据<strong>Extreme Value Theory</strong>，我们可以在原始分布未知的情况下计算极端事件的概率。但是<span class="math inline">\(\bar{G}_\gamma\)</span>分布中参数<span class="math inline">\(\gamma\)</span>是未知的，我们需要一种高效的方法来进行估计。<strong>The Peaks-Over-Threshold</strong> (POT) 方法是本文介绍的一种方法。</p>
<p><img src="https://i.loli.net/2020/06/24/hX2T1IkMAqfioZl.png" /></p>
<h2 id="peaks-over-threshold-approach">Peaks-Over-Threshold Approach</h2>
<p>POT方法依赖于Pickands-Balkema-De Haan定理（极值理论第二定理），维基百科版：</p>
<blockquote>
<p>考虑一个未知分布<span class="math inline">\(F\)</span>和随机变量<span class="math inline">\(X\)</span>，我们的目标是估计<span class="math inline">\(X\)</span>在超过确定阈值<span class="math inline">\(u\)</span>下的条件分布<span class="math inline">\(F_u\)</span>，定义为： <span class="math display">\[
F_u(y)=P(X-u\leq y|X&gt;u)=\frac{F(u+y)-F(u)}{1-F(u)}
\]</span> 其中<span class="math inline">\(0\leq y\leq x_F-u\)</span>，<span class="math inline">\(x_F\)</span>为<span class="math inline">\(F\)</span>的右端点。<span class="math inline">\(F_u\)</span>描述了超过特征阈值<span class="math inline">\(u\)</span>的分布，称为<strong>Conditional Excess Distribution Function</strong>。</p>
<p><strong>STATEMENT: </strong>(Pickands-Balkema-De Haan). 设<span class="math inline">\((X_1,X_2,\cdots)\)</span>为独立同分布随机变量序列，<span class="math inline">\(F_u\)</span>为相应的Conditional Excess Distribution Function。对于一大类的<span class="math inline">\(F\)</span>和很大的<span class="math inline">\(u\)</span>，<span class="math inline">\(F_u\)</span>能够很好的被Generalized Pareto Distribution所拟合： <span class="math display">\[
F_u(y)\rightarrow G_{k,\sigma}(y),\space\space \text{as } u\rightarrow \infty
\]</span> 其中： <span class="math display">\[
G_{k,\sigma}(y)=
\begin{cases}
1-(1+ky/\sigma)^{-1/k}, &amp;\text{if }k\neq 0\\
1-e^{-y/\sigma}, &amp;\text{if }k=0
\end{cases}
\]</span> 当<span class="math inline">\(k\geq 0\)</span>时<span class="math inline">\(\sigma&gt;0, y\geq 0\)</span>，<span class="math inline">\(k&lt;0\)</span>时<span class="math inline">\(0\leq y\leq -\sigma/k\)</span>。</p>
</blockquote>
<p>论文中给出的定理如下：</p>
<blockquote>
<p><strong>THEOREM: </strong>(Pickands-Balkema-De Haan). 累积概率密度函数<span class="math inline">\(F\in\mathcal{D}_\gamma\)</span>当且仅当函数<span class="math inline">\(\sigma\)</span>存在时，对所有<span class="math inline">\(x\in\mathbb{R}\)</span>在<span class="math inline">\(1+\gamma x&gt;0\)</span>的条件下有： <span class="math display">\[
\frac{\bar{F}(t+\sigma(t)x)}{\bar{F}(t)}\mathop{\rightarrow}\limits_{t\rightarrow\tau}(1+\gamma x)^{-\frac{1}{\gamma}}
\]</span></p>
</blockquote>
<p>上式可以写成如下形式： <span class="math display">\[
\bar{F}_t(x)=\mathbb{P}(X-t&gt;x|X&gt;t)\mathop{\sim}\limits_{t\rightarrow\tau}\left(1+\frac{\gamma x}{\sigma(t)}\right)^{-\frac{1}{\gamma}}
\]</span> 该式表明<span class="math inline">\(X\)</span>超过阈值<span class="math inline">\(t\)</span>的概率（写为<span class="math inline">\(X-t\)</span>）服从<strong>Generalized Pareto Distribution</strong> (GPD)，参数为<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\sigma\)</span>。POT主要是拟合GPD而不是EVT分布。</p>
<p>如果我们要估计参数<span class="math inline">\(\hat{\gamma}\)</span>和<span class="math inline">\(\hat{\sigma}\)</span>，分位数可以通过下式计算得到： <span class="math display">\[
z_q\simeq t+\frac{\hat{\sigma}}{\hat{\gamma}}\left(\left(\frac{qn}{N_t}\right)^{-\hat{\gamma}}-1\right)
\]</span></p>
<p>其中<span class="math inline">\(t\)</span>是一个“很高”的阈值，<span class="math inline">\(q\)</span>是给定的概率值，<span class="math inline">\(n\)</span>是所有观测样本的数量，<span class="math inline">\(N_t\)</span>是peaks的数量，即<span class="math inline">\(X_i&gt;t\)</span>的数量。为了进行高效的参数估计，文中使用了极大似然估计。</p>
<h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2>
<p>设<span class="math inline">\(X_1,\cdots,X_n\)</span>为独立同分布的随机变量，概率密度函数记为<span class="math inline">\(f_\theta\)</span>，<span class="math inline">\(\theta\)</span>为分布中的参数，那么似然函数可以写为：</p>
<p><span class="math display">\[
\mathcal{L}(X_1,\cdots,X_n;\theta)=\prod\limits_{i=1}^n f_\theta(X_i)
\]</span></p>
<p>在极大似然估计中，我们需要找到合适的参数使得似然函数最大化。在我们的问题中，似然函数如下： <span class="math display">\[
\log\mathcal{L}(\gamma,\sigma)=-N_t\log\sigma-\left(1+\frac{1}{\gamma}\right)\sum\limits_{i=1}^{N_t}\log\left(1+\frac{\gamma}{\sigma}Y_i\right)
\]</span> 其中<span class="math inline">\(Y_i&gt;0\)</span>表示<span class="math inline">\(X_i\)</span>超过阈值<span class="math inline">\(t\)</span>的部分。</p>
<p>文中使用了<strong>Grimshaw's Trick</strong>来将含两个参数的优化问题转换为只含一个参数的优化问题。记<span class="math inline">\(\ell(\gamma,\sigma)=\log\mathcal{L}(\gamma,\sigma)\)</span>，对于所有极值来说有<span class="math inline">\(\nabla \ell(\gamma, \sigma)=0\)</span>。Grimshaw's Trick表明对于满足<span class="math inline">\(\nabla \ell(\gamma, \sigma)=0\)</span>的一对<span class="math inline">\((\gamma^*,\sigma^*)\)</span>，<span class="math inline">\(x^*=\frac{\gamma^*}{\sigma^*}\)</span>为等式<span class="math inline">\(u(X)v(X)=1\)</span>的解，其中： <span class="math display">\[
\begin{align}
u(x)&amp;=\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\frac{1}{1+xY_i}\\
v(x)&amp;=1+\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\log(1+xY_i)
\end{align}
\]</span> 在找到满足该等式的解<span class="math inline">\(x^*\)</span>后，我们可以得到<span class="math inline">\(\gamma^*=v(x^*)-1\)</span>和<span class="math inline">\(\sigma^*=\gamma^*/x^*\)</span>，于是问题就变成了如何寻找方程的所有根。</p>
<p>因为<span class="math inline">\(\log\)</span>的存在，所以有<span class="math inline">\(1+xY_i&gt;0\)</span>。而<span class="math inline">\(Y_i\)</span>是正数，所以<span class="math inline">\(x^*\)</span>的范围一定在<span class="math inline">\(\left(-\frac{1}{Y^M},+\infty\right)\)</span>，其中<span class="math inline">\(Y^M=\max Y_i\)</span>。</p>
<p>Grimshaw（作者参考的一篇<a target="_blank" rel="noopener" href="https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485040">论文</a>）还给出了一个上界： <span class="math display">\[
x^*_{\text{max}}=2\frac{\bar{Y}-Y^m}{(Y^m)^2}
\]</span> 其中<span class="math inline">\(Y^m=\min Y_i\)</span>，<span class="math inline">\(\bar{Y}\)</span>为<span class="math inline">\(Y_i\)</span>的均值。详细的优化方法会在下文讨论。</p>
<p>背景部分到此结束，接下来的部分就是作者提出的新方法。</p>
<h1 id="methodology">Methodology</h1>
<p>Extreme Value Theory给出了在对原始分布未知的情况下估计使得<span class="math inline">\(\mathbb{P}(X&gt;z_q)&lt;q\)</span>的<span class="math inline">\(z_q\)</span>的方法。</p>
<p>本文据此提出了时间序列流的异常检测方法。首先根据已知的观测值<span class="math inline">\(X_1,\cdots,X_n\)</span>得到阈值<span class="math inline">\(z_q\)</span>，然后根据数据的特性运用两种不同方法来更新<span class="math inline">\(z_q\)</span>。对于平稳时间序列，使用SPOT；对于非平稳时间序列，使用DSPOT。</p>
<h2 id="initialization-step">Initialization Step</h2>
<p>在进行异常检测之前，需要根据已有的观测数据进行<span class="math inline">\(z_q\)</span>的估计。给定<span class="math inline">\(n\)</span>个观测值<span class="math inline">\(X_1,\cdots,X_n\)</span>和一个固定的概率值<span class="math inline">\(q\)</span>，我们的目标是估计阈值<span class="math inline">\(z_q\)</span>使得<span class="math inline">\(\mathbb{P}(X&gt;z_q)&lt;q\)</span>。其主要流程是首先设定一个较大的阈值<span class="math inline">\(t\)</span>，然后通过拟合GPD分布来计算<span class="math inline">\(z_q\)</span>。过程如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/24/fzeC8vuDtA6mEdl.png" /></p>
<p>算法流程如下所示：</p>
<p><img src="https://i.loli.net/2020/06/24/AEQpnZPiW3C4mr7.png" /></p>
<p><span class="math inline">\(Y_t\)</span>代表大于<span class="math inline">\(t\)</span>的观测值的集合，GPD分布的拟合使用了前文提到的Grimshaw's Trick。</p>
<h2 id="finding-anomalies-in-a-stream">Finding Anomalies in a Stream</h2>
<p>通过Initialization Step使用POT算法得到的<span class="math inline">\(z_q\)</span>，我们定义其为"Normality Bound"，用于后面的检测。在后面的步骤中，我们会根据新得到的观测值来更新<span class="math inline">\(z_q\)</span>。</p>
<h3 id="stationary-case">Stationary Case</h3>
<p>我们首先来讨论时间序列没有时间依赖性的情况（<span class="math inline">\(X_1,\cdots,X_n\)</span>之间独立同分布）。通过POT算法对所有观测值得到<span class="math inline">\(z_q\)</span>之后，Streaming POT (SPOT) 算法会检查<span class="math inline">\(X_n\)</span>之后的值（数据流场景，<span class="math inline">\(X_1,\cdots,X_n\)</span>是历史数据，还会有新的数据进来），如果大于<span class="math inline">\(z_q\)</span>，则将<span class="math inline">\(X_i\)</span>加入异常点集合中；如果大于<span class="math inline">\(t\)</span>但小于<span class="math inline">\(z_q\)</span>，则将<span class="math inline">\(X_i\)</span>加入观测值集合中，更新<span class="math inline">\(z_q\)</span>；其他情况我们<span class="math inline">\(X_i\)</span>是正常情况。算法流程图如下：</p>
<p><img src="https://i.loli.net/2020/06/24/h5yKnlCAYxbHu2R.png" /></p>
<h3 id="drifting-case">Drifting Case</h3>
<p>SPOT算法只适用于平稳分布的情况，但在现实生活中这样的假设过强了。于是作者提出了能处理时间依赖性的Streaming POT with Drift (DSPOT) 算法。</p>
<p><img src="https://i.loli.net/2020/06/25/O49XwQvVGH7k1ri.png" /></p>
<p>在DSPOT中，我们不使用<span class="math inline">\(X_i\)</span>的绝对值，而是用相对值<span class="math inline">\(X^\prime_i=X_i-M_i\)</span>，其中<span class="math inline">\(M_i\)</span>是<span class="math inline">\(i\)</span>时刻的局部特征，如Figure 4所示。最简单的实现是使用局部均值，即<span class="math inline">\(M_i=(1/d)\cdot\sum\limits_{k=1}^d X_{i-k}^*\)</span>，<span class="math inline">\(X_{i-1}^*,\cdots,X_{i-d}^*\)</span>是长度为<span class="math inline">\(d\)</span>的窗口。我们假设<span class="math inline">\(X^\prime_i\)</span>服从平稳分布的假设。</p>
<p>算法流程图如下所示：</p>
<p><img src="https://i.loli.net/2020/06/25/P6hOsD9dnNIHvUV.png" /></p>
<h2 id="numerical-optimization">Numerical Optimization</h2>
<p>现在剩下的问题就是优化了，前文已经提到对GPD的拟合已经被优化成一个参数的优化问题，下面将会详细讨论优化算法。</p>
<h3 id="reduction-of-the-optimal-parameters-search">Reduction of the Optimal Parameters Search</h3>
<p>前文已经得到了一个初步的<span class="math inline">\(x^*\)</span>的Bound，即<span class="math inline">\(x^*&gt;-\frac{1}{Y^M}\)</span>和<span class="math inline">\(x^*\leq 2\frac{\bar{Y}-Y^m}{(Y^m)^2}\)</span>，下面将给出一个更严格的Bound。</p>
<blockquote>
<p><strong>PROPOSITION: </strong>如果<span class="math inline">\(x^*\)</span>是<span class="math inline">\(u(x)v(x)=1\)</span>的解，那么： <span class="math display">\[
x^*\leq 0 \text{ or } x^*\geq 2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m}
\]</span></p>
</blockquote>
<p>证明见论文原文。</p>
<p>这样<span class="math inline">\(x^*\)</span>的范围就进一步缩小了，于是有<span class="math inline">\(u(x)v(X)=1\)</span>的解<span class="math inline">\(x^*\)</span>在以下范围之内： <span class="math display">\[
\left(-\frac{1}{Y^M},0\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]
\]</span></p>
<h3 id="how-can-we-maximize-the-likelihood-function">How Can We Maximize the Likelihood Function?</h3>
<p>接下来是优化的具体实现问题。文中首先设定了一个很小的值<span class="math inline">\(\epsilon&gt;0\space(\sim 10^{-8})\)</span>，然后在下面的范围内寻找函数<span class="math inline">\(w:x\mapsto u(x)v(x)-1\)</span>的根： <span class="math display">\[
\left[-\frac{1}{Y^M}+\epsilon,-\epsilon\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]
\]</span> 作者没有使用现有的寻找函数根的算法，而是转换为如下优化问题： <span class="math display">\[
\min\limits_{x_1,\cdots,x_k\in I}\sum\limits_{i=1}^k w(x_k)^2
\]</span> 其中<span class="math inline">\(I\)</span>就是<span class="math inline">\(x^*\)</span>的Bound。该问题是一个很典型的优化问题，可以被很多成熟的算法所解决。</p>
<h3 id="initial-threshold">Initial Threshold</h3>
<p>在算法的Initialization Step，需要事先设定一个阈值<span class="math inline">\(t\)</span>，如果设定的太大，那么<span class="math inline">\(Y_t\)</span>的数量就会很少。作者给出的建议是保证<span class="math inline">\(t&lt;z_q\)</span>，即<span class="math inline">\(t\)</span>对应的概率值应该小于<span class="math inline">\(1-q\)</span>。</p>
<h1 id="experiments">Experiments</h1>
<p>在实验部分，作者在合成数据和真实数据上试验了SPOT算法和DSPOT算法的有效性。</p>
<h2 id="dspot-reliability">(D)SPOT Reliability</h2>
<p>作者首先在合成数据上验证SPOT的有效性。具体做法是使用高斯分布生成数据（高斯分布的分位数能够直接计算），然后将SPOT得出的<span class="math inline">\(z_q\)</span>和理论值进行对比。误差定义如下： <span class="math display">\[
\text{error rate}=\left|\frac{z^{\text{SPOT}}-z^{\text{th}}}{z^{\text{th}}}\right|
\]</span> 下图是采用不同数量观测值的结果：</p>
<p><img src="https://i.loli.net/2020/06/25/GXlu2MAJaoqxyd4.png" /></p>
<h2 id="finding-anomalies-with-spot">Finding Anomalies with SPOT</h2>
<p>在这一节作者在真实数据集上进行了实验以验证SPOT算法的有效性，结果如下图：</p>
<p><img src="https://i.loli.net/2020/06/25/wTkZxKarFVDOlp6.png" /></p>
<p>在文中作者说算法的True Positive达到了<span class="math inline">\(86\%\)</span>，False Positive小于<span class="math inline">\(4\%\)</span>。</p>
<p><img src="https://i.loli.net/2020/06/25/RcUnwtHud7DjNXv.png" /></p>
<h2 id="finding-anomalies-with-dspot">Finding Anomalies with DSPOT</h2>
<p>在这一节作者使用DSPOT在真实数据集上进行了实验。窗口大小<span class="math inline">\(d=450\)</span>，预设的风险概率值<span class="math inline">\(q=10^{-3}\)</span>。结果如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/lIxqpnGtL7feVKs.png" /></p>
<p>在图中可以看出在<span class="math inline">\(8000\)</span> Minutes之后上界显著提高，作者分析了原因，认为是因为超过阈值<span class="math inline">\(t\)</span>的点<span class="math inline">\(Y_t\)</span>的存储是全局的，在前<span class="math inline">\(8000\)</span> Minutes算法存储了很多较高的<span class="math inline">\(Y_t\)</span>值，而在<span class="math inline">\(8000\)</span> Minutes之后，真实数据的趋势开始下降，但算法仍是根据全局的<span class="math inline">\(Y_t\)</span>来进行<span class="math inline">\(z_q\)</span>的计算（这一段没有特别明白）。作者给出的修正方法是只保存固定数量的Peaks。</p>
<p>下图是作者在股票数据上得到的实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/VeEo8OPbzyUxXrR.png" /></p>
<h2 id="performances">Performances</h2>
<p>作者还验证了算法的时间效率。</p>
<p><img src="https://i.loli.net/2020/06/25/Egh7CxsU2TtL6az.png" /></p>
<p>表中T代表的是每个Iteration的时间，M代表的是Peaks的比例，"bi-"前缀代表的是同时计算上界和下界。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-22T07:14:26.000Z" title="2019-10-22 3:14:26 ├F10: PM┤">2019-10-22</time>发表</span><span class="level-item"><time dateTime="2020-06-24T08:16:51.584Z" title="2020-6-24 4:16:51 ├F10: PM┤">2020-06-24</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Tutorial/">Tutorial</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/22/An-Introduction-to-Variational-Autoencoders/">An Introduction to Variational Autoencoders</a></h1><div class="content"><h1 id="deep-generative-models">Deep Generative Models</h1>
<p>生成模型是指一系列用于随机生成可观测数据的模型。假设在一个高维空间<span class="math inline">\(\mathcal{X}\)</span>中，存在一个随机向量<span class="math inline">\(\mathbf{X}\)</span>服从一个未知的分布<span class="math inline">\(p_r(x),x\in \mathcal{X}\)</span>。生成模型就是根据一些可观测的样本<span class="math inline">\(x^{(1)},x^{(2)},\cdots,x^{(N)}\)</span>来学习一个参数化的模型<span class="math inline">\(p_\theta(x)\)</span>来近似未知分布<span class="math inline">\(p_r(x)\)</span>。</p>
<p>生成模型主要用于密度估计和样本生成。</p>
<hr />
<p>密度估计即给定一组数据<span class="math inline">\(\mathcal{D}=\{x^{(i)}\},1\leq i\leq N\)</span>，假设他们都是从相同的概率密度函数<span class="math inline">\(p_r(x)\)</span>独立产生的。密度估计就是根据数据集<span class="math inline">\(\mathcal{D}\)</span>来估计其概率密度函数<span class="math inline">\(p_r(x)\)</span>。</p>
<p>如果将生成模型用于监督学习，那么就是输出标签的条件概率分布<span class="math inline">\(p(y|x)\)</span>，根据贝叶斯公式：</p>
<p><span class="math display">\[p(y|x)=\frac{p(x,y)}{\sum_y p(x,y)}\]</span></p>
<p>问题就变为了联合概率<span class="math inline">\(p(x,y)\)</span>的密度估计问题。</p>
<hr />
<p>样本生成即根据给定的概率分布<span class="math inline">\(p_\theta(x)\)</span>生成一些服从这个分布的样本，即采样。在含隐变量的生成模型中，生成<span class="math inline">\(x\)</span>的过程一般包含两步：</p>
<ol type="1">
<li>根据隐变量的分布<span class="math inline">\(p_\theta(z)\)</span>采样得到<span class="math inline">\(z\)</span>；</li>
<li>根据条件分布<span class="math inline">\(p_\theta(x|z;\theta)\)</span>进行采样得到<span class="math inline">\(x\)</span>。</li>
</ol>
<p>所以在生成模型中的重点是估计条件分布<span class="math inline">\(p(x|z;\theta)\)</span>。</p>
<h1 id="parameter-estimation-for-hidden-variable-with-em-algorithm">Parameter Estimation for Hidden Variable with EM Algorithm</h1>
<p>如果图模型中存在隐变量，就需要使用EM算法进行参数估计。</p>
<p>在一个包含隐变量的图模型中，令<span class="math inline">\(\mathbf{X}\)</span>为可观测变量集合，<span class="math inline">\(\mathbf{Z}\)</span>为隐变量集合，则一个样本<span class="math inline">\(x\)</span>的边际似然函数为：</p>
<p><span class="math display">\[p(x;\theta)=\sum_z p(x,z;\theta)\]</span></p>
<p>给定包含<span class="math inline">\(N\)</span>个训练样本的训练集<span class="math inline">\(\mathcal{D}=\{x^{(n)}\},1\leq i\leq N\)</span>，则训练集的对数边际似然为：</p>
<p><span class="math display">\[\begin{align}\mathcal{L}(\mathcal{D};\theta)&amp;=\frac{1}{N}\sum_{n=1}^N \log p(x^{(n)};\theta)\\&amp;=\frac{1}{N}\sum_{n=1}^N \log \sum_z p(x^{(n)},z;\theta)\end{align}\]</span></p>
<hr />
<p>这时，只要最大化整个训练集的对数边际似然<span class="math inline">\(\mathcal{L}(\mathcal{D};\theta)\)</span>，即可估计出最优的参数<span class="math inline">\(\theta^*\)</span>。不过在计算梯度的时候，需要在对数函数内部进行求和或积分计算。为了更好的计算<span class="math inline">\(\log p(x;\theta)\)</span>，我们引入一个额外的变分函数<span class="math inline">\(q(z)\)</span>，<span class="math inline">\(q(z)\)</span>为定义在隐变量<span class="math inline">\(z\)</span>上的分布。样本<span class="math inline">\(x\)</span>的对数边际似然函数为：</p>
<p><span class="math display">\[\begin{align}\log p(x;\theta)&amp;=\log \sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\\&amp;\geq\sum_z q(z)\log \frac{p(x,z;\theta)}{q(z)}\\&amp;\triangleq ELBO(q,x;\theta)\end{align}\]</span></p>
<p>其中<span class="math inline">\(ELBO(q,x;\theta)\)</span>为对数边际似然函数<span class="math inline">\(\log p(x;\theta)\)</span>的下界，称为证据下界。公式中使用了Jensen不等式(即对于凹函数<span class="math inline">\(g\)</span>，有<span class="math inline">\(g(\mathbb{E}[x])\geq\mathbb{E}[g(X)]\)</span>)。在这里，<span class="math inline">\(\frac{p(x,z;\theta)}{q(z)}\)</span>可视为<span class="math inline">\(q(z)\)</span>的函数，记为<span class="math inline">\(f(q(z))\)</span>，那么<span class="math inline">\(f(q(z))\)</span>的期望即<span class="math inline">\(\mathbb{E}[f(q(z))]=\sum_z q(z)f(q(z))=\sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\)</span>。而根据Jensen不等式，有<span class="math inline">\(g(\mathbb{E}[f(q(z))])\geq\mathbb{E}[g(f(q(z)))]\Leftrightarrow g(\sum_z q(z)\frac{p(x,z;\theta)}{q(z)})\geq \sum_z q(z)g(\frac{p(x,z;\theta)}{q(z)})\)</span>，在这里<span class="math inline">\(g\)</span>就是对数函数。</p>
<hr />
<p>根据Jensen不等式取等的条件：<span class="math inline">\(\frac{p(x,z;\theta)}{q(z)}=c\)</span>，<span class="math inline">\(c\)</span>为常数，有：</p>
<p><span class="math display">\[\begin{align}\sum_z p(x,z;\theta)&amp;=c\sum_z q(z)\\\Leftrightarrow\sum_z p(x,z;\theta)&amp;=c\cdot1\end{align}\]</span></p>
<p>因此：</p>
<p><span class="math display">\[\begin{align}q(z)&amp;=\frac{p(x,z;\theta)}{\sum_z p(x,z;\theta)}\\&amp;=\frac{p(x,z;\theta)}{p(x;\theta)}\\&amp;=p(z|x;\theta)\end{align}\]</span></p>
<p>所以，当且仅当<span class="math inline">\(q(z)=p(z|x;\theta)\)</span>时，<span class="math inline">\(\log p(x;\theta)=ELBO(q,x;\theta)\)</span>。</p>
<hr />
<p>于是最大化对数边际似然函数<span class="math inline">\(\log p(x;\theta)\)</span>的过程可以分解为两个步骤：</p>
<ol type="1">
<li>先找到近似分布<span class="math inline">\(q(z)\)</span>使得<span class="math inline">\(\log p(x;\theta)=ELBO(q,x;\theta)\)</span>；</li>
<li>再寻找参数<span class="math inline">\(\theta\)</span>最大化<span class="math inline">\(ELBO(q,x;\theta)\)</span>。</li>
</ol>
<p>这就是期望最大化(Expectation-Maximum,EM)算法。</p>
<hr />
<p>EM算法通过迭代的方法，不断重复直到收敛到某个局部最优解。在第<span class="math inline">\(t\)</span>步更新时，E步和M步分别为：</p>
<ol type="1">
<li><p>E步：固定参数<span class="math inline">\(\theta_t\)</span>，找到一个分布使<span class="math inline">\(ELBO(q,x;\theta_t)\)</span>最大，即等于<span class="math inline">\(\log p(x;\theta_t)\)</span>：<span class="math inline">\(q_{t+1}(z)=\text{arg}_q \max ELBO(q,x;\theta_t)\)</span>；</p></li>
<li><p>M步：固定<span class="math inline">\(q_{t+1}(z)\)</span>，找到一组参数使得证据下界最大，即：<span class="math inline">\(\theta_{t+1}=\text{arg}_\theta\max ELBO(q_{t+1},x;\theta)\)</span>。</p></li>
</ol>
<hr />
<p>对数边际似然也可以通过信息论的视角来进行分解：</p>
<p><span class="math display">\[\begin{align}\log p(x;\theta)&amp;=\sum_z q(z)\log p(x;\theta)\\&amp;=\sum_z q(z)(\log p(x,z;\theta)-\log p(z|x;\theta))\\&amp;=\sum_z q(z)\log\frac{p(x,z;\theta)}{q(z)}-\sum_z q(z)\log\frac{p(z|x;\theta)}{q(z)}\\&amp;=ELBO(q,x;\theta)+D_{KL}(q(z)\parallel p(z|x;\theta))\end{align}\]</span></p>
<p>其中<span class="math inline">\(D_{KL}(q(z)\parallel p(z|x;\theta))\)</span></p>
<h1 id="generative-model-with-hidden-variable">Generative Model with Hidden Variable</h1>
<p>假设一个生成模型包含不可观测的隐变量，其中可观测变量<span class="math inline">\(x\)</span>为一个高维空间中的随机向量，而不可观测的隐变量<span class="math inline">\(z\)</span>为一个相对低维空间中的随机向量。</p>
<p>这个生成模型的联合概率密度函数可以表达为：</p>
<p><span class="math display">\[p(x,z;\theta)=p(x|z;\theta)p(z;\theta)\]</span></p>
<p>其中<span class="math inline">\(p(z;\theta)\)</span>为隐变量<span class="math inline">\(z\)</span>的先验概率分布；<span class="math inline">\(p(x|z;\theta)\)</span>为已知<span class="math inline">\(z\)</span>条件下<span class="math inline">\(x\)</span>的概率分布。通常情况下，我们可以假设<span class="math inline">\(p(z;\theta)\)</span>和<span class="math inline">\(p(x|z;\theta)\)</span>服从某种带参的分布族，其形式已知，而参数可以通过最大似然来进行估计。</p>
<p>给定一个样本<span class="math inline">\(x\)</span>，其对数边际似然<span class="math inline">\(\log p(x;\theta)\)</span>可以分解为：</p>
<p><span class="math display">\[\log p(x;\theta)=ELBO(q,x;\theta,\phi)+D_{KL}(q(z;\phi)\parallel p(z|x;\theta))\]</span></p>
<p>其中<span class="math inline">\(q(z;\phi)\)</span>为额外引入的变分密度函数，<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>为证据下界：</p>
<p><span class="math display">\[ELBO(q,x;\theta,\phi)=\mathbb{E}_{z\sim q(z;\phi)}[\log{\frac{p(x,z;\theta)}{q(z;\phi)}}]\]</span></p>
<p>最大化<span class="math inline">\(\log p(x;\theta)\)</span>可以用EM算法来求解：</p>
<ul>
<li><strong>E-step:</strong> 寻找一个密度函数<span class="math inline">\(q(z;\phi)\)</span>使其等于或接近于后验密度函数<span class="math inline">\(p(z|x;\theta)\)</span>;</li>
<li><strong>M-step:</strong> 保持<span class="math inline">\(q(z;\phi)\)</span>固定，寻找<span class="math inline">\(\theta\)</span>来最大化<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>。</li>
</ul>
<p>在EM算法的每次迭代中，理论上最优的<span class="math inline">\(q(z;\phi)\)</span>为隐变量的后验概率密度函数<span class="math inline">\(p(z|x;\theta)\)</span>：</p>
<p><span class="math display">\[p(z|x;\theta)=\frac{p(x|z;\theta)p(z;\theta)}{\int_z p(x|z;\theta)p(z;\theta)\text{d}z}\]</span></p>
<p>后验密度函数<span class="math inline">\(p(z|x;\theta)\)</span>的计算是一个统计推断的问题，在一般情况下<span class="math inline">\(p(x|z;\theta)\)</span>也比较难以计算。</p>
<h1 id="variational-autoencoder">Variational Autoencoder</h1>
<p>变分自编码器(Variational Autoencoder, VAE)的主要思想是利用神经网络来分别建模两个复杂的条件概率密度函数：</p>
<ol type="1">
<li>用神经网络来产生变分分布<span class="math inline">\(q(z;\phi)\)</span>，称为推断网络。推断网络的输入为<span class="math inline">\(x\)</span>，输出为变分分布<span class="math inline">\(q(z|x;\phi)\)</span>；</li>
<li>用神经网络来产生概率分布<span class="math inline">\(p(x|z;\theta)\)</span>，称为生成网络。生成网络的输入为<span class="math inline">\(z\)</span>，输出为概率分布<span class="math inline">\(p(x|z;\theta)\)</span>。</li>
</ol>
<p><img src="https://i.loli.net/2020/06/24/B1d9UtTzNfjG6e2.png" /></p>
<p>VAE的图模型如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/24/GAhy281seQ3tbZT.png" /></p>
<h2 id="variational-network">Variational Network</h2>
<p>假设<span class="math inline">\(q(z|x;\phi)\)</span>是服从对角化协方差的高斯分布：</p>
<p><span class="math display">\[q(z|x;\phi)=\mathcal{N}(z;\mu_I,\sigma^2_I I)\]</span></p>
<p>其中<span class="math inline">\(\mu_I\)</span>和<span class="math inline">\(\sigma_I^2\)</span>是高斯分布的均值和方差，可以通过推断网络<span class="math inline">\(f_I(x;\phi)\)</span>来预测：</p>
<p><span class="math display">\[
\left[\begin{matrix}\mu_I\\\sigma_I\end{matrix}\right]=f_I(x;\phi)
\]</span> 推断网络<span class="math inline">\(f_I(x;\phi)\)</span>可以是一般的全连接网络或卷积网络，比如一个两层的神经网络：</p>
<p><span class="math display">\[\begin{align}h&amp;=\sigma(W^{(1)}x+b^{(1)})\\\mu_I&amp;=W^{(2)}h+b^{(2)}\\\sigma_I&amp;=\text{softplus}(W^{(3)}h+b^{(3)})\end{align}\]</span></p>
<p>其中所有网络参数<span class="math inline">\(\{W^{(1)},W^{(2)},W^{(3)},b^{(1)},b^{(2)},b^{(3)}\}\)</span>即对应了变分参数<span class="math inline">\(\phi\)</span>。</p>
<hr />
<p>推断网络的目标是使得<span class="math inline">\(q(z|x;\phi)\)</span>来尽可能接近真实的后验<span class="math inline">\(p(z|x;\theta)\)</span>，需要找到变分参数<span class="math inline">\(\phi^*\)</span>来最小化两个分布的KL散度：</p>
<p><span class="math display">\[\phi^*=\text{arg}_\phi\min{D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))}\]</span></p>
<p>由于<span class="math inline">\(p(z|x;\theta)\)</span>未知，故KL散度无法直接计算，不过由于<span class="math inline">\(D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))=\log p(x;\theta)-ELBO(q,x;\theta,\phi)\)</span>，所以可以直接最大化证据下界，有：</p>
<p><span class="math display">\[\phi^*=\text{arg}_\phi\max{ELBO(q,x;\theta,\phi)}\]</span></p>
<h2 id="generative-network">Generative Network</h2>
<p>生成模型的联合分布可以分解为两部分：隐变量<span class="math inline">\(z\)</span>的先验分布<span class="math inline">\(p(z;\theta)\)</span>和条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>。为简单起见，一般假设隐变量<span class="math inline">\(z\)</span>的先验分布为标准正态分布<span class="math inline">\(\mathcal{N}(z|0,I)\)</span>，隐变量每一维之间都是独立的。条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>可以通过生成网络来建模，我们同样用参数化的分布族来表示条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>，这些分布族的函数可以用生成网络计算得到。根据变量<span class="math inline">\(x\)</span>的类型不同，可以假设<span class="math inline">\(p(x|z;\theta)\)</span>服从不同的分布族。如果<span class="math inline">\(x\in\{0,1\}^d\)</span>是<span class="math inline">\(d\)</span>维的二值向量，可以假设<span class="math inline">\(\log p(x|z;\theta)\)</span>服从多变量的伯努利分布，即：</p>
<p><span class="math display">\[\begin{align}p(x|z;\theta)&amp;=\prod\limits_{i=1}^d p(x_i|z;\theta)\\&amp;=\prod\limits_{i=1}^d \gamma_i^{x_i}(1-\gamma_i)^{(1-x_i)}\end{align}\]</span></p>
<p>如果<span class="math inline">\(x\in\mathbb{R}^d\)</span>是<span class="math inline">\(d\)</span>维的连续向量，可以假设<span class="math inline">\(p(x|z;\theta)\)</span>服从对角化协方差的高斯分布，即：</p>
<p><span class="math display">\[p(x|z;\theta)=\mathcal{N}(x;\mu_G,\sigma_G^2 I)\]</span></p>
<hr />
<p>生成网络的目标是找到一组<span class="math inline">\(\theta^*\)</span>最大化证据下界<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>：</p>
<p><span class="math display">\[\theta^*=\text{arg}_\theta\max ELBO(q,x;\theta,\phi)\]</span></p>
<h2 id="model-combination">Model Combination</h2>
<p>推断网络和生成网络的目标都是最大化证据下界因此总的目标函数为：</p>
<p><span class="math display">\[\begin{align}\max_{\theta,\phi}ELBO(q,x;\theta,\phi)&amp;=\max_{\theta,\phi}\mathbb{E}_{z\sim q(z;\phi)}[\log\frac{p(x|z;\theta)p(z;\theta)}{q(z;\theta)}]\\&amp;=\max_{\theta,\phi}\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]-D_{KL}(q(z|x;\phi)\parallel p(z;\theta))\end{align}\]</span></p>
<p>其中先验分布<span class="math inline">\(p(z;\theta)=\mathcal{N}(z|0,I)\)</span>。</p>
<p>公式中<span class="math inline">\(\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]\)</span>一般通过采样的方式进行计算，最后取平均值。</p>
<h2 id="model-training">Model Training</h2>
<p>给定数据集<span class="math inline">\(\mathcal{D}\)</span>，包含<span class="math inline">\(N\)</span>个从未知数据分布中抽取的独立同分布样本<span class="math inline">\(x^{(1)},x^{(2)},\cdots,x^{(N)}\)</span>。变分自编码器的目标函数为：</p>
<p><span class="math display">\[\mathcal{J}(\phi,\theta|\mathcal{D})=\sum\limits_{n=1}^N(\frac{1}{M}\sum\limits_{m=1}^M\log p(x^{(n)}|z^{(n,m)};\theta)-D_{KL}(q(z|x^{(n)};\phi)\parallel\mathcal{N}(z;0,I)))\]</span></p>
<p>如果采用随机梯度下降法，每次从数据集中采一个样本<span class="math inline">\(x\)</span>，然后根据<span class="math inline">\(q(z|x;\phi)\)</span>采一个隐变量<span class="math inline">\(z\)</span>，则目标函数变为：</p>
<p><span class="math display">\[\mathcal{J}(\phi,\theta|x)=\log p(x|z;\theta)-D_{KL}(q(z|x;\phi)\parallel\mathcal{N}(z;0,I))\]</span></p>
<p>假设<span class="math inline">\(q(z|x;\phi)\)</span>是正态分布，KL散度可直接算出：</p>
<p><span class="math display">\[D_{KL}(\mathcal{N}(\mu_1,\Sigma_1)\parallel\mathcal(\mu_2,\Sigma_2))\\=\frac{1}{2}(\text{tr}(\sigma_I^2 I)+\mu_I^T\mu_I-d-\log(|\sigma_I^2 I|))\]</span></p>
<hr />
<p>再参数化是将一个参数为<span class="math inline">\(u\)</span>的函数<span class="math inline">\(f(u)\)</span>，通过一个函数<span class="math inline">\(u=g(v)\)</span>，转换为参数为<span class="math inline">\(v\)</span>的函数<span class="math inline">\(\hat{f}(v)=f(g(v))\)</span>。在变分自编码器中，一个问题是如何求随机变量<span class="math inline">\(z\)</span>关于<span class="math inline">\(\phi\)</span>的导数。但由于是采样的方式，无法直接刻画<span class="math inline">\(z\)</span>和<span class="math inline">\(\phi\)</span>之间的函数关系，因此也无法计算导数。</p>
<p>如果<span class="math inline">\(z\sim q(z|x;\phi)\)</span>的随机性独立于参数<span class="math inline">\(\phi\)</span>，我们可以通过再参数化的方法来计算导数。假设<span class="math inline">\(q(z|x;\phi)\)</span>为正态分布<span class="math inline">\(\mathcal{N}(\mu_I,\sigma^2_I I)\)</span>，其中<span class="math inline">\(\mu_I\)</span>和<span class="math inline">\(\sigma_I\)</span>是推断网络<span class="math inline">\(f_I(x;\phi)\)</span>的输出。我们可以通过下面的方式采样<span class="math inline">\(z\)</span>：</p>
<p><span class="math display">\[z=\mu_I+\sigma_I\odot \varepsilon\]</span></p>
<p>其中<span class="math inline">\(\varepsilon\sim\mathcal{N}(0,I)\)</span>。这样<span class="math inline">\(z\)</span>和<span class="math inline">\(\mu_I,\sigma_I\)</span>的关系从采样关系变为函数关系。</p>
<hr />
<p>如果进一步假设<span class="math inline">\(p(x|z;\theta)\)</span>服从高斯分布<span class="math inline">\(\mathcal{N}(x|\mu_G,I)\)</span>，其中<span class="math inline">\(\mu_G=f_G(z;\theta)\)</span>是生成网络的输出，则目标函数可以简化为：</p>
<p><span class="math display">\[\mathcal{J}(\phi,\theta|x)=-\parallel x-\mu_G\parallel^2+D_{KL}(\mathcal{N}(\mu_I,\sigma_I)\parallel\mathcal{N}(0,I))\]</span></p>
<p>其中第一项可以近似看作是输入<span class="math inline">\(x\)</span>的重构正确性，第二项可以看作是正则化项。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-18T15:17:07.000Z" title="2019-10-18 11:17:07 ├F10: PM┤">2019-10-18</time>发表</span><span class="level-item"><time dateTime="2020-06-25T08:25:06.176Z" title="2020-6-25 4:25:06 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/RNN/">RNN</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/18/Recurrent-Neural-Networks-for-Multivariate-Time-Series-with-Missing-Values/">Recurrent Neural Networks for Multivariate Time Series with Missing Values</a></h1><div class="content"><h1 id="abstract">Abstract</h1>
<p>文中提出了一种可以处理带缺失值多为时间序列的GRU模型：<strong>GRU-D</strong>。本模型不仅可以捕捉时间序列中的长期依赖模式，并且还能利用时间序列中的缺失模式来达到更好的时间序列预测效果。</p>
<p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41598-018-24271-9">原文</a></p>
<h1 id="methodology">Methodology</h1>
<h2 id="notations">Notations</h2>
<p>记包含<span class="math inline">\(D\)</span>个变量的多变量时间序列为<span class="math inline">\(X=(x_1,x_2,\cdots,x_T)^T\in\mathbb{R}^{T\times D}\)</span>，其中对于每个<span class="math inline">\(t\in\{1,2,\cdots,T\},x_t\in\mathbb{R}^D\)</span>表示时间序列在时间<span class="math inline">\(t\)</span>的观测值，<span class="math inline">\(x_t^d\)</span>表示<span class="math inline">\(x_t\)</span>的第<span class="math inline">\(d\)</span>个成分。记<span class="math inline">\(s_t\in\mathbb{R}\)</span>为<span class="math inline">\(t\)</span>时刻的时间戳，并假设第一个观测值的时间戳为<span class="math inline">\(0\)</span>。对于包含缺失值的时间序列，我们用<strong>Masking Vector</strong> <span class="math inline">\(m_t\in\{0,1\}\)</span>进行标记，同时对每个<span class="math inline">\(x_t^d\)</span>维护距离上一个观测值的<strong>Time Interval</strong> <span class="math inline">\(\delta_t^d\in\mathbb{R}\)</span>，公式如下： <span class="math display">\[
m_t^d=\begin{cases}1, &amp;\text{if }x_t^d\text{ is observed}\\0, &amp;\text{otherwise}\end{cases}
\]</span></p>
<p><span class="math display">\[
\delta_t^d=\begin{cases}s_t-s_{t-1}+\delta_{t-1}^d, &amp;t&gt;1,m_{t-1}^d=0\\s_t-s_{t-1}, &amp;t&gt;1, m_{t-1}^d=1\\0, &amp;t=1\end{cases}
\]</span></p>
<p>下图是一些示例：</p>
<p><img src="https://i.loli.net/2020/06/25/C4FKQw2AZ9xkalo.png" /></p>
<p>在本文中，我们主要关注时间序列的分类问题，即给定数据集<span class="math inline">\(\mathcal{D}=\{(X_n,s_n,M_n)\}_{n=1}^N\)</span>，我们要对每个样本的类别进行预测<span class="math inline">\(l_n\in\{1,\cdots,L\}\)</span>。</p>
<h2 id="gru-rnn-for-time-series-classification">GRU-RNN for Time Series Classification</h2>
<p>GRU是一种改进版本的RNN，其最大不同是加入了门控机制。GRU单元的结构如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/wpKQsxEklizVTtm.png" style="zoom: 33%;" /></p>
<p>GRU包含了重置门和更新门，其中重置门<span class="math inline">\(R_t\)</span>负责控制上一时间的隐状态<span class="math inline">\(h_{t-1}\)</span>有多少部分需要保留，而更新门则决定由<span class="math inline">\(R_t\)</span>计算出来的候选隐状态<span class="math inline">\(\tilde{h}_t\)</span>有多少部分需要保留。最后当前时间的隐状态由<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(\tilde{h}_t\)</span>共同算出。GRU的状态更新公式如下： <span class="math display">\[
\begin{align}
R_t&amp;=\sigma(W_rx_t+U_rh_{t-1}+b_r)\\
Z_t&amp;=\sigma(W_zx_t+U_zh_{t-1}+b_z)\\
\tilde{h}_t&amp;=\text{tanh}(Wx_t+U(R_t\odot h_{t-1})+b)\\
h_t&amp;=(1-Z_t)\odot h_{t-1}+Z_t\odot \tilde{h}_t
\end{align}
\]</span> 文中提出了一些处理缺失值的简单方法：</p>
<ol type="1">
<li>直接用均值替代：<span class="math inline">\(x_t^d\leftarrow m_t^dx_t^d+(1-m_t^d)\tilde{x}^d\)</span>，其中<span class="math inline">\(\tilde{x}^d=\frac{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d x_{t,n}^d}{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d\tilde{x}^d}\)</span>。这种方法称为<strong>GRU-Mean</strong>；</li>
<li>用上一个观测值替代：<span class="math inline">\(x_t^d\leftarrow m_t^d x_t^d+(1-m_t^d)x_{t^\prime}^d\)</span>。这种方法称为<strong>GRU-Forward</strong>；</li>
<li>不填充，将是否缺失，距离上一个观测值的时间作为额外信息输入：<span class="math inline">\(x_t^{(n)}\leftarrow[x_t^{(n)};m_t^{(n)};\delta_t^{(n)}]\)</span>。这种方法称为<strong>GRU-Simple</strong>。</li>
</ol>
<h3 id="gru-d-model-with-trainable-decays">GRU-D: Model with Trainable Decays</h3>
<p>文中提出了时间序列缺失值的两个性质：一个是在上一个观测值距离很远的情况下缺失值倾向于接近一个默认的值，第二个是缺失值的影响会随着时间减弱。为了体现上述两点，文中提出了GPU-D模型，模型框架如下：</p>
<p><img src="https://i.loli.net/2020/06/25/aXbS4ADkLfeHPCR.png" style="zoom:67%;" /></p>
<p>在模型中，<strong>Decay Rates</strong>被设定为一个带参数的函数和GRU一起训练： <span class="math display">\[
\gamma_t=\exp\{-\max(0,W_\gamma\delta_t+b_\gamma)\}
\]</span></p>
<p><span class="math display">\[
\hat{x}_t^d=m_t^dx_t^d+(1-m_t^d)(\gamma_{x_t}^dx_{t^\prime}^d+(1-\gamma_{x_t}^d)\tilde{x}^d)
\]</span> 其中<span class="math inline">\(x_{t^\prime}^d\)</span>是第<span class="math inline">\(d\)</span>个变量的上一个观测值，<span class="math inline">\(\tilde{x}^d\)</span>是第<span class="math inline">\(d\)</span>个变量的经验均值。这样<span class="math inline">\(\hat{x}_t^d\)</span>就代表经过<strong>Input Decay</strong>的输入。</p>
<p>文中提到只用<strong>Input Decay</strong>是不够的，除此之外作者还使用了<strong>Hidden State Decay</strong>，即对<span class="math inline">\(h_{t-1}\)</span>进行Decay，公式如下： <span class="math display">\[
\hat{h}_{t-1}=\gamma_{h_t}\odot h_{t-1}
\]</span> 用Decay之后的<span class="math inline">\(\hat{x}_t\)</span>和<span class="math inline">\(\hat{h}_{t-1}\)</span>替换原始的GRU公式就得到了GRU-D模型： <span class="math display">\[
\begin{align}
R_t&amp;=\sigma(W_r\hat{x}_t-U_r\hat{h}_{t-1}+V_rm_t+b_r)\\
Z_t&amp;=\sigma(W_z\hat{x}_t+U_z\hat{h}_{t-1}+V_zm_t+b_z)\\
\tilde{h}_t&amp;=\text{tanh}(W\hat{x}_t+U(R_t\odot \hat{h}_{t-1})+Vm_t+b)\\
h_t&amp;=(1-z_t)\odot \hat{h}_{t-1}+z_t\odot\tilde{h}_t
\end{align}
\]</span></p>
<h1 id="experiments">Experiments</h1>
<h2 id="baseline-imputation-methods">Baseline Imputation Methods</h2>
<p>下图为文中比较中用到的Baseline：</p>
<p><img src="https://i.loli.net/2020/06/25/BFQdwMOXLc15mnl.png" /></p>
<h2 id="baseline-prediction-methods">Baseline Prediction Methods</h2>
<p>下图为文中用到的用来预测的Baseline：</p>
<p><img src="https://i.loli.net/2020/06/25/qFxDRBLvNpAIjMs.png" /></p>
<h2 id="results">Results</h2>
<p>文中用到的数据集如下：</p>
<ul>
<li><em>Gesture phase segmentation dataset (Gesture)</em>.</li>
<li><em>PhysioNet Challenge 2012 dataset (PhysioNet)</em>.</li>
<li><em>MIMIC-Ⅲ dataset (MIMIC-Ⅲ)</em>.</li>
</ul>
<p>下图展示了不同方法在人工合成数据集上的表现：</p>
<p><img src="https://i.loli.net/2020/06/25/6GVYuxoFP5yHAjf.png" /></p>
<p>下表展示了不同模型在预测任务表现的对比：</p>
<p><img src="https://i.loli.net/2020/06/25/qTZsgtGewh19Y8V.png" style="zoom: 67%;" /></p>
<p>下表展示了不同方法在MIMIC-Ⅲ和PhysioNet数据集上的多任务表现：</p>
<p><img src="https://i.loli.net/2020/06/25/dQiqebwYTCVfm5W.png" /></p>
<p>下图分别展示了模型学到的<strong>Input Decay</strong>和<strong>Hidden State Decay</strong>：</p>
<p><img src="https://i.loli.net/2020/06/25/2F7MrgOXA9nuZqC.png" style="zoom:67%;" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-18T15:00:57.000Z" title="2019-10-18 11:00:57 ├F10: PM┤">2019-10-18</time>发表</span><span class="level-item"><time dateTime="2020-09-12T02:54:14.061Z" title="2020-9-12 10:54:14 ├F10: AM┤">2020-09-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/18/Robust-Anomaly-Detection-for-Multivariate-Time-Series-through-Stochastic-Recurrent-Neural-Network/">Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</a></h1><div class="content"><h1 id="abstract">Abstract</h1>
<p>本文提出了<em>OmniAnomaly</em>：一种针对多变量时间序列的随机循环神经网络异常检测算法。该模型运用了一系列技术来捕捉多变量时间序列的正常模式，并在检测阶段基于重构误差来检测异常，同时本文还提供了一定的理论解释。</p>
<p><a target="_blank" rel="noopener" href="https://www.kdd.org/kdd2019/accepted-papers/view/robust-anomaly-detection-for-multivariate-time-series-through-stochastic-re">原文</a></p>
<h1 id="contribution">Contribution</h1>
<ol type="1">
<li>提出了<em>OmniAnomaly</em>，一种基于随机循环神经网络的多变量时间序列异常检测算法；</li>
<li>提出了针对多变量时间序列异常检测的解释方法；</li>
<li>通过实验证明了<em>OmniAnomaly</em>中所用的关键技术的有效性，包括GRU，planar NF, stochastic variable connection和adjusted Peaks-Over-Threshold method；</li>
<li>通过大量的实验我们证明了<em>OmniAnomaly</em>的有效性；</li>
<li>发布了代码和数据集。</li>
</ol>
<h1 id="background">Background</h1>
<h2 id="linear-gaussian-state-space-model">Linear Gaussian State Space Model</h2>
<p>状态空间模型（State Space Model, SSM）的概念来自于控制理论，在这里我们主要讨论其在时间序列中的应用。其大概思想是我们认为时间序列在时刻<span class="math inline">\(t\)</span>的观测值<span class="math inline">\(z_t\)</span>是一个隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>的条件分布<span class="math inline">\(p(z_t|\boldsymbol{l}_t)\)</span>，而这个隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>刻画了时间序列的内在规律，同时隐含状态会随着时间更新，即服从条件分布<span class="math inline">\(p(\boldsymbol{l}_t|\boldsymbol{l}_{t-1})\)</span>。</p>
<p>在线性状态空间模型（Linear State Space Model）中我们以如下的方式刻画隐含状态的更新： <span class="math display">\[
\boldsymbol{l}_t=\boldsymbol{F}_t\boldsymbol{l}_{t-1}+\boldsymbol{g}_t\varepsilon_t, \space\space\space\varepsilon_t\sim\mathcal{N}(0,1)
\]</span> <span class="math inline">\(\boldsymbol{F}_t\)</span>为确定的状态转移矩阵，而<span class="math inline">\(\boldsymbol{g}_t\varepsilon_t\)</span>则表示了状态转移的随机性。</p>
<p>观测值<span class="math inline">\(z_t\)</span>从隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>计算而来： <span class="math display">\[
\begin{align}
z_t&amp;=y_t+\sigma_t\epsilon_t,\\
y_t&amp;=\boldsymbol{a}_t^\top\boldsymbol{l}_{t-1}+b_t,\\
\epsilon_t&amp;\sim\mathcal{N}(0,1)
\end{align}
\]</span> 其中<span class="math inline">\(\boldsymbol{a}_t\in\mathbb{R}^L,\sigma_t\in \mathbb{R},b_t\in\mathbb{R}\)</span>都是额外的参数。初始状态<span class="math inline">\(\boldsymbol{l}_0\)</span>则从一个独立的高斯分布得来，即<span class="math inline">\(\boldsymbol{l}_0\sim N(\boldsymbol\mu_0,\text{diag}(\boldsymbol{\sigma}_0^2))\)</span>。</p>
<p>令参数集合<span class="math inline">\(\Theta_t=(\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0,\boldsymbol{F}_t,\boldsymbol{g}_t,\boldsymbol{a}_t,b_t,\sigma_t),\forall t&gt;0\)</span>，一般来说参数集合不会随着时间变化，即每个时刻<span class="math inline">\(t\)</span>共享同样的参数<span class="math inline">\(\Theta_t=\Theta,\forall t&gt;0\)</span>。对参数的估计可以采用极大似然估计： <span class="math display">\[
\begin{align}
\Theta^*_{1:T}&amp;=\arg\max_{\Theta_{1:T}}p(z_{1:T}|\Theta_{1:T}),\\
\end{align}
\]</span> 其中： <span class="math display">\[
\begin{align}
p(z_{1:T}|\Theta_{1:T})&amp;=p(z_1|\Theta_1)\prod\limits_{t=2}^T p(z_t|z_{1:t-1},\Theta_{1:t})\\
&amp;=\int p(\boldsymbol{l}_0)\left[\prod\limits_{t=1}^T p(z_t|\boldsymbol{l}_t)p(\boldsymbol{l}_t|\boldsymbol{l}_{t-1})\right]\mathrm{d}\boldsymbol{l}_{0:T}
\end{align}
\]</span></p>
<h2 id="planar-normalizing-flow">Planar Normalizing Flow</h2>
<h3 id="normalizing-flows">Normalizing Flows</h3>
<p>VAE采用一个变分分布<span class="math inline">\(q_\phi(z|x)\)</span>来近似真实的后验分布<span class="math inline">\(p(z|x)\)</span>，并推导出<span class="math inline">\(\log p_\theta(x)\)</span>的下界（称为ELBO）来作为优化目标函数： <span class="math display">\[
\begin{align}
\log p_\theta(x)&amp;=\log \int p_\theta(x|z)p(z)\mathrm{d}z\\
&amp;=\log\int\frac{q_\phi(z|x)}{q_\phi(z|x)}p_\theta(x|z)p(z)\mathrm{d}z\\
&amp;\geq-D_{KL}[q_\phi(z|x)\parallel p(z)]+\mathbb{E}_q[\log p_\theta(x|z)]
\end{align}
\]</span> <span class="math inline">\(\log p_\theta(x)\)</span>与ELBO取等的条件是<span class="math inline">\(D_{KL}[q_\phi(z|x)\parallel p(z)]\)</span>，表明变分分布完全匹配了真实的后验分布。但在实际应用中，真实的后验分布可能会非常复杂，而我们的变分分布通常是一个确定的较为简单的分布，如高斯分布。这样变分分布可能很难对真实后验分布得到一个很好的拟合。</p>
<p>一个解决方案是使用标准化流（Normalizing Flows）。标准化流是从一个相对简单的分布出发，执行一系列可逆的映射，将原始简单的分布转化为一个复杂的分布。</p>
<p>首先考虑一个光滑的、可逆的映射<span class="math inline">\(f:\mathbb{R}^d\mapsto \mathbb{R}^d\)</span>，记<span class="math inline">\(g=f^{-1}\)</span>，那么<span class="math inline">\(g\circ f(\mathbf{z})=\mathbf{z}\)</span>。令<span class="math inline">\(\mathbf{z}^\prime=f(\mathbf{z})\)</span>，那么<span class="math inline">\(\mathbf{z}^\prime\)</span>的分布为： <span class="math display">\[
q(\mathbf{z}^\prime)=q(\mathbf{z})\left|\text{det}\frac{\partial f^{-1}}{\partial \mathbf{z}^\prime}\right|=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}
\]</span> 式中<span class="math inline">\(q(\mathbf{z}^\prime)=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}\)</span>说明了<span class="math inline">\(\mathbf{z}^\prime\)</span>的分布等于<span class="math inline">\(\mathbf{z}\)</span>的分布乘上<span class="math inline">\(f\)</span>的Jacobian矩阵的行列式的倒数。那么对于映射多次的情况： <span class="math display">\[
\mathbf{z}_K=f_K\circ\cdots\circ f_2\circ f_1(\mathbf{z}_0)
\]</span> <span class="math inline">\(\mathbf{z}_K\)</span>的分布可以通过链式计算得到： <span class="math display">\[
\ln q_K(\mathbf{z}_K)=\ln q_0(\mathbf{z}_0)-\sum\limits_{k=1}^K\ln\left|\text{det}\frac{\partial f_k}{\partial \mathbf{z}_{k-1}}\right|
\]</span></p>
<h3 id="planar-flows">Planar Flows</h3>
<p>考虑一个变换族： <span class="math display">\[
f(\mathbf{z})=\mathbf{z}+\mathbf{u}h(\mathbf{w}^\top\mathbf{z}+b)
\]</span> 其中<span class="math inline">\(\lambda=\{\mathbf{w}\in \mathbb{R}^d,\mathbf{u}\in\mathbb{R}^d,b\in\mathbb{R}\}\)</span>为参数集合，<span class="math inline">\(h(\cdot)\)</span>为元素级的非线性函数（如各种激活函数）。令<span class="math inline">\(\psi(\mathbf{z})=h^\prime(\mathbf{w}^\top\mathbf{z}+b)\mathbf{w}\)</span>，则<span class="math inline">\(f\)</span>的Jacobian矩阵行列式绝对值等于： <span class="math display">\[
\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|=\left|\text{det}(\mathbf{I}+\mathbf{u}\psi(\mathbf{z})^\top)\right|=\left|1+\mathbf{u}^\top\psi(\mathbf{z})\right|
\]</span> 但是<span class="math inline">\(f\)</span>并不保证总是可逆的，如<span class="math inline">\(h(x)=\tanh(x)\)</span>时，<span class="math inline">\(f\)</span>可逆的条件是<span class="math inline">\(\mathbf{w}^\top \mathbf{u}\geq-1\)</span>。</p>
<p>下面讨论如何保证可逆的条件。考虑将<span class="math inline">\(\mathbf{z}\)</span>分解为<span class="math inline">\(\mathbf{z}=\mathbf{z}_\bot+\mathbf{z}_\parallel\)</span>，其中<span class="math inline">\(\mathbf{z}_\bot\)</span>与<span class="math inline">\(\mathbf{w}\)</span>正交，<span class="math inline">\(\mathbf{z}_\parallel\)</span>与<span class="math inline">\(\mathbf{w}\)</span>平行，那么： <span class="math display">\[
f(z)=\mathbf{z}_\bot+\mathbf{z}_\parallel+\mathbf{u}h(\mathbf{w}^\top \mathbf{z}_\parallel +b)
\]</span> 实际上得到<span class="math inline">\(\mathbf{z}_\parallel\)</span>之后可以很容易的得到<span class="math inline">\(\mathbf{z}_\bot\)</span>，令<span class="math inline">\(\mathbf{y}=f(\mathbf{z})\)</span>，有： <span class="math display">\[
\mathbf{z}_\bot=\mathbf{y}-\mathbf{z}_\parallel-\mathbf{u}h(\mathbf{w}^\top\mathbf{z}_\parallel+b)
\]</span> 而<span class="math inline">\(\mathbf{z}_\parallel\)</span>与<span class="math inline">\(\mathbf{w}\)</span>平行，易知<span class="math inline">\(\mathbf{z}_\parallel=\alpha\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}\)</span>，其中<span class="math inline">\(\alpha\in\mathbb{R}\)</span>。</p>
<p>对式(16)两边同时乘以<span class="math inline">\(\mathbf{w}^\top\)</span>可得： <span class="math display">\[
\mathbf{w}^\top f(\mathbf{z})=\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)
\]</span> 当<span class="math inline">\(\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\)</span>对于<span class="math inline">\(\alpha\)</span>是非递减函数的时候，<span class="math inline">\(f\)</span>是可逆的。因为<span class="math inline">\(\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\)</span>是非递减函数时有<span class="math inline">\(1+\mathbf{w}^\top\mathbf{u}h^\prime(\alpha+b)\geq 0\equiv \mathbf{w}^\top \mathbf{u}\geq -\frac{1}{h^\prime(\alpha + b)}\)</span>，而<span class="math inline">\(0\leq h^\prime(\alpha + b) \leq 1\)</span>（<span class="math inline">\(\tanh\)</span>函数的性质），所以总是有<span class="math inline">\(\mathbf{w}^\top \mathbf{u}\geq-1\)</span>。</p>
<p>对于任意一个<span class="math inline">\(\mathbf{u}\)</span>，我们可以通过特定的方式构造一个<span class="math inline">\(\hat{\mathbf{u}}\)</span>使得<span class="math inline">\(\mathbf{w}^\top\hat{\mathbf{u}}&gt;-1\)</span>，即令<span class="math inline">\(\hat{\mathbf{u}}(\mathbf{w},\mathbf{u})=\mathbf{u}+[m(\mathbf{w}^\top\mathbf{u})-(\mathbf{w}^\top\mathbf{u})]\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}\)</span>，其中<span class="math inline">\(m(x)=-1+\log(1+e^x)\)</span>。</p>
<p><img src="https://i.loli.net/2020/06/25/uPyplhWBazROEw4.png" /></p>
<h1 id="methodology">Methodology</h1>
<h2 id="problem-statement">Problem Statement</h2>
<p>本文针对的是多变量时间序列<span class="math inline">\(x=\{x_1,x_2,\cdots,x_N\}\in R^{M\times N}\)</span>，<span class="math inline">\(N\)</span>为时间长度，其中某一时刻的观测值<span class="math inline">\(x_t\in R^M\)</span>为一个<span class="math inline">\(M\)</span>维的向量。作者使用<span class="math inline">\(x_{t-T:t}\in R^{M\times(T+1)}\)</span>来表示<span class="math inline">\(t-T\)</span>到<span class="math inline">\(t\)</span>之间的时间序列。</p>
<p><img src="https://i.loli.net/2020/06/25/4eHhs82uOzI5tG3.png" /></p>
<h2 id="overall-structure">Overall Structure</h2>
<p>算法的总体框架如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/wd8maAoVb3Fk9vP.png" /></p>
<p>预处理模块主要是对数据进行标准化以及窗口切分。训练模块则根据输入的数据对正常模式进行捕捉，输出异常分数。在线检测模块则会定期执行。</p>
<h2 id="network-architecture">Network Architecture</h2>
<p>模型的总体结构如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/Lp7D81EvxVsywXQ.png" /></p>
<p>在qnet中，首先GRU被用来建模样本的时间依赖关系，之后VAE将样本<span class="math inline">\(\mathbf{x}\)</span>映射到隐空间<span class="math inline">\(\mathbf{z}\)</span>。文中使用了Linear Gaussian State Space Model来建模隐变量之间的时间依赖关系。除此之外，作者还使用了Planar Normalizing Flow来将隐变量映射到复杂的非高斯分布。在pnet中，隐变量<span class="math inline">\(\mathbf{z}_{t-T:t}\)</span>被用来重建<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>，直观上来说，对样本的好的隐变量表示可以带来更好的重构效果。</p>
<p>从细节上来说，在时间<span class="math inline">\(t\)</span>，qnet的输入为<span class="math inline">\(\mathbf{x}_t\)</span>和<span class="math inline">\(\mathbf{e}_{t-1}\)</span>，两者经过GRU Cell之后会产生<span class="math inline">\(t\)</span>时间的<span class="math inline">\(\mathbf{e_t}\)</span>。<span class="math inline">\(\mathbf{e}_t\)</span>是GRU捕捉时间依赖性的关键，可以认为它包含了<span class="math inline">\(\mathbf{x}_{1:t}\)</span>的信息。之后<span class="math inline">\(\mathbf{e}_t\)</span>会和<span class="math inline">\(\mathbf{z}_{t-1}\)</span>进行拼接，进入标准的VAE变分网络结构，通过网络输出的参数<span class="math inline">\(\mu_{z_t},\sigma_{z_t}\)</span>采样得到隐变量<span class="math inline">\(\mathbf{z}_t^0\)</span>，此时隐变量可以说捕捉了时间依赖性。</p>
<p>网络中涉及到的公式如下所示：</p>
<p><span class="math display">\[
\begin{align}
e_t&amp;=(1-c_t^e)\circ\text{tanh}(w^ex_t+u^e(r_t^e\circ e_{t-1})+b^e)+c_t^e\circ e_{t-1}\\
\mu_{z_t}&amp;=w^{\mu_z}h^\phi([z_{t-1},e_t])+b^{\mu_z}\\
\sigma_{z_t}&amp;=\text{softplus}(w^{\sigma_z}h^\phi([z_{t-1},e_t])+b^{\sigma_z})+\epsilon^{\sigma_z}
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(r_t^e=\text{sigmoid}(\mathbf{w}^{r^e}\mathbf{x}_t+\mathbf{u}^{r^e}\mathbf{e}_{t-1}+b^{r^e})\)</span>是GRU中的重置门，<span class="math inline">\(c_t^e=\text{sigmoid}(\mathbf{w}^{c^e}\mathbf{x}_t+\mathbf{u}^{c^e}\mathbf{e}_{t-1}+b^{c^e})\)</span>是GRU中的更新门。</p>
<p>此时<span class="math inline">\(\mathbf{z}_t^0\)</span>服从高斯分布，为了拟合复杂的后验分布，我们使用Planar Normalizing Flow来对<span class="math inline">\(\mathbf{z}_t^0\)</span>进行变换，最后得到经<span class="math inline">\(K\)</span>次变换后的随机变量<span class="math inline">\(\mathbf{z}_t^K\)</span>。</p>
<p>在时间<span class="math inline">\(t\)</span>，pnet试图通过<span class="math inline">\(\mathbf{z}_t^K\)</span>来重构<span class="math inline">\(\mathbf{x}_t\)</span>。首先<span class="math inline">\(\mathbf{z}\)</span>空间中的变量会根据Linear Gaussian State Space Model来进行“连接“，公式为<span class="math inline">\(\mathbf{z}_t=\mathbf{O}_\theta(\mathbf{T}_\theta\mathbf{z}_{t-1}+\mathbf{v}_t)+\boldsymbol{\epsilon}_t\)</span>，其中<span class="math inline">\(\mathbf{O}_\theta\)</span>和<span class="math inline">\(\mathbf{T}_\theta\)</span>为状态转移矩阵，<span class="math inline">\(\mathbf{v}_t\)</span>和<span class="math inline">\(\boldsymbol{\epsilon}_t\)</span>为随机噪声。之后<span class="math inline">\(\mathbf{z}_t\)</span>和<span class="math inline">\(\mathbf{d}_{t-1}\)</span>会作为GRU的输入，产生<span class="math inline">\(\mathbf{d}_t\)</span>。之后<span class="math inline">\(\mathbf{d}_t\)</span>会经过标准VAE中的生成网络，通过网络输出的高斯分布参数<span class="math inline">\(\mu_{x_t},\sigma_{x_t}\)</span>采样得到重构后的样本<span class="math inline">\(\mathbf{x}^\prime_t\)</span>。pnet中涉及到的公式如下所示： <span class="math display">\[
\begin{align}
d_t&amp;=(1-c_t^d)\circ\text{tanh}(w^dz_t+u^d(r_t^d\circ d_{t-1})+b^d)+c_t^d\circ d_{t-1}\\
\mu_{x_t}&amp;=w^{\mu_x}h^\theta(d_t)+b^{\mu_x}\\
\sigma_{x_t}&amp;=\text{softplus}(w^{\sigma_x}h^\theta(d_t)+b^{\sigma_x})+\epsilon^{\sigma_x}
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(r_t^d=\text{sigmoid}(\mathbf{w}^{r^d}\mathbf{x}_t+\mathbf{u}^{r^d}\mathbf{d}_{t-1}+b^{r^d})\)</span>是GRU中的重置门，<span class="math inline">\(c_t^d=\text{sigmoid}(\mathbf{w}^{c^d}\mathbf{x}_t+\mathbf{u}^{c^d}\mathbf{d}_{t-1}+b^{c^d})\)</span>是GRU中的更新门。</p>
<h2 id="offline-model-training">Offline Model Training</h2>
<p>和传统VAE类似，模型的训练可以通过优化ELBO来完成。记长度为<span class="math inline">\(T+1\)</span>的输入序列为<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>，隐空间变量采样次数为<span class="math inline">\(L\)</span>，第<span class="math inline">\(l\)</span>个隐空间变量为<span class="math inline">\(\mathbf{l}^{(l)}_{t-T:t}\)</span>，损失函数可以写成如下形式：</p>
<p><span class="math display">\[
\tilde{\mathcal{L}}(\mathbf{x}_{t-T:t})\approx\frac{1}{L}\sum_{t=1}^L[\log(p_\theta(\mathbf{x}_{t-T:t}|\mathbf{z}_{t-T:t}^{(l)}))+\log(p_\theta(\mathbf{z}_{t-T:t}^{(l)}))-\log(q_\phi(\mathbf{z}_{t-T:t}^{(l)}|\mathbf{x}_{t-T:t}))]
\]</span></p>
<p>第一项<span class="math inline">\(\log(p_\theta(\mathbf{x}_{t-T:t}|\mathbf{z}_{t-T:t}^{(l)}))\)</span>可以看作是重构误差；第二项<span class="math inline">\(\log(p_\theta(\mathbf{z}_{t-T:t}))=\sum_{i=t-T}^t \log(p_\theta(\mathbf{z}_i|\mathbf{z}_{i-1}))\)</span>通过Linear Gaussian State Space Model计算；第三项<span class="math inline">\(-\log(q_\phi(\mathbf{z}_{t-T:t}|\mathbf{x}_{t-T:t}))=-\sum_{i=t-T}^t\log(q_\phi(\mathbf{z}_i|\mathbf{z}_{i-1},\mathbf{x}_{t-T:i}))\)</span>为隐变量<span class="math inline">\(\mathbf{z}\)</span>后验分布的估计，同时<span class="math inline">\(\mathbf{z}_i\)</span>是经Planar Normalizing Flow转换过的。</p>
<h2 id="online-detection">Online Detection</h2>
<p>在训练好模型之后，就可以进行异常检测了。在时间<span class="math inline">\(t\)</span>，我们通过根据长度为<span class="math inline">\(T+1\)</span>的序列<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>来重构<span class="math inline">\(\mathbf{x}_t\)</span>，并根据重构概率<span class="math inline">\(\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))\)</span>来判定异常。定义<span class="math inline">\(\mathbf{x}_t\)</span>对应的异常分数<span class="math inline">\(S_t=\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))\)</span>，高异常分数代表样本<span class="math inline">\(\mathbf{x}_t\)</span>能够以大概率重构（因为模型是用正常样本训练，可以认为模型建模的是正常样本的分布，重构概率高就代表符合正常分布）。给定阈值之后便可根据异常分数来进行异常的判定。</p>
<h2 id="automatic-threshold-selection">Automatic Threshold Selection</h2>
<p>在异常检测阶段，需要根据设定的阈值和每个样本的异常分数来判断该样本是否为异常，所以阈值的选择十分重要。文中用到了一种根据<strong>Extreme Value Theory</strong>自动选择阈值的算法。对于一个分布，其中的极端事件往往位于分布的末尾，而Extreme Value Theory第一定理给出不管原始分布如何，这些极端事件的分布服从一个带参的分布族。因此，可以在对数据分布未知的情况下估计极端事件的分布。</p>
<p>除了Extreme Value Theory第一定理之外，Extreme Value Theory第二定理给出随机变量大于特定阈值<span class="math inline">\(t\)</span>的分布可以用Generalized Pareto Distribution来描述。作者使用了基于Extreme Value Theory第二定理的Peaks-Over-Threshold算法来进行阈值的选择。因为Extreme Value Theory第二定理给出随机变量大于特定阈值<span class="math inline">\(t\)</span>的分布，而在本文的场景中我们需要刻画的异常点的分布应该是小于一个给定阈值的分布，所以需要修改一下公式。</p>
<p>对于给定的数据，模型会给出对应的异常分数序列<span class="math inline">\(\{S_1,S_2,\cdots,S_{N^\prime}\}\)</span>，给定预先设定的阈值<span class="math inline">\(th\)</span>，<span class="math inline">\(S_i\)</span>极端部分（即小于<span class="math inline">\(th\)</span>的部分）的分布符合Generalized Pareto Distribution，公式如下： <span class="math display">\[
\bar{F}(s)=P(th-S&gt;s|S&lt;th)\sim(1+\frac{\gamma s}{\beta})^{-\frac{1}{\gamma}}
\]</span></p>
<p>其中<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>为分布的形状参数，本文使用极大似然估计来对参数进行估计。设参数的估计值分别为<span class="math inline">\(\hat{\gamma}\)</span>和<span class="math inline">\(\hat{\beta}\)</span>，最终的阈值<span class="math inline">\(th_F\)</span>由拟合得到的分布的分位数确定：</p>
<p><span class="math display">\[
th_F\simeq th-\frac{\hat{\beta}}{\hat{\gamma}}((\frac{qN^\prime}{N^\prime_{th}})^{-\hat{\gamma}}-1)
\]</span></p>
<p>其中<span class="math inline">\(q\)</span>为期望<span class="math inline">\(S&lt;th\)</span>的概率，<span class="math inline">\(N^\prime\)</span>为观测值的数量，<span class="math inline">\(N^\prime_{th}\)</span>为<span class="math inline">\(S_i&lt;th\)</span>的个数。</p>
<h2 id="anomaly-interpretation">Anomaly Interpretation</h2>
<p><span class="math display">\[
\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))=\sum_{i=1}^M\log(p_\theta(x_t^i|\mathbf{z}_{t-T:t}))
\]</span></p>
<h1 id="experiments">Experiments</h1>
<h2 id="datasets-and-metrics">Datasets and Metrics</h2>
<h2 id="overall-performance">Overall Performance</h2>
<p><img src="https://i.loli.net/2020/06/25/yYLiWfXBDQklbPU.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/HlORKBEu7ij6Vor.png" /></p>
<h2 id="effects-of-major-techniques">Effects of Major Techniques</h2>
<p><img src="https://i.loli.net/2020/06/25/eKXGJ9ZlSmq8r4Q.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/dlibhyOBXNmPrkw.png" /></p>
<h2 id="visualization-on-z-space-representations">Visualization on Z-Space Representations</h2>
<p><img src="https://i.loli.net/2020/06/25/MQXv5pZAejgVJ9s.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/jCkovq2lWrP6KXy.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/JSZuUkzNyci41DA.png" /></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/Research/">上一页</a></div><div class="pagination-next"><a href="/categories/Research/page/3/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/Research/">1</a></li><li><a class="pagination-link is-current" href="/categories/Research/page/2/">2</a></li><li><a class="pagination-link" href="/categories/Research/page/3/">3</a></li></ul></nav></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>