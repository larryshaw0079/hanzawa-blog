<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>分类: Technical Notes - Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="website"><meta property="og:title" content="Hanzawa の 部屋"><meta property="og:url" content="http://qfxiao.me/"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://qfxiao.me/img/og_image.png"><meta property="article:author" content="Hanzawa"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://qfxiao.me"},"headline":"Hanzawa の 部屋","image":["http://qfxiao.me/img/og_image.png"],"author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":null}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">分类</a></li><li class="is-active"><a href="#" aria-current="page">Technical Notes</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-09-09T03:33:25.000Z" title="2020-9-9 11:33:25 ├F10: AM┤">2020-09-09</time>发表</span><span class="level-item"><time dateTime="2021-02-19T10:21:57.175Z" title="2021-2-19 6:21:57 ├F10: PM┤">2021-02-19</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Machine-Learning/">Machine Learning</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/09/Model-Selection-and-Evaluation-Machine-Learning-Basics/">Model Selection and Evaluation: Machine Learning Basics</a></h1><div class="content"><h1 id="overfitting">Overfitting</h1>
<p>我们将模型输出与真实值之间的差异称为误差，如对于分类问题，我们可以使用模型分类错误的样本数量占总样本数的比例。模型在训练集（我们收集到的数据）上的误差称作是<strong>训练误差 (training error)</strong>，而在新样本（这里指的是新的样本而不是测试集，训练集测试集是从我们收集到的数据上人为划分出来的）上的误差称作是<strong>泛化误差 (generalization error)</strong>。对于机器学习算法，我们希望算法能学到数据背后的普遍规律，所以我们总是希望模型的泛化误差越小越好。</p>
<p>不过测试集对训练过程来说是未知的，所以模型只能尽量从训练集中发掘数据的普遍规律，要是模型把训练集学得”太好“了，很可能把训练集中不属于普遍规律的部分特点作为了一般性质，这就会导致泛化性能下降，我们称这种情况为<strong>过拟合 (overfitting)</strong>。反之，模型对训练集的特性学得不够，就会出现<strong>欠拟合 (underfitting)</strong>。关于过拟合和欠拟合，周志华老师的《机器学习》中有一张很好的图：</p>
<p><img src="https://i.loli.net/2020/09/09/tVPeQfu9CWLaSi6.png" style="zoom:67%;" /></p>
<p>一般来说，欠拟合比较容易克服，可以通过增加模型的复杂度来实现。而过拟合则比较难解决，一般而言可以通过增加数据量、加正则化约束来改善。</p>
<h1 id="model-selection">Model Selection</h1>
<p>对于一个机器学习任务，一般我们有多种模型供我们选择，并且模型也有不同的超参数，我们希望得到泛化性能尽可能高的模型。不过根据前面的讨论，新样本是未知的，所以没法直接得到泛化误差，而过拟合的存在使得我们不能贸然的根据模型在我们收集到的数据上的表现来选择模型（训练误差低不代表泛化误差低）。</p>
<p>我们假设无论是我们收集到的数据还是新样本都是从数据的真实分布中独立同分布采样得来，为此我们可以从数据中划分出一部分”测试集“，然后将模型在测试集上的表现作为泛化误差的近似，而剩下的部分用来模型训练。</p>
<p>那么如何划分训练集和测试集呢？比较常见的方法是”<span class="math inline">\(k\)</span>折交叉验证法“ (<span class="math inline">\(k\)</span>-fold cross validation)，一般<span class="math inline">\(k\)</span>常取<span class="math inline">\(10\)</span>，其基本思想如下图所示：</p>
<p><img src="https://i.loli.net/2020/09/09/8KEWDe3qMTsQoUx.png" style="zoom: 50%;" /></p>
<p><span class="math inline">\(k\)</span>折交叉验证法首先将数据集均匀地划分为<span class="math inline">\(k\)</span>个部分，然后进行<span class="math inline">\(k\)</span>个循环，在每个循环中将第<span class="math inline">\(k\)</span>份作为测试集，其余的作为训练集，最后得到的结果进行平均。</p>
<h1 id="evaluation-metrics">Evaluation Metrics</h1>
<p>前面我们讨论了评测的框架，但是没有说具体的评测指标。实际上评测指标要根据任务来确定，并且不同的评测指标也有自己的特点。</p>
<h2 id="regression">Regression</h2>
<p>回归任务比较常用的评测标准是<strong>均方误差 (Mean Squared Error)</strong>： <span class="math display">\[
\text{MSE}(f;D)=\frac{1}{m}\sum_{i=1}^m (f(x_i)-y_i)^2
\]</span> 和<strong>平均绝对误差 (Mean Absolute Error)</strong>： <span class="math display">\[
\text{MAE}(f;D)=\frac{1}{m}\sum_{i=1}^m |f(x_i)-y_i|
\]</span></p>
<h2 id="classification">Classification</h2>
<h3 id="binary-classification">Binary Classification</h3>
<p>对于分类任务，最简单的想法是使用模型分类正确的比例来作为评测标准，我们称之为<strong>准确率 (Accuracy)</strong>： <span class="math display">\[
\text{ACC}(f;D)=\frac{1}{m}\sum_{i=1}^m \mathbb{I}(f(x_i)=y_i)
\]</span> 但准确率并不能满足我们的所有要求，比如说对于新冠病毒的分类任务，我们可能会更关注于对于所有患有新冠的病人，模型到底查出来了多少，而对于模型误把正常病人当作是患病的情况没有那么关注。对于二分类问题，我们可以将样例根据其真实类别与模型预测的类别划分为真正例 (true positive, TP)、假正例 (false positive, FP)、真反例 (true negative, TN) 和假反例 (false negative, FN) 四种，形成<strong>混淆矩阵 (Confusion Matrix)</strong>：</p>
<p><img src="https://i.loli.net/2020/09/09/xY8KAldMZSRwHnB.jpg" /></p>
<p>下面给一个具体的例子：</p>
<p><img src="https://i.loli.net/2020/09/09/tCLxvZfNoEgqWID.png" /></p>
<p>比如我们的任务是预测一张手写数字图片是不是<span class="math inline">\(5\)</span>，根据上图，右下角就是我们正确预测的是<span class="math inline">\(5\)</span>的图片，左下角就是本来是<span class="math inline">\(5\)</span>，但被预测成不是<span class="math inline">\(5\)</span>的图片；左上角是本来不是<span class="math inline">\(5\)</span>，我们也正确地预测出其不是<span class="math inline">\(5\)</span>的，右上角是本来不是<span class="math inline">\(5\)</span>却被预测成是<span class="math inline">\(5\)</span>的。是不是有点被绕晕了😀，只要记住T和F代表的是预测结果对还是不对，P和N代表的是模型预测当前样本是正例还是负例。</p>
<p>基于混淆矩阵，我们可以定义<strong>查准率 (Precision)</strong> 和<strong>查全率 (Recall)</strong> 这两个评测标准。</p>
<p>查准率，顾名思义，对于检测出来的正例，有多少是真正的正例，即查的准不准，公式为： <span class="math display">\[
\text{Precision}=\frac{TP}{TP+FP}
\]</span> 分母就是模型预测为正例的样本总数。</p>
<p>查全率，就对应刚才举的新冠的例子，我们比较在乎对于数据集中的正例，有多少被查出来了，公式为： <span class="math display">\[
\text{Recall} = \frac{TP}{TP+FN}
\]</span> 分母就是真实类别为正例的样本总数。一般来说，查准率和查全率是相互矛盾的，除非是特别简单的任务，很难兼顾查准率和查全率。</p>
<p><strong>F1分数 (F1 Score)</strong> 综合了查准率和查全率： <span class="math display">\[
\text{F}1=\frac{2\cdot\text{Precision}\cdot\text{Recall}}{\text{Precision}+\text{Recall}}
\]</span> 查准率和查全率中任意一项较低都会导致F1分数较低。有时候我们对待查准率和查全率的权重不同，这时候可以使用F<span class="math inline">\(_\beta\)</span>分数： <span class="math display">\[
\text{F}_\beta=\frac{(1+\beta^2)\cdot \text{Precision}\cdot\text{Recall}}{(\beta^2\cdot\text{Precision})+\text{Recall}}
\]</span> <span class="math inline">\(\beta=1\)</span>时等价于F1分数，<span class="math inline">\(\beta&gt;1\)</span>代表偏重查全率，<span class="math inline">\(\beta&lt;1\)</span>代表偏重查准率。</p>
<h3 id="multi-classification">Multi-classification</h3>
<p>前面的讨论都是基于二分类任务，如果是多分类任务的话， 对于每一类，我们将该类作为正例，其他类别作为负例，都能得到一个混淆矩阵。如果我们在每个混淆矩阵上计算评测指标，然后进行平均，这样就得到宏查准率 (macro-Precision)、宏查全率 (macro-Recall) 和宏F1分数 (macro-F1)。如果我们事先将混淆矩阵的TP、FP、TN、FN先进行平均，再计算评测指标，就得到了微查准率 (micro-Precision)、微查全率 (micro-Recall) 和微F1分数 (micro-F1)。</p>
<h3 id="pr-curve">PR-Curve</h3>
<p>很多情况下模型的输出是样本为正例的“概率值”或者是分数，分数越高的样本代表越可能是正例。这种时候需要人为划定阈值，规定高于阈值的样本是正例。不过阈值的划分相当于超参数的选取，同时我们会认为一个鲁棒的模型的性能应该不受阈值选取的左右。这个时候我们可以使用<strong>PR曲线 (Precision-Recall Curve)</strong>，即遍历所有可能的阈值，对于每个阈值，计算其对应的Precision和Recall，然后画在图上，最后会得到一系列离散的点（理论上应该是连续曲线，不过阈值是连续值，我们只能取离散值），形成PR-曲线。</p>
<p><img src="https://i.loli.net/2020/09/09/ecP8flTYZIDUBrV.png" alt="2-class Precision-Recall curve: AP=0.88" style="zoom:67%;" /></p>
<p>模型性能越好，曲线就会越接近右上角的点，我们可以把PR曲线的曲线下面积 (PR-AUC) 作为评测标准。</p>
<h3 id="roc-curve">ROC-Curve</h3>
<p>ROC全称是<strong>受试者工作特征 (Receiver Operating Characteristic) 曲线</strong>, 和PR曲线类似，ROC曲线也是遍历不同的阈值计算点，不过ROC曲线计算的是真正例率 (True Positive Rate, TPR) 和假正例率 (False Positive Rate, FPR)，两者定义分别是： <span class="math display">\[
\begin{align}
\text{TPR}=\frac{TP}{TP+FN}\\
\text{FPR}=\frac{FP}{TN+FP}
\end{align}
\]</span> 其中TPR就是查全率，而FPR是所有负例中没有检测出来的比例，这一项是越低越好。</p>
<p><img src="https://i.loli.net/2020/09/09/CdHrDaKAns4EN93.png" style="zoom:67%;" /></p>
<p>模型性能越好，曲线就会越接近左上角的点，我们可以把ROC曲线的曲线下面积 (ROC-AUC) 作为评测标准。</p>
<p>下面来总结一下PR曲线和ROC曲线之间的优缺点。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>纵轴</th>
<th>横轴</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>PR</strong></td>
<td><span class="math inline">\(\text{Precision}=\frac{TP}{TP+FP}\)</span></td>
<td><span class="math inline">\(\text{Recall} = \frac{TP}{TP+FN}\)</span></td>
</tr>
<tr class="even">
<td><strong>ROC</strong></td>
<td><span class="math inline">\(\text{TPR}=\frac{TP}{TP+FN}\)</span></td>
<td><span class="math inline">\(\text{FPR}=\frac{FP}{TN+FP}\)</span></td>
</tr>
</tbody>
</table>
<p><img src="https://i.loli.net/2020/09/09/xY8KAldMZSRwHnB.jpg" /></p>
<p>ROC的优点：</p>
<ul>
<li>相比PR仅关注正例，ROC同时关注正例和负例</li>
<li>相比PR不易受到正负例相对数量的影响，ROC的两个指标的计算都只涉及到P、N中的一列，正例或负例增加对总体影响不大。而PR曲线就不一样，两个指标的计算都涉及到了P、N两列，那么正例或负例样本数量的变化会造成较大影响。如负例突然增大，那么FP也会增大，这样Precision会降低，而Recall却不变。</li>
</ul>
<p>ROC的缺点：</p>
<ul>
<li>对于类别不平衡问题，存在大量负例，这样会带来大量的FP，而ROC的FPR却不会因为FP的大幅增长而剧烈改变，结果是这一类错误很难在ROC曲线中体现出来。所以ROC会呈现出一个过于乐观的评价。</li>
</ul>
<p>我们来尝试下是否如此，下面是用随机森林分类器，测试样本正负例数量比为1:1的情况下的ROC曲线和PR曲线：</p>
<p><img src="https://i.loli.net/2020/09/10/yqMDQh5oTs96SJm.png" style="zoom:50%;" /></p>
<p><img src="https://i.loli.net/2020/09/10/zFW941YTMl8yxiP.png" style="zoom:50%;" /></p>
<p>在我们将正负例数量比调整为1:9之后（总数量相同），可以看到ROC曲线比较稳定，没出现较大变化，而PR曲线则出现了剧烈变化：</p>
<p><img src="https://i.loli.net/2020/09/10/HbnLtfGsiVc17q5.png" style="zoom:50%;" /></p>
<p><img src="https://i.loli.net/2020/09/10/FfYrb3SnkeZ9JTv.png" alt="4" style="zoom:50%;" /></p>
<h1 id="bias-and-variance">Bias and Variance</h1>
<p>在机器学习中，偏差-方差分解是解释学习算法泛化性能的重要工具。假设我们要预测某一个地区的房子的房价，每一套房子都是一个样本，我们认为每个样本都是从总体分布<span class="math inline">\(P(X)\)</span>独立同分布采样得来的。不过我们不可能得到所有的样本，我们采样得到的训练集只是其中一个子集。那么，即使对于同样的测试样本，使用不同的训练集（训练集大小相同）训练出来的模型对测试样本的预测也是不一样的，我们将这一部分模型自身的不稳定性用<strong>方差 (Variance) </strong>来描述：<span class="math inline">\(\text{Var}(X)=E_D\left[(\hat f(X)-E[\hat f(X)])^2\right]\)</span>，方差越小代表模型稳定性越强。模型输出的期望与真实值之间的差距我们用<strong>偏差 (Bias) </strong>来描述：<span class="math inline">\(\text{Bias}(X)=E[\hat f(X)]-f(X)\)</span>。</p>
<p>泛化误差与方差、偏差有下列关系： <span class="math display">\[
\begin{align}
\text{Err}(X)&amp;=E\left[(y-\hat f(X))^2\right]\\
&amp;=E\left[(f(X)+\varepsilon-\hat f(X))^2\right]\\
&amp;= (E[\hat f(X)]-f(X))^2 + E\left[(\hat f(X)-E[\hat f(X)])^2\right]+\sigma_{\varepsilon}^2\\
&amp;=\text{Bias}^2 + \text{Variance} + \text{Random Error}
\end{align}
\]</span> 也就是说泛化误差可以分解为方差、偏差和随机噪声之和。偏差刻画了模型的期望预测与真实值之间的偏离程度，方差刻画了不同训练集对模型性能的影响，他们之间的关系如下图所示：</p>
<p><img src="https://i.loli.net/2020/09/09/2IpmrwJGfqORU3z.png" style="zoom: 33%;" /></p>
<p>图中左上角的部分是比较理想的情况，即方差和偏差都较小。但实际上方差和偏差往往是相互冲突的，如下图所示：</p>
<p><img src="https://i.loli.net/2020/09/09/GsREiklKYfuX3Ub.png" style="zoom:67%;" /></p>
<p>模型复杂度不足的时候，模型的拟合能力不够强，偏差主导了泛化误差，而随着模型复杂度的提高，模型的拟合能力逐渐提高，训练数据的扰动会造成模型发生显著变化，这时方差逐渐主导了泛化误差。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-09-02T10:47:55.000Z" title="2020-9-2 6:47:55 ├F10: PM┤">2020-09-02</time>发表</span><span class="level-item"><time dateTime="2021-02-19T10:22:31.857Z" title="2021-2-19 6:22:31 ├F10: PM┤">2021-02-19</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Machine-Learning/">Machine Learning</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/02/Machine-Learning-Classification-Algorithms-Decision-Trees/">Machine Learning Classification Algorithms: Decision Trees</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>（PS：本文内容是学习高级树模型（GBDT，XGBoost）的基础，强烈建议在看那些内容之前先了解本文的内容！）</p>
<p>本文主要是介绍常用的三种决策树模型：ID3、C4.5和CART。决策树（Decision Tree）是一种<strong>有监督分类模型</strong>（稍加改造可进行回归任务）。</p>
<p>比如我们要判断一个瓜是不是好瓜，对于人来说，要判断一个瓜是不是好瓜，可能会先去看看色泽，然后看看根蒂，然后再敲一敲听听声音，这样经过一系列的决策过程。</p>
<p><img src="https://i.loli.net/2020/09/02/WuGQ79g4NcHqJDh.png" style="zoom:50%;" /></p>
<p>决策树正是模拟了这样的过程。给定数据集，决策树会不断地选择最佳的特征将数据集进行切分（如选择色泽，然后将数据分为青绿、乌黑、浅白这几个子集），然后递归地进行下去，直到达到停止条件：</p>
<ol type="1">
<li>每个叶子节点的样本都属于同一个类别</li>
<li>没有可供划分的特征，或者集合中每个样本所有特征取值都相同</li>
<li>决策树达到预先指定的最大深度</li>
</ol>
<p>所以决策树算法要解决的关键问题就是如何去选择当前最好的划分特征。</p>
<h1 id="id3">ID3</h1>
<p>ID3算法根据信息熵来进行特征的划分。信息熵是衡量一个随机变量信息量的度量，如果把数据集的标签<span class="math inline">\(y\)</span>看作是随机变量，那么<span class="math inline">\(y\)</span>的熵越小代表不确定性越小（集合里几乎都是一种类别的样本），熵越大代表不确定性越大（集合包含不同类别的样本），其公式为： <span class="math display">\[
Ent(D)=-\sum_{k=1}^{|\mathcal Y|}p_k\log p_k
\]</span> <span class="math inline">\(Ent(D)\)</span>代表集合<span class="math inline">\(D\)</span>对应的熵，<span class="math inline">\(|\mathcal Y|\)</span>是类别数量，二分类就是<span class="math inline">\(|\mathcal Y|=2\)</span>，<span class="math inline">\(p_k\)</span>为第<span class="math inline">\(k\)</span>个类别对应的概率（频率）。很自然的，我们可以根据划分前后熵的变化来确定划分特征的选择，如果划分之后熵减小的最多，那么这个特征也是最好的。假设我们选定特征<span class="math inline">\(a\)</span>来对集合进行划分，特征<span class="math inline">\(a\)</span>共有<span class="math inline">\(V\)</span>个离散取值，那么划分之后将会产生<span class="math inline">\(V\)</span>个子集，我们记每个子集为<span class="math inline">\(D^v, v=1,\cdots, V\)</span>。那么，信息增益可以写为： <span class="math display">\[
Gain(D,a)=Ent(D)-\sum_{v=1}^V \frac{|D^v|}{|D|}Ent(D^v)
\]</span> 不过，ID3存在两个致命的缺点：</p>
<ol type="1">
<li>无法对连续取值的特征进行计算</li>
<li>对取值较多的特征具有很大的偏向性（极端的情况，把样本编号作为特征，由于每个样本的编号都不同，分裂之后每个自己只有一个样本/类别，熵是最小的）</li>
</ol>
<h1 id="c4.5">C4.5</h1>
<p>C4.5算法在ID3的基础上做了诸多改进。C4.5解决了ID3对于取值数目较多的特征的偏向性问题，其采用的方案很直观，即对信息增益除以一个系数，特征取值数目越多的特征系数越大，该划分标准被称作是信息增益率： <span class="math display">\[
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
\]</span> 其中<span class="math inline">\(IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log \frac{|D^v|}{|D|}\)</span>。其实<span class="math inline">\(IV(a)\)</span>可以看作是“划分之后每个样本属于集合<span class="math inline">\(v\)</span>的概率”这个随机变量的熵，划分的子集越多，划分之后属于哪个集合就越不确定，所以熵就越大。</p>
<p>不过信息增益率反而会对特征取值数目少的特征有所偏好，所以C4.5算法是先计算信息增益，确定信息增益高于平均值的候选集，再从中选择信息增益率最高的特征。</p>
<p>除此之外，C4.5还能处理连续取值的特征，其做法是“离散化”，即将连续取值划分为若干个离散的区间，一般二分比较常用。设连续特征<span class="math inline">\(a\)</span>，假设其出现了<span class="math inline">\(n\)</span>个取值，将其排序得到<span class="math inline">\(\{a^1,a^2,\cdots,a^n\}\)</span>，我们考虑每两个相邻节点的中点集合<span class="math inline">\(T_a=\{\frac{a^i+a^{i+1}}{2}|1\leq i \leq n-1\}\)</span>，之后我们就可以像考察离散属性值一样选择最优划分。</p>
<p>下图是在breast cancer数据上决策树的可视化（图片太大了，可以点开放大🔍看）：</p>
<p><img src="https://i.loli.net/2020/09/10/RNyxp8EM6BCsOHK.png" /></p>
<h1 id="cart">CART</h1>
<p>CART (Classification and Regression Trees) 是一种应用广泛的决策树模型，既可应用于分类任务也可应用于回归任务。</p>
<h2 id="cart-regression">CART Regression</h2>
<p>我们先来说说CART怎么进行回归。在回归问题中，CART使用了MSE作为划分准则： <span class="math display">\[
\frac{1}{N}\sum_{i=1}^N (f(x_i)-y_i)^2
\]</span> 如果CART有<span class="math inline">\(M\)</span>片叶子，那么相当于CART将输入划分成了<span class="math inline">\(M\)</span>个单元<span class="math inline">\(R_m, m=1,\cdots,M\)</span>，也即有<span class="math inline">\(M\)</span>个输出，那么该CART在数据集上的MSE为： <span class="math display">\[
\frac{1}{N}\sum_{m=1}^M\sum_{x_i\in R_m} (c_m-y_i)^2
\]</span> 这里<span class="math inline">\(c_j\)</span>为叶子节点<span class="math inline">\(j\)</span>的输出，一般选为对应样本的均值<span class="math inline">\(c_m=\text{avg}(y_i|x_i\in R_m)\)</span>。这样，剩下的问题就是如何确定每次的切分特征和切分点了。假设选择的特征是<span class="math inline">\(j\)</span>，切分点<span class="math inline">\(s\)</span>，那么该划分方案对应的损失为： <span class="math display">\[
\min_{c_1}\sum_{x_i\in R_1\{j,s\}}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2\{j,s\}}(y_i-c_2)^2
\]</span> 遍历所有的<span class="math inline">\(j\)</span>和<span class="math inline">\(s\)</span>，我们就能找到最佳的特征和切分点： <span class="math display">\[
\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1\{j,s\}}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2\{j,s\}}(y_i-c_2)^2\right]
\]</span></p>
<p>算法流程大致如下：</p>
<blockquote>
<p><strong>CART Decision Tree Algorithm</strong></p>
<p>INPUT: 数据集 <span class="math inline">\(D=\{(x_1,y_1),\cdots,(x_N,y_N)\}\)</span></p>
<p>OUTPUT: 预测值<span class="math inline">\(\{\hat y_1,\cdots,\hat y_N\}\)</span></p>
<p>PROCEDURE:</p>
<p><strong>1. 选取当前最优切分特征变量<span class="math inline">\(j^*\)</span>与最优切分点<span class="math inline">\(s^*\)</span></strong></p>
<p>设当前选择的切分变量为<span class="math inline">\(j\)</span>，切分点为<span class="math inline">\(s\)</span>那么可以根据切分点将数据集分为两个子集，一个是<span class="math inline">\(R_1(j,s)=\left\{x|x^{(j)}\leq s\right\}\)</span>，另一个是<span class="math inline">\(R_2(j,s)=\left\{x|x^{(j)}&gt; s\right\}\)</span>。 遍历所有的<span class="math inline">\(j\)</span>，求解 <span class="math display">\[
\min_{j,s}\left[\min_{c_1}\sum\limits_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]
\]</span> 注意<span class="math inline">\(\hat c_1=\frac{1}{N_1}\sum\limits_{x_i\in R_1(j,s)}y_i\)</span> <strong>2. 用选定的<span class="math inline">\((j^*,s^*)\)</span>来划分区域并计算输出值</strong></p>
<p>此时，我们还需要确定这两个区域（划分到同一个区域的样本对应的输出是相同的）的输出值<span class="math inline">\(c_1\)</span>和<span class="math inline">\(c_2\)</span>，其确定方式是使得对应区域上的均方误差最小。这样我们相当于得到了给定<span class="math inline">\(j,s\)</span>下的损失，所以只要找出使得损失最小的<span class="math inline">\(j^*,s^*\)</span>即可：</p>
<p><strong>3. 递归地对划分出来的两个区域重复步骤1和步骤2，直到满足停止条件</strong></p>
<p><strong>4. 最后将输入空间划分为<span class="math inline">\(M\)</span>，输出<span class="math inline">\(f(x)=\sum_{m=1}^M \hat c_m I(x\in R_m)\)</span></strong></p>
</blockquote>
<p>下图是在波士顿房价数据上决策树的可视化（图片太大了，可以点开放大🔍看）：</p>
<p><img src="https://i.loli.net/2020/09/09/knOHuvsfyorTpxt.png" /></p>
<h2 id="cart-classification">CART Classification</h2>
<p>从前面的讨论可以看到，CART回归树是一棵二叉树，对于分类任务，CART也是一棵二叉树。我们先来介绍CART的划分准则，再来介绍它是怎么进行划分的。</p>
<p>采用基尼系数作为准则，基尼系数的计算依赖于基尼值： <span class="math display">\[
\begin{align}
Gini(D)&amp;=1-\sum_{k=1}^{|\mathcal Y|}p_k^2
\end{align}
\]</span> 直观上来说，基尼值表示随机抽取两个样本，其类别不一致的概率</p>
<p>如果说一个特征越好，那么划分之后其每个子集对应的基尼值应该越小越好。基尼系数的定义为： <span class="math display">\[
Gini\_index(D,a)=\sum_{v=1}^V \frac{|D^v|}{|D|}Gini(D^v)
\]</span></p>
<p>对于离散取值特征，CART不会根据不同取值个数进行划分，而是和连续值类似，会确定一个“划分点”，将样本进行二分。比如对于特征<span class="math inline">\(a\)</span>，其对应取值为<span class="math inline">\(\{a^1,a^2,\cdots,a^n\}\)</span>，CART会考察每个取值，将样本集划分为特征<span class="math inline">\(a\)</span>是不是等于<span class="math inline">\(a^i\)</span>两部分，然后计算基尼系数，最终会采用基尼系数最小的取值作为划分点。</p>
<p>下图是在breast cancer数据上决策树的可视化（图片太大了，可以点开放大🔍看）：</p>
<p><img src="https://i.loli.net/2020/09/10/mMf2EVkI1NaQLdS.png" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-08-25T18:09:24.000Z" title="2020-8-26 2:09:24 ├F10: AM┤">2020-08-26</time>发表</span><span class="level-item"><time dateTime="2021-02-19T10:23:31.925Z" title="2021-2-19 6:23:31 ├F10: PM┤">2021-02-19</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Machine-Learning/">Machine Learning</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/26/Machine-Learning-Classification-Algorithms-Support-Vector-Machine/">Machine Learning Classification Algorithms: Support Vector Machine</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>Still working on it😅...</p>
<p><a target="_blank" rel="noopener" href="http://blog.pluskid.org/?page_id=683">blog</a></p>
<h1 id="hyperplane">Hyperplane</h1>
<p>超平面可以从代数和几何两方面来理解。超平面的代数定义可以看作是方程： <span class="math display">\[
a_1x_1+\cdots+a_nx_n=d
\]</span> 的所有解形成的集合，其中<span class="math inline">\(a_1,\cdots,a_n\)</span>为不全为<span class="math inline">\(0\)</span>的实数，<span class="math inline">\(d\)</span>也是实数。</p>
<p>从几何上来说，超平面可以看作是除空间<span class="math inline">\(R^n\)</span>自身外维度最大的仿射空间。</p>
<p><img src="https://i.loli.net/2020/09/07/WiMJQSe7lN8upfw.jpg" /></p>
<h1 id="maximum-margin-classifier">Maximum Margin Classifier</h1>
<p><img src="https://i.loli.net/2020/08/26/vjuyCGXMr4msaUK.png" style="zoom:67%;" /></p>
<p>要谈SVM就得先谈线性分类器，其设置是这样的。对于<span class="math inline">\(D\)</span>维空间，我们有一堆数据<span class="math inline">\(X\)</span>，进行二分类任务，标签记为<span class="math inline">\(y\)</span>，其中<span class="math inline">\(y=-1\)</span>和<span class="math inline">\(y=1\)</span>分别代表不同的类别。我们的任务就是找到一个超平面，将正负例切分开来（先假设数据是线性可分的），这个超平面的方程可以表示为： <span class="math display">\[
w^\top x+b=0
\]</span> 我们令<span class="math inline">\(f(x)=w^\top x+b\)</span>，对于<span class="math inline">\(f(x)&lt;0\)</span>的样本，我们赋予其类别<span class="math inline">\(-1\)</span>，对于<span class="math inline">\(f(x)&gt;0\)</span>的样本，我们可以赋予其类别<span class="math inline">\(1\)</span>。对于相同的分类结果，我们可以找出无限种超平面。不过，对于那些样本特别靠近超平面的情况，鲁棒性并不好。为什么呢？因为这时只要超平面有轻微的变化，样本的分类结果就会发生变化。直观上来说，我们希望样本到超平面的距离越大越好。</p>
<p>我们先定义函数间隔的概念，函数间隔<span class="math inline">\(\hat \gamma=y(w^\top x+b)\)</span>，乘以<span class="math inline">\(y\)</span>的目的主要是保持非负性，表示起来方便。可见函数间隔的大小并不能表示样本距离，因为同一个超平面，法向量<span class="math inline">\(w\)</span>可以任意增大，函数间隔也会相应增大。</p>
<p>下面来推导点<span class="math inline">\(x\)</span>到超平面的距离。设<span class="math inline">\(x\)</span>在超平面上的投影为<span class="math inline">\(x_0\)</span>，到超平面的距离为<span class="math inline">\(\gamma\)</span>，<span class="math inline">\(w\)</span>为法向量，那么有： <span class="math display">\[
x=x_0+\gamma\frac{w}{\parallel w\parallel}
\]</span> 将上式带入到超平面方程可以得到 <span class="math display">\[
\gamma=\frac{w^\top}{\parallel w\parallel}x+\frac{b}{\parallel w\parallel}
\]</span> 我们称<span class="math inline">\(\gamma\)</span>为几何间隔。</p>
<p><img src="https://i.loli.net/2020/08/26/b6qLJWzHwFAPDne.png" /></p>
<p>可以很容易看出函数间隔和几何间隔的关系： <span class="math display">\[
\gamma = \frac{\hat \gamma}{\parallel w\parallel}
\]</span> 前面提到我们希望几何间隔越大越好，于是可以直接最大化<span class="math inline">\(\gamma\)</span>，得到： <span class="math display">\[
\begin{align}
\max \space &amp;\gamma\\
s.t. \space &amp; y_i(w^\top x_i+b)=\hat\gamma_i\geq\hat\gamma, \space i=1,\cdots,n
\end{align}
\]</span> 这里<span class="math inline">\(\hat \gamma=\gamma \parallel w\parallel\)</span>，根据前面的分析我们知道，对于同一个超平面，函数间隔<span class="math inline">\(\hat\gamma\)</span>可以随着<span class="math inline">\(\parallel w\parallel\)</span>的变化而变化，所以为了找到最优的<span class="math inline">\(\gamma\)</span>，我们可以考虑固定<span class="math inline">\(\parallel w\parallel\)</span>或者<span class="math inline">\(\hat\gamma\)</span>，这里我们固定<span class="math inline">\(\hat \gamma=1\)</span>，所以有： <span class="math display">\[
\begin{align}
\max &amp; \space \frac{1}{\parallel w\parallel},\\ s.t. \space&amp; y_i(w^\top x_i+b)\geq 1, \space i=1,\cdots,n
\end{align}
\]</span></p>
<p>下面的约束条件代表前提是所有样本分类正确，而<span class="math inline">\(\max\frac{1}{\parallel w\parallel}\)</span>代表最大化间隔。为了方便，我们将其化为等价的最小化形式： <span class="math display">\[
\begin{align}
\min &amp; \space \frac{1}{2}\parallel w\parallel^2,\\ s.t. &amp; y_i(w^\top x_i+b)\geq 1, \space i=1,\cdots,n
\end{align}
\]</span> 其中那些<span class="math inline">\(y_i(w^\top x_i+b)=1\)</span>的样本就是“支持向量”。这个优化问题是典型的二次凸优化问题，可以调用现成的算法去解决。不过我们可以使用拉格朗日乘子法来更高效的解决。</p>
<h1 id="dual-problem">Dual Problem</h1>
<p>拉格朗日乘子法可以将有<span class="math inline">\(d\)</span>个变量和<span class="math inline">\(k\)</span>个约束条件的最优化问题转化成有<span class="math inline">\(d+k\)</span>个变量的无约束最优化问题求解。</p>
<h2 id="lagrange-multiplier">Lagrange Multiplier</h2>
<p>对于以下有约束优化问题： <span class="math display">\[
\begin{align}
\min_x \space &amp; f(x)\\
\text{s.t.} \space &amp; h_i(x)=0 \space (i=1,\cdots,m),\\
&amp;g_j(x) \leq 0 \space (j=1,\cdots,n)
\end{align}
\]</span></p>
<p>引入拉格朗日乘子<span class="math inline">\(\boldsymbol\lambda = (\lambda_1,\lambda_2,\cdots,\lambda_n)^\top\)</span>和<span class="math inline">\(\boldsymbol\mu=(\mu_1,\mu_2,\cdots,\mu_m)^\top\)</span>，相应的广义拉格朗日函数 (generalized Lagrange function) 为： <span class="math display">\[
L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu)=f(\boldsymbol x)+\sum_{j=1}^n \lambda_j g_j(\boldsymbol x)+\sum_{i=1}^m \mu_i h_i(\boldsymbol x)
\]</span></p>
<p>其中<span class="math inline">\(\lambda_j\)</span>，<span class="math inline">\(\mu_i\)</span>被称作是拉格朗日乘子，<span class="math inline">\(\lambda_j \geq 0\)</span>。</p>
<h3 id="primal-problem">Primal Problem</h3>
<p>现在我们来讨论原问题的等价性。假设给定某个<span class="math inline">\(x\)</span>，如果<span class="math inline">\(x\)</span>违反约束条件，即存在某个<span class="math inline">\(x\)</span>使得<span class="math inline">\(h_i(x)\neq 0\)</span>或者<span class="math inline">\(g_j(x)&gt;0\)</span>，那么就有： <span class="math display">\[
\max\limits_{\boldsymbol\lambda,\boldsymbol\mu:\lambda_j\geq 0} L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu)=+\infty
\]</span> 如果存在某个<span class="math inline">\(x\)</span>使得<span class="math inline">\(h_i(x)\neq 0\)</span>，那么可以令<span class="math inline">\(\lambda_j \rightarrow +\infty\)</span>，如果存在<span class="math inline">\(g_j(x)&gt;0\)</span>，那么可令<span class="math inline">\(\mu_ih_i(x)\rightarrow +\infty\)</span>。</p>
<p>如果考虑以下极小化问题： <span class="math display">\[
p^*=\min_x\max\limits_{\boldsymbol\lambda,\boldsymbol\mu:\lambda_j\geq 0} L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu)
\]</span> 他与原始带约束最优化问题是等价的（因为不符合约束时会有<span class="math inline">\(+\infty\)</span>，而我们考虑的是极小化问题），我们将其记为原问题 (Primal problem)。</p>
<h3 id="dual-problem-1">Dual Problem</h3>
<p>如果先考虑最小化<span class="math inline">\(x\)</span>，再考虑最大化<span class="math inline">\(\boldsymbol\lambda\)</span>和<span class="math inline">\(\boldsymbol\mu\)</span>，这时有： <span class="math display">\[
\max\limits_{\boldsymbol\lambda,\boldsymbol\mu:\lambda_j\geq 0}\min_x L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu)
\]</span> 对偶问题 (Dual problem) <span class="math display">\[
d^*=\max\limits_{\boldsymbol\lambda,\boldsymbol\mu:\lambda_j\geq 0}\min_x L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu)
\]</span> 原问题和对偶问题的关系 <span class="math display">\[
d^*=\max\limits_{\boldsymbol\lambda,\boldsymbol\mu:\lambda_j\geq 0}\min_x L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu) \leq \min_x\max\limits_{\boldsymbol\lambda,\boldsymbol\mu:\lambda_j\geq 0} L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu) = p^*
\]</span></p>
<h3 id="kkt-condition">KKT Condition</h3>
<blockquote>
<p>对于原问题和对偶问题，设<span class="math inline">\(f(x)\)</span>和<span class="math inline">\(g_i(x)\)</span>为凸函数，<span class="math inline">\(h_i(x)\)</span>为仿射函数，并且不等式约束<span class="math inline">\(c_i(x)\)</span>是严格可行的，则<span class="math inline">\(x^*\)</span>，<span class="math inline">\(\lambda^*\)</span>，<span class="math inline">\(\mu^*\)</span>分别是原问题和对偶问题的解的充分必要条件是满足下面的Karush-Kuhn-Tucker (KKT) 条件： <span class="math display">\[
\begin{cases}
\nabla_x L(x^*,\lambda^*,\mu^*)=0 &amp;\\ 
\lambda^*_j g_j(x^*)=0 &amp; j=1,\cdots n\\
g_j(x^*)\leq 0 &amp; j=1,\cdots n\\
\lambda_j^*\geq 0 &amp; j=1,\cdots n\\
h_i(x^*) = 0 &amp; i = 1, \cdots m
\end{cases}
\]</span></p>
</blockquote>
<p>这告诉我们</p>
<h2 id="dual-form-of-svm-optimization">Dual Form of SVM Optimization</h2>
<p>支持向量机优化的对偶问题可以写为： <span class="math display">\[
L(w,b,\alpha)=\frac{1}{2}\parallel w\parallel^2-\sum_{i=1}^n \alpha_i(y_i(w^\top x_i+b)-1)
\]</span> 我们先令： <span class="math display">\[
\begin{align}
\frac{\partial L}{\partial w}=0&amp;\Rightarrow w=\sum_{i=1}^n\alpha_i y_i x_i\\
\frac{\partial L}{\partial b}=0&amp;\Rightarrow \sum_{i=1}^n\alpha_i y_i =0
\end{align}
\]</span> 带回到<span class="math inline">\(L\)</span>得到： <span class="math display">\[
\begin{align}
L(w,b,\alpha)&amp;=\frac{1}{2}\sum_{i,j=1}^n\alpha_i\alpha_j y_i y_j x^\top_i x_j-\sum_{i,j=1}^n \alpha_i\alpha_jy_iy_jx^\top_ix_j-b\sum_{i=1}^n\alpha_iy_i+\sum_{i=1}^n\alpha_i\\
&amp;=\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j x^\top_i x_j
\end{align}
\]</span> 于是得到关于<span class="math inline">\(\alpha\)</span>的对偶优化问题： <span class="math display">\[
\begin{align}
\max_\alpha &amp;\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j x^\top_i x_j\\
\text{s.t. }&amp; \alpha_i\geq 0, i=1,\cdots,n\\
&amp; \sum_{i=1}^n \alpha_i y_i = 0
\end{align}
\]</span></p>
<p>前面有提到我们根据<span class="math inline">\(f(x)=w^\top x + b\)</span>的输出来判定样本类别，而刚才得到<span class="math inline">\(w=\sum_{i=1}^n\alpha_i y_i x_i\)</span>，于是： <span class="math display">\[
\begin{align}
f(x) &amp;= (\sum_{i=1}^n \alpha_iy_ix_i)^\top x+b\\
&amp;= \sum_{i=1}^n \alpha_i y_i \langle x_i, x\rangle + b
\end{align}
\]</span> 最后的<span class="math inline">\(\sum_{i=1}^n \alpha_i y_i \langle x_i, x\rangle + b\)</span>值得特别注意，这意味着我们对于测试样本<span class="math inline">\(x\)</span>的预测，只需要计算它与训练集的内积即可，同时由于所有非支持向量对应的<span class="math inline">\(\alpha\)</span>都是<span class="math inline">\(0\)</span>，我们只需要求一小部分内积。同时这个内积计算也是后面核方法应用的前提。</p>
<h1 id="kernel">Kernel</h1>
<p>到目前为止，我们的讨论都是在数据是线性可分的前提下进行讨论的，那么对于线性不可分的情况呢？答案是使用核方法。</p>
<p><img src="https://i.loli.net/2020/09/08/kSTVgelDjWqtu8v.png" /></p>
<p>核方法的思想是，对于原始不可分的数据，我们假设原始数据通过一个映射<span class="math inline">\(\phi(\cdot)\)</span>就变得线性可分了。核方法相当于对数据找到了一种新的表示，如上图没法用一个超平面直接分割，但通过<span class="math inline">\(\phi(\cdot)\)</span>映射之后就变得可分了。原始的分类函数为： <span class="math display">\[
f(x)= \sum_{i=1}^n \alpha_i y_i \langle x_i, x\rangle + b
\]</span> 加上映射之后变为： <span class="math display">\[
f(x)= \sum_{i=1}^n \alpha_i y_i \langle \phi(x_i), \phi(x)\rangle + b
\]</span> 优化问题也变为： <span class="math display">\[
\begin{align}
\max_\alpha &amp;\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j \langle\phi(x_i), \phi(x_j)\rangle\\
\text{s.t. }&amp; \alpha_i\geq 0, i=1,\cdots,n\\
&amp; \sum_{i=1}^n \alpha_i y_i = 0
\end{align}
\]</span> 我们把计算两个向量在映射后的空间中的内积的函数叫做核函数 <span class="math display">\[
f(x)= \sum_{i=1}^n \alpha_i y_i k(x_i, x) + b
\]</span> 优化问题改为： <span class="math display">\[
\begin{align}
\max_\alpha &amp;\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j k(\phi(x_i), \phi(x_j))\\
\text{s.t. }&amp; \alpha_i\geq 0, i=1,\cdots,n\\
&amp; \sum_{i=1}^n \alpha_i y_i = 0
\end{align}
\]</span> 实际上，通过核函数，我们隐式地定义了一个映射<span class="math inline">\(\phi(\cdot)\)</span></p>
<p>常用核函数</p>
<table>
<thead>
<tr class="header">
<th>名称</th>
<th>表达式</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>线性核</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>多项式核</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>RBF核</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>拉普拉斯核</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Sigmoid核</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="soft-margin">Soft Margin</h1>
<p>数据线性不可分的情况，除了数据本身结构非线性的原因之外（核方法），还有可能是因为噪声或者离群点。为了处理这种情况，我们可以允许一部分点在一定程度上偏离超平面，具体来说就是原来的约束条件<span class="math inline">\(y_i(w^\top x_i+b)\geq 1, \space i=1,\cdots,n\)</span>变成了： <span class="math display">\[
y_i(w^\top x_i+b)\geq 1-\xi_i, \space i=1,\cdots,n
\]</span> 其中<span class="math inline">\(\xi_i\geq 0\)</span>称作是松弛变量，代表样本<span class="math inline">\(i\)</span>允许的偏离程度。当然松弛变量不可能无限大，所以我们需要将<span class="math inline">\(\xi_i\)</span>加入到优化目标函数中使其尽量小，于是有： <span class="math display">\[
\begin{align}
\min &amp; \space \frac{1}{2}\parallel w\parallel^2+C\sum_{i=1}^n \xi_i,\\ s.t. &amp; y_i(w^\top x_i+b)\geq 1-\xi_i, \space i=1,\cdots,n
\end{align}
\]</span> 其中<span class="math inline">\(C\)</span>为控制最优化<span class="math inline">\(\parallel w\parallel\)</span>和松弛变量这两项的权重。这里的优化函数还是对偶问题之前的形式，我们马上会讨论对偶问题。</p>
<h1 id="numerical-optimization">Numerical Optimization</h1>
<p>这里讨论SVM高效求解的Sequential Minimal Optimization (SMO)算法。</p>
<p>坐标下降法是一种非梯度优化算法，</p>
<p><img src="https://i.loli.net/2020/09/08/I6AonzFRHGVBU3t.png" /></p>
<p><img src="https://i.loli.net/2020/09/08/Hmr79nMlK4C8GeJ.png" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-08-25T16:28:26.000Z" title="2020-8-26 12:28:26 ├F10: AM┤">2020-08-26</time>发表</span><span class="level-item"><time dateTime="2021-02-28T05:38:18.079Z" title="2021-2-28 1:38:18 ├F10: PM┤">2021-02-28</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Machine-Learning/">Machine Learning</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/08/26/Machine-Learning-Ensemble-Algorithms-GBDT-and-XGBoost/">Machine Learning Ensemble Algorithms: GBDT and XGBoost</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>本文主要介绍GBDT和XGBoost，在学习本文内容之前建议先学习<a target="_blank" rel="noopener" href="http://hanzawa.me/2020/09/02/Machine-Learning-Classification-Algorithms-Decision-Trees/">决策树相关内容</a>。</p>
<p>下面是一些有用的参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">XGBoost Documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6133937.html">AdaBoost blog</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6140514.html">GBDT blog</a></p>
<p><a target="_blank" rel="noopener" href="http://wepon.me/files/gbdt.pdf">slide</a></p>
<p><a target="_blank" rel="noopener" href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">陈天奇slide</a></p>
<p><a target="_blank" rel="noopener" href="https://snaildove.github.io/2018/10/01/8.Booting-Methods_LiHang-Statistical-Learning-Methods/">blog</a></p>
<p><a target="_blank" rel="noopener" href="https://snaildove.github.io/2018/10/02/get-started-XGBoost/">blog</a></p>
<h1 id="preliminaries">Preliminaries</h1>
<p>实际上，GBDT和梯度下降、XGBoost和牛顿法之间是存在密切关系的，这里我们先回顾一下梯度下降算法和牛顿法的基础知识。</p>
<h2 id="taylor-formulation">Taylor Formulation</h2>
<p>函数<span class="math inline">\(f(x)\)</span>在点<span class="math inline">\(x_0\)</span>处的泰勒展开为： <span class="math display">\[
f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
\]</span> 特别的，一阶展开为： <span class="math display">\[
f(x)\approx f(x_0)+f^\prime(x_0)(x-x_0)
\]</span> 二阶展开为： <span class="math display">\[
f(x)\approx f(x_0)+f^\prime(x_0)(x-x_0) + f^{\prime\prime}(x_0)\frac{(x-x_0)^2}{2}
\]</span> 迭代形式：假设<span class="math inline">\(x^t=x^{t-1}+\Delta x\)</span>，将<span class="math inline">\(f(x)\)</span>在<span class="math inline">\(x^{t-1}\)</span>处进行泰勒展开 <span class="math display">\[
\begin{align}
f(x^t) &amp;= f(x^{t-1}+\Delta x)\\
&amp;\approx f(x^{t-1})+f^\prime(x^{t-1})\Delta x + f^{\prime\prime}(x^{t-1})\frac{\Delta x^2}{2}
\end{align}
\]</span></p>
<h2 id="gradient-descend-method">Gradient Descend Method</h2>
<p>设参数<span class="math inline">\(\theta\)</span>，那么参数对应的损失函数为<span class="math inline">\(L(\theta)\)</span>，</p>
<p>设当前步数为<span class="math inline">\(t\)</span>，那么<span class="math inline">\(t-1\)</span>步时的参数为<span class="math inline">\(\theta^{t-1}\)</span>，将<span class="math inline">\(L(\theta^t)\)</span>在<span class="math inline">\(\theta^{t-1}\)</span>处展开得到：</p>
<p><span class="math display">\[
L(\theta^t) \approx L(\theta^{t-1})+ L^\prime(\theta^{t-1})\Delta\theta
\]</span> 我们想求的<span class="math inline">\(\theta^t=\theta^{t-1}+\Delta \theta\)</span></p>
<h2 id="newtons-method">Newton's Method</h2>
<p>将<span class="math inline">\(L(\theta^t)\)</span>在<span class="math inline">\(\theta^{t-1}\)</span>处进行二阶泰勒展开 <span class="math display">\[
\begin{align}
L(\theta^t) &amp;= L(\theta^{t-1}+\Delta \theta)\\
&amp;\approx L(\theta^{t-1})+L^\prime(\theta^{t-1})\Delta \theta + L^{\prime\prime}(\theta^{t-1})\frac{\Delta \theta^2}{2}
\end{align}
\]</span> 记一阶导数和二阶导数分别为<span class="math inline">\(g\)</span>和<span class="math inline">\(H\)</span>，那么 <span class="math display">\[
L(\theta^t)=L(\theta^{t-1})+g\Delta \theta + H\frac{\Delta \theta^2}{2}
\]</span> 要使得迭代后的结果尽量小，即<span class="math inline">\(g\Delta \theta + H\frac{\Delta \theta^2}{2}\)</span>尽量小，那么有<span class="math inline">\(\frac{\left(g\Delta \theta + H\frac{\Delta \theta^2}{2}\right)}{\partial\Delta\theta}=0\)</span></p>
<p>求得<span class="math inline">\(\Delta \theta=H^{-1}g\)</span>，故<span class="math inline">\(\theta^{t}=\theta^{t-1}+\Delta \theta=\theta^{t-1}-\frac{g}{h}\)</span>。如果<span class="math inline">\(\theta\)</span>是一个向量，那么<span class="math inline">\(\theta^{t}=\theta^{t-1}-H^{-1}g\)</span>，这里<span class="math inline">\(H\)</span>为海森矩阵。</p>
<h1 id="gradient-boosting-decision-tree-gbdt">Gradient Boosting Decision Tree (GBDT)</h1>
<p>我们首先来看基于树的Boosting模型中，非常经典的梯度提升树 (Gradient Boosting Decision Tree)。</p>
<h2 id="the-additive-model">The Additive Model</h2>
<p>首先GBDT是一个加法模型，即最终模型由一系列树模型乘以对应权重相加得来： <span class="math display">\[
F_T(x;w)=\sum_{t=0}^T\alpha_t h_t(x;w_t)=\sum_{t=0}^T f_t(x;w_t)
\]</span> 我们的目标是使得<span class="math inline">\(F\)</span>的损失函数最小化： <span class="math display">\[
F_T^*=\mathop{\arg\min}\limits_{F}\sum_{i=1}^N L(y_i, F_T(x_i;w))
\]</span></p>
<p>直接优化这个损失函数复杂度是很高的，GBDT实际上运用了一种类似贪心的策略来优化这个函数，将优化过程分解成了迭代的步骤。</p>
<p>回想梯度下降算法进行优化的步骤，我们有参数<span class="math inline">\(\theta\)</span>，损失函数<span class="math inline">\(L(\theta)\)</span>是<span class="math inline">\(\theta\)</span>的函数，我们希望找到最优的<span class="math inline">\(\theta^*\)</span>使得<span class="math inline">\(L(\theta^*)\)</span>最小，于是我们使用了迭代优化的步骤。假设迭代执行到第<span class="math inline">\(t\)</span>步，也就是说我们现在的参数<span class="math inline">\(\theta^{t-1}\)</span>为前面<span class="math inline">\(t-1\)</span>步增量之和：<span class="math inline">\(\theta^{t-1}=\sum_{j=1}^{t-1}\Delta \theta_j\)</span>，每一步的增量记为<span class="math inline">\(\Delta \theta_t\)</span>。当前的增量<span class="math inline">\(\Delta \theta_{t}\)</span>是怎么计算得到的呢？大家都知道是采用的损失函数在<span class="math inline">\(\theta^{t-1}\)</span>的负梯度乘以一个步长，即<span class="math inline">\(\Delta \theta_t=-\alpha_t \frac{\partial L(\theta)}{\partial \theta^{t-1}}\)</span>。</p>
<p>梯度下降相当于是在参数空间<span class="math inline">\(\theta\)</span>找到最合适的参数<span class="math inline">\(\theta^*\)</span>使得损失函数<span class="math inline">\(L(\theta)\)</span>最小化，如果我们把模型<span class="math inline">\(F_T\)</span>看作是函数空间，我们的目的是在函数空间中找到最优的<span class="math inline">\(F_T^*\)</span>使得损失函数最小化，在这一个角度上GBDT和梯度下降就统一起来了。每一步的基模型<span class="math inline">\(f_t\)</span>就相当于梯度下降中的增量<span class="math inline">\(\Delta \theta\)</span>，所以我们就得到了GBDT每一的优化目标，即损失函数<span class="math inline">\(L\)</span>对于<span class="math inline">\(F_{t-1}\)</span>的负梯度。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>梯度下降</th>
<th>GBDT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>损失函数</td>
<td><span class="math inline">\(L(\theta)\)</span></td>
<td><span class="math inline">\(L(F_t)\)</span></td>
</tr>
<tr class="even">
<td>参数</td>
<td><span class="math inline">\(\theta^t\)</span></td>
<td><span class="math inline">\(F_t\)</span></td>
</tr>
<tr class="odd">
<td>增量</td>
<td><span class="math inline">\(\Delta \theta_t=-\alpha_t g_t\)</span></td>
<td><span class="math inline">\(f_t=-\alpha_t g_t\)</span></td>
</tr>
<tr class="even">
<td>步长</td>
<td><span class="math inline">\(\alpha_t\)</span></td>
<td><span class="math inline">\(\alpha_t\)</span></td>
</tr>
<tr class="odd">
<td>初始值</td>
<td><span class="math inline">\(\theta_0\)</span></td>
<td><span class="math inline">\(f_0\)</span></td>
</tr>
</tbody>
</table>
<h2 id="gradient-boosting-tree-for-regression">Gradient Boosting Tree for Regression</h2>
<p>我们先来讨论GBDT解决回归问题的算法。前面我们已经讨论过，在每一步GBDT的优化目标是损失函数的负梯度，那么现在的问题就是如何求得每一步最优的基模型（GBDT的基模型选用的是CART）。GBDT的算法步骤如下：</p>
<blockquote>
<p><strong>Gradient Boosting Tree Algorithm</strong></p>
<p>INPUT: 训练样本<span class="math inline">\(\{(x_1,y_1),\cdots,(x_m,y_m)\}\)</span>，迭代轮数<span class="math inline">\(T\)</span>，损失函数<span class="math inline">\(L\)</span></p>
<p>OUTPUT: 强模型<span class="math inline">\(F_T\)</span></p>
<ol type="1">
<li>初始化弱学习器<span class="math inline">\(f_0\)</span>，直接使用一个基模型在训练集上进行训练</li>
<li>在步骤<span class="math inline">\(t=1...T\)</span>，对于每个样本计算负梯度<span class="math inline">\(r_{ti}=\left[\frac{\partial L(y_i,F_{t-1}(x_i))}{\partial F_{t-1}}\right]\)</span></li>
<li>在<span class="math inline">\((x_i,r_{ti})\)</span>上训练得到一个CART回归树，确定树的结构</li>
<li>假设一共有<span class="math inline">\(J\)</span>个叶子节点，那么对每个叶子节点计算最佳输出值<span class="math inline">\(c_{tj}=\mathop{\arg\min}\limits_{c_{tj}}\sum_{x_i\in R_{tj}} L(y_i,F_{t-1}(x_i)+c_{tj})\)</span>（其中<span class="math inline">\(c_{tj}\)</span>代表第<span class="math inline">\(j\)</span>个叶子的输出，<span class="math inline">\(R_{tj}\)</span>代表第<span class="math inline">\(j\)</span>个叶子对应的样本集合），确定每个叶子节点的输出</li>
<li>更新强学习器<span class="math inline">\(F_t=F_{t-1}+f_t\)</span>，回到步骤2直到达到迭代轮数</li>
<li>最终得到强学习器的表达式：<span class="math inline">\(f(x)=f_0(x)+\sum\limits_{t=1}^T\sum\limits_{j=1}^J c_{tj}\mathrm I(x\in R_{tj})\)</span></li>
</ol>
</blockquote>
<p>于是我们就得到了最终模型<span class="math inline">\(F_T\)</span>。</p>
<h2 id="gradient-boosting-tree-for-classification">Gradient Boosting Tree for Classification</h2>
<p>在处理分类任务时，由于输出是离散的值</p>
<p>一种方法是使用指数损失函数，此时GBDT退化为AdaBoost；另一种方法是借鉴逻辑回归的方法，去建模真实值的概率</p>
<h3 id="binary-classification">Binary Classification</h3>
<h3 id="multi-class-classfication">Multi-class Classfication</h3>
<h2 id="gbdt-sumarry">GBDT Sumarry</h2>
<p>优点：</p>
<ol type="1">
<li>可以灵活处理</li>
<li>相对SVM，调参较少</li>
<li>使用某些损失函数对异常值的鲁棒性高</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>难以并行训练</li>
</ol>
<h1 id="xgboost">XGBoost</h1>
<p>前面我们讲了梯度下降和牛顿法，刚才又讨论了GBDT和梯度下降的关系，那么XGBoost是否和牛顿法有什么关系呢？答案是肯定的。GBDT利用了损失函数在<span class="math inline">\(F_{t-1}\)</span>的一阶展开（即一阶导数信息），而XGBoost则利用了损失函数在<span class="math inline">\(F_{t-1}\)</span>的二阶展开，这也是XGBoost和GBDT最根本的区别。下面我们将详细讲解XGBoost算法。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>牛顿法</th>
<th>XGBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>损失函数</td>
<td><span class="math inline">\(L(\theta)\)</span></td>
<td><span class="math inline">\(L(F_t)\)</span></td>
</tr>
<tr class="even">
<td>参数</td>
<td><span class="math inline">\(\theta^t\)</span></td>
<td><span class="math inline">\(F_t\)</span></td>
</tr>
<tr class="odd">
<td>增量</td>
<td><span class="math inline">\(\Delta \theta_t=-\alpha_t H^{-1}_tg_t\)</span></td>
<td><span class="math inline">\(f_t=-\alpha_t H^{-1}_tg_t\)</span></td>
</tr>
<tr class="even">
<td>步长</td>
<td><span class="math inline">\(\alpha_t\)</span></td>
<td><span class="math inline">\(\alpha_t\)</span></td>
</tr>
<tr class="odd">
<td>初始值</td>
<td><span class="math inline">\(\theta_0\)</span></td>
<td><span class="math inline">\(f_0\)</span></td>
</tr>
</tbody>
</table>
<h2 id="regularization">Regularization</h2>
<p>XGBoost相比GBDT的另一大改进是加入了正则化项，即控制每个树的复杂度。衡量树的复杂度的度量有很多，XGBoost采用的是每棵树叶子节点的个数<span class="math inline">\(T\)</span>和每个叶子节点输出<span class="math inline">\(w\)</span>的平方和： <span class="math display">\[
\Omega(f)=\gamma T+\frac{1}{2}\lambda\parallel w\parallel^2
\]</span></p>
<p>这一步主要是为了进一步降低每个弱学习器的方差。</p>
<h2 id="objective-function">Objective Function</h2>
<p>加上正则项之后总的损失函数变为： <span class="math display">\[
L=\sum_{i=1}^N \ell(y_i, F_T(x_i))+\Omega(F_T)
\]</span> 和GBDT类似，我们来推导第<span class="math inline">\(t\)</span>步的优化公式，对于第<span class="math inline">\(t\)</span>步，我们的损失函数为： <span class="math display">\[
\begin{align}
L_t&amp;=\sum_{i=1}^N \ell(y_i,F_t(x_i))+\Omega(F_t)\\
&amp;=\sum_{i=1}^N \ell(y_i, F_{t-1}(x_i) + f_t(x_i))+\Omega(F_t)
\end{align}
\]</span> 将损失函数在<span class="math inline">\(F_{t-1}\)</span>处进行二阶泰勒展开，得到 <span class="math display">\[
L_t \approx \left[\sum_{i=1}^N \ell(y_i, F_{t-1}) + g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i) \right] + \Omega(F_t)
\]</span> 其中<span class="math inline">\(g_i=\frac{\partial \ell(y_i, F_{t-1})}{\partial F_{t-1}}\)</span>，<span class="math inline">\(h_i=\frac{\partial \ell(y_i, F_{t-1}) ^2}{\partial^2 F_{t-1}}\)</span>，分别代表损失函数对<span class="math inline">\(F_{t-1}\)</span>的一阶导和二阶导。</p>
<p>由于我们要优化的是本轮的基模型<span class="math inline">\(f_t\)</span>，<span class="math inline">\(\ell(y_i, F_{t-1})\)</span>已经是固定的了，相当于常数，把常数项去掉，得到： <span class="math display">\[
\begin{align}
\tilde L_t &amp;= \left[\sum_{i=1}^N  g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i) \right] + \Omega(f_t)\\
&amp;=\left[\sum_{i=1}^N  g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i) \right] + \gamma T + \frac{1}{2}\lambda \parallel w \parallel^2
\end{align}
\]</span> 我们都知道样本<span class="math inline">\(x_i\)</span>在树<span class="math inline">\(f_t\)</span>上的输出取决于<span class="math inline">\(x_i\)</span>在哪个叶子节点。假设树<span class="math inline">\(f_t\)</span>一共有<span class="math inline">\(J\)</span>个叶节点，记<span class="math inline">\(q(x_i)=j\)</span>代表样本<span class="math inline">\(x_i\)</span>经过决策树对应的叶节点是<span class="math inline">\(j\)</span>，<span class="math inline">\(I_j\)</span>代表叶子节点<span class="math inline">\(j\)</span>的所有样本下标集合，<span class="math inline">\(w_j\)</span>代表叶子节点<span class="math inline">\(j\)</span>的输出，我们可以将损失函数改写为： <span class="math display">\[
\begin{align}
\tilde L_t &amp;= \sum_{j=1}^J\left[\sum_{i\in I_j}g_i w_j+\frac{1}{2}(\sum_{i\in I_j}h_i +\lambda)w_j^2\right]+\gamma T\\
&amp;= \sum_{j=1}^J\left[G_j w_j + \frac{1}{2}(H_j+\lambda)w_j^2 \right] + \gamma T
\end{align}
\]</span> 其中<span class="math inline">\(G_j=\sum_{i\in I_j}g_i\)</span>和<span class="math inline">\(H_j=\sum_{i\in I_j}h_i\)</span>为简记，分别代表损失函数在叶子节点<span class="math inline">\(j\)</span>对应的所有样本上的一阶导之和与二阶导之和。</p>
<p>到现在，我们还剩两个问题需要解决，一个是确定树<span class="math inline">\(f_t\)</span>的最优结构，也就是怎么去分裂节点，另一个是确定每个叶子节点的最优输出。我们可以先确定下一个问题，找到另一个问题的最优答案，再来确定剩下的问题。</p>
<p>这里先去寻找树<span class="math inline">\(f_t\)</span>每一个叶子节点对应的最优输出。和牛顿法的推导类似，为了使损失函数下降的最快，我们令<span class="math inline">\(G_j w_j + \frac{1}{2}(H_j+\lambda)w_j^2\)</span>的导数为<span class="math inline">\(0\)</span>，得到： <span class="math display">\[
w_j^*=-\frac{G_j}{H_j + \lambda}
\]</span> 加上正则项有： <span class="math display">\[
\tilde L_t^*=-\frac{1}{2}\sum_{j=1}^J\frac{G_j^2}{H_j+\lambda}+\gamma T
\]</span></p>
<h2 id="splitting-strategy">Splitting Strategy</h2>
<p>现在来确定树<span class="math inline">\(f_t\)</span>的最优结构。最优结构的确定实际上使用了一种类似贪心的策略，和决策树类似，我们从一个只有根节点的树出发（所有样本都在根节点这一叶子节点上），不断分裂节点来降低<span class="math inline">\(\tilde L_t^*\)</span>。在每一步的分裂中，我们会希望<span class="math inline">\(\frac{G_j^2}{H_j+\lambda}\)</span>越大越好，于是： <span class="math display">\[
Gain = \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma
\]</span></p>
<p>我们希望挑选能使得<span class="math inline">\(Gain\)</span>最大的特征和特征分裂点，而选择的策略又有很多种，下面介绍三种。</p>
<h3 id="exact-greedy-algorithm-for-split-finding">Exact Greedy Algorithm for Split Finding</h3>
<p>最简单的方法是枚举所有特征，然后对于这个特征下的所有可能取值进行排序，然后遍历分裂点，找到使得<span class="math inline">\(gain\)</span>最高的那个。这样做的好处是找到的分裂点确定是最好的，不过坏处是时间复杂度过高。</p>
<h3 id="approximate-algorithm-for-split-finding">Approximate Algorithm for Split Finding</h3>
<p>一个比较容易想到的优化方案是不去遍历所有可能的分裂点，而是只考察其中的分位数，如下图展示了三分位数方法：</p>
<p><img src="https://i.loli.net/2020/09/10/HoPR9XlpZSm1Gju.png" /></p>
<p>这样需要考察的点就大大减少。</p>
<p>同时分位数的选择由有global和local之分，global是指在训练之前我们就可以提前对每个特征的分位数进行预处理，local是指每次分裂前计算分位数点。直观上来说global需要更多的分位点数，而local则需要更多的计算量。</p>
<p>实际上，XGBoost还会使用二阶导信息<span class="math inline">\(h_i\)</span>对样本进行夹权，如下图所示：</p>
<p><img src="https://i.loli.net/2020/09/15/nxloKNmTHwJRqI9.png" /></p>
<h3 id="sparsity-aware-split-finding">Sparsity-aware Split Finding</h3>
<p>稀疏感知分裂算法 (Sparsity-aware Split Finding)</p>
<h2 id="other-features">Other Features</h2>
<p>除了上面提到的之外，XGBoost还有很多工程优化。</p>
<h3 id="block-structure-and-parallelism">Block Structure and Parallelism</h3>
<p>XGBoost预先对特征进行了排序，</p>
<p>每个特征的增益的计算可以并行进行</p>
<h3 id="column-sample">Column Sample</h3>
<p>借鉴随机森林，即每次只用一部分特征进行特征选择，进一步降低过拟合</p>
<h3 id="shrinkage">Shrinkage</h3>
<p>在每次迭代会对叶子节点的权总乘以一个系数，让后面的树有更大的学习空间。</p>
<h3 id="custom-loss-function">Custom Loss Function</h3>
<h3 id="missing-values">Missing Values</h3>
<h1 id="lightgbm">LightGBM</h1>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/227782064">parameter tuning</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-02T04:26:13.000Z" title="2020-6-2 12:26:13 ├F10: PM┤">2020-06-02</time>发表</span><span class="level-item"><time dateTime="2020-08-01T04:00:16.325Z" title="2020-8-1 12:00:16 ├F10: PM┤">2020-08-01</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/02/%E9%9D%A2%E5%90%91OpenPAI%E7%9A%84Docker%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E5%8F%8AOpenPAI%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/">面向OpenPAI的Docker镜像配置及OpenPAI基本使用方法</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>实验室服务器集群采用OpenPAI来进行GPU资源的管理，而OpenPAI采用了Docker作为基础，即代码都放在Docker容器中运行。由于Docker的使用、Docker镜像的配置都有一定的门槛，所以这里写一篇Tutorial来进行介绍。本文不是网上资料的拼凑，而是经过本人走弯路踩坑形成的"Best practice"。主要内容包括Docker的介绍、Docker的基本使用、如何配置自己的Docker镜像以及OpenPAI平台的基本使用，但不包括Docker和OpenPAI的安装。</p>
<blockquote>
<p>2020.8.1 Update: 加入通过HDFS读取容器保存的文件的方法</p>
</blockquote>
<h1 id="docker-from-scratch">Docker from Scratch</h1>
<p>要理解Docker是什么，从虚拟机开始讲可能会比较好理解。虚拟机大家可能都很熟悉了，比如说我用的系统是Windows，但我需要Linux系统来作为一个Flask编写的网站的服务器，但是又不想单独安装Linux系统，于是可以使用虚拟机来解决这个问题。安装VMWare Workstation，去官网下载Ubuntu系统镜像，然后在VMWare中安装好系统，然后从头配置Flask相关环境。实际上我需要的仅仅是一个Flask运行环境而已，而使用虚拟机却需要如此“大费周章”，这时Docker出现了，网上有大量现成的Flask Docker镜像，配置好了你所需的Flask环境，你只需要下载这些镜像，然后运行它，你就得到了一个Flask运行环境，而与你当前使用的系统无关。如果你需要一个Tomcat的运行环境，那么去找一个Tomcat的Docker镜像就行。Docker将需求或者说服务绑定在了Docker镜像中（<strong>轻量化</strong>，一个需求对应一个Docker镜像，每个镜像都很小），你有什么需求，去找相应的镜像即可（或者自己写一个），镜像的运行是以虚拟机的形式存在，所以他们之间也是互不干扰的。同时，你在写好一个Docker镜像之后，你还可以<strong>分享</strong>给别人，这样其他人就不用重新配置，直接运行你给他的镜像即可。Docker有两个比较关键的概念：</p>
<ul>
<li><strong>镜像 Images：</strong> 这里的镜像不是指我们安装系统时下载的ISO镜像，Docker镜像就是把你需要的东西（一个系统+需要的服务）集中到一起，相当于做菜的菜谱；</li>
<li><strong>容器 Containers：</strong> 如果一个Docker镜像启动了，那么就会有一个Docker容器产生，相当于按照菜谱做出来的菜。</li>
</ul>
<p><img src="https://i.loli.net/2020/06/24/H2x9mnPwkVQyO6p.png" alt="img" style="zoom: 33%;" /></p>
<p><img src="https://i.loli.net/2020/06/24/7k6S3yecX2lbvQY.png" alt="img" style="zoom: 33%;" /></p>
<p>这一节我们先不讨论如何自己写Docker镜像，只是先讨论Docker的基本操作。</p>
<h2 id="basic-operations">Basic Operations</h2>
<p>Docker新安装好当然是没有什么镜像的，首先我们使用<code>docker pull hello-world</code>来下载一个测试镜像。</p>
<blockquote>
<p>拉取镜像 <code>docker pull &lt;image_name&gt;</code></p>
</blockquote>
<p>在输入之后，Docker会自动在远程服务器上查找对应的镜像进行下载。由于我的电脑上已经有这个镜像了，所以显示是下面的样子：</p>
<p><img src="https://i.loli.net/2020/06/24/rvqcAwzOYJtF6T8.png" /></p>
<p>接下来，我们输入<code>docker run hello-world</code>运行这个镜像。</p>
<blockquote>
<p>运行镜像<code>docker run &lt;image_name&gt;</code></p>
</blockquote>
<p>可以看到，Docker输出了一些信息就自己退出了，这和我们理解的虚拟机不太一样。在Docker里面，我们既可以创建一个完整的系统，用户在运行之后就可以正常使用这个操作系统，也可以创建一个简单的服务，默认运行完一些指令就退出了。这里的<code>hello-world</code>镜像这是输出了一些信息后就自动退出了，因为这就是这个镜像的全部内容。</p>
<p><img src="https://i.loli.net/2020/06/24/Zb1VKjFyMh8gHf2.png" /></p>
<p>我们尝试来运行一个完整的系统，先用<code>docker pull ubuntu</code>拉取Ubuntu Docker镜像：</p>
<p><img src="https://i.loli.net/2020/06/24/zEerb2gPwYWX8O7.png" /></p>
<p>接下来我们使用：</p>
<p><img src="https://i.loli.net/2020/06/24/6cMz1WGH4fgpBsS.png" /></p>
<blockquote>
<p><code>-it</code>的意思是什么？根据<code>docker run --help</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-i, --interactive                    Keep STDIN open even if not attached</span><br><span class="line">    --ip string                      IPv4 address (e.g., 172.30.100.104)</span><br><span class="line">    --ip6 string                     IPv6 address (e.g., 2001:db8::33)</span><br><span class="line">    --ipc string                     IPC mode to use</span><br><span class="line">    --isolation string               Container isolation technology</span><br><span class="line">    --kernel-memory bytes            Kernel memory limit</span><br><span class="line">-t, --tty                            Allocate a pseudo-TTY</span><br><span class="line">    --ulimit ulimit                  Ulimit options (default [])</span><br><span class="line">Copy</span><br></pre></td></tr></table></figure>
<p>其实<code>-it</code>是<code>-i</code>和<code>-t</code>的合并写法，意思是运行后进入这个容器并且启用shell，不然运行之后就会放到后台而不会进入容器中。而<code>--rm</code>则代表容器退出之后会被删除（镜像不会被删除），每次运行实际上会创建一个新的容器，如果不加<code>--rm</code>或退出之后不手动删除的话会看到一堆停止运行的容器。</p>
</blockquote>
<p>输入<code>cat /etc/issue</code>可以看到默认拉取的是最新的Ubuntu 20.04 LTS：</p>
<p><img src="https://i.loli.net/2020/06/24/wldBMFtx3eIk98C.png" /></p>
<h1 id="build-customized-docker-images">Build Customized Docker Images</h1>
<p>如果没有现成的Docker镜像能满足我们的需求，我们可以考虑自己写一个。要自定义一个Docker镜像需要两步，第一步是编写Dockerfile，第二步是使用<code>docker build</code>命令构建镜像。Dockerfile可以看作是一个脚本，描述了我们构建镜像所需要的全部命令，比如要构建一个用于Python科学计算的Docker镜像，我们需要在Dockerfile中编写安装Python的命令，安装Numpy、Scipy等常用包的命令等等。我们先来上手编写Dockerfile，这里我准备写一个包含<a target="_blank" rel="noopener" href="https://hexo.io/">hexo博客框架</a>的镜像，这个框架需要node作为基础环境，不过我们不需要在Dockerfile里写安装node的命令。因为类似于<code>C++</code>或<code>Python</code>中的对象的继承，Dockerfile也可以“继承”，这意味着我们不必从头写起。我们先来看一下完整的Dockfile和效果，再来一一解释。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM node</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">RUN npm install -g hexo-cli</span><br><span class="line">EXPOSE 4000</span><br><span class="line">CMD hexo init blog &amp;&amp; cd blog &amp;&amp; hexo generate &amp;&amp; hexo serverCopy</span><br></pre></td></tr></table></figure>
<p>运行结果如下所示，可以看到Docker按照我们写的Dockerfile一行一行的进行镜像的构建：</p>
<p><img src="https://i.loli.net/2020/06/24/tZ3JW4SPNhxUwEn.png" /></p>
<p>现在来解释Docerfile里的内容。<code>FROM &lt;docker image&gt;</code>表示继承其他的镜像，这里我们使用node官方的镜像。接下来是安装hexo，<code>RUN &lt;command&gt;</code>表示执行命令，这里我们直接用<code>npm install -g hexo-cli</code>进行安装。由于要浏览博客网页需要开放端口，而Docker容器运行的时候和外部主机是完全隔断的，要使外部主机访问Docker容器端口，需要暴露端口。<code>EXPOSE &lt;port&gt;</code>代表暴露端口，这里用的是4000端口。之后是创建博客和启动本地服务，<code>CMD &lt;command&gt;</code>和<code>RUN &lt;command&gt;</code>的区别是RUN会在构建的时候执行，而CMD是在容器启动之后才会执行。<code>hexo init blog &amp;&amp; cd blog &amp;&amp; hexo generate &amp;&amp; hexo server</code>分别代表初始化博客、进入博客所在文件夹、生成博客网站、启动本地服务器。更多指令可以参考<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">官方文档</a>。</p>
<p>然后我们使用<code>docker build -t test_hexo .</code>命令构建镜像。</p>
<blockquote>
<p>构建镜像 <code>docker build -t &lt;image_name&gt; &lt;direcotry&gt;</code></p>
</blockquote>
<p>运行镜像：</p>
<p><img src="https://i.loli.net/2020/06/24/m4FSe15kMIDCHXy.png" /></p>
<p>可以看到容器启动后开始执行博客初始化。</p>
<p><img src="https://i.loli.net/2020/06/24/BEdibvgPTMz4sn8.png" /></p>
<p>最后在<code>locahost:4000</code>上启动了一个本地服务器，在浏览器中输入这个地址，可以看到刚刚构建好的博客：</p>
<p><img src="https://i.loli.net/2020/06/24/vmSUb5QH6l4afzF.png" /></p>
<p>值得注意的是，在Dockerfile中我们暴露了4000端口，使用<code>-p</code>标签可以达到同样的效果：<code>docker run -p &lt;docker_port&gt;:&lt;local_port&gt; &lt;image_name&gt;</code>。比如<code>docker run -p 9999:8888 xxxx</code>代表将Docker容器中的9999端口转发到外部主机的8888端口。如果你是在远程服务器上使用的Docker，那么端口只是被转发到了远程服务器上，还得手动将远程服务器再转发到你本机上才能直接在本机浏览器上看到页面。</p>
<h2 id="build-docker-images-with-aliyun-container-registry">Build Docker Images with Aliyun Container Registry</h2>
<p>因为某些原因，如果在构建镜像的时候需要通过<code>apt-get update</code>更新源，会发现无论如何都会卡住。这个时候可以使用<a target="_blank" rel="noopener" href="https://cr.console.aliyun.com/">阿里云容器镜像服务</a>，在阿里的服务器上构建好镜像，再拉取到自己的机器上。注册好帐号之后，点击创建镜像仓库：</p>
<p><img src="https://i.loli.net/2020/06/24/u4DGHv3En1iLI5z.png" /></p>
<p>这里仓库类型如果没有特殊需求建议使用公开，然后填写一些基本信息：</p>
<p><img src="https://i.loli.net/2020/06/24/Qipw5hAg136afnL.png" /></p>
<p>之后设置代码源，其实就是告诉阿里云从哪儿获取Dockerfile，我这里用的是Github，所以需要先在阿里云中关联Github账号，然后在Github中创建一个用来放Dockerfile的仓库。构建设置里有一个“海外机器构建”，这正是我们使用阿里云容器服务的主要目的，勾选。</p>
<p><img src="https://i.loli.net/2020/06/24/ksvZJG9lKfdh6aR.png" /></p>
<p>镜像仓库创建好之后，点进去，在构建页面点击添加规则：</p>
<p><img src="https://i.loli.net/2020/06/24/irBm3QcqjVhGMsv.png" /></p>
<p>按下图进行设置即可，镜像版本就是你想要的镜像名字：</p>
<p><img src="https://i.loli.net/2020/06/24/7pAFvM8CJQbq6mU.png" /></p>
<p>点击“立即构建”：</p>
<p><img src="https://i.loli.net/2020/06/24/GlMwqDInL7zVOpf.png" /></p>
<p>等待一段时间后，如果构建成功，便可以进行拉取了，在镜像仓库的基本信息页面可以看到地址：</p>
<p><img src="https://i.loli.net/2020/06/24/57jmEv8PYkr2nQG.png" /></p>
<p>将阿里云上的镜像拉取到本机之后一般会想要对镜像改名，可以使用<code>docker tag &lt;old_name&gt; &lt;new_name&gt;</code>。</p>
<h1 id="build-docker-images-for-deep-learning">Build Docker Images for Deep Learning</h1>
<h2 id="startup">Startup</h2>
<p>在Docker中配置适用于OpenPAI的深度学习镜像不是一件容易的事，会有很多的坑，这里专门说一下如何配置。推荐在阿里云容器镜像服务中进行构建，会少很多麻烦。</p>
<p>第一步是初始镜像，由于需要用到CUDA，这里可以根据自己的需求（比如不同CUDA版本支持的GPU驱动版本不一样，还有Tensorflow不同版本对CUDA和cuDNN要求也不一样）从Nvidia的Dockerhub<a target="_blank" rel="noopener" href="https://hub.docker.com/r/nvidia/cuda">官方页面</a>选择合适的CUDA和cuDNN版本：</p>
<p><img src="https://i.loli.net/2020/06/24/pftdhmHiWkTq8Fj.png" /></p>
<p>这里我们选择CUDA10.1 + cuDNN7：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia&#x2F;cuda:10.1-cudnn7-devel-ubuntu18.04Copy</span><br></pre></td></tr></table></figure>
<p>这一条主要是解决乱码问题以及定义用到的软件包的版本，这里Miniconda版本设置为4.5.4的原因是这是最后一个自带Python3.6的版本，我在这儿为了稳定所以用了Python3.6，大家也可以安装最新版的Miniconda：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ENV LANG&#x3D;C.UTF-8 LC_ALL&#x3D;C.UTF-8</span><br><span class="line">ENV HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">LABEL HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">ENV MINICONDA_VERSION&#x3D;4.5.4Copy</span><br></pre></td></tr></table></figure>
<p>接下来安装必须的包，大家可以根据需求自行调整，<code>-y</code>标签代表Yes，即自动同意安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">RUN DEBIAN_FRONTEND&#x3D;noninteractive &amp;&amp; \</span><br><span class="line">    apt-get -y update &amp;&amp; \</span><br><span class="line">    apt-get -y install build-essential \</span><br><span class="line">        wget \</span><br><span class="line">        git \</span><br><span class="line">        curl \</span><br><span class="line">        unzip \</span><br><span class="line">        automake \</span><br><span class="line">        openjdk-8-jdk \</span><br><span class="line">        openssh-server \</span><br><span class="line">        openssh-client \</span><br><span class="line">        lsof \</span><br><span class="line">        libcupti-dev &amp;&amp; \</span><br><span class="line">    apt-get clean &amp;&amp; \</span><br><span class="line">    rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*Copy</span><br></pre></td></tr></table></figure>
<p>安装Miniconda并设置环境变量，<code>-b</code>标签可以让Miniconda无交互自动安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN wget --quiet https:&#x2F;&#x2F;repo.anaconda.com&#x2F;miniconda&#x2F;Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh &amp;&amp; &#x2F;bin&#x2F;bash Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh -b -p &#x2F;opt&#x2F;miniconda \</span><br><span class="line">&amp;&amp; rm Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh</span><br><span class="line">ENV PATH &#x2F;opt&#x2F;miniconda&#x2F;bin:$PATHCopy</span><br></pre></td></tr></table></figure>
<p>安装Hadoop，OpenPAI平台会用到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN wget -qO- http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;hadoop&#x2F;common&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;.tar.gz | \</span><br><span class="line">    tar xz -C &#x2F;usr&#x2F;local &amp;&amp; \</span><br><span class="line">    mv &#x2F;usr&#x2F;local&#x2F;hadoop-$&#123;HADOOP_VERSION&#125; &#x2F;usr&#x2F;local&#x2F;hadoopCopy</span><br></pre></td></tr></table></figure>
<p><code>ENV</code>的作用是配置环境变量。配置JAVA和Hadoop环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ENV JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-8-openjdk-amd64 \</span><br><span class="line">    HADOOP_INSTALL&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop \</span><br><span class="line">    NVIDIA_VISIBLE_DEVICES&#x3D;all</span><br><span class="line"></span><br><span class="line">ENV HADOOP_PREFIX&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_BIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;bin \</span><br><span class="line">    HADOOP_SBIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;sbin \</span><br><span class="line">    HADOOP_HDFS_HOME&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_COMMON_LIB_NATIVE_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native \</span><br><span class="line">    HADOOP_OPTS&#x3D;&quot;-Djava.library.path&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native&quot;Copy</span><br></pre></td></tr></table></figure>
<p>设置PATH环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ENV PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin:$&#123;HADOOP_BIN_DIR&#125;:$&#123;HADOOP_SBIN_DIR&#125; \</span><br><span class="line">LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;targets&#x2F;x86_64-linux&#x2F;lib&#x2F;stubs:$&#123;JAVA_HOME&#125;&#x2F;jre&#x2F;lib&#x2F;amd64&#x2F;serverCopy</span><br></pre></td></tr></table></figure>
<p>完整的Dockerfile如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia&#x2F;cuda:10.1-cudnn7-devel-ubuntu18.04</span><br><span class="line"></span><br><span class="line">ENV LANG&#x3D;C.UTF-8 LC_ALL&#x3D;C.UTF-8</span><br><span class="line">ENV HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">LABEL HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">ENV MINICONDA_VERSION&#x3D;4.5.4</span><br><span class="line"></span><br><span class="line">RUN DEBIAN_FRONTEND&#x3D;noninteractive &amp;&amp; \</span><br><span class="line">    apt-get -y update &amp;&amp; \</span><br><span class="line">    apt-get -y install build-essential \</span><br><span class="line">        wget \</span><br><span class="line">        git \</span><br><span class="line">        curl \</span><br><span class="line">        unzip \</span><br><span class="line">        automake \</span><br><span class="line">        openjdk-8-jdk \</span><br><span class="line">        openssh-server \</span><br><span class="line">        openssh-client \</span><br><span class="line">        lsof \</span><br><span class="line">        libcupti-dev &amp;&amp; \</span><br><span class="line">    apt-get clean &amp;&amp; \</span><br><span class="line">    rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*</span><br><span class="line"></span><br><span class="line">RUN wget --quiet https:&#x2F;&#x2F;repo.anaconda.com&#x2F;miniconda&#x2F;Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh &amp;&amp; &#x2F;bin&#x2F;bash Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh -b -p &#x2F;opt&#x2F;miniconda \</span><br><span class="line">&amp;&amp; rm Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh</span><br><span class="line">ENV PATH &#x2F;opt&#x2F;miniconda&#x2F;bin:$PATH</span><br><span class="line">    </span><br><span class="line">RUN wget -qO- http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;hadoop&#x2F;common&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;.tar.gz | \</span><br><span class="line">    tar xz -C &#x2F;usr&#x2F;local &amp;&amp; \</span><br><span class="line">    mv &#x2F;usr&#x2F;local&#x2F;hadoop-$&#123;HADOOP_VERSION&#125; &#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line">    </span><br><span class="line">ENV JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-8-openjdk-amd64 \</span><br><span class="line">    HADOOP_INSTALL&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop \</span><br><span class="line">    NVIDIA_VISIBLE_DEVICES&#x3D;all</span><br><span class="line"></span><br><span class="line">ENV HADOOP_PREFIX&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_BIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;bin \</span><br><span class="line">    HADOOP_SBIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;sbin \</span><br><span class="line">    HADOOP_HDFS_HOME&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_COMMON_LIB_NATIVE_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native \</span><br><span class="line">    HADOOP_OPTS&#x3D;&quot;-Djava.library.path&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native&quot;</span><br><span class="line"></span><br><span class="line">ENV PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin:$&#123;HADOOP_BIN_DIR&#125;:$&#123;HADOOP_SBIN_DIR&#125;:$PATH \</span><br><span class="line">LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;targets&#x2F;x86_64-linux&#x2F;lib&#x2F;stubs:$&#123;JAVA_HOME&#125;&#x2F;jre&#x2F;lib&#x2F;amd64&#x2F;serverCopy</span><br></pre></td></tr></table></figure>
<p>建议先把这一部分进行构建，作为基础镜像，后面要配置其他环境（如安装Pytorch框架登），就不用重复构建这部分，还减少了出错的可能性。这里说一下，启动带CUDA的Docker镜像需要在<code>docker run</code>加上额外的参数<code>--runtime nvidia</code>。</p>
<p>接下来安装深度学习框架。</p>
<h2 id="configure-pytorch">Configure PyTorch</h2>
<p>假设上面的镜像我们命名为xiaoqinfeng/base，那么构建PyTorch的Dockerfile可以像下面这么写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM xiaoqinfeng&#x2F;base</span><br><span class="line"></span><br><span class="line">RUN pip install -U pip</span><br><span class="line">RUN pip install numpy scipy pandas matplotlib tqdm</span><br><span class="line">RUN pip install torch&#x3D;&#x3D;1.5.0+cu101 torchvision&#x3D;&#x3D;0.6.0+cu101 -f https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;torch_stable.htmlCopy</span><br></pre></td></tr></table></figure>
<p>因为这里我用的CUDA10.1，其他版本的CUDA安装指令可能不太一样，具体可以参考<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">官网</a>。</p>
<h2 id="configure-tensorflow">Configure Tensorflow</h2>
<p>如果是安装Tensorflow，那么构建Tensorflow的Dockerfile可以像下面这么写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM xiaoqinfeng&#x2F;base</span><br><span class="line"></span><br><span class="line">RUN pip install -U pip</span><br><span class="line">RUN pip install numpy scipy pandas matplotlib tqdm tensorflow-gpuCopy</span><br></pre></td></tr></table></figure>
<p>这里会自动安装最新版本的Tensorflow2。Tensorflow不同版本对CUDA和cuDNN版本甚至Python版本的支持都不太一样，可以参考<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source#linux">官网</a>的说明。</p>
<h1 id="deep-learning-with-openpai">Deep Learning with OpenPAI</h1>
<h2 id="what-is-openpai">What is OpenPAI</h2>
<p>OpenPAI是一个分布式深度学习计算资源管理平台，对于我们用户来说，只需要定义好Docker镜像，然后编写好任务设置，提交到平台之后，平台便会自动分配计算资源来运行任务。</p>
<p>OpenPAI界面：</p>
<p><img src="https://i.loli.net/2020/06/24/CDrZjgYR7lNcieh.png" style="zoom: 50%;" /></p>
<p><img src="https://i.loli.net/2020/06/24/FGJpVCbls7YP4aM.png" style="zoom: 50%;" /></p>
<p>下面我们来讲讲怎么向OpenPAI平台提交任务。</p>
<h2 id="submit-jobs-to-openpai">Submit Jobs to OpenPAI</h2>
<h3 id="pack-code-data-files">Pack Code &amp; Data Files</h3>
<p>假设你已经完成了代码的编写和测试，你的目录结构可能看起来是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── README.md</span><br><span class="line">├── data</span><br><span class="line">│   └── dataset.csv</span><br><span class="line">├── main.py</span><br><span class="line">└── src</span><br><span class="line">    ├── data.py</span><br><span class="line">    └── net.pyCopy</span><br></pre></td></tr></table></figure>
<p>因为OpenPAI会创建一个虚拟容器来运行你的代码，所以你的数据和代码必须要以某种方式传送到OpenPAI上的虚拟容器中。我们先来打包，在代码目录下执行<code>tar -cvf files.tar ./</code>。之后，运行<code>python -m http.server &lt;port&gt;</code>。打开浏览器输入<code>&lt;server_ip&gt;:&lt;port&gt;</code>应该就能看到你的文件了：</p>
<p><img src="https://i.loli.net/2020/06/24/Cbgiw3XKnGYsNHp.png" /></p>
<p>由于这个http进程需要一直运行，所以建议使用<code>screen</code>放到后台执行。</p>
<h3 id="configure-tasks">Configure Tasks</h3>
<p>像OpenPAI提交任务可以采用网页提交也可以使用VSCode插件，这里我们采用网页提交。登入OpenPAI界面，点击Submit Job：</p>
<p><img src="https://i.loli.net/2020/06/24/bhXAcJfOmo8RtT2.png" /></p>
<p>可以看到提交任务的界面：</p>
<p><img src="https://i.loli.net/2020/06/24/KME8GuN49gyY1ph.png" /></p>
<p>Job name大家可以自己设置。在Command一栏，是执行任务所需的全部命令，首先我们要做的就是将代码数据压缩包下载到容器中并解压：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget &lt;server_ip&gt;:&lt;port&gt;&#x2F;files.tar</span><br><span class="line"></span><br><span class="line">tar -xvf files.tarCopy</span><br></pre></td></tr></table></figure>
<p>然后是运行代码，假设我这里的任务比较简单，只有一行main.py的调用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.pyCopy</span><br></pre></td></tr></table></figure>
<p>如果任务的执行比较复杂，也只需把命令填到Command里即可，OpenPAI会自动执行。接下来是设置配置，可以选GPU的数量，内存大小等等：</p>
<p><img src="https://i.loli.net/2020/06/24/T7ryOxjpJP4FWYR.png" style="zoom:67%;" /></p>
<p>然后是镜像的选择：</p>
<p><img src="https://i.loli.net/2020/06/24/3n7SpNlsLjBRvgE.png" style="zoom:67%;" /></p>
<p>要注意在本机上构建好镜像之后，需要把镜像重命名为<code>&lt;repository_address&gt;/&lt;image_name&gt;</code>的格式（我们的<code>&lt;repository&gt;</code>是<code>lin-ai-27:5000</code>，假设我的镜像名是<code>xiaoqinfeng/pytorch</code>，那就是改成<code>lin-ai-27:5000/xiaoqinfeng/pytorch</code>），然后执行<code>docker push</code>推送到Docker镜像服务器上才能在OpenPAI上使用。</p>
<p>提交之后，可以在Jobs界面看到任务的运行情况：</p>
<p><img src="https://i.loli.net/2020/06/24/ZD4cUgOCIqnyHdB.png" style="zoom: 50%;" /></p>
<h1 id="misc">Misc</h1>
<h2 id="store-files-in-containers">Store Files in Containers</h2>
<p>我们往往需要在程序运行的时候保存文件，如checkpoints等。在OpenPAI上执行程序的话文件是保存在程序中的，如果我们想要在运行完之后把文件复制到本地电脑上呢？这个时候就需要在任务的配置文件里加上复制文件到HDFS的语句。首先确认你的HDFS的URL：如<code>hdfs://172.31.246.52:9000/你的OpenPAI用户名/</code>。</p>
<p>如果要创建文件夹，则可以使用<code>hdfs dfs -mkdir -p &lt;HDFS URL&gt;+&lt;New Folder&gt;</code>。这里<code>&lt;New Folder&gt;</code>是你要创建的的文件夹的路径，用起来和Linux的<code>mkdir</code>命令其实是差不多的。</p>
<p>要复制文件（夹）则使用<code>hdfs dfs -cp &lt;Source Dir&gt; &lt;Dest Dir&gt;</code>。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-02T02:26:13.000Z" title="2020-6-2 10:26:13 ├F10: AM┤">2020-06-02</time>发表</span><span class="level-item"><time dateTime="2020-12-21T14:05:52.434Z" title="2020-12-21 10:05:52 ├F10: PM┤">2020-12-21</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA11-cuDNN8-Tensorflow-Pytorch/">Ubuntu20.04LTS 深度学习环境配置 CUDA11 + cuDNN8 + Tensorflow + Pytorch</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>Ubuntu的最新LTS版本也更新到了20.04，在给新机器配置深度学习环境的时候发现比以前容易了许多，特此写一篇Tutorial。这里的安装方法只针对Ubuntu20.04LTS，对于其他版本的系统可能不太适用。</p>
<h1 id="install-gpu-drivers">Install GPU Drivers</h1>
<p>这里假设安装系统之后已经做好了必要的配置（安装常用软件依赖、修改国内源等）。Ubuntu20.04中GPU驱动可以直接通过GUI界面安装，十分方便，方法是找到软件与更新 (Software &amp; Updates)，在附加驱动 (additional drivers) 选项卡中选择驱动版本，一般是选择“专有，tested” (proprietary, tested) 那个，之后点Apply Changes，重启。</p>
<p><img src="https://i.loli.net/2020/06/24/xevtVosYOH7D68I.png" /></p>
<p>如果是服务器的话，也可以采用命令行的安装方法。在命令行输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu-drivers devices</span><br></pre></td></tr></table></figure>
<p>会列出驱动信息：</p>
<figure>
<img src="https://i.loli.net/2020/12/19/CGWnoRFPz23r46K.png" alt="" /><figcaption>image-20201219103330645</figcaption>
</figure>
<p>可以看到推荐驱动版本是455，直接使用命令<code>ubuntu-drivers autoinstall</code>就可以自动安装推荐版本的驱动了。</p>
<h1 id="install-cuda-cudnn">Install CUDA &amp; cuDNN</h1>
<p>最常用的方式是按照<a href="%5BCUDA%20Toolkit%2011.0%20Download%20%7C%20NVIDIA%20Developer%5D(https://developer.nvidia.com/cuda-11.0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal)">官网</a>的指导来安装CUDA，之后需要设置环境变量，在<code>~/.bashrc</code>中添加下列指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-11.0/bin<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-11.0/lib64\</span><br><span class="line">                         <span class="variable">$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span></span><br></pre></td></tr></table></figure>
<p>还可以选择简单的方式，直接使用apt-get来安装CUDA：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install nvidia-cuda-toolkit</span><br></pre></td></tr></table></figure>
<p>不过这样会直接安装最新的CUDA版本，而PyTorch等深度学习框架不一定对最新的CUDA版本都有很好的支持，所以不推荐这种方式。</p>
<p>安装完CUDA之后输入<code>nvcc --version</code>可以测试是否安装成功，输入<code>nvidia-smi</code>可以看到GPU信息和CUDA版本。</p>
<p>之后安装cuDNN，进入<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">官网</a>，选择Download cuDNN：</p>
<p><img src="https://i.loli.net/2020/06/24/VYBRoINFDC9ObA1.png" /></p>
<p>会要求登录，如果没有账号的注册一个即可。在这里根据CUDA版本选择适合的cuDNN，我这里是CUDA10.2。我们选择deb包的方式安装，下载下图中圈出来的三个deb包，依次用<code>sudo dpkg -i xxx.deb</code>命令安装。</p>
<p><img src="https://i.loli.net/2020/06/24/qcDG5t3JY6vfoQM.png" /></p>
<h1 id="configure-python">Configure Python</h1>
<p>为了更好地管理Python包和虚拟环境，我们需要安装Anaconda。使用Anaconda之后，我们可以创建虚拟环境，虚拟环境之间互不干扰。做科学实验我们一般需要安装大量的Python包，有的包之间甚至还有冲突，如果我们把他们都安装在同一个环境下就会难以管理，甚至出冲突。而有了虚拟环境之后，我们可以把不同需求放在不同虚拟环境中，比如深度学习开发放在一个虚拟环境中（安装Tensorflow等），网站开发放在一个虚拟环境中（安装Flask等）。Anaconda默认自带大量的包，不过我们一般会创建新的虚拟环境去安装新的包，所以这里我们选用Miniconda。Miniconda和Anaconda唯一的区别是不会自带大量Python包，这里大家自行选择。Anaconda国内镜像下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/，Miniconda国内镜像下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/。</p>
<p>比如说我们下载的是<code>Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>，执行<code>bash Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>即可安装。显示一大屏用户协议哪儿按<code>q</code>可以直接跳过，其他选项的默认的输入<code>yes</code>即可。在提示是否需要conda init的时候记得输入<code>yes</code>。</p>
<p>安装成功之后，重开一个终端，可以看到现在处于<code>base</code>环境中：</p>
<p><img src="https://i.loli.net/2020/06/24/giml1UKzWuyqTjr.png" /></p>
<p>我们先配置一下国内镜像，执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.condarc</span><br></pre></td></tr></table></figure>
<p>然后粘贴下列文本使用清华源：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;r</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;pro</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  msys2: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  bioconda: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  menpo: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  pytorch: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  simpleitk: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br></pre></td></tr></table></figure>
<p>之后我们输入<code>conda create -n &lt;your_name&gt;</code>创建一个新的虚拟环境，如果需要指定Python版本，则<code>conda create --n &lt;your_name&gt; python=&lt;python_version&gt;</code>。之后输入<code>conda activate &lt;your_name&gt;</code>进入虚拟环境，如果需要退出，则使用<code>conda deactivate</code>。</p>
<p>安装常用包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install --yes numpy scipy pandas matplotlib tqdm pip jupyter</span><br></pre></td></tr></table></figure>
<p><code>--yes</code>的作用是手动输入<code>y</code>来确认是否安装，这里列出的是一些最常用的Python包，大家可以根据自己的需求自行调整。<code>conda install</code>为Anaconda中安装Python包的方式。</p>
<h1 id="install-pytorch">Install PyTorch</h1>
<p>这里来安装PyTorch环境，推荐使用<code>conda create -n pytorch</code>创建一个专有虚拟环境，然后使用<code>conda install</code>安装常用包。对于安装PyTorch，我们可以使用<code>conda</code>也可以使用<code>pip</code>安装。<code>pip</code>是另外一个安装Python包的工具，由于不检查依赖所以比<code>conda</code>安装速度快，而且包的数量比<code>conda</code>多，使用也更广泛。同样<code>pip</code>也可以使用国内镜像加速下载，详见https://mirrors.tuna.tsinghua.edu.cn/help/pypi/。</p>
<p>对于CUDA11.0，官方给出的用<code>conda</code>安装PyTorch的命令（CUDA11.1也是这个）是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch</span><br></pre></td></tr></table></figure>
<p>用<code>pip</code>安装PyTorch的命令是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure>
<p>对于其他版本的CUDA安装命令可能不一样，可以去<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">官网</a>查看。</p>
<p>可以使用以下命令来测试GPU版本的PyTorch是否正常工作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(torch.cuda.is_available())&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="install-tensorflow">Install Tensorflow</h1>
<p>安装Tensorflow环境同样推荐创建一个专有虚拟环境。对于Tensorflow2的安装，使用<code>pip</code>十分方便，使用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow tensorflow-gpu</span><br></pre></td></tr></table></figure>
<p>即可。要安装其他版本的Tensorflow可以使用<code>pip install tensorflow==&lt;tf_version&gt; tensorflow-gpu==&lt;tf_version&gt;</code>来指定版本。不过不同版本的Tensorflow要求的CUDA版本都有所不同，可以参考<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source#linux">官网</a>的说明。</p>
<p>可以使用以下命令来测试GPU版本的Tensorflow是否正常工作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import tensorflow as tf; tf.config.list_physical_devices(&#x27;GPU&#x27;)&quot;</span></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-01-31T03:25:59.000Z" title="2020-1-31 11:25:59 ├F10: AM┤">2020-01-31</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:39:27.592Z" title="2020-6-25 1:39:27 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/">Geant4 安装教程与调试环境配置</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>Geant4安装的教程很多，版本都很旧了，这里写一个新版本（10.6）基于Ubuntu的安装教程，并且开启CLion IDE调试。</p>
<h1 id="step-1-download-packages">Step 1: Download Packages</h1>
<p>首先进入官网(<a target="_blank" rel="noopener" href="http://geant4.web.cern.ch/support/download">http://geant4.web.cern.ch/support/download</a>)下载源代码（推荐tar.gz格式）及数据文件，解压。新建一个文件夹专门用来放<code>Geant4</code>相关文件，新建data，source，build文件夹，将Geant4的文件复制进来并按如下结构组织：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build</span><br><span class="line">├── data</span><br><span class="line">│   ├── G4ABLA3.1</span><br><span class="line">│   ├── G4EMLOW7.9</span><br><span class="line">│   ├── G4ENSDFSTATE2.2</span><br><span class="line">│   ├── G4INCL1.0</span><br><span class="line">│   ├── G4NDL4.6</span><br><span class="line">│   ├── G4PARTICLEXS2.1</span><br><span class="line">│   ├── G4PII1.3</span><br><span class="line">│   ├── G4SAIDDATA2.0</span><br><span class="line">│   ├── G4TENDL1.3.2</span><br><span class="line">│   ├── PhotonEvaporation5.5</span><br><span class="line">│   ├── RadioactiveDecay5.4</span><br><span class="line">│   └── RealSurface2.1.1</span><br><span class="line">└── <span class="built_in">source</span></span><br><span class="line">    └── geant4.10.06</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2020/06/25/OuZaAJ3WyEYwsG7.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/R5Tmk68AhPbaDHY.png" /></p>
<h1 id="step-2-install-dependencies">Step 2: Install Dependencies</h1>
<p>安装编译所需环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential cmake</span><br></pre></td></tr></table></figure>
<p>安装相关依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libgl1-mesa-dev libglu1-mesa-dev libxt-dev libxmu-dev libxi-dev zlib1g-dev libgl2ps-dev libexpat1-dev libxerces-c-dev</span><br></pre></td></tr></table></figure>
<p>如果要用到QT需要单独安装QT。</p>
<h1 id="step-3-compile">Step 3: Compile</h1>
<p>进入build文件夹，用cmake命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake ../<span class="built_in">source</span>/geant4.10.06/ -DCMAKE_BUILD_TYPE=DEBUG -DGEANT4_USE_GDML=ON -DGEANT4_USE_OPENGL_X11=ON -DGEANT4_USE_RAYTRACER_X11=ON -DGEANT4_BUILD_MULTITHREADED=ON</span><br></pre></td></tr></table></figure>
<p>其中<code>../source/geant4.10.06/</code>替换成换成（如果版本不一样）你自己的Geant4源代码所在目录，需要QT则加上<code>-DGEANT4_USE_QT=ON</code>。如果不需要调试则把<code>-DCMAKE_BUILD_TYPE=DEBUG</code>改成<code>-DCMAKE_BUILD_TYPE=RELEASE</code>。<code>-DGEANT4_BUILD_MULTITHREADED=ON</code>是多线程，视情况开启。</p>
<p>完成之后开始编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -jX</span><br></pre></td></tr></table></figure>
<p><code>-jX</code>为多线程编译，如<code>-j8</code>。</p>
<p>编译完成之后进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<h1 id="step-4-configure">Step 4: Configure</h1>
<p>安装的默认路径在<code>/usr/local/share/Geant4-10.6.0</code>，将下载的数据文件复制到该文件夹：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp -r ./data/ /usr/<span class="built_in">local</span>/share/Geant4-10.6.0/</span><br></pre></td></tr></table></figure>
<p>之后，在<code>~/.bashrc</code>里添加<code>/usr/local/share/Geant4-10.6.0/geant4make/geant4make.sh</code>，如果你的版本和我的不一样，相应修改即可。</p>
<h1 id="step-5-clion-configuration">Step 5: CLion Configuration</h1>
<p>最后我们来配置CLion环境，配好之后可以在IDE中编写<code>Geant4</code>代码，还可以断点调试，非常方便。安装CLion的过程这里省略，打开一个<code>Geant4</code>自带的例子或者自己新建一个项目，打开<code>Edit Configurations</code>。</p>
<p><img src="https://i.loli.net/2020/06/25/W1xXHUqIvofyKQk.png" /></p>
<p>随便打开一个终端，输入一下命令获取环境变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env | grep G4</span><br></pre></td></tr></table></figure>
<p>在<code>Environment variables</code>填入刚才获取的环境变量（复制之后按一下粘贴就可以了），然后把<code>Working directory</code>设置成当前文件夹。</p>
<p><img src="https://i.loli.net/2020/06/25/HrslFTOau51y8tX.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/5UKaCgvnWjYmNTG.png" /></p>
<p>现在就大功告成了！</p>
</div></article></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>