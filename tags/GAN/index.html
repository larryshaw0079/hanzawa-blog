<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>标签: GAN - Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="website"><meta property="og:title" content="Hanzawa の 部屋"><meta property="og:url" content="https://larryshaw0079.github.io/hanzawa-blog"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://larryshaw0079.github.io/img/og_image.png"><meta property="article:author" content="Hanzawa"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://larryshaw0079.github.io/hanzawa-blog"},"headline":"Hanzawa の 部屋","image":["https://larryshaw0079.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":null}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">标签</a></li><li class="is-active"><a href="#" aria-current="page">GAN</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-06T04:03:27.000Z" title="2020-6-6 12:03:27 ├F10: PM┤">2020-06-06</time>发表</span><span class="level-item"><time dateTime="2020-06-25T08:14:29.250Z" title="2020-6-25 4:14:29 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/06/Generative-Probabilistic-Novelty-Detection-with-Adversarial-Autoencoders/">Generative Probabilistic Novelty Detection with Adversarial Autoencoders</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>这篇文章介绍了一种基于概率分布的异常检测方法。其基本思想是假设正常样本服从定义在流形<span class="math inline">\(M\)</span>上的分布，而对于任意一点<span class="math inline">\(\bar x\)</span>，通过投影到流形<span class="math inline">\(M\)</span>上<span class="math inline">\(x^\parallel\)</span>，可以分解为平行于切空间的部分<span class="math inline">\(x^\parallel\)</span>和正交与切空间的部分<span class="math inline">\(x^\bot\)</span>。原始的坐标<span class="math inline">\(\bar x\)</span>被转换到<span class="math inline">\(x^\parallel\)</span>局部坐标系中，然后似然通过转换后的坐标系进行计算。</p>
<h1 id="methodology">Methodology</h1>
<h2 id="generative-probabilistic-novelty-detection">Generative Probabilistic Novelty Detection</h2>
<p>我们假设训练数据<span class="math inline">\(x_1,\cdots,x_N\)</span>，其中<span class="math inline">\(x_i\in\mathbb{R}^m\)</span>，从一个分布采样的来，并带有随机噪声<span class="math inline">\(\xi\)</span>： <span class="math display">\[
x_i=f(z_i)+\xi_i, \space\space\space i=1,\cdots,N
\]</span> 其中<span class="math inline">\(z_i\in\mathbb{R}^n\)</span>，<span class="math inline">\(f:\Omega\mapsto\mathbb{R}^m\)</span>定义了一个<span class="math inline">\(n\)</span>维带参流形<span class="math inline">\(\mathcal{M}\equiv f(\Omega)\)</span>。注意这里噪声的加入使得样本的值域扩展到了整个实数空间。同时假设存在<span class="math inline">\(g:\mathbb{R}^m\mapsto\mathbb{R}^n\)</span>，对任意<span class="math inline">\(x\in\mathcal{M}\)</span>都有<span class="math inline">\(f(g(x))=x\)</span>。<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>后面会通过神经网络实现。</p>
<p>对于一个测试样本<span class="math inline">\(\bar{x}\in\mathbb{R}^m\)</span>，我们可以得到其在<span class="math inline">\(M\)</span>上的投影，这是通过逆变换<span class="math inline">\(\bar z = g(\bar x)\)</span>得到对应<span class="math inline">\(z\)</span>的然后再通过<span class="math inline">\(\bar x^{\parallel}=f(\bar z)\)</span>得到。<span class="math inline">\(f\)</span>在<span class="math inline">\(\bar z\)</span>的一阶泰勒展开为： <span class="math display">\[
f(z)=f(\bar z)+J_f(\bar z)(z-\bar z)+O(\parallel z-\bar z\parallel ^2)
\]</span> <img src="https://i.loli.net/2020/06/25/oi9xKMO3ID7jANJ.png" style="zoom:67%;" /></p>
<p>其中<span class="math inline">\(J_f(\bar z)\)</span>为<span class="math inline">\(f\)</span>在点<span class="math inline">\(\bar z\)</span>的雅各比矩阵。<span class="math inline">\(\mathcal T=\text{span}(J_f(\bar z))\)</span>代表点<span class="math inline">\(\bar z\)</span>处由<span class="math inline">\(J_f(\bar z)\)</span>的<span class="math inline">\(n\)</span>个独立向量组成的切空间。通过对<span class="math inline">\(J_f(\bar z)\)</span>进行奇异值分解<span class="math inline">\(J_f(\bar z)=U^\parallel SV^\top\)</span>。 <span class="math display">\[
\bar w=U^\top\bar x=\left[\begin{matrix}U^{\parallel^\top}\bar x\\ U^{\bot^\top}\bar x\end{matrix}\right]=\left[\begin{matrix}\bar w^\parallel\\ \bar w^\bot\end{matrix}\right]
\]</span> 坐标<span class="math inline">\(\bar w\)</span>可以分解为平行于<span class="math inline">\(\mathcal T\)</span>和正交于<span class="math inline">\(\mathcal T\)</span>两部分。</p>
<p>定义在施加变换前后的坐标系上的概率分布<span class="math inline">\(p_X(x)\)</span>和<span class="math inline">\(p_W(w)\)</span>是等价的，不过对于<span class="math inline">\(p_W(w)\)</span>，我们假设平行部分和正交部分是独立的，即： <span class="math display">\[
p_X(x)=p_W(w)=p_W(w^\parallel,w^\bot)=p_{W^\parallel}(w^\parallel)p_{W^\bot}(w^\bot)
\]</span> 这一假设的依据是随机噪声部分假设主要是往流形之外偏离的，即与<span class="math inline">\(\mathcal T\)</span>正交，所以<span class="math inline">\(W^\bot\)</span>主要是反映噪声的部分。而噪声与样本分布相独立的假设是合理的。于是，异常分数可以定义为： <span class="math display">\[
p_X(\bar x)=p_{W^\parallel}(\bar w^\parallel)p_{W^\bot}(\bar w^\bot)=\begin{cases}\geq \gamma \Rightarrow \text{Inlier}\\&lt;\gamma\Rightarrow\text{Outlier}\end{cases}
\]</span></p>
<h2 id="computing-the-distribution-of-data-samples">Computing the Distribution of Data Samples</h2>
<p>上面的异常分数需要计算<span class="math inline">\(p_{W^\parallel}(\bar w^\parallel)\)</span>和<span class="math inline">\(p_{W^\bot}(\bar w^\bot)\)</span>。给定测试样本<span class="math inline">\(\bar x\)</span>，投影到流形<span class="math inline">\(\bar x^\parallel=f(g(\bar x))\)</span>。<span class="math inline">\(\bar w^\parallel\)</span>可以重写为<span class="math inline">\(\bar w^\parallel=U^{\parallel^\top}\bar x=U^{\parallel^\top}(\bar x-\bar x^{\parallel})+U^{\parallel^\top}\bar x^\parallel=U^{\parallel^\top}\bar x^\parallel\)</span>，即我们假设<span class="math inline">\(U^{\parallel^\top}(\bar x-\bar x^\parallel)\approx 0\)</span>。于是有<span class="math inline">\(w^\parallel(z)=U^{\parallel^\top}f(\bar z)+SV^\top(z-\bar z)+O(\parallel z-\bar z\parallel^2)\)</span>。</p>
<p>如果<span class="math inline">\(Z\)</span>为定义在流形上的概率分布，那么： <span class="math display">\[
p_{W^\parallel}(w^\parallel)=|\text{det}S^{-1}|p_Z(z)
\]</span> <span class="math inline">\(p_{W^\bot}(w^\bot)\)</span>由半径为<span class="math inline">\(\parallel w^\bot\parallel\)</span>的超球体<span class="math inline">\(\mathcal S^{m-n-1}\)</span>来进行估计： <span class="math display">\[
p_{W^\bot}(w^\bot)\approx\frac{\Gamma(\frac{m-n}{2})}{2\pi^{\frac{m-n}{2}}\parallel w^\bot\parallel^{m-n}}p_{\parallel W^\bot\parallel}(\parallel w^\bot\parallel)
\]</span></p>
<p>其中<span class="math inline">\(\Gamma(\cdot)\)</span>代表Gamma函数。</p>
<h2 id="manifold-learning-with-adversarial-autoencoders">Manifold Learning with Adversarial Autoencoders</h2>
<p>为了学习映射<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>，我们使用了AAE框架，如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/sQhO3D4gKJqBPXv.png" style="zoom: 67%;" /></p>
<p>除了常规的AAE外，我们还为<span class="math inline">\(x\)</span>添加了一个额外的判别器。</p>
<h3 id="adversarial-losses">Adversarial Losses</h3>
<p>对于隐变量<span class="math inline">\(z\)</span>，对抗损失函数为： <span class="math display">\[
\mathcal L_{adv-d_z}(x,g,D_z)=E[\log(D_z(\mathcal N(0,1)))]+E[\log(1-D_z(g(x)))]
\]</span> 对于样本<span class="math inline">\(x\)</span>，对抗损失函数为： <span class="math display">\[
\mathcal L_{adv-d_x}(x,D_x,f)=E[\log(D_x(x))]+E[\log(1-D_x(f(\mathcal N(0,1))))]
\]</span></p>
<h3 id="autoencoder-loss">Autoencoder Loss</h3>
<p><span class="math display">\[
\mathcal L_\text{error}(x,g,f)=-E_z[\log(p(f(g(x))|x))]
\]</span></p>
<h3 id="full-objective">Full Objective</h3>
<p><span class="math display">\[
\mathcal L(x,g,D_z,D_x,f)=\mathcal L_{adv-d_z}+\mathcal L_{adv-d_x}+\lambda \mathcal L_\text{error}
\]</span></p>
<p>下图为模型重构的例子：</p>
<p><img src="https://i.loli.net/2020/06/25/i7ytlgjoIbYV6uF.png" style="zoom:67%;" /></p>
<h1 id="experiments">Experiments</h1>
<h2 id="datasets">Datasets</h2>
<ul>
<li><strong>MNIST. </strong> 手册数字识别数据集。</li>
<li><strong>The Coil-100. </strong>包含7200张100个不同物体的不同角度的图片。</li>
<li><strong>Fashion-MNIST. </strong> 手册数字识别数据集彩色版。</li>
<li><strong>Others. </strong> 前三个数据集都是采用一个类作为inlier，而其他类作为outlier。在这一设置中inlier采样自数据集CIFAR-10(CIFAR-100)，而outlier采样自TinyImageNet、LSUN和iSUN。</li>
</ul>
<h2 id="results">Results</h2>
<h3 id="mnist-dataset">MNIST Dataset</h3>
<p><img src="https://i.loli.net/2020/06/25/5a71oidmK2ZGLyh.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/4lcGeHDrhbdKN5W.png" style="zoom:67%;" /></p>
<h3 id="coil-100-dataset">Coil-100 Dataset</h3>
<p><img src="https://i.loli.net/2020/06/25/ofVGBgR7a3WmyvU.png" style="zoom:67%;" /></p>
<h3 id="fashion-mnist">Fashion-MNIST</h3>
<p><img src="https://i.loli.net/2020/06/25/avURoBw6ny8SIEq.png" /></p>
<h3 id="cifar-10-cifar-100">CIFAR-10 (CIFAR-100)</h3>
<p><img src="https://i.loli.net/2020/06/25/piteKy1m9kvQ6EU.png" style="zoom: 67%;" /></p>
<h3 id="ablation">Ablation</h3>
<p><img src="https://i.loli.net/2020/06/25/xgni9wBtYkheGZq.png" /></p>
<p><img src="https://i.loli.net/2020/06/25/idhqkCbAKvzMt68.png" style="zoom:67%;" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-16T12:19:10.000Z" title="2019-10-16 8:19:10 ├F10: PM┤">2019-10-16</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:38:07.244Z" title="2020-6-25 1:38:07 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Time-Series-Imputation/">Time Series Imputation</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/16/GAIN-Missing-Data-Imputation-using-Generative-Adversarial-Nets/">GAIN: Missing Data Imputation using Generative Adversarial Nets</a></h1><div class="content"><h1 id="abstract">Abstract</h1>
<p>本文基于GAN提出了一种时间序列缺失值填充（Time Series Imputation）的方法。其主要的思路为生成器<span class="math inline">\(G\)</span>从隐空间<span class="math inline">\(Z\)</span>生成完整的样本，而判别器<span class="math inline">\(D\)</span>则输出样本中不同部分为真实的概率。除此之外，作者提出了使用Hint Vector来揭示原始数据中缺失部分的信息，来优化训练过程。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.02920">原文</a></p>
<h1 id="methodology">Methodology</h1>
<h2 id="problem-formulation">Problem Formulation</h2>
<p>考虑一个<span class="math inline">\(d\)</span>维的空间<span class="math inline">\(\mathcal{X}=\mathcal{X}_1\times \cdots\times \mathcal{X}_d\)</span>，设<span class="math inline">\(\mathbf{X}=(X_1,\cdots,X_d)\)</span>维空间<span class="math inline">\(\mathcal{X}\)</span>上的随机向量（即理想的完整的时间序列），记其分布为<span class="math inline">\(P(\mathbf{X})\)</span>。设<span class="math inline">\(\mathbf{M}=(M_1,\cdots,M_d)\)</span>为Mask向量表示<span class="math inline">\(\mathbf{X}\)</span>中被观察到的部分。（即标识时间序列哪些部分有缺失），取值为<span class="math inline">\(\{0,1\}^d\)</span>。</p>
<p>对于每一个<span class="math inline">\(i\in\{1,\cdots,d\}\)</span>，我们定义一个新空间<span class="math inline">\(\tilde{\mathcal{X}}=\mathcal{X}\cup\{*\}\)</span>，其中<span class="math inline">\(*\)</span>表示不属于任意<span class="math inline">\(\mathcal{X}_i\)</span>的一个点。令<span class="math inline">\(\tilde{\mathcal{X}}=\tilde{\mathcal{X}_1}\times\cdots\times\tilde{\mathcal{X}_d}\)</span>，同时定义一个新的随机变量（即我们观测到的含有缺失值的时间序列）<span class="math inline">\(\tilde{\mathbf{X}}=(\tilde{X}_1,\cdots,\tilde{X}_d)\in \tilde{\mathcal{X}}\)</span>： <span class="math display">\[
\tilde{X}_i=\begin{cases}X_i,&amp;\text{if } M_i=1\\*,&amp;\text{otherwise}\end{cases}
\]</span> 假设数据集的形式为<span class="math inline">\(\mathcal{D}=\{(\tilde{x}^i,m^i)\}^n_{i=1}\)</span>，我们的任务是从<span class="math inline">\(P(\mathbf{X}|\tilde{\mathbf{X}}=\tilde{x}^i)\)</span>上采样来对缺失值进行填充。</p>
<h2 id="model-architecture">Model Architecture</h2>
<p>模型的架构如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/wCK3J8MoTASrj9Y.png" style="zoom:67%;" /></p>
<h3 id="generator">Generator</h3>
<p>生成器的输入有三项：<span class="math inline">\(\tilde{\mathbf{X}}\)</span>，<span class="math inline">\(\mathbf{M}\)</span>和随机噪声<span class="math inline">\(\mathbf{Z}\)</span>，输出设为<span class="math inline">\(\bar{\mathbf{X}}\)</span>。设生成器为映射<span class="math inline">\(G: \tilde{\mathcal{X}}\times\{0,1\}^d\times[0,1]^d\rightarrow \mathcal{X}\)</span>，而<span class="math inline">\(\mathbf{Z}\)</span>为<span class="math inline">\(d\)</span>维的高斯噪声。生成器的输出和填充后的时间序列定义为： <span class="math display">\[
\begin{align}
\bar{\mathbf{X}}&amp;=G(\tilde{\mathbf{X}},\mathbf{M},(1-\mathbf{M})\odot\mathbf{Z})\\
\hat{\mathbf{X}}&amp;=\mathbf{M}\odot\tilde{\mathbf{X}}+(1-\mathbf{M})\odot\bar{\mathbf{X}}
\end{align}
\]</span> <span class="math inline">\(\bar{\mathbf{X}}\)</span>即为生成器的直接输出，因为其实有些部分没有缺失，生成器还是会为每个部分输出值。</p>
<p><span class="math inline">\(\hat{\mathbf{X}}\)</span>为填充后的时间序列，对于缺失的部分采用生成器的输出进行填充。</p>
<h3 id="discriminator">Discriminator</h3>
<p>和原始的GAN不同的是，我们不需要判断整个样本是真实的或者是生成的，而是需要判断样本的那些部分是真实的或者是生成的，所以判别器为映射<span class="math inline">\(D: \mathcal{X}\rightarrow[0,1]^d\)</span>。判别器的具体目标函数将在后面讨论。</p>
<h3 id="hint">Hint</h3>
<p>Hint是一种提示机值，是一个和<span class="math inline">\(\mathbf{X}\)</span>相同维度的随机变量<span class="math inline">\(\mathbf{H}\)</span>，其分布依赖于<span class="math inline">\(\mathbf{M}\)</span>。<span class="math inline">\(\mathbf{H}\)</span>是由用户自己定义的，相当于一种不完整的<span class="math inline">\(\mathbf{M}\)</span>，用来作为判别器的额外输入。</p>
<h3 id="objective">Objective</h3>
<p>我们训练判别器最大化正确预测<span class="math inline">\(\mathbf{M}\)</span>的概率，而生成器最小化判别器正确预测<span class="math inline">\(\mathbf{M}\)</span>的概率，目标函数如下： <span class="math display">\[
\begin{align}
V(D,G)=&amp;\mathbb{E}_{\hat{X},M,H}[\mathbf{M}^T\log D(\hat{\mathbf{X}},\mathbf{H})\\&amp;+(1-\mathbf{M})^T\log(1-D(\hat{\mathbf{X}},\mathbf{H}))]
\end{align}
\]</span> 按照标准的GAN可以将优化函数写成以下的形式： <span class="math display">\[
\min_G\max_D V(D,G)
\]</span> 在这里判别器的任务可以看作是一个二分类，而目标函数就是二值交叉熵的定义，因此可以写为： <span class="math display">\[
\mathcal{L}(a,b)=\sum\limits_{i=1}^d[a_i\log(b_i)+(1-a_i)\log(1-b_i)]
\]</span> <span class="math inline">\(\mathbf{M}\)</span>可以看作Ground Truth，记<span class="math inline">\(\hat{\mathbf{M}}=D(\hat{\mathbf{X},\mathbf{H}})\)</span>，即判别器输出的预测，因此优化函数可以简记为： <span class="math display">\[
\min_G\max_D\mathbb{E}[\mathcal{L}(\mathbf{M},\hat{\mathbf{M}})]
\]</span></p>
<h2 id="gain-algorithm">GAIN Algorithm</h2>
<p>下面讨论GAIN算法的训练流程。</p>
<p>本文通过理论讨论，给出了生成Hint Vector的一个方法，首先定义随机变量<span class="math inline">\(\mathbf{B}=(B_1,\cdots,B_d)\in\{0,1\}^d\)</span>，<span class="math inline">\(\mathbf{B}\)</span>通过从<span class="math inline">\(\{1,\cdots,d\}\)</span>随机均匀采样一个<span class="math inline">\(k\)</span>，然后由下列公式得到： <span class="math display">\[
B_j=\begin{cases}1, &amp;\text{if }j\neq k\\0, &amp;\text{if }j=k\end{cases}
\]</span> 定义空间<span class="math inline">\(\mathcal{H}=\{0,0.5,1\}^d\)</span>，Hint Vector为<span class="math inline">\(\mathbf{H}=\mathbf{B}\odot\mathbf{M}+0.5(1-\mathbf{B})\in\mathcal{H}\)</span>。</p>
<p>判别器的训练过程如下：固定生成器<span class="math inline">\(G\)</span>，对一个大小为<span class="math inline">\(k_D\)</span>的mini-batch，独立同分布采样<span class="math inline">\(k_D\)</span>个<span class="math inline">\(z\)</span>和<span class="math inline">\(b\)</span>，用来计算<span class="math inline">\(\mathbf{Z}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>。判别器的损失函数定义如下： <span class="math display">\[
\mathcal{L}_D(m,\hat{m},b)=\sum\limits_{i:b_i=0}[m_i\log(\hat{m}_i)+(1-m_i)\log(1-\hat{m}_i)]
\]</span> 判别器的优化函数为： <span class="math display">\[
\min_D-\sum\limits_{j=1}^{k_D}\mathcal{L}_D(m(j),\hat{m}(j),b(j))
\]</span> 其中<span class="math inline">\(\hat{m}(j)=D(\hat{x}(j),m(j))\)</span>。</p>
<p>在优化了判别器之后，需要优化生成器，对一个大小为<span class="math inline">\(k_G\)</span>的mini-batch，生成器的损失函数包含两个部分，一个是在缺失部分的损失：</p>
<p><span class="math display">\[
\mathcal{L}_G(m,\hat{m},b)=-\sum\limits_{i:b_i=0}(1-m_i)\log(\hat{m}_i)
\]</span> 一个是未缺失部分的损失： <span class="math display">\[
\mathcal{L}_M(x,x^\prime)=\sum\limits_{i=1}^d m_iL_M(x_i,x_i^\prime)
\]</span> 其中： <span class="math display">\[
L_M(x_i,x_i^\prime)=\begin{cases}(x_i^\prime-x_i)^2, &amp;\text{if }x_i\text{ is continuours},\\-x_i\log(x_i^\prime), &amp;\text{if }x_i\text{ is binary}.\end{cases}
\]</span> 最终的优化函数为： <span class="math display">\[
\min_G\sum\limits_{j=1}^{k_G}\mathcal{L}_G(m(j),\hat{m}(j),b(j))+\alpha\mathcal{L}_M(\tilde{x}(j),\hat{x}(j))
\]</span> 算法流程如下：</p>
<p><img src="https://i.loli.net/2020/06/25/znABi6x9mJuvDOX.png" style="zoom:67%;" /></p>
<h1 id="experiments">Experiments</h1>
<p>下表为在5个不同数据集上实验，与其他5种方法对比的结果：</p>
<p><img src="https://i.loli.net/2020/06/25/EenX2YO8aDxQAk3.png" style="zoom:67%;" /></p>
<p>上图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例、样本数量、特征维度下的对比曲线图。</p>
<p>下表为使用不同模型对时间序列进行填充之后，使用逻辑回归进行回归任务的性能：</p>
<p><img src="https://i.loli.net/2020/06/25/PzeWKdGu4wADshV.png" style="zoom:67%;" /></p>
<p>下图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例下的AUROC曲线图：</p>
<p><img src="https://i.loli.net/2020/06/25/IKtoTj8xgG1yJDk.png" style="zoom:67%;" /></p>
<p>下表展示的是作者对时间序列填充算法保持特征-标签关系的能力。作者分别用完整的数据和填充后的数据用逻辑回归模型进行训练，将两者的权重求绝对值和均方根的结果。</p>
<p><img src="https://i.loli.net/2020/06/25/uy2jPcnbtSrC6vI.png" style="zoom:67%;" /></p>
</div></article></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>