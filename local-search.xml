<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Grokking the Interview Coding Questions: High Frequency Problems</title>
    <link href="/2020/08/29/Grokking-the-Interview-Coding-Questions/"/>
    <url>/2020/08/29/Grokking-the-Interview-Coding-Questions/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文不会对算法的原理做过多的讲解，主要是记录常见问题的思路。。。</p><p>PS：本文代码皆通过LeetCode提交通过。。。</p><p>推荐两个网站：</p><ul><li><p><a href="https://oi-wiki.org/" target="_blank" rel="noopener">🐱‍🏍OI WIKI</a></p></li><li><p><a href="https://en.cppreference.com/w/" target="_blank" rel="noopener">📓C++文档</a> 也有中文版</p></li></ul><p>Still working on it😀...</p><h1 id="sorting-arrays">Sorting &amp; Arrays</h1><h2 id="basic-sorting">Basic Sorting</h2><p><a href="https://leetcode-cn.com/problems/sort-an-array/" target="_blank" rel="noopener">🐱‍💻Leetcode 912</a></p><p>不多说了，都是基础，几个<span class="math inline">\(O(N^2)\)</span>的算法实现起来没什么问题，主要是下面两个注意不要写错：</p><ul><li>快速排序</li></ul><pre><code class="hljs cpp"><span class="hljs-comment">// 要修改原数组的内容，引用&amp;是必须的</span><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">QuickSort</span><span class="hljs-params">(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; &amp;arr)</span> </span>&#123;    <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">std</span>::pair&lt;<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>&gt;&gt; ranges; <span class="hljs-comment">// 用来保存需要迭代的范围</span>    ranges.push_back(<span class="hljs-built_in">std</span>::make_pair(<span class="hljs-number">0</span>, arr.size()<span class="hljs-number">-1</span>)); <span class="hljs-comment">// 初始化，闭区间</span>        <span class="hljs-keyword">while</span> (!ranges.empty()) &#123;        <span class="hljs-keyword">auto</span> range = ranges.back(); <span class="hljs-comment">// 取出栈顶</span>        ranges.pop_back();                <span class="hljs-keyword">if</span> (range.first &gt;= range.second) <span class="hljs-keyword">continue</span>; <span class="hljs-comment">// 只有一个数的情况直接跳过</span>                <span class="hljs-keyword">int</span> mid_val = arr[range.second]; <span class="hljs-comment">// 最后一个数作为anchor</span>        <span class="hljs-keyword">int</span> left = range.first, right = range.second<span class="hljs-number">-1</span>;                <span class="hljs-keyword">while</span> (left &lt; right) &#123;            <span class="hljs-keyword">while</span> (left &lt; right &amp;&amp; arr[left] &lt; mid_val) left++;            <span class="hljs-keyword">while</span> (left &lt; right &amp;&amp; arr[right] &gt;= mid_val) right--;            <span class="hljs-built_in">std</span>::swap(arr[left], arr[right]);        &#125;                <span class="hljs-comment">// 处理特殊情况</span>        <span class="hljs-keyword">if</span> (arr[left] &gt;= arr[range.second]) <span class="hljs-built_in">std</span>::swap(arr[left], arr[range.second]);        <span class="hljs-keyword">else</span> left++;                ranges.push_back(<span class="hljs-built_in">std</span>::make_pair(range.first, left<span class="hljs-number">-1</span>));        ranges.push_back(<span class="hljs-built_in">std</span>::make_pair(left+<span class="hljs-number">1</span>, range.second));    &#125;&#125;</code></pre><ul><li>堆排序</li></ul><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">HeadAdjust</span><span class="hljs-params">(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; arr, <span class="hljs-keyword">int</span> start, <span class="hljs-keyword">int</span> end)</span> </span>&#123;    <span class="hljs-keyword">int</span> dad = start;    <span class="hljs-keyword">int</span> son = dad*<span class="hljs-number">2</span>+<span class="hljs-number">1</span>;    <span class="hljs-keyword">while</span> (son &lt;= end) &#123;            &#125;&#125;<span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">HeapSort</span><span class="hljs-params">(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; arr)</span> </span>&#123;    &#125;</code></pre><h2 id="top-k-element-第k大小元素">Top K Element 第K大/小元素</h2><p><a href="https://leetcode-cn.com/problems/kth-largest-element-in-an-array/" target="_blank" rel="noopener">🐱‍💻Leetcode 215</a> （同类题：<a href="https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof/" target="_blank" rel="noopener">🐱‍💻剑指Offer 40</a>）</p><p><strong>题目中说了给定的<span class="math inline">\(K\)</span>是合法的，所以下面的代码没有检查<span class="math inline">\(K\)</span>的合法性</strong></p><ul><li>先排序再求解 <span class="math inline">\(O(N\log N)\)</span> （⭐）</li></ul><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">findKthLargest</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> k)</span> </span>&#123;        <span class="hljs-built_in">std</span>::sort(nums.begin(), nums.end());        <span class="hljs-keyword">return</span> nums[nums.size()-k];    &#125;&#125;;</code></pre><ul><li>堆 <span class="math inline">\(O(N\log K)\)</span> （⭐⭐）</li></ul><p>注意求最小的<span class="math inline">\(K\)</span>个数的话用大顶堆，不然把全部数push到堆中然后一个一个弹出的话复杂度是<span class="math inline">\(O(N\log N)\)</span>，和排序没什么区别了。</p><p>使用<code>C++</code>自带<code>priority_queue</code>的版本：</p><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">TopK</span><span class="hljs-params">(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; arr, <span class="hljs-keyword">int</span> k)</span> </span>&#123;    <span class="hljs-built_in">std</span>::priority_queue&lt;<span class="hljs-keyword">int</span>, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;, <span class="hljs-built_in">std</span>::greater&lt;<span class="hljs-keyword">int</span>&gt;&gt; pq; <span class="hljs-comment">// greater是小顶堆，less是大顶堆</span>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> num : arr) &#123;        <span class="hljs-keyword">if</span> (pq.size() &lt; k) &#123;            pq.push(num);        &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (num &gt; pq.top()) &#123;            <span class="hljs-comment">// 如果栈顶元素大于当前元素</span>            pq.pop();            pq.push(num);        &#125;    &#125;        <span class="hljs-keyword">return</span> pq.top();&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">findKthLargest</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> k)</span> </span>&#123;        <span class="hljs-keyword">return</span> TopK(nums, k);    &#125;&#125;;</code></pre><ul><li>快排变体 <span class="math inline">\(O(N)\)</span> （⭐⭐⭐）</li></ul><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">TopK</span><span class="hljs-params">(<span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; arr, <span class="hljs-keyword">int</span> k)</span> </span>&#123;  <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">std</span>::pair&lt;<span class="hljs-keyword">int</span>, <span class="hljs-keyword">int</span>&gt;&gt; ranges;  ranges.push_back(<span class="hljs-built_in">std</span>::make_pair(<span class="hljs-number">0</span>, arr.size()<span class="hljs-number">-1</span>));  <span class="hljs-keyword">while</span> (!ranges.empty()) &#123;    <span class="hljs-keyword">auto</span> range = ranges.back();    ranges.pop_back();    <span class="hljs-keyword">if</span> (range.first &gt;= range.second) &#123;      <span class="hljs-keyword">if</span> (range.first == k || range.second == k) &#123;        <span class="hljs-keyword">return</span> arr[k];      &#125;      <span class="hljs-keyword">continue</span>;    &#125;    <span class="hljs-keyword">int</span> mid_val = arr[range.second];    <span class="hljs-keyword">int</span> left = range.first, right = range.second<span class="hljs-number">-1</span>;    <span class="hljs-keyword">while</span> (left &lt; right) &#123;      <span class="hljs-keyword">while</span> (left &lt; right &amp;&amp; arr[left] &lt; mid_val) left++;      <span class="hljs-keyword">while</span> (left &lt; right &amp;&amp; arr[right] &gt;= mid_val) right--;      <span class="hljs-built_in">std</span>::swap(arr[left], arr[right]);    &#125;    <span class="hljs-keyword">if</span> (arr[left] &gt;= arr[range.second]) <span class="hljs-built_in">std</span>::swap(arr[left], arr[range.second]);    <span class="hljs-keyword">else</span> left++;    <span class="hljs-keyword">if</span> (left == k) <span class="hljs-keyword">return</span> arr[left];    ranges.push_back(<span class="hljs-built_in">std</span>::make_pair(range.first, left<span class="hljs-number">-1</span>));    ranges.push_back(<span class="hljs-built_in">std</span>::make_pair(left+<span class="hljs-number">1</span>, range.second));  &#125;  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">findKthLargest</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums, <span class="hljs-keyword">int</span> k)</span> </span>&#123;        <span class="hljs-keyword">return</span> TopK(nums, nums.size()-k);    &#125;&#125;;</code></pre><h1 id="list">List</h1><h2 id="add-two-numbers-链表相加">Add Two Numbers 链表相加</h2><p><a href="https://leetcode-cn.com/problems/add-two-numbers/" target="_blank" rel="noopener">🐱‍💻Leetcode 2</a></p><h2 id="intersection-of-two-lists-链表相交节点">Intersection of Two Lists 链表相交节点</h2><p><a href="https://leetcode-cn.com/problems/intersection-of-two-linked-lists/" target="_blank" rel="noopener">🐱‍💻Leetcode 160</a></p><ul><li>HashTable法 <span class="math inline">\(O(M+N)\)</span>（⭐⭐）</li></ul><p>空间复杂度<span class="math inline">\(O(M)\)</span>或<span class="math inline">\(O(N)\)</span>，这里使用<code>unordered_set</code>实现：</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function">ListNode *<span class="hljs-title">getIntersectionNode</span><span class="hljs-params">(ListNode *headA, ListNode *headB)</span> </span>&#123;        <span class="hljs-keyword">if</span> (headA == <span class="hljs-literal">nullptr</span> || headB == <span class="hljs-literal">nullptr</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">nullptr</span>;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">unordered_set</span>&lt;ListNode*&gt; us;                ListNode* current = headA;        <span class="hljs-keyword">while</span> (current) &#123;            us.insert(current);            current = current-&gt;next;        &#125;        ListNode* res = <span class="hljs-literal">nullptr</span>;        current = headB;        <span class="hljs-keyword">while</span> (current) &#123;            <span class="hljs-keyword">if</span> (us.find(current) != us.end()) &#123;                res = current;                <span class="hljs-keyword">break</span>;            &#125;            current = current-&gt;next;        &#125;        <span class="hljs-keyword">return</span> res;    &#125;&#125;;</code></pre><ul><li>双指针法 <span class="math inline">\(O(M+N)\)</span>（⭐⭐⭐）</li></ul><p>空间复杂度<span class="math inline">\(O(1)\)</span></p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function">ListNode *<span class="hljs-title">getIntersectionNode</span><span class="hljs-params">(ListNode *headA, ListNode *headB)</span> </span>&#123;        <span class="hljs-keyword">if</span> (headA == <span class="hljs-literal">nullptr</span> || headB == <span class="hljs-literal">nullptr</span>) <span class="hljs-keyword">return</span> <span class="hljs-literal">nullptr</span>;        ListNode* a = headA, *b = headB;        <span class="hljs-keyword">while</span> (a != b) &#123;            <span class="hljs-keyword">if</span> (a == <span class="hljs-literal">nullptr</span>) &#123;                a = headB;            &#125; <span class="hljs-keyword">else</span> &#123;                a = a-&gt;next;            &#125;            <span class="hljs-keyword">if</span> (b == <span class="hljs-literal">nullptr</span>) &#123;                b = headA;            &#125; <span class="hljs-keyword">else</span> &#123;                b = b-&gt;next;            &#125;        &#125;        <span class="hljs-keyword">return</span> a;    &#125;&#125;;</code></pre><h2 id="reverse-the-list-翻转链表">Reverse the List 翻转链表</h2><p><a href="https://leetcode-cn.com/problems/reverse-linked-list/" target="_blank" rel="noopener">🐱‍💻Leetcode 206</a></p><p>面试中出现频率还挺高的</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function">ListNode* <span class="hljs-title">enumerate</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;        <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt; buffer;        <span class="hljs-keyword">while</span> (head != <span class="hljs-literal">nullptr</span>) &#123;            buffer.push_back(head-&gt;val);            head = head-&gt;next;        &#125;        ListNode* res = <span class="hljs-keyword">new</span> ListNode(buffer.back());        ListNode* current = res;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = buffer.size()<span class="hljs-number">-2</span>; i &gt;= <span class="hljs-number">0</span>; i--) &#123;            current-&gt;next = <span class="hljs-keyword">new</span> ListNode(buffer[i]);            current = current-&gt;next;        &#125;        <span class="hljs-keyword">return</span> res;    &#125;    <span class="hljs-function">ListNode* <span class="hljs-title">reverseList</span><span class="hljs-params">(ListNode* head)</span> </span>&#123;        <span class="hljs-keyword">if</span> (head == <span class="hljs-literal">nullptr</span>) <span class="hljs-keyword">return</span> head;        <span class="hljs-keyword">return</span> enumerate(head);    &#125;&#125;;</code></pre><h2 id="merge-lists-合并2或k个链表">Merge Lists 合并（2或K个）链表</h2><p><a href="https://leetcode-cn.com/problems/merge-k-sorted-lists/" target="_blank" rel="noopener">🐱‍💻Leetcode 23</a></p><p>最简单的hard题🤣？？？优先队列法写起来是最舒服的，效率也是最高的，时间复杂度<span class="math inline">\(O(nk\times\log nk)\)</span></p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span>    <span class="hljs-built_in">std</span>::priority_queue&lt;<span class="hljs-built_in">std</span>::pair&lt;<span class="hljs-keyword">int</span>, ListNode*&gt;, <span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-built_in">std</span>::pair&lt;<span class="hljs-keyword">int</span>, ListNode*&gt;&gt;, <span class="hljs-built_in">std</span>::greater&lt;<span class="hljs-built_in">std</span>::pair&lt;<span class="hljs-keyword">int</span>, ListNode*&gt;&gt;&gt; q;<span class="hljs-keyword">public</span>:    <span class="hljs-function">ListNode* <span class="hljs-title">mergeKLists</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;ListNode*&gt;&amp; lists)</span> </span>&#123;        <span class="hljs-keyword">if</span> (lists.empty()) <span class="hljs-keyword">return</span> <span class="hljs-literal">nullptr</span>;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> <span class="hljs-built_in">list</span> : lists) &#123;            <span class="hljs-keyword">if</span> (<span class="hljs-built_in">list</span> != <span class="hljs-literal">nullptr</span>) &#123;                q.push(<span class="hljs-built_in">std</span>::make_pair(<span class="hljs-built_in">list</span>-&gt;val, <span class="hljs-built_in">list</span>));            &#125;        &#125;        <span class="hljs-keyword">auto</span>* dummy = <span class="hljs-keyword">new</span> ListNode(<span class="hljs-number">-1</span>);        <span class="hljs-keyword">auto</span>* current = dummy;        <span class="hljs-keyword">while</span> (q.size()) &#123;            <span class="hljs-keyword">auto</span> pair = q.top();            <span class="hljs-keyword">auto</span>* p = pair.second;            q.pop();            current-&gt;next = p;            <span class="hljs-keyword">if</span> (p-&gt;next) q.push(<span class="hljs-built_in">std</span>::make_pair(p-&gt;next-&gt;val, p-&gt;next));            current = current-&gt;next;        &#125;        <span class="hljs-keyword">return</span> dummy-&gt;next;    &#125;&#125;;</code></pre><h2 id="sort-list-排序链表">Sort List 排序链表</h2><p><a href="https://leetcode-cn.com/problems/sort-list/" target="_blank" rel="noopener">🐱‍💻Leetcode 148</a></p><h1 id="mathematics">Mathematics</h1><h2 id="power-of-two-判断2的幂">Power of Two 判断<span class="math inline">\(2\)</span>的幂</h2><p><a href="https://leetcode-cn.com/problems/power-of-two/" target="_blank" rel="noopener">🐱‍💻Leetcode 231</a></p><p><span class="math inline">\(n\)</span>为<span class="math inline">\(2\)</span>的幂只需检查<span class="math inline">\(n\)</span>与<span class="math inline">\(n-1\)</span>的与是否为<span class="math inline">\(0\)</span>即可，如<span class="math inline">\(8\)</span>的二进制表示（假设只有<span class="math inline">\(8\)</span>位）为<code>[0001000]</code>，<span class="math inline">\(7\)</span>的二进制为<code>[0000111]</code>。</p><p>一行解决。记得判断大于<span class="math inline">\(0\)</span>。</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">bool</span> <span class="hljs-title">isPowerOfTwo</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n)</span> </span>&#123;        <span class="hljs-keyword">return</span> n &gt; <span class="hljs-number">0</span> &amp;&amp; (n&amp;(n<span class="hljs-number">-1</span>))==<span class="hljs-number">0</span>;    &#125;&#125;;</code></pre><h1 id="graph">Graph</h1><h1 id="misc">Misc</h1><h2 id="single-number-只出现过一次的数字">Single Number 只出现过一次的数字</h2><p><a href="https://leetcode-cn.com/problems/single-number/" target="_blank" rel="noopener">🐱‍💻Leetcode 136</a></p><p>运用位运算的性质</p><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span> &#123;</span><span class="hljs-keyword">public</span>:    <span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">singleNumber</span><span class="hljs-params">(<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>&gt;&amp; nums)</span> </span>&#123;        <span class="hljs-keyword">int</span> res = <span class="hljs-number">0</span>;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">auto</span> i : nums) res ^= i;        <span class="hljs-keyword">return</span> res;    &#125;&#125;;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Technical Notes</category>
      
      <category>Interview</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
      <tag>Data Structure</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsupervised Representation Learning by Predicting Random Distances</title>
    <link href="/2020/08/24/Unsupervised-Representation-Learning-by-Predicting-Random-Distances/"/>
    <url>/2020/08/24/Unsupervised-Representation-Learning-by-Predicting-Random-Distances/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>针对高维表格数据的表示学习，作者提出了基于预测预计变换后的距离的无监督表示学习框架RDP，并进行了理论上的讨论。To be finished...</p><p><a href="https://arxiv.org/abs/1912.12186" target="_blank" rel="noopener">论文地址</a> <a href="https://github.com/billhhh/RDP" target="_blank" rel="noopener">代码地址</a></p><h1 id="proposed-method">Proposed Method</h1><h2 id="random-distance-prediction-model">Random Distance Prediction Model</h2><p>对于很多下游任务来说，高维数据对模型效率和性能都很大，所以学习低维的有意义（能够最大限度保存原始空间的信息）的表示十分重要。本文的大致思想是给定一个确定的随机映射将样本映射到一个新的空间，然后构造数据集，输入时任意一对样本，标签是两个样本在新的空间的距离，之后训练一个模型来学习这个距离。作者认为通过该任务的训练，模型能够学到有意义的低维表示。模型的框架如下图：</p><p><img src="https://i.loli.net/2020/07/19/vRV32EgLiYkWaQN.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>其中<span class="math inline">\(\phi(\mathbf x;\Theta):\mathbb R^D\mapsto\mathbb R^M\)</span>为孪生神经网络（Siamese Neural Network），将数据映射到<span class="math inline">\(M\)</span>的新空间。损失函数为：</p><p><span class="math display">\[\mathcal L_{rdp}(\mathbf x_i,\mathbf x_j)=l(\langle \phi(\mathbf x_i;\Theta),\phi(\mathbf x_j;\Theta)\rangle,\langle\eta(\mathbf x_i),\eta(\mathbf x_j)\rangle)\]</span></p><p>其中<span class="math inline">\(\eta(\cdot)\)</span>为已知的映射，<span class="math inline">\(l(\cdot)\)</span>为衡量两个输入相似程度的度量。具体的来说，文中选取了简单的实现方案，即采用内积作为映射后的样本的距离度量：</p><p><span class="math display">\[\mathcal L_{rdp}(\mathbf x_i,\mathbf x_j)=\left(\phi(\mathbf x_i;\Theta)\cdot\phi(\mathbf x_j;\Theta)-\eta(\mathbf x_i)\cdot\eta(\mathbf x_j)\right)^2\]</span></p><p><span class="math inline">\(\eta(\cdot)\)</span>为现成的映射。至于为什么要这么做，可以先接着看下面原文给出的理论分析，然后我再说说我自己的理解。</p><h2 id="incorporating-task-dependent-complementary-auxiliary-loss">Incorporating Task-Dependent Complementary Auxiliary Loss</h2><p>对于特定的下游任务，作者提出可以整合额外的误差函数来提高模型行性能。比如说针对聚类任务可以使用重构误差：</p><p><span class="math display">\[\mathcal L_{aux}^{clu}(\mathbf x)=(\mathbf x-\phi^\prime(\phi(\mathbf x;\Theta); \Theta^\prime))^2\]</span></p><p>其中<span class="math inline">\(\phi(\cdot)\)</span>和<span class="math inline">\(\phi^\prime(\cdot):\mathbb R^M\mapsto\mathbb R^D\)</span>分别为编码器和解码器。</p><p>对于异常检测任务，可以使用下式： <span class="math display">\[\mathcal L_{aux}^{ad}(\mathbf x)=(\phi(\mathbf x;\Theta)-\eta(\mathbf x))^2\]</span></p><p>这一个Loss本来是出现在强化学习的论文中，用来检测一个状态<span class="math inline">\(\mathbf x\)</span>出现的频率，如果预测误差较小，说明这个样本之前见过或见过类似的，否则没怎么见过，可以认为是异常。由于本文的目的主要是降维加保留原始空间信息，可以认为使用线性变换的话此目的已经达到了。</p><h2 id="theoretical-analysis">Theoretical Analysis</h2><h3 id="using-linear-projection">Using Linear Projection</h3><p>这里讨论使用线性映射的情况，设数据集<span class="math inline">\(\mathcal X\subset\mathbb R^{N\times D}\)</span>，映射矩阵<span class="math inline">\(\mathbf A\subset\mathbb R^{K\times D}\)</span>为一随机矩阵，映射之后的数据为<span class="math inline">\(\mathbf A\mathcal X^\top\)</span>。对于<span class="math inline">\(\epsilon\in(0,\frac{1}{2})\)</span>和<span class="math inline">\(K=\frac{20\log n}{\epsilon^2}\)</span>，存在<span class="math inline">\(f:\mathbb R^D\mapsto\mathbb R^K\)</span>使得对于所有的<span class="math inline">\(\mathbf x_i,\mathbf x_j\in\mathcal X\)</span>有：</p><p><span class="math display">\[(1-\epsilon)\parallel\mathbf x_i-\mathbf x_j \parallel^2\leq \parallel f(\mathbf x_i)-f(\mathbf x_j)\parallel^2\leq (1+\epsilon)\parallel\mathbf x_i-\mathbf x_j\parallel^2\]</span></p><p>如果<span class="math inline">\(\mathbf A\)</span>的每个元素独立采样自标准正态分布那么有：</p><p><span class="math display">\[\text{Pr}\left((1-\epsilon)\parallel\mathbf x\parallel^2\leq\parallel\frac{1}{\sqrt{K}}\mathbf A\mathbf x\parallel^2\leq(1+\epsilon)\parallel\mathbf x\parallel^2\right)\geq 1-2e^{\frac{-(\epsilon^2-\epsilon^3)K}{4}}\]</span></p><p>在该随机映射下有：</p><p><span class="math display">\[\text{Pr}(|\hat{\mathbf x}_i\cdot\hat{\mathbf x}_j-f(\hat{\mathbf x}_i)\cdot f(\hat{\mathbf x}_j)|\geq\epsilon)\leq 4e^{\frac{-(\epsilon^2-\epsilon^3)\cdot K}{4}}\]</span></p><p>直观的解释就是说使用线性映射的情况下，只要使用的变换矩阵采样自标准正态分布，那么变换之后样本对之间的距离信息能够以一定的概率保留。</p><h3 id="using-non-linear-projection">Using Non-Linear Projection</h3><p>这里作者试图说明，在某些条件下，非线性随机映射的作用和核函数接近。对于一个确定的随机映射函数<span class="math inline">\(g:\mathbb R^D\mapsto\mathbb R^K\)</span>，在某些特定的条件下，函数<span class="math inline">\(g\)</span>和核函数存在下列关系：</p><p><span class="math display">\[k(\mathbf x_i,\mathbf x_j)=\langle\psi(\mathbf x_i),\psi(\mathbf x_j)\rangle\approx g(\mathbf x_i)\cdot g(\mathbf x_j)\]</span></p><p>这个条件是函数<span class="math inline">\(g\)</span>为一个乘以一个线性矩阵<span class="math inline">\(\mathbf A\)</span>然后在经过一个具备平移不变性的傅里叶基函数（如cosine）。由于核函数能够保留原始空间的信息，所以作者认为使用非线性函数也能保留原始空间的信息。</p><blockquote><p>PS: 感觉作者在理论部分的讨论还是有点模糊，因为把一个随机的映射作为（伪）监督信息来进行学习，神经网络学到的不也就是随机噪声信息吗？对于这个方法work的原因，我在这里不负责任的分析一下。</p></blockquote><h3 id="learning-class-structure-by-random-distance-prediction">Learning Class Structure by Random Distance Prediction</h3><p>这一节主要解释为什么神经网络<span class="math inline">\(\phi(\cdot)\)</span>学到的要比随机映射<span class="math inline">\(\eta(\cdot)\)</span>要好。模型的优化目标可以写成如下的形式：</p><p><span class="math display">\[\mathop{\arg\min}_{\Theta}\sum_{\mathbf x_i,\mathbf x_j\in\mathcal X}(\phi(\mathbf x_i;\Theta)\cdot\phi(\mathbf x_j;\Theta)-y_{ij})^2\]</span></p><p>其中<span class="math inline">\(y_{ij}=\eta(\mathbf x_i)\cdot\eta(\mathbf x_j)\)</span>。设<span class="math inline">\(\mathbf Y_\eta\in\mathbb R^{N\times N}\)</span>为距离矩阵。这个目标函数是在最小化每一对样本在经过<span class="math inline">\(\phi(\cdot)\)</span>和<span class="math inline">\(\eta(\cdot)\)</span>映射后之间的距离的差距。通过公式(7)和公式(8)我们知道，在合适的条件下，随机映射<span class="math inline">\(\eta(\cdot)\)</span>能够保留原始空间的距离信息（即原始空间相近的样本在映射后也相近）。不过，上述公式的成立都依赖于对数据分布的一定假设，当真实的数据不满足条件时，结论就会有所偏差。</p><h1 id="experiments">Experiments</h1><h2 id="performance-evaluation-in-anomaly-detection">Performance Evaluation in Anomaly Detection</h2><h3 id="experimental-settings">Experimental Settings</h3><p><img src="https://i.loli.net/2020/07/20/3G7DNKjwfQkiIz4.png" srcset="/img/loading.gif" /></p><p>异常分数定义为<span class="math inline">\(\mathcal S(\mathbf x)=(\phi(\mathbf x;\Theta)-\eta(\mathbf x))^2\)</span>。</p><h3 id="comparison-to-the-state-of-the-art-competing-methods">Comparison to the State-of-the-art Competing Methods</h3><p><img src="https://i.loli.net/2020/07/20/8Ie2Q3mpdPHtrYF.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/07/20/OEcQSvZmfBz1ACt.png" srcset="/img/loading.gif" /></p><h3 id="ablation-study">Ablation Study</h3><p><img src="https://i.loli.net/2020/07/20/7GtKlN8q5Mvygre.png" srcset="/img/loading.gif" /></p><h2 id="performance-evaluation-in-clustering">Performance Evaluation in Clustering</h2><h3 id="experimental-settings-1">Experimental Settings</h3><p><img src="https://i.loli.net/2020/07/20/9xW12MVkoXgFZ6J.png" srcset="/img/loading.gif" /></p><h3 id="comparison-to-the-state-of-the-art-competing-methods-1">Comparison to the State-of-the-art Competing Methods</h3><p><img src="https://i.loli.net/2020/07/20/pUZ64aX1xWiLf2q.png" srcset="/img/loading.gif" /></p><figure><img src="https://i.loli.net/2020/07/20/VrnXuJsymiMItUf.png" srcset="/img/loading.gif" alt="" /><figcaption>image-20200720014002063</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Representation Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Representation Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Effective End-to-end Unsupervised Outlier Detection via Linear Priority of Discriminative Network</title>
    <link href="/2020/07/14/Effective-End-to-end-Unsupervised-Outlier-Detection-via-Linear-Priority-of-Discriminative-Network/"/>
    <url>/2020/07/14/Effective-End-to-end-Unsupervised-Outlier-Detection-via-Linear-Priority-of-Discriminative-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文针对无监督异常检测提出了<span class="math inline">\(E^3\space{Outlier}\)</span>。作者使用自监督学习的方法，通过构建有监督任务在没有标签的情况下学习高层语义特征。PS：这篇文章的方法和NIPS18上的<em>Deep Anomaly Detection Using Geometric Transformations</em>（后面简称GEOM）颇为相似，但是不知为啥没有在实验中进行比较。后面我会分析一些两篇文章方法上的异同。</p><h1 id="proposed-method">Proposed Method</h1><h2 id="surrogate-supervision-based-effective-representation-learning-for-uod">Surrogate Supervision Based Effective Representation Learning for UOD</h2><p>这里作者提到了使用重构的模型来进行异常检测的不足：重构模型采用像素级别的损失函数（如mean square error），而这太过于严格和细节，并不能学到高层语义特征。</p><p>为此，作者提出了<em>surrogate supervision based discriminative network</em> (SSD)。具体操作和GEOM类似，首先预定义大小为<span class="math inline">\(K\)</span>的几何变换集合<span class="math inline">\(\mathcal O=\{O(\cdot|y)\}_{y=1}^K\)</span>。对每一个样本<span class="math inline">\(\mathbf x\)</span>，在经过<span class="math inline">\(K\)</span>个集合变换之后会得到<span class="math inline">\(K\)</span>个变换后的样本（第<span class="math inline">\(y\)</span>个变换产生的样本即记为<span class="math inline">\(\mathbf x^{(y)}=O(\mathbf x|y)\)</span>），每个样本对应的pseudo label即为变换的序号或者说种类。之后在新的数据集上（大小为原来的<span class="math inline">\(K\)</span>倍）训练<span class="math inline">\(K\)</span>分类网络。网络的输出为<span class="math inline">\(P(\mathbf x^{(y^\prime)}|\boldsymbol\theta)=[P^{(y)}(\mathbf x^{(y^\prime)}|\boldsymbol\theta)]_{y=1}^K\)</span>，每个维度代表输入样本对应的变换的概率。总的损失函数为： <span class="math display">\[\min_\theta\frac{1}{N}\sum_{i=1}^{N}\mathcal L_{SS}(\mathbf x_i|\theta)\]</span></p><p>其中<span class="math inline">\(\mathcal L_{SS}(\mathbf x_i|\theta)\)</span>代表每个样本对应的Loss，这个Loss可以由分类器在<span class="math inline">\(K\)</span>个变换上的交叉熵损失来确定：</p><p><span class="math display">\[\mathcal L_{SS}(\mathbf x_i|\boldsymbol\theta)=-\frac{1}{K}\sum_{y=1}^K\log(P^{(y)}(\mathbf x_i^{(y)}|\boldsymbol\theta))=-\frac{1}{K}\sum_{y=1}^K\log(P^{(y)}(O(\mathbf x_i|y)|\boldsymbol\theta))\]</span> <img src="https://i.loli.net/2020/07/14/ULAdYpzsoGfFwtD.png" srcset="/img/loading.gif" /></p><p>变换集合<span class="math inline">\(\mathcal O\)</span>由一系列基本变换的组合确定。作者将这些基本变换分为了：1) 旋转 2) 翻转 3) 平移，包括横向和纵向 4) Patch置换（参考图1(a)中的Patch Re-arranging）。最终的变换集合<span class="math inline">\(\mathcal O\)</span>由三个子集组成，分别是<span class="math inline">\(\mathcal O_{RA}\)</span>（代表Regular Affine，其中每个变换为旋转<span class="math inline">\(90°\)</span>的倍数、翻转、横向平移和纵向平移这四个基本变换的叠加），<span class="math inline">\(\mathcal O_{IA}\)</span>（代表Irregular Affine，其中每个变换为进行<span class="math inline">\(30°\)</span>的倍数且不为<span class="math inline">\(90°\)</span>的倍数角度的旋转、翻转这两个基本变换的叠加）和<span class="math inline">\(\mathcal O_{PR}\)</span>（只包含Patch Re-arranging）。</p><p>为了验证SSD学到的特征的有效性，作者将CAE提取的特征和SSD提取的特征分别用孤立森林进行异常检测，发现SSD效果更好（见图1(b)）。</p><p>到这里为止本文和GEOM基本没有大的区别。值得注意的是在所采用的几何变换中，采用了非线性变换（进行<span class="math inline">\(30°\)</span>的倍数且不为<span class="math inline">\(90°\)</span>的倍数角度的旋转）。而在GEOM中，提到过使用非线性变换的话效果会比较差，至于具体的影响如何，可能需要实验来确定。</p><h2 id="inlier-priority-the-foundation-of-end-to-end-uod">Inlier Priority: The Foundation of End-to-end UOD</h2><p>在这里作者主要对在训练集包含少量异常的情况下做出的理论分析，作者将其称为<em>Inlier Priority</em>，原句如下：</p><blockquote><p><em>Inlier Priority</em>: Despite that inliers/outliersare indiscriminately fed into SSD for training, SSD will prioritize the minimization of inliers’ loss.</p></blockquote><h3 id="priority-by-gradient-magnitude">Priority by Gradient Magnitude</h3><p>对于第<span class="math inline">\(c\)</span>个类来说，设<code>softmax</code>层和倒数第二层之间的权重矩阵为<span class="math inline">\(\mathbf w_c=[w_{s,c}]^{(L+1)}_{s=1}\)</span>，损失函数记为<span class="math inline">\(\mathcal L\)</span>，梯度记为<span class="math inline">\(\nabla_{\mathbf w_c}\mathcal L=[\nabla_{w_{s,c}}\mathcal L]^{(L+1)}_{s=1}\)</span>。设训练集<span class="math inline">\(X^{(c)}\)</span>包含<span class="math inline">\(N_{in}\)</span>个正常样本，<span class="math inline">\(N_{out}\)</span>个异常样本。记正常样本和异常样本对应的梯度分别为<span class="math inline">\(\parallel\nabla^{(in)}_{\mathbf w_c}\mathcal L\parallel\)</span>和<span class="math inline">\(\parallel\nabla^{(out)}_{\mathbf w_c}\mathcal L\parallel\)</span>，在网络只有一个隐层且采用<code>Sigmoid</code>作为激活函数时，两者梯度的期望之比有如下关系：</p><p><span class="math display">\[\frac{E(\parallel\nabla^{(in)}_{\mathbf w_c}\mathcal L\parallel^2)}{E(\parallel\nabla^{(out)}_{\mathbf w_c}\mathcal L\parallel^2)}\approx\frac{N^2_{in}}{N^2_{out}}\]</span></p><p>在训练集中，正常样本和异常样本的数量是极不均衡的，<span class="math inline">\(N_{in}\gg N_{out}\)</span>，所以有<span class="math inline">\(E(\parallel\nabla^{(in)}_{\mathbf w_c}\mathcal L\parallel^2)\gg E(\parallel\nabla^{(out)}_{\mathbf w_c}\mathcal L\parallel^2)\)</span>。</p><p>在使用更复杂的网络时，作者通过实验展示了正常样本和异常样本对应的梯度大小的比较：</p><p><img src="https://i.loli.net/2020/07/16/iPa7h9HWqrnZvgx.png" srcset="/img/loading.gif" /></p><h3 id="priority-by-network-updating-direction">Priority by Network Updating Direction</h3><p>这里作者通过梯度更新的方向来进行了理论上的解释。对于一个Batch的数据<span class="math inline">\(X\)</span>，梯度为<span class="math inline">\(-\nabla_\theta\mathcal L(X)=-\frac{1}{N}\sum_i\nabla_\theta\mathcal L(\mathbf x_i)\)</span>，如果将该梯度在Batch中某一样本<span class="math inline">\(\mathbf x_i\)</span>对应的梯度的方向上进行分解<span class="math inline">\(-\nabla_\theta\mathcal L(\mathbf x_i):d_i=-\nabla_\theta\mathcal L(X)\cdot\frac{-\nabla_\theta\mathcal L(\mathbf x_i)}{\parallel -\nabla_\theta\mathcal L(\mathbf x_i)\parallel}\)</span>，这代表了总的Loss在多大程度上减小样本<span class="math inline">\(\mathbf x_i\)</span>对应的Loss，由于一个Batch即包含正常样本，也可能包含异常样本，所以作者将两者对应的梯度方向贡献进行了可视化：</p><p><img src="https://i.loli.net/2020/07/16/U5fVk8YOEGPx3Q7.png" srcset="/img/loading.gif" /></p><p>可以看到随着训练的进行，正常样本对应的贡献更高。</p><p>PS: 我以为作者会对基于几何变换的异常检测为什么有效做一些理论上的解释，不过却没有。这里只是对在训练集包含少量异常的情况下做出的理论分析，而这个实际上直觉上就很显然了。</p><h2 id="scoring-strategies-for-uod">Scoring Strategies for UOD</h2><p>作者采用了三种方法来计算异常分数：</p><h3 id="pseudo-label-based-score-pl">Pseudo Label based Score (PL)</h3><p>对于一个测试样本<span class="math inline">\(\mathbf x\)</span>，对其进行<span class="math inline">\(K\)</span>个几何变换，通过分类器会得到<span class="math inline">\(K\)</span>个输出，对于第<span class="math inline">\(k\)</span>个输出，我们只取其第<span class="math inline">\(k\)</span>个分量，最后把他们加起来除以<span class="math inline">\(K\)</span>：</p><p><span class="math display">\[S_{pl}(\mathbf x)=\frac{1}{K}\sum_{y=1}^K P^{(y)}(\mathbf x^{(y)}|\boldsymbol\theta)\]</span></p><h3 id="maximum-probability-based-score-mp">Maximum Probability based Score (MP)</h3><p>这里稍有不同，对于第<span class="math inline">\(k\)</span>个输出，我们取其值最大的分量，而不是第<span class="math inline">\(k\)</span>个分量：</p><p><span class="math display">\[S_{mp}(\mathbf x)=\frac{1}{K}\sum_{y=1}^K\max_t P^{(t)}(\mathbf x^{(y)}|\boldsymbol\theta)\]</span></p><h3 id="negative-entropy-based-score-ne">Negative Entropy based Score (NE)</h3><p>作者认为，标签为One-Hot向量，分类器的输出分布越“尖峰”就越接近于正常样本，而越“平均”就越接近于异常样本，所以作者提出使用熵来描述分类器输出的“尖锐度”： <span class="math display">\[S_{ne}(\mathbf x)=-\frac{1}{K}\sum_{y=1}^K H(P(\mathbf x^{(y)}|\boldsymbol\theta))=\frac{1}{K}\sum_{y=1}^K\sum_{t=1}^K P^{(t)}(\mathbf x^{(y)}|\boldsymbol\theta)\log(P^{(t)}(\mathbf x^{(y)}|\boldsymbol\theta))\]</span> 这里作者对第一种方法得到的结果进行了可视化：</p><p><img src="https://i.loli.net/2020/07/16/PgzqXIdJ67s3BtG.png" srcset="/img/loading.gif" /></p><p>PS：对比NIPS18 的Dirichlet Normality Score</p><ol type="1"><li>也用到了全部<span class="math inline">\(K\)</span>个维度的信息</li><li>相当于对分类器的输出做了迪利克雷分布的先验假设，然后通过训练集的输出估计分布参数。因为直觉上对于正常分布来说，分类器的输出分布形状上都类似一个尖峰，但对于不同的数据集来说具体形状还是会有所差异</li></ol><h1 id="experiments">Experiments</h1><h2 id="experiment-setup">Experiment Setup</h2><p>数据集用到了MNIST, Fashion-MNIST (F-MNIST) , CIFAR10, SVHN和CIFAR100。为了模拟无监督异常检测的环境，人为在训练集中加入异常样本，异常的比例<span class="math inline">\(\rho\)</span>从<span class="math inline">\(5\%\)</span>到<span class="math inline">\(25\%\)</span>以<span class="math inline">\(5\%\)</span>的步长递增。评测标准采用AUPR和AUROC。</p><h2 id="uod-performance-comparison-and-discussion">UOD Performance Comparison and Discussion</h2><p>下表展示了模型性能对比结果：</p><p><img src="https://i.loli.net/2020/07/16/G3agKPuJBowFmIW.png" srcset="/img/loading.gif" /></p><p>下图展示了在不同的Outlier Ratio下的性能对比：</p><p><img src="https://i.loli.net/2020/07/16/h6iYjBkrwQdFvMJ.png" srcset="/img/loading.gif" /></p><p>下图展示了在不同的变换集合，网络结构，异常分数的条件下的性能：</p><p><img src="https://i.loli.net/2020/07/16/waWAi7zI3QpOcf6.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Self-Supervised Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PRML Notes 1.1: Probability Distributions - Binary and Multinomial Variables</title>
    <link href="/2020/06/22/PRML-Notes-1-1-Probability-Distributions-Binary-and-Multinomial-Variables/"/>
    <url>/2020/06/22/PRML-Notes-1-1-Probability-Distributions-Binary-and-Multinomial-Variables/</url>
    
    <content type="html"><![CDATA[<h1 id="overview">Overview</h1><p>这是PRML (Pattern Recognition and Machine Learning) 第二章<code>Probability Distributions</code>笔记的第一部分，主要包括<code>2.1. Binary Variables</code>和<code>2.2. Multinomial Variables</code>这两节。</p><h1 id="probability-distributions-for-binary-variables">Probability Distributions for Binary Variables</h1><h2 id="intro">Intro</h2><p>这一节主要针对二值随机变量的建模，即<span class="math inline">\(x\in\{0,1\}\)</span>。这里可以想象为我们有一个硬币，<span class="math inline">\(x=1\)</span>代表正面朝上，而<span class="math inline">\(x=0\)</span>代表反面朝上，并且正面朝上的概率为<span class="math inline">\(\mu\)</span>，即： <span class="math display">\[p(x=1|\mu)=\mu\]</span> 其中<span class="math inline">\(0\leqslant \mu \leqslant 1\)</span>。<span class="math inline">\(x\)</span>的概率分布可以写为： <span class="math display">\[\text{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x}\]</span> 也就是我们熟知的<strong>伯努利分布 (Bernoulli Distribution)</strong>。其均值和方差分别为： <span class="math display">\[\begin{align}\mathbb E[x]&amp;=\mu\\\text{var}[x] &amp;= \mu(1-\mu)\end{align}\]</span> 现在来考虑参数估计任务。假设我们正在进行一个投硬币的实验，每一次投币都服从伯努利分布且相互独立，我们将每次采集到的观测值组成数据集<span class="math inline">\(\mathcal D=\{x_1,\cdots,x_N\}\)</span>，则似然函数为： <span class="math display">\[p(\mathcal D|\mu)=\prod_{n=1}^N p(x_n|\mu)=\prod_{n=1}^N \mu^{x_n}(1-\mu)^{1-x_n}\]</span> 如果采用极大似然估计的话，我们可以最大化似然函数，这等价于最大化对数似然： <span class="math display">\[\ln p(\mathcal D|\mu)=\sum_{n=1}^{N}\ln p(x_n|\mu)=\sum_{n=1}^N\{x_n\ln\mu+(1-x_n)\ln(1-\mu)\}\]</span> 令其导数为0得到极值点： <span class="math display">\[\mu_{ML}=\frac{1}{N}\sum_{n=1}^N x_n\]</span> 这相当于样本均值，不过这样做会有严重的问题。假设我们的数据集为<span class="math inline">\(\mathcal D=\{1,1,1\}\)</span>，也就是说我们只收集到了三个样本，并且都是正例，我们会得到<span class="math inline">\(\mu_{ML}=1\)</span>，而这显然是严重过拟合的。稍后我们会说说如何应对这种情况（加入先验）。</p><h2 id="binomial-distribution">Binomial Distribution</h2><p>我们同样可以对多次伯努利实验进行概率建模。记<span class="math inline">\(m\)</span>为成功的次数，<span class="math inline">\(N\)</span>为数据集大小，可知这个概率应该与<span class="math inline">\(\mu^m(1-\mu)^{N-m}\)</span>成正比。乘以标准化系数后即我们熟知的<strong>二项分布 (Binomial Distribution)</strong>： <span class="math display">\[\text{Bin}(m|N,\mu)=\binom{N}{m}\mu^m(1-\mu)^{N-m}\]</span></p><p>其中： <span class="math display">\[\binom{N}{m}=\frac{N!}{(N-m)!m!}\]</span> 其均值和方差分别为： <span class="math display">\[\begin{align}\mathbb E[m]&amp;=N\mu\\\text{var}[m]&amp;=N\mu(1-\mu)\end{align}\]</span></p><h2 id="beta-distribution">Beta Distribution</h2><p>现在我们来讨论如何解决刚才提到的最大似然估计过拟合问题。为了解决这个问题，我们使用贝叶斯的思路，对<span class="math inline">\(\mu\)</span>引入了先验分布<span class="math inline">\(p(\mu)\)</span>。而这个分布需要具有良好的解释性和数学性质。</p><p>根据贝叶斯定理： <span class="math display">\[p(\mu|\mathcal D)=\frac{p(\mathcal D|\mu)p(\mu)}{p(\mathcal D)}\]</span> 而<span class="math inline">\(p(\mathcal D)=\int_0^1 p(\mathcal D|\mu)p(\mu)\mathrm d\mu\)</span>只受数据集影响，而数据集是固定的，所以为常数，因此<span class="math inline">\(p(\mu|\mathcal D)\propto p(\mathcal D|\mu)p(\mu)\)</span>。而似然函数为<span class="math inline">\(\mu^x(1-\mu)^{1-x}\)</span>的乘积，如果先验也采用<span class="math inline">\(\mu\)</span>和<span class="math inline">\(1-\mu\)</span>的幂的乘积的形式，那么后验分布也将和先验形式相同，这种性质在统计学中被称为<strong>先验共轭 (conjugacy)</strong>。</p><p>这里我们直接给出这个先验分布，再来分析它的性质。这个分布叫做<strong>Beta分布 (Beta Distribution)</strong><span class="math inline">\(P(\mu|a,b)\sim \text{Beta}(a,b)\)</span>： <span class="math display">\[\begin{align}\text{Beta}(\mu|a,b) &amp;= \frac{\Gamma(a+b)}{\Gamma(a\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\ &amp;= \frac{1}{B(a,b)}\mu^{a-1}(1-\mu)^{b-1}\end{align}\]</span> <span class="math inline">\(B(\boldsymbol \alpha,\beta)\)</span>称为B函数，为一个标准化函数： <span class="math display">\[\begin{align}B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\end{align}\]</span> 其目的是为了使整个概率分布积分等于1而存在的。Gamma函数的定义为： <span class="math display">\[\Gamma(x)=\int_0^{\infty}s^{x-1}e^{-s}\mathrm d s\]</span> Gamma函数有一个性质：</p><p><span class="math display">\[\Gamma(x+1)=x\Gamma(x)\]</span> 证明为： <span class="math display">\[\begin{align*}\Gamma(x+1) &amp;= \int_{0}^{\infty} {s^{x} e^{-s} ds} \\&amp;= \big[s^{x} (-e^{-s})\big] \big|_{0}^{\infty} - \int_{0}^{\infty} {(x s^{x-1}) (-e^{-s}) ds} \\&amp;= (0 - 0) + x \int_{0}^{\infty} {s^{x-1} e^{-s} ds} \\&amp;= x \Gamma(x)\end{align*}\]</span></p><p>除此之外：</p><p><span class="math display">\[\Gamma(1)=1\\\Gamma(\frac{1}{2})=\sqrt{\pi}\]</span></p><p>可以验证： <span class="math display">\[\int_0^1\text{Beta}(\mu|a,b)\mathrm d\mu=1\]</span></p><p>Beta分布的均值和方差为：</p><p><span class="math display">\[\mathbb E[\mu]=\frac{a}{a+b}\\\text{var}[\mu]=\frac{ab}{(a+b)^2(a+b+1)}\]</span></p><p>因为后验分布与先验和似然函数的乘积成比例，那么： <span class="math display">\[p(\mu|m,l,a,b)\propto\mu^{m+a-1}(1-\mu)^{l+b-1}\]</span></p><p>其中<span class="math inline">\(l=N-m\)</span>。乘上标准化因子，就得到： <span class="math display">\[p(\mu|m,l,a,b)=\frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1}\]</span> 得到的仍然是Beta分布，相当于把<span class="math inline">\(a\rightarrow{m+a}\)</span>，<span class="math inline">\(b\rightarrow{l+b}\)</span>。同时不难发现，参数<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>都有比较直观的意义。<span class="math inline">\(a\)</span>可以看作是历史记录中，成功的次数，<span class="math inline">\(b\)</span>可以看作是历史记录中失败的次数，比如<span class="math inline">\(a=2\)</span>，<span class="math inline">\(b=3\)</span>，根据经验成功的概率应该在<span class="math inline">\(\frac{2}{2+3}=0.4\)</span>左右，即我们的先验为成功的概率为<span class="math inline">\(0.4\)</span>（见下图左下角的子图）。如果在实验中，又进行了<span class="math inline">\(7\)</span>次实验，其中<span class="math inline">\(m=6\)</span>，<span class="math inline">\(l=1\)</span>，由于成功的次数变多了，<span class="math inline">\(a=2+6=8\)</span>，<span class="math inline">\(b=3+1=4\)</span>，直觉上来说我们对成功概率的估计应当相应提高，大概为<span class="math inline">\(\frac{8}{8+4}\approx 0.67\)</span>左右。这时的Beta分布如右下角的图的样子，也印证了我们的直觉。</p><p><img src="https://i.loli.net/2020/06/25/3hcj18ELXl4iWy6.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>以下为不同参数对应的Beta分布的互动演示：</p><div><iframe width="650px" height="450px" frameborder="0" style="dispaly:block；" src="http://qfxiao.me/html/beta_distribution_vis.html"></iframe></div><p>最后，Beta还有一个有趣的应用就是，如果我们不断接收到新的观测数据，那么旧的后验分布则可以作为新的先验分布将参数更新下去 。这相当于说，基于已有的观测数据，我们提出一个先验Beta分布，然后根据新得到的一批观测数据，用先验Beta分布计算一个似然函数，将似然函数和先验Beta分布乘起来，归一化后得到了新的后验分布，只要不断有新的观测数据接收到，就可以把后验分布作为新的先验，不断更新下去。这样做的优势是对于大数据集，我们不需要整个数据集，而是只需要一批一批的更新即可。</p><h1 id="probability-distributions-for-multinomial-variables">Probability Distributions for Multinomial Variables</h1><h2 id="intro-1">Intro</h2><p>前面我们讨论了二值随机变量，现在我们将其扩展到多值变量。设一个<span class="math inline">\(K\)</span>维向量<span class="math inline">\(\mathbf x\)</span>，当<span class="math inline">\(x_k\)</span>为<span class="math inline">\(1\)</span>的时候其他元素都为<span class="math inline">\(0\)</span>，如<span class="math inline">\(K=6,x_3=1\)</span>时<span class="math inline">\(\mathbf x\)</span>表示为<span class="math inline">\(\mathbf x=(0,0,1,0,0,0)^\top\)</span>。如果<span class="math inline">\(p(x_k=1)=\mu_k\)</span>，那么<span class="math inline">\(\mathbf x\)</span>的概率分布为： <span class="math display">\[p(\mathbf x|\boldsymbol \mu)=\prod_{k=1}^{K}\mu_k^{x_k}\]</span> <span class="math inline">\(\mu_k\)</span>满足<span class="math inline">\(\sum_k \mu_k=1\)</span>和<span class="math inline">\(\mu_k\geqslant 0\)</span>，该分布被称作是<strong>Categorical Distribution</strong>。易知其均值为： <span class="math display">\[\begin{align}\mathbb E[\mathbf x|\boldsymbol \mu]=\sum_{\mathbf x}p(\mathbf x|\boldsymbol \mu)\mathbf x=\boldsymbol \mu\end{align}\]</span> 假设我们有大小为<span class="math inline">\(N\)</span>的数据集<span class="math inline">\(\mathcal D\)</span>，每个样本服从该分布且相互独立，那么似然函数： <span class="math display">\[p(\mathcal D|\boldsymbol \mu)=\prod_{n=1}^N\prod_{k=1}^K \mu_k^{x_{nk}}=\prod_{k=1}^K \mu_k^{\sum_n x_{nk}}=\prod_{k=1}^K\mu_k^{m_k}\]</span> 其中<span class="math inline">\(m_k=\sum_n x_{nk}\)</span>，即<span class="math inline">\(x_k=1\)</span>的数量。为了最大化对数似然同时保证<span class="math inline">\(\sum_k \mu_k=1\)</span>，我们可以用拉格朗日乘子法： <span class="math display">\[\sum_{k=1}^K m_k\ln \mu_k+\lambda\left(\sum_{k=1}^K\mu_k-1\right)\]</span> 我们得到<span class="math inline">\(\mu_k=-m_k/\lambda\)</span>。通过<span class="math inline">\(\sum_k \mu_k=1\)</span>得出<span class="math inline">\(\lambda=-N\)</span>，故最后我们有： <span class="math display">\[\mu_k^{ML}=\frac{m_k}{N}\]</span></p><p>这相当于是<span class="math inline">\(x_k=1\)</span>的数量除以总数。</p><h2 id="multinomial-distribution">Multinomial Distribution</h2><p>类似的，我们可以对多次实验进行建模，假设进行<span class="math inline">\(N\)</span>次独立实验，概率分布可以写为：</p><p><span class="math display">\[\text{Mult}(m_1,m_2,\cdots,m_K|\boldsymbol\mu,N)=\binom{N}{m_1m_2\cdots m_K}\prod_{k=1}^K\mu_k^{m_k}\]</span></p><p>这也是我们熟知的<strong>多项分布 (Multinomial Distribution)</strong>，其中<span class="math inline">\(\binom{N}{m_1m_2\cdots m_K}\)</span>为正则化因子： <span class="math display">\[\binom{N}{m_1m_2\cdots m_K}=\frac{N!}{m_1!m_2!\cdots m_K!}\]</span> 注意<span class="math inline">\(\sum\limits_{k=1}^K m_k=N\)</span>。</p><h2 id="dirichlet-distribution">Dirichlet Distribution</h2><p>有了前面Beta的启发，我们同样可以对多项分布的参数<span class="math inline">\(\mu_k\)</span>建立共轭先验。首先根据似然函数，我们知道先验应当与<span class="math inline">\(\mu_k\)</span>的幂的乘积成比例：</p><p><span class="math display">\[p(\boldsymbol \mu|\boldsymbol \alpha) \propto \prod_{k=1}^{K}\mu_k^{a_{k-1}}\]</span></p><p>其中<span class="math inline">\(0\leqslant \mu_k\leqslant 1\)</span>且<span class="math inline">\(\sum_k\mu_k=1\)</span>。和Beta分布不同，由于要满足<span class="math inline">\(\sum\mu_k=1\)</span>，所以<span class="math inline">\(\{\mu_k\}\)</span>的取值会位于<span class="math inline">\(K-1\)</span>的单纯型上，如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/N8CSMvmlRz91Gqy.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>加上标准化因子，我们就得到了所谓的先验分布，称之为<strong>迪利克雷分布 (Dirichlet Distribution)</strong>： <span class="math display">\[\text{Dir}(\boldsymbol \mu|\boldsymbol\alpha)=\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_K)}\prod_{k=1}^K\mu_k^{a_{k-1}}\]</span></p><p>其中<span class="math inline">\(\Gamma(\cdot)\)</span>为Gamma函数，<span class="math inline">\(\alpha_0=\sum\limits_{k=1}^K\alpha_k\)</span>。下图为不同条件下的迪利克雷分布的可视化：</p><p><img src="https://i.loli.net/2020/06/25/amGtuvPNoOM7kWZ.png" srcset="/img/loading.gif" /></p><p><span class="math inline">\(\boldsymbol \mu\)</span>的后验与先验和似然函数的乘积成正比：</p><p><span class="math display">\[p(\boldsymbol\mu|\mathcal D,\boldsymbol\alpha)\propto p(\mathcal D|\boldsymbol\mu)p(\boldsymbol\mu|\boldsymbol\alpha)\propto\prod_{k=1}^K \mu_k^{\alpha_k+m_k-1}\]</span></p><p>不难验证：</p><p><span class="math display">\[\begin{align}p(\boldsymbol\mu|\mathcal D,\boldsymbol\alpha) &amp;= \text{Dir}(\boldsymbol\mu|\boldsymbol\alpha+\mathbf m)\\&amp;=\frac{\Gamma(\alpha_0+N)}{\Gamma(\alpha_1+m_1)\cdots\Gamma(\alpha_K+m_K)}\prod_{k=1}^K\mu_k^{\alpha_k+m_k-1}\end{align}\]</span></p><p>即后验同样为迪利克雷分布。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Statistics</tag>
      
      <tag>Probability</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Time2Graph: Revisiting Time Series Modeling with Dynamic Shapelets</title>
    <link href="/2020/06/13/Time2Graph-Revisiting-Time-Series-Modeling-with-Dynamic-Shapelets/"/>
    <url>/2020/06/13/Time2Graph-Revisiting-Time-Series-Modeling-with-Dynamic-Shapelets/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文旨在提供一种可解释的高效的时间序列建模（表示学习）方法来更好地服务分类任务。Shapelet在时间序列分类任务上体现了良好的可解释性。不过传统的基于Shapelet的方法忽略了Shapelet在不同时间片段上的动态性，即整个时间维度上不同的时间片段可能适合用不同的Shapelet。作者基于此设计了动态的<em>time-aware shapelet</em>，并且定义了<em>shapelet evolution graph</em>来捕获Shapelet在时间维度上的动态变化。</p><p><a href="https://arxiv.org/abs/1911.04143" target="_blank" rel="noopener">📰Get Paper</a></p><h1 id="preliminaries">Preliminaries</h1><p>时间序列集合<span class="math inline">\(T=\{t_1,\cdots,t_{|T|}\}\)</span>包含若干条时序数据<span class="math inline">\(t=\{x_1,\cdots,x_n\}\)</span>。一个<span class="math inline">\(t\)</span>的片段<span class="math inline">\(s\)</span>是<span class="math inline">\(t\)</span>的一个连续子序列。如果<span class="math inline">\(t\)</span>能被切分成<span class="math inline">\(m\)</span>个长度都为<span class="math inline">\(l\)</span>的片段，那么我们就有<span class="math inline">\(t=\{\{x_{l*k+1},\cdots,x_{l*k+l}\},0\leq k\leq m-1\}\)</span>。两个长度相等的片段之间距离很好度量，直接计算欧式距离即可，那么两个片段长度不相等的情况呢？这就需要对其（Alignment）的概念。</p><blockquote><p><strong>Definition 1 </strong> <strong><em>Alignment</em></strong>. 给定两个长度分别为<span class="math inline">\(l_i\)</span>和<span class="math inline">\(l_j\)</span>的序列<span class="math inline">\(s_i\)</span>和<span class="math inline">\(s_j\)</span>，一个<em>alignment</em> <span class="math inline">\(a=(a_1,a_2)\)</span>是一个满足以下条件的长度为<span class="math inline">\(p\)</span>的下标序列： <span class="math display">\[1\leq a_k(1)\leq\cdots\leq a_k(p)=l_k,\\a_k(n+1)-a_k(n)\leq 1,\\\text{for }k=i,j,\text{ and }1\leq n\leq p-1\]</span></p></blockquote><p>上述公式可能比较抽象，其实看了下图就不难理解：</p><p><img src="https://i.loli.net/2020/07/07/UE2GtoYK1vhDpLi.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>片段<span class="math inline">\(s_i\)</span>中的某个点<span class="math inline">\(a\)</span>，与片段<span class="math inline">\(s_j\)</span>中的某个点<span class="math inline">\(b\)</span>形成对应，然后在<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>之间连一条虚拟的线（不能与已有的线交叉），一直这么做直到短的那个片段中的每个点都找到对应，就是一个合理的<em>alignment</em>。对于两个长度不一样的片段<span class="math inline">\(s_i\)</span>和<span class="math inline">\(s_j\)</span>，会有很多种<em>alignment</em>。我们把<span class="math inline">\(s_i\)</span>和<span class="math inline">\(s_j\)</span>所有可能的<em>alignment</em>记为<span class="math inline">\(\mathcal{A}(s_i,s_j)\)</span>。在定义了<em>alignment</em>之后就可以定义DTW了。DTW (<em>Dynamic Time Warping</em>) 定义为在给定一个预定义的距离度量<span class="math inline">\(\tau\)</span>和所有可能的<em>alignment</em> <span class="math inline">\(\mathcal{A}(s_i,s_j)\)</span>的情况下，最小的距离<span class="math inline">\(\tau\)</span>： <span class="math display">\[d_\text{DTW}(s_i,s_j)=\min_{a\in\mathcal{A}(s_i,s_j)}\tau(s_i,s_j|a)\]</span></p><p>进一步的，因为时间序列<span class="math inline">\(t\)</span>也可以看作是一个片段，我们可以定义一个子序列<span class="math inline">\(s\)</span>和时间序列<span class="math inline">\(t\)</span>之间的距离度量： <span class="math display">\[D(s,t)=\min_{1\leq k\leq m} d(s,s_k)\]</span></p><p>这里<span class="math inline">\(\boldsymbol s_k\)</span>为时序<span class="math inline">\(\boldsymbol t\)</span>分解成的片段。之后Shapelet可以通过片段与时序的距离定义为最具有辨识度的有代表性的片段：</p><blockquote><p><strong>Definition 2 </strong> <strong>Shapelet. </strong>一个Shapelet <span class="math inline">\(\boldsymbol v\)</span>是对于特定类别时序的最具有代表性的片段。考虑时序分类任务，给定时序集合<span class="math inline">\(T\)</span>，可以通过与<span class="math inline">\(\boldsymbol v\)</span>相似或不相似而分成两个子集合，与<span class="math inline">\(\boldsymbol v\)</span>相似的集合与<span class="math inline">\(\boldsymbol v\)</span>的距离应当尽量小，与<span class="math inline">\(\boldsymbol v\)</span>不相似的集合与<span class="math inline">\(\boldsymbol v\)</span>的距离应当尽量大，此时损失函数可以形式化为： <span class="math display">\[\mathcal L=-g(S_{pos}(\boldsymbol v,T),S_{neg}(\boldsymbol v,T))\]</span></p></blockquote><p><span class="math inline">\(\mathcal L\)</span>描述了在shapelet <span class="math inline">\(\boldsymbol v\)</span>下正负样本集的相异性。<span class="math inline">\(S_{*}(\boldsymbol v,T)\)</span>表示特定时序集合与<span class="math inline">\(\boldsymbol v\)</span>的距离集合，<span class="math inline">\(g(\cdot,\cdot)\)</span>为接受两个有限集合为输入的可微函数，并且能够度量两个集合的距离。</p><h1 id="framework">Framework</h1><p>本文主要是提出了一种时间序列表示学习方法。基于Shapelet在不同的时间片段上的作用是不同的观察，作者为不同的时间片段赋予了不同的Shapelet，而不是像传统方法一样整个时序对应一个Shapelet。接着基于这些Shapelet作者构造了图，并通过图嵌入得到了嵌入向量，作为时序的表示。</p><p><img src="https://i.loli.net/2020/06/25/mKJH4c2EMAljavG.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h2 id="time-aware-shapelet-extraction">Time-Aware Shapelet Extraction</h2><p>第一步是捕获Shapelet在时间维度上的动态影响。我们定义了两个参数来定量的测量shapelet在不同时间上的动态性。第一个是局部因子<span class="math inline">\(\boldsymbol w_n\)</span>，用来控制shapelet内部<span class="math inline">\(n\)</span>个元素的权重，那么shapelet <span class="math inline">\(\boldsymbol v\)</span>和片段<span class="math inline">\(\boldsymbol s\)</span>的距离为： <span class="math display">\[\begin{align}\hat{d}(\boldsymbol v,\boldsymbol s|\boldsymbol w) &amp;= \tau(\boldsymbol v,\boldsymbol s|\boldsymbol a^*,\boldsymbol w)\\&amp; = \left(\sum_{k=1}^{p}\boldsymbol w_{\boldsymbol a^*_1(k)}\cdot(\boldsymbol v_{\boldsymbol a^*_1(k)}-\boldsymbol s_{\boldsymbol a^*_2(k)})^2\right)^{\frac{1}{2}}\end{align}\]</span> 其中<span class="math inline">\(\boldsymbol a^*\)</span>为DTW距离下的最佳对齐。</p><p>第二个是全局因素<span class="math inline">\(\boldsymbol u_m\)</span>，这主要是通过对不同片段<span class="math inline">\(\boldsymbol s\)</span>施加不同的权重实现的，于是shapelet <span class="math inline">\(\boldsymbol v\)</span>和时间序列<span class="math inline">\(\boldsymbol t\)</span>的距离可以重写为： <span class="math display">\[\hat{D}(\boldsymbol v,\boldsymbol t|\boldsymbol w,\boldsymbol u)=\min_{1\leq k\leq m}\boldsymbol u_k\cdot\hat{d}(\boldsymbol v,\boldsymbol s_k|\boldsymbol w)\]</span> 其中<span class="math inline">\(\boldsymbol t\)</span>被分割为<span class="math inline">\(m\)</span>个片段：<span class="math inline">\(\boldsymbol t=\{\boldsymbol s_1,\cdots,\boldsymbol s_m\}\)</span>。对于分类任务，具体的来说，我们先生成一堆Shapelet候选集，然后通过有监督的方法来挑选最佳的Shapelet和对应的参数<span class="math inline">\(\boldsymbol w\)</span>和<span class="math inline">\(\boldsymbol u\)</span>。</p><p>计算shapelet候选集的算法如下：</p><p><img src="https://i.loli.net/2020/06/25/srW4Shk79XBFL3U.png" srcset="/img/loading.gif" /></p><p>在获取了Shapelet候选集合之后，我们有带有标签的时序集合<span class="math inline">\(T\)</span>，对于每一个Shapelet我们可以优化：</p><p><span class="math display">\[\hat{\mathcal L}=-g(S_{pos}(\boldsymbol v,T),S_{neg}(\boldsymbol v,T))+\lambda\parallel \boldsymbol w\parallel+\epsilon\parallel \boldsymbol u\parallel\]</span></p><p>来获取最优的<span class="math inline">\(\hat{\boldsymbol w}\)</span>和<span class="math inline">\(\hat{\boldsymbol u}\)</span>。然后，我们可以挑选出使得<span class="math inline">\(\hat{\mathcal L}\)</span>最小的前<span class="math inline">\(K\)</span>个Shapelet。整个过程的算法流程如下：</p><p><img src="https://i.loli.net/2020/06/25/5b2TcjuBzlIFrPm.png" srcset="/img/loading.gif" /></p><h2 id="shapelet-evolution-graph">Shapelet Evolution Graph</h2><p>在获取了Shapelet之后，为了捕获Shapelet之间的相关性，我们定义了<em>Shapelet Evolution Graph</em>。</p><blockquote><p><strong>Definition 3 Shapelet Evolution Graph. </strong> <em>Shapelet Evolution Graph</em>为一个有向带权图<span class="math inline">\(G=(V,E)\)</span>，<span class="math inline">\(V\)</span>为<span class="math inline">\(K\)</span>个Shapelet，每条带有权重<span class="math inline">\(w_{ij}\)</span>的边<span class="math inline">\(e_{ij}\in E\)</span>代表两个Shapelet <span class="math inline">\(\boldsymbol v_i \in V\)</span>和<span class="math inline">\(\boldsymbol v_j \in V\)</span>被分配给相邻片段的概率。</p></blockquote><h3 id="graph-construction">Graph Construction</h3><p>这里来说一下，建图的具体过程。首先顶点为Shapelet，之后来进行边的构造。对于每一个片段<span class="math inline">\(\boldsymbol s_i\)</span>，我们会计算Shapelet到该片段的距离，距离越近代表这个Shapelet与片段越匹配。之后会设定一个阈值<span class="math inline">\(\delta\)</span>，然后将与片段的距离低于这个阈值的Shapelet分配给这个片段（一个Shapelet可能会分配给多个不同片段）。对于<span class="math inline">\(\boldsymbol s_i\)</span>的所有shapetlet我们记为<span class="math inline">\(\boldsymbol v_{i,*}\)</span>，我们会按照Shape到片段的距离进行归一化：</p><p><span class="math display">\[\boldsymbol p_{i,j}=\frac{\max(\hat{d}_{i,*}(\boldsymbol v_{i,*},\boldsymbol s_i))-\hat{d}_{i,j}(\boldsymbol v_{i,j},\boldsymbol s_i)}{\max(\hat{d}_{i,*}(\boldsymbol v_{i,*},\boldsymbol s_i))-\min(\hat{d}_{i,*}(\boldsymbol v_{i,*},\boldsymbol s_i))}\]</span></p><p>其中<span class="math inline">\(\hat{d}_{i,*}(\boldsymbol v_{i,*},\boldsymbol s_i)=\boldsymbol u_*[i]*\hat{d}(\boldsymbol v_{i,*},\boldsymbol s_i|\boldsymbol w_*)&lt;\delta\)</span>。这样对于每个片段<span class="math inline">\(\boldsymbol s_i\)</span>所分配的Shapelet对应的<span class="math inline">\(\boldsymbol p\)</span>之和会等于<span class="math inline">\(1\)</span>。对每一对相邻的片段<span class="math inline">\((\boldsymbol s_i,\boldsymbol s_{i+1})\)</span>的Shapelet <span class="math inline">\(\boldsymbol v_{i,j}\)</span>和<span class="math inline">\(\boldsymbol v_{i+1,k}\)</span>，我们创建一条连接<span class="math inline">\(\boldsymbol v_{*,j}\)</span>和<span class="math inline">\(\boldsymbol v_{*,k}\)</span>的边<span class="math inline">\(e_{j,k}\)</span>，权重为<span class="math inline">\(\boldsymbol p_{i,j}\cdot\boldsymbol p_{i+1,k}\)</span>。最后，所有重复的边会被合并。</p><p><img src="https://i.loli.net/2020/07/07/FzGEbWJflHmQa9R.png" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p>如上图所示，假设有两个片段，每个片段分配了<span class="math inline">\(3\)</span>个Shapelet，Shapelet <span class="math inline">\(B\)</span>在片段<span class="math inline">\(1\)</span>对应的概率是<span class="math inline">\(p_{12}\)</span>，Shapelet <span class="math inline">\(C\)</span>在片段<span class="math inline">\(2\)</span>对应的概率是<span class="math inline">\(p_{23}\)</span>，那么由于片段<span class="math inline">\(1\)</span>和<span class="math inline">\(2\)</span>是相邻片段，会在<span class="math inline">\(B\)</span>和<span class="math inline">\(C\)</span>之间连一条边，边的权重为<span class="math inline">\(p_{12}*p_{23}\)</span>。</p><p>建图的算法流程图如下：</p><p><img src="https://i.loli.net/2020/06/25/aq9tGuApSbiC67c.png" srcset="/img/loading.gif" /></p><h2 id="representation-learning">Representation Learning</h2><p>之后，我们使用DeepWalk算法来获取获取每个结点（Shapelet）的嵌入表示。对于时序<span class="math inline">\(\boldsymbol t=\{\boldsymbol s_1,\cdots,\boldsymbol s_m\}\)</span>即对应的Shapelet <span class="math inline">\(\{\boldsymbol v_{1,*},\cdots, v_{m,*}\}\)</span>和对应的概率<span class="math inline">\(\{\boldsymbol p_{1,*},\cdots,\boldsymbol p_{m,*}\}\)</span>，每个Shaplet <span class="math inline">\(\boldsymbol v_{i,j}\)</span>的表示记为<span class="math inline">\(\boldsymbol \mu(\boldsymbol v_{i,j})\)</span>。片段<span class="math inline">\(\boldsymbol s_i\)</span>对应的嵌入向量为对应的Shapelet嵌入向量与对应的概率值加权求和：</p><p><span class="math display">\[\boldsymbol\Phi_i=\left(\sum_j p_{i,j}\cdot\boldsymbol \mu(\boldsymbol v_{i,j})\right),\space 1\leq i \leq m\]</span></p><p>算法流程如下：</p><p><img src="https://i.loli.net/2020/06/25/O5exTgsVLuRWQ42.png" srcset="/img/loading.gif" /></p><h1 id="experiments">Experiments</h1><h2 id="experimental-setup">Experimental Setup</h2><p>文中用了<em>Earthquakes</em> (EQS)、<em>WormsTwoClass</em> (WTC)、<em>Strawberry</em> (STB)、<em>Electricity Consumption Records</em> (ECR)和<em>Network Traffic Flow</em> (NTF) 这五个数据集，其中后两个为作者自己收集的数据集。五个数据集对应的统计信息如下：</p><p><img src="https://i.loli.net/2020/06/25/zHjTKkUJB8fGwdi.png" srcset="/img/loading.gif" /></p><p>文中与多个Baseline进行了比较，包括:</p><ul><li><strong>Distance-based Models: </strong>文中使用了不同的距离度量与基于1-NN的模型进行组合，包括Euclidean Distance (ED)、Dynamic Time Warping (DTW)、Weighted DTW (WDTW)、Complexity-Invariant Distance (CID) 和 Derivative DTW (DDTW)；</li><li><strong>Feature-based Models: </strong>文中分别使用了提取特征（均值、标准差等）和原始序列来训练XGBoost。除此之外，还使用了 Bag-of-Patterns (BoP)、Time Series Forest (TSF)、Elastic Ensembles (EE) 和 基于SAX的 Vector Space Model (SAXVSM)；</li><li><strong>Shapelet-based Models: </strong>这部分模型包括 Learn Time Series Shapelets (LS)、Fast Shapelets (FS)、和 Learned Pattern Similarity (LPS)；</li><li><strong>Deep Learning Models: </strong>这部分模型包括MLP、LSTM和VAE。</li></ul><h2 id="comparison-results">Comparison Results</h2><p>对于前三个公共数据集评测标准采用Accuracy，后两个数据集因为样本类比不均衡，所以采用了Precision、Recall和F1作为评测标准。结果如下：</p><p><img src="https://i.loli.net/2020/06/25/jxKFpDVXdmc5tbE.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>在EQS数据集上，Time2Graph打败了所有Baseline，而在WTC和STB这两个数据集上也达到了较好的效果。在ECR和NTF这两个真实数据集上，Time2Graph在F1上打败了所有Baseline。</p><h2 id="parameter-analysis">Parameter Analysis</h2><p>本节对Shapelet的数量<span class="math inline">\(K\)</span>、嵌入维度<span class="math inline">\(B\)</span>和片段长度<span class="math inline">\(l\)</span>进行了参数分析。结果如下：</p><p><img src="https://i.loli.net/2020/06/25/hPfqAvNnOlEBQuw.png" srcset="/img/loading.gif" /></p><h2 id="case-study-of-time-aware-shapelets">Case Study of Time-Aware Shapelets</h2><p>本节作者对提出的<em>Time-Aware Shapetlet</em>进行了细致的探究。第一个问题是不同Shapelet的区分能力是否不同？下图(a)里，作者在使用Shapelet进行二分类的任务中，将Shapelet按Loss（图中灰色的线）进行排序，并且绘制了对应的正负样本距离的KL散度（橘红色的点）。可以看到，在Loss曲线和KL散度呈反比关系。KL散度越高，我们可以认为该Shapelet的区分度越高，这说明不同Shapelet的区分度的确不同，并且这会与最终效果直接挂钩。图(b)展示了不同Shapelet的均值和方差（原文没有说清楚是什么的均值和方差）。</p><p>除此之外，作者和流行的Shapelet提取算法<em>LS</em>进行了比较，如图(c)和图(d)。从图中可以看到对于不同时间，本文的算法提取的Shapelet的确是具有时间动态性的。</p><p><img src="https://i.loli.net/2020/06/25/ed5PwksC2l3OWE8.png" srcset="/img/loading.gif" /></p><h2 id="case-study-of-the-shapelet-evolution-graph">Case Study of the Shapelet Evolution Graph</h2><p>本节作者对<em>Shapelet Evolution Graph</em>进行了细致的探究。下图分别为一月份和七月份的<em>Shapelet Evolution Graph</em>。在一月，45号Shapelet的度较大，而且对应的时间因素在一月和二月也较大（图中深色部分）。说明45号Shapelet在一月份具有代表性。而在七月，45号Shapelet的重要性降低，而42号Shapelet在七月的重要性很高。</p><p><img src="https://i.loli.net/2020/06/25/N2Vij8ODQaRuALJ.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Time Series Modeling</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Shapelet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Probabilistic Novelty Detection with Adversarial Autoencoders</title>
    <link href="/2020/06/06/Generative-Probabilistic-Novelty-Detection-with-Adversarial-Autoencoders/"/>
    <url>/2020/06/06/Generative-Probabilistic-Novelty-Detection-with-Adversarial-Autoencoders/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>这篇文章介绍了一种基于概率分布的异常检测方法。其基本思想是假设正常样本服从定义在流形<span class="math inline">\(M\)</span>上的分布，而对于任意一点<span class="math inline">\(\bar x\)</span>，通过投影到流形<span class="math inline">\(M\)</span>上<span class="math inline">\(x^\parallel\)</span>，可以分解为平行于切空间的部分<span class="math inline">\(x^\parallel\)</span>和正交与切空间的部分<span class="math inline">\(x^\bot\)</span>。原始的坐标<span class="math inline">\(\bar x\)</span>被转换到<span class="math inline">\(x^\parallel\)</span>局部坐标系中，然后似然通过转换后的坐标系进行计算。</p><h1 id="methodology">Methodology</h1><h2 id="generative-probabilistic-novelty-detection">Generative Probabilistic Novelty Detection</h2><p>我们假设训练数据<span class="math inline">\(x_1,\cdots,x_N\)</span>，其中<span class="math inline">\(x_i\in\mathbb{R}^m\)</span>，从一个分布采样的来，并带有随机噪声<span class="math inline">\(\xi\)</span>： <span class="math display">\[x_i=f(z_i)+\xi_i, \space\space\space i=1,\cdots,N\]</span> 其中<span class="math inline">\(z_i\in\mathbb{R}^n\)</span>，<span class="math inline">\(f:\Omega\mapsto\mathbb{R}^m\)</span>定义了一个<span class="math inline">\(n\)</span>维带参流形<span class="math inline">\(\mathcal{M}\equiv f(\Omega)\)</span>。注意这里噪声的加入使得样本的值域扩展到了整个实数空间。同时假设存在<span class="math inline">\(g:\mathbb{R}^m\mapsto\mathbb{R}^n\)</span>，对任意<span class="math inline">\(x\in\mathcal{M}\)</span>都有<span class="math inline">\(f(g(x))=x\)</span>。<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>后面会通过神经网络实现。</p><p>对于一个测试样本<span class="math inline">\(\bar{x}\in\mathbb{R}^m\)</span>，我们可以得到其在<span class="math inline">\(M\)</span>上的投影，这是通过逆变换<span class="math inline">\(\bar z = g(\bar x)\)</span>得到对应<span class="math inline">\(z\)</span>的然后再通过<span class="math inline">\(\bar x^{\parallel}=f(\bar z)\)</span>得到。<span class="math inline">\(f\)</span>在<span class="math inline">\(\bar z\)</span>的一阶泰勒展开为： <span class="math display">\[f(z)=f(\bar z)+J_f(\bar z)(z-\bar z)+O(\parallel z-\bar z\parallel ^2)\]</span> <img src="https://i.loli.net/2020/06/25/oi9xKMO3ID7jANJ.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>其中<span class="math inline">\(J_f(\bar z)\)</span>为<span class="math inline">\(f\)</span>在点<span class="math inline">\(\bar z\)</span>的雅各比矩阵。<span class="math inline">\(\mathcal T=\text{span}(J_f(\bar z))\)</span>代表点<span class="math inline">\(\bar z\)</span>处由<span class="math inline">\(J_f(\bar z)\)</span>的<span class="math inline">\(n\)</span>个独立向量组成的切空间。通过对<span class="math inline">\(J_f(\bar z)\)</span>进行奇异值分解<span class="math inline">\(J_f(\bar z)=U^\parallel SV^\top\)</span>。 <span class="math display">\[\bar w=U^\top\bar x=\left[\begin{matrix}U^{\parallel^\top}\bar x\\ U^{\bot^\top}\bar x\end{matrix}\right]=\left[\begin{matrix}\bar w^\parallel\\ \bar w^\bot\end{matrix}\right]\]</span> 坐标<span class="math inline">\(\bar w\)</span>可以分解为平行于<span class="math inline">\(\mathcal T\)</span>和正交于<span class="math inline">\(\mathcal T\)</span>两部分。</p><p>定义在施加变换前后的坐标系上的概率分布<span class="math inline">\(p_X(x)\)</span>和<span class="math inline">\(p_W(w)\)</span>是等价的，不过对于<span class="math inline">\(p_W(w)\)</span>，我们假设平行部分和正交部分是独立的，即： <span class="math display">\[p_X(x)=p_W(w)=p_W(w^\parallel,w^\bot)=p_{W^\parallel}(w^\parallel)p_{W^\bot}(w^\bot)\]</span> 这一假设的依据是随机噪声部分假设主要是往流形之外偏离的，即与<span class="math inline">\(\mathcal T\)</span>正交，所以<span class="math inline">\(W^\bot\)</span>主要是反映噪声的部分。而噪声与样本分布相独立的假设是合理的。于是，异常分数可以定义为： <span class="math display">\[p_X(\bar x)=p_{W^\parallel}(\bar w^\parallel)p_{W^\bot}(\bar w^\bot)=\begin{cases}\geq \gamma \Rightarrow \text{Inlier}\\&lt;\gamma\Rightarrow\text{Outlier}\end{cases}\]</span></p><h2 id="computing-the-distribution-of-data-samples">Computing the Distribution of Data Samples</h2><p>上面的异常分数需要计算<span class="math inline">\(p_{W^\parallel}(\bar w^\parallel)\)</span>和<span class="math inline">\(p_{W^\bot}(\bar w^\bot)\)</span>。给定测试样本<span class="math inline">\(\bar x\)</span>，投影到流形<span class="math inline">\(\bar x^\parallel=f(g(\bar x))\)</span>。<span class="math inline">\(\bar w^\parallel\)</span>可以重写为<span class="math inline">\(\bar w^\parallel=U^{\parallel^\top}\bar x=U^{\parallel^\top}(\bar x-\bar x^{\parallel})+U^{\parallel^\top}\bar x^\parallel=U^{\parallel^\top}\bar x^\parallel\)</span>，即我们假设<span class="math inline">\(U^{\parallel^\top}(\bar x-\bar x^\parallel)\approx 0\)</span>。于是有<span class="math inline">\(w^\parallel(z)=U^{\parallel^\top}f(\bar z)+SV^\top(z-\bar z)+O(\parallel z-\bar z\parallel^2)\)</span>。</p><p>如果<span class="math inline">\(Z\)</span>为定义在流形上的概率分布，那么： <span class="math display">\[p_{W^\parallel}(w^\parallel)=|\text{det}S^{-1}|p_Z(z)\]</span> <span class="math inline">\(p_{W^\bot}(w^\bot)\)</span>由半径为<span class="math inline">\(\parallel w^\bot\parallel\)</span>的超球体<span class="math inline">\(\mathcal S^{m-n-1}\)</span>来进行估计： <span class="math display">\[p_{W^\bot}(w^\bot)\approx\frac{\Gamma(\frac{m-n}{2})}{2\pi^{\frac{m-n}{2}}\parallel w^\bot\parallel^{m-n}}p_{\parallel W^\bot\parallel}(\parallel w^\bot\parallel)\]</span></p><p>其中<span class="math inline">\(\Gamma(\cdot)\)</span>代表Gamma函数。</p><h2 id="manifold-learning-with-adversarial-autoencoders">Manifold Learning with Adversarial Autoencoders</h2><p>为了学习映射<span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>，我们使用了AAE框架，如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/sQhO3D4gKJqBPXv.png" srcset="/img/loading.gif" style="zoom: 67%;" /></p><p>除了常规的AAE外，我们还为<span class="math inline">\(x\)</span>添加了一个额外的判别器。</p><h3 id="adversarial-losses">Adversarial Losses</h3><p>对于隐变量<span class="math inline">\(z\)</span>，对抗损失函数为： <span class="math display">\[\mathcal L_{adv-d_z}(x,g,D_z)=E[\log(D_z(\mathcal N(0,1)))]+E[\log(1-D_z(g(x)))]\]</span> 对于样本<span class="math inline">\(x\)</span>，对抗损失函数为： <span class="math display">\[\mathcal L_{adv-d_x}(x,D_x,f)=E[\log(D_x(x))]+E[\log(1-D_x(f(\mathcal N(0,1))))]\]</span></p><h3 id="autoencoder-loss">Autoencoder Loss</h3><p><span class="math display">\[\mathcal L_\text{error}(x,g,f)=-E_z[\log(p(f(g(x))|x))]\]</span></p><h3 id="full-objective">Full Objective</h3><p><span class="math display">\[\mathcal L(x,g,D_z,D_x,f)=\mathcal L_{adv-d_z}+\mathcal L_{adv-d_x}+\lambda \mathcal L_\text{error}\]</span></p><p>下图为模型重构的例子：</p><p><img src="https://i.loli.net/2020/06/25/i7ytlgjoIbYV6uF.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="experiments">Experiments</h1><h2 id="datasets">Datasets</h2><ul><li><strong>MNIST. </strong> 手册数字识别数据集。</li><li><strong>The Coil-100. </strong>包含7200张100个不同物体的不同角度的图片。</li><li><strong>Fashion-MNIST. </strong> 手册数字识别数据集彩色版。</li><li><strong>Others. </strong> 前三个数据集都是采用一个类作为inlier，而其他类作为outlier。在这一设置中inlier采样自数据集CIFAR-10(CIFAR-100)，而outlier采样自TinyImageNet、LSUN和iSUN。</li></ul><h2 id="results">Results</h2><h3 id="mnist-dataset">MNIST Dataset</h3><p><img src="https://i.loli.net/2020/06/25/5a71oidmK2ZGLyh.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/4lcGeHDrhbdKN5W.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="coil-100-dataset">Coil-100 Dataset</h3><p><img src="https://i.loli.net/2020/06/25/ofVGBgR7a3WmyvU.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="fashion-mnist">Fashion-MNIST</h3><p><img src="https://i.loli.net/2020/06/25/avURoBw6ny8SIEq.png" srcset="/img/loading.gif" /></p><h3 id="cifar-10-cifar-100">CIFAR-10 (CIFAR-100)</h3><p><img src="https://i.loli.net/2020/06/25/piteKy1m9kvQ6EU.png" srcset="/img/loading.gif" style="zoom: 67%;" /></p><h3 id="ablation">Ablation</h3><p><img src="https://i.loli.net/2020/06/25/xgni9wBtYkheGZq.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/idhqkCbAKvzMt68.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>GAN</tag>
      
      <tag>Novelty Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Estimate the Implicit Likelihoods of GANs with Application to Anomaly Detection</title>
    <link href="/2020/06/06/Estimate-the-Implicit-Likelihoods-of-GANs-with-Application-to-Anomaly-Detection/"/>
    <url>/2020/06/06/Estimate-the-Implicit-Likelihoods-of-GANs-with-Application-to-Anomaly-Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><h1 id="preliminaries">Preliminaries</h1><h2 id="flow-based-models-recap">Flow-based Models Recap</h2><h3 id="jacobian-matrix-and-determinant">Jacobian Matrix and Determinant</h3><p>设函数<span class="math inline">\(f:\mathbb{R}^n\mapsto\mathbb{R}^m\)</span>，由<span class="math inline">\(f\)</span>的一阶偏导数组成的矩阵叫做Jacobian矩阵： <span class="math display">\[\mathbf J=\left[\begin{matrix}\frac{\partial f_1}{\partial x_1} &amp;\cdots &amp;\frac{\partial f_1}{\partial x_n}\\\vdots &amp;\ddots &amp;\vdots\\\frac{\partial f_m}{\partial x_1}&amp; \dots&amp; \frac{\partial f_m}{\partial x_n}\end{matrix}\right]\]</span></p><h3 id="change-of-variable-theorem">Change of Variable Theorem</h3><p>给定已知概率分布的随机变量<span class="math inline">\(z\sim\pi(z)\)</span>，新随机变量由可逆映射<span class="math inline">\(f\)</span>给出<span class="math inline">\(x=f(z)\)</span>，<span class="math inline">\(z=f^{-1}(x)\)</span>。由于： <span class="math display">\[\int p(x)\mathrm{d}x=\int \pi(z)\mathrm d z=1\]</span> 所以<span class="math inline">\(p(x)\)</span>可以由下式得到： <span class="math display">\[p(x)=\pi(z)\left|\frac{\mathrm d z}{\mathrm d x}\right|=\pi(f^{-1}(x))\left|\frac{\mathrm d f^{-1}}{\mathrm d x}\right|=\pi(f^{-1}(x))|(f^{-1})^\prime(x)|\]</span> <span class="math inline">\(\frac{\mathrm d f^{-1}}{\mathrm d x}\)</span>相当于对逆函数<span class="math inline">\(f^{-1}\)</span>求导，而<span class="math inline">\(z=f^{-1}(x)\)</span>。</p><p>对于多变量的情况： <span class="math display">\[\mathbf z\sim \pi(\mathbf z), \mathbf x=f(\mathbf z),\mathbf z = f^{-1}(\mathbf x)\\p(\mathbf x) = \pi(\mathbf z)\left|\text{det}\frac{\mathrm d \mathbf z}{\mathrm d \mathbf x}\right|= \pi(f^{-1}(\mathbf x))\left|\text{det}\frac{\mathrm d f^{-1}}{\mathrm d \mathbf x}\right|\]</span> <span class="math inline">\(\left|\text{det}\frac{\mathrm d f^{-1}}{\mathrm d \mathbf x}\right|\)</span>为逆函数Jacobian矩阵行列式的绝对值。</p><h2 id="deep-generative-models-as-manifolds">Deep Generative Models as Manifolds</h2><p>一个深度生成模型可以表示为从低维隐空间<span class="math inline">\(\mathcal Z \subseteq R^d\)</span>映射到流形<span class="math inline">\(\mathcal M\subseteq R^D\)</span>的嵌入函数<span class="math inline">\(g:\mathcal Z\rightarrow \mathcal X\)</span>，<span class="math inline">\(D \ll d\)</span>。通常假设<span class="math inline">\(g\)</span>为光滑映射，那么<span class="math inline">\(\mathcal M\)</span>为一个嵌入流形。<span class="math inline">\(g\)</span>在<span class="math inline">\(z\in\mathcal Z\)</span>的雅各比矩阵<span class="math inline">\(\mathbf J_g(z)\)</span>提供了在<span class="math inline">\(x=g(z)\in\mathcal X\)</span>处的切空间，即<span class="math inline">\(\mathbf J_g:T_z\mathcal Z\rightarrow T_x\mathcal X\)</span>。这个映射不是满射，并且值域限制在流形<span class="math inline">\(\mathcal M\)</span>在<span class="math inline">\(x=g(z)\)</span>的切空间，记为<span class="math inline">\(T_x\mathcal M\)</span>。<span class="math inline">\(\mathcal M\)</span>与真实数据流形接近，即<span class="math inline">\(\mathcal M_{data}\)</span>。</p><p>考虑一个定义在隐空间<span class="math inline">\(\mathcal Z\)</span>每一点<span class="math inline">\(z\)</span>上的Riemannian metric： <span class="math display">\[\mathbf M(z)=\mathbf J_g(z)^\top\mathbf J_g(z)\]</span> 给定两个切空间向量<span class="math inline">\(u,v\in T_z\in\mathcal Z\)</span>，他们的内积定义为<span class="math inline">\(&lt;u,v&gt;=u^\top \mathbf M(z)v\)</span>。</p><p>考虑一个光滑曲线<span class="math inline">\(\gamma:t\in[a,b]\rightarrow\mathcal Z\)</span>， <span class="math display">\[L(\gamma)=\int^b_a\sqrt{\hat{\gamma}(t)^\top M_{\gamma(t)}\hat{\gamma}(t)}\mathrm d t\]</span></p><h2 id="change-of-variable-on-manifolds">Change of Variable on Manifolds</h2><p>在流形上的<strong>Change of Variable Theorem</strong>是类似的：</p><p><span class="math display">\[p_\mathcal{X}(x)=p_\mathcal{Z}(g^{-1}(x))\text{det}(\mathbf J_g(g^{-1}(x))^\top\mathbf J_g(g^{-1}(x)))^{-\frac{1}{2}}\]</span></p><p>我们假设隐空间分布<span class="math inline">\(p_\mathcal{Z}\)</span>服从标准正态先验分布<span class="math inline">\(N(0,\mathbf I_d)\)</span>，于是我们可以计算出<span class="math inline">\(x\)</span>的似然：</p><p><span class="math display">\[\log(p_\mathcal{X}(g(z)))=\log(p_\mathcal{Z}(z))-\frac{1}{2}\log(\text{det}(\mathbf J_g(z)^\top\mathbf J_g(z)))\]</span></p><p>通常我们没法知道真实世界中<span class="math inline">\(\mathcal M_{data}\)</span>的切空间维度是多少，设定<span class="math inline">\(d=\text{dim}\mathcal Z&gt;T_x\mathcal M_{data}\)</span>会导致<span class="math inline">\(\mathbf J_g\)</span>不是满秩矩阵，这时<span class="math inline">\(\text{det}(\mathbf J_g(z)^\top\mathbf J_g(z))=0\)</span>，即无法应用上式计算似然。</p><h2 id="density-estimation-with-neural-networks">Density Estimation with Neural Networks</h2><h2 id="anomaly-detection-with-generative-models">Anomaly Detection with Generative Models</h2><h1 id="proposed-method">Proposed Method</h1><p>对于GAN模型，给定隐变量<span class="math inline">\(z\)</span>和生成器<span class="math inline">\(g\)</span>，我们可以通过公式(8)计算生成样本<span class="math inline">\(g(z)\)</span>的对数似然，为了保证Jacobian矩阵为满秩，我们可以选定较低的隐变量维度<span class="math inline">\(d\)</span>，不过这样会伤害GAN模型的性能。为了解决这个问题，我们需要使用一个<em>inference network</em>来将<span class="math inline">\(x\)</span>映射回<span class="math inline">\(z\)</span>，</p><h2 id="the-variance-network-of-the-generator">The Variance Network of the Generator</h2><p>文中对生成器<span class="math inline">\(g\)</span>进行了扩展：</p><p><span class="math display">\[f(z) = g(z) + \sigma(z)\odot\epsilon,\\g:\mathcal Z\rightarrow\mathcal X, \sigma:\mathcal Z\rightarrow\mathbb{R},\epsilon\sim N(0,\mathbf I_D)\]</span></p><p><span class="math inline">\(g\)</span>代表均值，<span class="math inline">\(\sigma\)</span>代表方差，实际上<span class="math inline">\(\mathbb E_{\epsilon\sim N(0,\mathbf I_D)}f(z)=g(z)\)</span>。为了保证方差在少样本或无样本的区域更大，方差网络采用RBF网络实现。</p><blockquote><p>RBF网络： <span class="math display">\[h(x)=\exp\left(-\frac{(x-c)^2}{r^2}\right)\]</span> <img src="https://i.loli.net/2020/06/25/ho5FJsOBjxi2G78.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><p><img src="https://i.loli.net/2020/06/25/rtRLwyK2VigOvTh.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p></blockquote><p>首先，从隐空间采样大量的样本，之后使用<strong>K-means</strong>算法将其分为<span class="math inline">\(K\)</span>个聚簇，<span class="math inline">\(c_k\)</span>为第<span class="math inline">\(k\)</span>个簇的中心，<span class="math inline">\(C_k\)</span>为第<span class="math inline">\(k\)</span>个簇的样本数量。对于任何样本<span class="math inline">\(x\)</span>，为了计算其对应的方差，需要用一个<em>inference network</em> <span class="math inline">\(h\)</span>来将其映射到隐空间，即<span class="math inline">\(z=h(x)\)</span>。RBF网络通过输入<span class="math inline">\(z\)</span>到每个簇中心的距离来返回<span class="math inline">\(x\)</span>对应的方差，RBF函数由下式给出： <span class="math display">\[\sigma(z)=(\mathbf W^2\mathbf v(z))^{-\frac{1}{2}},\\v_k(z) = \exp(-\lambda_k\parallel z-c_k\parallel^2_2), \space k=1,\cdots,K\\\lambda_k=\frac{1}{2}\left(\frac{a}{|C_k|}\sum_{z_j\in C_k}\parallel z_j-c_k\parallel_2\right)^{-2}\]</span></p><p>其中<span class="math inline">\(\alpha\)</span>为核的超参数，<span class="math inline">\(\mathbf W\)</span>为模型参数。给定生成器<span class="math inline">\(g\)</span>，我们可以通过最小化<span class="math inline">\(f(z)\)</span>和<span class="math inline">\(x\)</span>之间的距离来优化参数<span class="math inline">\(\mathbf W\)</span>。</p><p><span class="math display">\[\begin{align}\bar{\mathbf M}^z_f&amp;=\mathbb E_{\epsilon\sim N(0,\mathbf I_D)}\mathbf J_f^\top(z)\mathbf J_f(z)\\&amp;=\mathbf J_g^\top(z)\mathbf J_g(z)+\mathbf J_\sigma^\top(z)\mathbf J_\sigma(z)\end{align}\]</span></p><blockquote><p><strong>LEMMA 1. </strong> 对于<span class="math inline">\(K\geq \text{dim}(\mathcal Z)+1\)</span>和满秩矩阵<span class="math inline">\(\mathbf W^2\)</span>，<span class="math inline">\(\bar{\mathbf M}_f^z\)</span>为一个满秩矩阵。生成样本的对数似然由下式给出： <span class="math display">\[\mathbb E_{\epsilon\sim N(0,\mathbf I_D)}\log(p_\mathcal{X}(f(z)))=\log(p_\mathcal{Z}(z))-\frac{1}{2}\log(\text{det}(\bar{\mathbf M}_f^z))\]</span></p></blockquote><h2 id="the-inference-network-learning">The Inference Network Learning</h2><p><img src="https://i.loli.net/2020/06/25/AZfIrq2v8RVipj1.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><span class="math display">\[\max_{h,\sigma}\mathbb E_{x\sim p_{data}(x)}\left[\log(p(x|z))|_{z=h(x)}\right]\]</span></p><p><span class="math inline">\(p(x|z)\)</span></p><p><span class="math display">\[\max_{h,\sigma}\mathbb E_{x\sim p_{data}(x), z=h(x)}\left[\log(p(x|z))-\text{KL}\right]\]</span></p><blockquote><p><strong>LEMMA 2. </strong> <span class="math display">\[f(z)\sim GP(g(z), \Lambda_\sigma)\]</span></p></blockquote><p><span class="math display">\[L_{h,\sigma}=\mathbb E_{x\sim p_{data}}\left[\sum_{i=1}^D[-\frac{1}{2}(g_i(h(x))-x_i)^2/\sigma_i^2(h(x))-\log\sigma_i(h(x))]-\text{KL}(q(z|x)\parallel p(z))\right]\]</span></p><h2 id="stabilize-training">Stabilize Training</h2><p><span class="math display">\[\min_{g,h}\mathbb E_{z\sim p(z)}[\parallel z-h(g(z))\parallel^2]\]</span></p><h2 id="algorithm">Algorithm</h2><p><img src="https://i.loli.net/2020/06/25/PjJF9wBM2XS38TQ.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><blockquote><p><strong>LEMMA 3. </strong></p></blockquote><h1 id="experiments">Experiments</h1><p><img src="https://i.loli.net/2020/06/25/GlqvJFOtEnbNoUP.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/Z8yDQU9McsT5GXI.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/NmBten8igTUV76L.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/8phKZkiRxdLyBT6.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/noBqN9gWfPuFy8w.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/sJnwi5RzLAU3D6x.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/OvtLenWDSEi37lX.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/TIJbpsvGkwz3rVF.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Classification-based Anomaly Detection for General Data</title>
    <link href="/2020/06/02/Classification-based-Anomaly-Detection-for-General-Data/"/>
    <url>/2020/06/02/Classification-based-Anomaly-Detection-for-General-Data/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文主要是对<a href="http://qfxiao.me/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/">NIPS18这篇异常检测文章</a>的改进，首先是利用了标签信息来提升算法的表现，其次是将算法扩展到了非图像数据。作者对现有的异常检测算法进行了回顾：</p><ul><li><strong>Reconstruction Methods： </strong>这一部分方法假设异常样本和正常样本能够通过重构任务来进行区分。通过在正常样本上学习重构任务，之后对于正常样本，模型能够很好地进行重构，而异常样本则会有较高的重构误差。</li><li><strong>Distributional Methods： </strong>这一部分方法将异常检测看作是密度估计问题。通过对正常样本的分布进行估计，异常样本在该正常分布下的似然将会很低。</li><li><strong>Classification-based Methods： </strong>这一部分方法主要是指的单分类方法和通过几何变换构造分类任务的方法。本文使用的就是这类方法。</li></ul><h1 id="proposed-method">Proposed Method</h1><h2 id="classification-based-anomaly-detection">Classification-based Anomaly Detection</h2><p>假设所有数据位于空间<span class="math inline">\(R^L\)</span>内，而正常数据位于子空间<span class="math inline">\(X\subset R^L\)</span>内。我们假设所有的异常样本位于<span class="math inline">\(X\)</span>之外。为了检测异常，我们希望学习一个分类器<span class="math inline">\(C\)</span>使得对于所有的<span class="math inline">\(x\in X\)</span>有<span class="math inline">\(C(x)=1\)</span>，而对所有的<span class="math inline">\(x\in R^L\backslash X\)</span>有<span class="math inline">\(C(x)=0\)</span>。</p><p>单分类方法的思想是直接学习<span class="math inline">\(P(x\in X)\)</span>，代表的方法有One-Class SVM，DSVDD等。传统的OC-SVM直接在原始空间或者核空间学习分类器。比较新的方法，如Deep-SVDD则是先将样本转换到一个特征空间，然后在这个特征空间上学习使得半径<span class="math inline">\(R\)</span>最小的超球体（球心<span class="math inline">\(c_0\)</span>），来覆盖住所有正常样本。异常的判定则通过计算<span class="math inline">\(\parallel f(x)-c_0\parallel^2-R^2\)</span>来实现。不过学习一个好的样本到特征空间的变换并不是一件容易的事情，比如说<span class="math inline">\(f(x)=0, \forall x \in X\)</span>就是一个使得超球体最小的解。所以需要很多trick来避免诸如此类的情况。</p><p><em>Geometric-transformation classification</em> (GEOM) 则将数据空间<span class="math inline">\(X\)</span>通过<span class="math inline">\(M\)</span>个几何变换转换到一系列子空间<span class="math inline">\(X_1,\cdots,X_M\)</span>。之后训练一个分类器来预测样本<span class="math inline">\(T(x,m)\)</span>对应的几何变换的种类<span class="math inline">\(m\)</span>。转换后的正常图片空间记为<span class="math inline">\(\cup_m X_m\)</span>，所以该方法尝试估计以下条件概率： <span class="math display">\[P(m^\prime|T(x,m))=\frac{P(T(x,m)\in X_{m^\prime})P(m^\prime)}{\sum_{\bar{m}}P(T(x,m)\in X_{\bar{m}})P(\tilde{m})}-\frac{P(T(x,m)\in X_{m^\prime})}{\sum_{\bar{m}}P(T(x,m)\in X_{\bar{m}})}\]</span></p><p>对于异常的样本<span class="math inline">\(x\in R^L\backslash X\)</span>，在经过几何变换之后，都不会位于正确的子空间中，即<span class="math inline">\(T(x,m)\in R^L\backslash X_m\)</span>。之后，使用<span class="math inline">\(P(m|T(x,m))\)</span>来判定异常。</p><p>作者认为，这种方法的问题是分类器<span class="math inline">\(P(m^\prime|T(x,m))\)</span>只在正常数据上训练，而对于异常样本的异常分数会出现方差很大的问题。</p><p>一种解决方式是加入异常样本进行训练，但是作者认为在有的任务中标签很难获取，于是作者使用了另外一种方法来解决这个问题。</p><h2 id="distance-based-multiple-transformation-classification">Distance-based Multiple Transformation Classification</h2><p>和GEOM一样，先对每个样本进行<span class="math inline">\(M\)</span>个几何变换，然后学习一个特征提取器<span class="math inline">\(f(x)\)</span>，将<span class="math inline">\(X_m\)</span>映射到特征空间。之后和OC-SVM类似，假设特征<span class="math inline">\(\{f(x)|x\in X_m\}\)</span>为球心为<span class="math inline">\(c_m=\frac{1}{N}\sum_{x\in X} f(T(x,m))\)</span>的超球体。样本属于某一类<span class="math inline">\(m^\prime\)</span>的概率由下式给出：</p><p><span class="math display">\[P(m^\prime|T(x,m))=\frac{e^{-\parallel f(T(x,m))-c_{m^\prime}\parallel^2}}{\sum_{\bar m}e^{-\parallel f(T(x,m))-c_{\bar m}\parallel^2}}\]</span></p><p>目标函数采用的是Triplet Loss：</p><p><span class="math display">\[L=\sum_i\max(\parallel f(T(x_i,m))-c_m\parallel^2+s-\min_{m^\prime\neq m}\parallel f(T(x_i,m))-c_{m^\prime}\parallel^2,0)\]</span></p><p><span class="math inline">\(\parallel f(T(x_i,m))-c_m\parallel^2\)</span>相当于最小化了类内距离，<span class="math inline">\(\min_{m^\prime\neq m}\parallel f(T(x_i,m))-c_{m^\prime}\parallel^2\)</span>最大化了每个类对应的集簇间距离。在检测阶段，为了避免一些数值问题，作者做了一些平滑操作：</p><p><span class="math display">\[\tilde P(m^\prime|T(x,m))=\frac{e^{-\parallel f(T(x,m))-c_{m^\prime}\parallel^2+\epsilon}}{\sum_{\tilde m}e^{-\parallel f(T(x,m))-c_{\tilde m}\parallel^2+M\cdot\epsilon}}\]</span></p><p>最后的评判分数由下式给出：</p><p><span class="math display">\[Score(x)=-\log P(x\in X)=-\sum_m\log \tilde{P}(T(x,m)\in X_m)=-\sum_m\log\tilde{P}(m|T(x,m))\]</span></p><p>算法流程图如下：</p><p><img src="https://i.loli.net/2020/06/24/r48h1RJxcXF6YDM.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="parameterizing-the-set-of-transformations">Parameterizing the Set of Transformations</h2><p>在GEOM中，由于使用的几何变换都是针对图像的，所以对于其他类型的数据并不适用。本文中作者对非图像数据设计了以下变换：</p><p><span class="math display">\[T(x,m)=W_mx+b_m\]</span></p><p>不同的参数<span class="math inline">\(W_m\)</span>和<span class="math inline">\(b_m\)</span>即为不同的几何变换，可以考虑采用随机采样的方式。</p><h1 id="experiments">Experiments</h1><h2 id="image-experiments">Image Experiments</h2><p>对于图像数据的异常检测实验，作者采用了CIFAR10、FasionMNIST这两个数据集，实验结果如下：</p><p><img src="https://i.loli.net/2020/06/24/j4Y29tB6k1Aipgo.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/24/D3opwrLnSGmcsyM.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="tabular-data-experiments">Tabular Data Experiments</h2><p>对于非图像数据，作者采用了几个小的数据集：Arrhythmia、Thyroid、KDD和KDDRev。采用的Baseline包括OC-SVM、E2E-AE、LOF、DAGMM和FB-AE (Feature Bagging Autoencoder)。对于几何变换的参数，采样自标准正态分布。结果如下：</p><p><img src="https://i.loli.net/2020/06/24/e6PfIDOVwlzrSWi.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="remark">Remark</h1><p>结合<a href="https://openreview.net/forum?id=H1lK_lBtvS" target="_blank" rel="noopener">OpenReview</a>上的一些讨论，这里提出一些问题和总结：</p><ul><li>KDD数据集太简单了，正常、异常样本能够很容易被分开；</li><li>对于图像数据作者只使用了CIFAR10和FashionMNIST这两个比较小的数据集，而在GEOM中还使用了CIFAR100和CatsVsDogs。并且GEOM原文中提到数据集（指图像大小）越大，GEOM的优势就越明显，所以在本文的实验中只使用这两个数据集说服力略显不够；</li><li>关于评测标准的问题，作者在图像数据中用的是AUROC，而非图像数据用的是F1 score。像AUPR、AUROC这种评测标准往往更加全面，而F1 score依赖于阈值的选取。如果是遍历阈值找到最好的那个F1 score，则无法全面考察模型的鲁棒性，模型有可能只是在特定的阈值下表现很好，而阈值稍微偏差一下性能可能就会大幅下降。我看到的大多数异常检测文章都是使用AUROC或者F1加上AUROC作为评测指标；</li><li>文中在第二节“CLASSIFICATION-BASED ANOMALY DETECTION”的末尾两段关于GEOM方法的缺点说的很模糊。异常分数的方差大到底指的是什么；</li><li>关于作者提出的变换<span class="math inline">\(T(x,m)=W_mx+b_m\)</span>并没有用到图像数据的实验上，而且在实验中<span class="math inline">\(b_m\)</span>这个参数实际上是被忽略掉了的，<span class="math inline">\(b_m\)</span>的作用究竟如何不得而知。而且GEOM中的几何变换的Motivation在原文中是做了实验充分讨论了的，GEOM的作者认为这些几何变换保留了图像的高阶语义信息。而本文中的变换中的参数只是随机采样而来，并不存在说保留原始数据中的结构信息。如果忽略掉这一层变换，那就类似于加了神经网络提取特征的OC-SVM。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面向OpenPAI的Docker镜像配置及OpenPAI基本使用方法</title>
    <link href="/2020/06/02/%E9%9D%A2%E5%90%91OpenPAI%E7%9A%84Docker%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E5%8F%8AOpenPAI%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2020/06/02/%E9%9D%A2%E5%90%91OpenPAI%E7%9A%84Docker%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E5%8F%8AOpenPAI%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>实验室服务器集群采用OpenPAI来进行GPU资源的管理，而OpenPAI采用了Docker作为基础，即代码都放在Docker容器中运行。由于Docker的使用、Docker镜像的配置都有一定的门槛，所以这里写一篇Tutorial来进行介绍。本文不是网上资料的拼凑，而是经过本人走弯路踩坑形成的"Best practice"。主要内容包括Docker的介绍、Docker的基本使用、如何配置自己的Docker镜像以及OpenPAI平台的基本使用，但不包括Docker和OpenPAI的安装。</p><blockquote><p>2020.8.1 Update: 加入通过HDFS读取容器保存的文件的方法</p></blockquote><h1 id="docker-from-scratch">Docker from Scratch</h1><p>要理解Docker是什么，从虚拟机开始讲可能会比较好理解。虚拟机大家可能都很熟悉了，比如说我用的系统是Windows，但我需要Linux系统来作为一个Flask编写的网站的服务器，但是又不想单独安装Linux系统，于是可以使用虚拟机来解决这个问题。安装VMWare Workstation，去官网下载Ubuntu系统镜像，然后在VMWare中安装好系统，然后从头配置Flask相关环境。实际上我需要的仅仅是一个Flask运行环境而已，而使用虚拟机却需要如此“大费周章”，这时Docker出现了，网上有大量现成的Flask Docker镜像，配置好了你所需的Flask环境，你只需要下载这些镜像，然后运行它，你就得到了一个Flask运行环境，而与你当前使用的系统无关。如果你需要一个Tomcat的运行环境，那么去找一个Tomcat的Docker镜像就行。Docker将需求或者说服务绑定在了Docker镜像中（<strong>轻量化</strong>，一个需求对应一个Docker镜像，每个镜像都很小），你有什么需求，去找相应的镜像即可（或者自己写一个），镜像的运行是以虚拟机的形式存在，所以他们之间也是互不干扰的。同时，你在写好一个Docker镜像之后，你还可以<strong>分享</strong>给别人，这样其他人就不用重新配置，直接运行你给他的镜像即可。Docker有两个比较关键的概念：</p><ul><li><strong>镜像 Images：</strong> 这里的镜像不是指我们安装系统时下载的ISO镜像，Docker镜像就是把你需要的东西（一个系统+需要的服务）集中到一起，相当于做菜的菜谱；</li><li><strong>容器 Containers：</strong> 如果一个Docker镜像启动了，那么就会有一个Docker容器产生，相当于按照菜谱做出来的菜。</li></ul><p><img src="https://i.loli.net/2020/06/24/H2x9mnPwkVQyO6p.png" srcset="/img/loading.gif" alt="img" style="zoom: 33%;" /></p><p><img src="https://i.loli.net/2020/06/24/7k6S3yecX2lbvQY.png" srcset="/img/loading.gif" alt="img" style="zoom: 33%;" /></p><p>这一节我们先不讨论如何自己写Docker镜像，只是先讨论Docker的基本操作。</p><h2 id="basic-operations">Basic Operations</h2><p>Docker新安装好当然是没有什么镜像的，首先我们使用<code>docker pull hello-world</code>来下载一个测试镜像。</p><blockquote><p>拉取镜像 <code>docker pull &lt;image_name&gt;</code></p></blockquote><p>在输入之后，Docker会自动在远程服务器上查找对应的镜像进行下载。由于我的电脑上已经有这个镜像了，所以显示是下面的样子：</p><p><img src="https://i.loli.net/2020/06/24/rvqcAwzOYJtF6T8.png" srcset="/img/loading.gif" /></p><p>接下来，我们输入<code>docker run hello-world</code>运行这个镜像。</p><blockquote><p>运行镜像<code>docker run &lt;image_name&gt;</code></p></blockquote><p>可以看到，Docker输出了一些信息就自己退出了，这和我们理解的虚拟机不太一样。在Docker里面，我们既可以创建一个完整的系统，用户在运行之后就可以正常使用这个操作系统，也可以创建一个简单的服务，默认运行完一些指令就退出了。这里的<code>hello-world</code>镜像这是输出了一些信息后就自动退出了，因为这就是这个镜像的全部内容。</p><p><img src="https://i.loli.net/2020/06/24/Zb1VKjFyMh8gHf2.png" srcset="/img/loading.gif" /></p><p>我们尝试来运行一个完整的系统，先用<code>docker pull ubuntu</code>拉取Ubuntu Docker镜像：</p><p><img src="https://i.loli.net/2020/06/24/zEerb2gPwYWX8O7.png" srcset="/img/loading.gif" /></p><p>接下来我们使用：</p><p><img src="https://i.loli.net/2020/06/24/6cMz1WGH4fgpBsS.png" srcset="/img/loading.gif" /></p><blockquote><p><code>-it</code>的意思是什么？根据<code>docker run --help</code>：</p><pre><code class="hljs routeros">-i, --interactive                    Keep STDIN open even <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> attached    --ip string                      IPv4<span class="hljs-built_in"> address </span>(e.g., 172.30.100.104)    --ip6 string                    <span class="hljs-built_in"> IPv6 address </span>(e.g., 2001:db8::33)    --ipc string                     IPC mode <span class="hljs-keyword">to</span> use    --isolation string               Container isolation technology    --kernel-memory bytes            Kernel memory limit-t, --tty                            Allocate a pseudo-TTY    --ulimit ulimit                  Ulimit options (default [])Copy</code></pre><p>其实<code>-it</code>是<code>-i</code>和<code>-t</code>的合并写法，意思是运行后进入这个容器并且启用shell，不然运行之后就会放到后台而不会进入容器中。而<code>--rm</code>则代表容器退出之后会被删除（镜像不会被删除），每次运行实际上会创建一个新的容器，如果不加<code>--rm</code>或退出之后不手动删除的话会看到一堆停止运行的容器。</p></blockquote><p>输入<code>cat /etc/issue</code>可以看到默认拉取的是最新的Ubuntu 20.04 LTS：</p><p><img src="https://i.loli.net/2020/06/24/wldBMFtx3eIk98C.png" srcset="/img/loading.gif" /></p><h1 id="build-customized-docker-images">Build Customized Docker Images</h1><p>如果没有现成的Docker镜像能满足我们的需求，我们可以考虑自己写一个。要自定义一个Docker镜像需要两步，第一步是编写Dockerfile，第二步是使用<code>docker build</code>命令构建镜像。Dockerfile可以看作是一个脚本，描述了我们构建镜像所需要的全部命令，比如要构建一个用于Python科学计算的Docker镜像，我们需要在Dockerfile中编写安装Python的命令，安装Numpy、Scipy等常用包的命令等等。我们先来上手编写Dockerfile，这里我准备写一个包含<a href="https://hexo.io/" target="_blank" rel="noopener">hexo博客框架</a>的镜像，这个框架需要node作为基础环境，不过我们不需要在Dockerfile里写安装node的命令。因为类似于<code>C++</code>或<code>Python</code>中的对象的继承，Dockerfile也可以“继承”，这意味着我们不必从头写起。我们先来看一下完整的Dockfile和效果，再来一一解释。</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> node--------------------------------------------------------------<span class="hljs-keyword">RUN</span><span class="bash"> npm install -g hexo-cli</span><span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">4000</span><span class="hljs-keyword">CMD</span><span class="bash"> hexo init blog &amp;&amp; <span class="hljs-built_in">cd</span> blog &amp;&amp; hexo generate &amp;&amp; hexo serverCopy</span></code></pre><p>运行结果如下所示，可以看到Docker按照我们写的Dockerfile一行一行的进行镜像的构建：</p><p><img src="https://i.loli.net/2020/06/24/tZ3JW4SPNhxUwEn.png" srcset="/img/loading.gif" /></p><p>现在来解释Docerfile里的内容。<code>FROM &lt;docker image&gt;</code>表示继承其他的镜像，这里我们使用node官方的镜像。接下来是安装hexo，<code>RUN &lt;command&gt;</code>表示执行命令，这里我们直接用<code>npm install -g hexo-cli</code>进行安装。由于要浏览博客网页需要开放端口，而Docker容器运行的时候和外部主机是完全隔断的，要使外部主机访问Docker容器端口，需要暴露端口。<code>EXPOSE &lt;port&gt;</code>代表暴露端口，这里用的是4000端口。之后是创建博客和启动本地服务，<code>CMD &lt;command&gt;</code>和<code>RUN &lt;command&gt;</code>的区别是RUN会在构建的时候执行，而CMD是在容器启动之后才会执行。<code>hexo init blog &amp;&amp; cd blog &amp;&amp; hexo generate &amp;&amp; hexo server</code>分别代表初始化博客、进入博客所在文件夹、生成博客网站、启动本地服务器。更多指令可以参考<a href="https://docs.docker.com/engine/reference/builder/" target="_blank" rel="noopener">官方文档</a>。</p><p>然后我们使用<code>docker build -t test_hexo .</code>命令构建镜像。</p><blockquote><p>构建镜像 <code>docker build -t &lt;image_name&gt; &lt;direcotry&gt;</code></p></blockquote><p>运行镜像：</p><p><img src="https://i.loli.net/2020/06/24/m4FSe15kMIDCHXy.png" srcset="/img/loading.gif" /></p><p>可以看到容器启动后开始执行博客初始化。</p><p><img src="https://i.loli.net/2020/06/24/BEdibvgPTMz4sn8.png" srcset="/img/loading.gif" /></p><p>最后在<code>locahost:4000</code>上启动了一个本地服务器，在浏览器中输入这个地址，可以看到刚刚构建好的博客：</p><p><img src="https://i.loli.net/2020/06/24/vmSUb5QH6l4afzF.png" srcset="/img/loading.gif" /></p><p>值得注意的是，在Dockerfile中我们暴露了4000端口，使用<code>-p</code>标签可以达到同样的效果：<code>docker run -p &lt;docker_port&gt;:&lt;local_port&gt; &lt;image_name&gt;</code>。比如<code>docker run -p 9999:8888 xxxx</code>代表将Docker容器中的9999端口转发到外部主机的8888端口。如果你是在远程服务器上使用的Docker，那么端口只是被转发到了远程服务器上，还得手动将远程服务器再转发到你本机上才能直接在本机浏览器上看到页面。</p><h2 id="build-docker-images-with-aliyun-container-registry">Build Docker Images with Aliyun Container Registry</h2><p>因为某些原因，如果在构建镜像的时候需要通过<code>apt-get update</code>更新源，会发现无论如何都会卡住。这个时候可以使用<a href="https://cr.console.aliyun.com/" target="_blank" rel="noopener">阿里云容器镜像服务</a>，在阿里的服务器上构建好镜像，再拉取到自己的机器上。注册好帐号之后，点击创建镜像仓库：</p><p><img src="https://i.loli.net/2020/06/24/u4DGHv3En1iLI5z.png" srcset="/img/loading.gif" /></p><p>这里仓库类型如果没有特殊需求建议使用公开，然后填写一些基本信息：</p><p><img src="https://i.loli.net/2020/06/24/Qipw5hAg136afnL.png" srcset="/img/loading.gif" /></p><p>之后设置代码源，其实就是告诉阿里云从哪儿获取Dockerfile，我这里用的是Github，所以需要先在阿里云中关联Github账号，然后在Github中创建一个用来放Dockerfile的仓库。构建设置里有一个“海外机器构建”，这正是我们使用阿里云容器服务的主要目的，勾选。</p><p><img src="https://i.loli.net/2020/06/24/ksvZJG9lKfdh6aR.png" srcset="/img/loading.gif" /></p><p>镜像仓库创建好之后，点进去，在构建页面点击添加规则：</p><p><img src="https://i.loli.net/2020/06/24/irBm3QcqjVhGMsv.png" srcset="/img/loading.gif" /></p><p>按下图进行设置即可，镜像版本就是你想要的镜像名字：</p><p><img src="https://i.loli.net/2020/06/24/7pAFvM8CJQbq6mU.png" srcset="/img/loading.gif" /></p><p>点击“立即构建”：</p><p><img src="https://i.loli.net/2020/06/24/GlMwqDInL7zVOpf.png" srcset="/img/loading.gif" /></p><p>等待一段时间后，如果构建成功，便可以进行拉取了，在镜像仓库的基本信息页面可以看到地址：</p><p><img src="https://i.loli.net/2020/06/24/57jmEv8PYkr2nQG.png" srcset="/img/loading.gif" /></p><p>将阿里云上的镜像拉取到本机之后一般会想要对镜像改名，可以使用<code>docker tag &lt;old_name&gt; &lt;new_name&gt;</code>。</p><h1 id="build-docker-images-for-deep-learning">Build Docker Images for Deep Learning</h1><h2 id="startup">Startup</h2><p>在Docker中配置适用于OpenPAI的深度学习镜像不是一件容易的事，会有很多的坑，这里专门说一下如何配置。推荐在阿里云容器镜像服务中进行构建，会少很多麻烦。</p><p>第一步是初始镜像，由于需要用到CUDA，这里可以根据自己的需求（比如不同CUDA版本支持的GPU驱动版本不一样，还有Tensorflow不同版本对CUDA和cuDNN要求也不一样）从Nvidia的Dockerhub<a href="https://hub.docker.com/r/nvidia/cuda" target="_blank" rel="noopener">官方页面</a>选择合适的CUDA和cuDNN版本：</p><p><img src="https://i.loli.net/2020/06/24/pftdhmHiWkTq8Fj.png" srcset="/img/loading.gif" /></p><p>这里我们选择CUDA10.1 + cuDNN7：</p><pre><code class="hljs angelscript">FROM nvidia/cuda:<span class="hljs-number">10.1</span>-cudnn7-devel-ubuntu18<span class="hljs-number">.04</span>Copy</code></pre><p>这一条主要是解决乱码问题以及定义用到的软件包的版本，这里Miniconda版本设置为4.5.4的原因是这是最后一个自带Python3.6的版本，我在这儿为了稳定所以用了Python3.6，大家也可以安装最新版的Miniconda：</p><pre><code class="hljs routeros">ENV <span class="hljs-attribute">LANG</span>=C.UTF-8 <span class="hljs-attribute">LC_ALL</span>=C.UTF-8ENV <span class="hljs-attribute">HADOOP_VERSION</span>=2.7.2LABEL <span class="hljs-attribute">HADOOP_VERSION</span>=2.7.2ENV <span class="hljs-attribute">MINICONDA_VERSION</span>=4.5.4Copy</code></pre><p>接下来安装必须的包，大家可以根据需求自行调整，<code>-y</code>标签代表Yes，即自动同意安装：</p><pre><code class="hljs livescript">RUN DEBIAN_FRONTEND=noninteractive &amp;&amp; <span class="hljs-string">\</span>    apt-get -y update &amp;&amp; <span class="hljs-string">\</span>    apt-get -y install build-essential <span class="hljs-string">\</span>        wget <span class="hljs-string">\</span>        git <span class="hljs-string">\</span>        curl <span class="hljs-string">\</span>        unzip <span class="hljs-string">\</span>        automake <span class="hljs-string">\</span>        openjdk-<span class="hljs-number">8</span>-jdk <span class="hljs-string">\</span>        openssh-server <span class="hljs-string">\</span>        openssh-client <span class="hljs-string">\</span>        lsof <span class="hljs-string">\</span>        libcupti-dev &amp;&amp; <span class="hljs-string">\</span>    apt-get clean &amp;&amp; <span class="hljs-string">\</span>    rm -rf <span class="hljs-regexp">/var/lib/apt/lists/</span>*Copy</code></pre><p>安装Miniconda并设置环境变量，<code>-b</code>标签可以让Miniconda无交互自动安装：</p><pre><code class="hljs awk">RUN wget --quiet https:<span class="hljs-regexp">//</span>repo.anaconda.com<span class="hljs-regexp">/miniconda/</span>Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh &amp;&amp; <span class="hljs-regexp">/bin/</span>bash Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh -b -p <span class="hljs-regexp">/opt/mi</span>niconda \&amp;&amp; rm Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.shENV PATH <span class="hljs-regexp">/opt/mi</span>niconda<span class="hljs-regexp">/bin:$PATHCopy</span></code></pre><p>安装Hadoop，OpenPAI平台会用到：</p><pre><code class="hljs awk">RUN wget -qO- http:<span class="hljs-regexp">//</span>archive.apache.org<span class="hljs-regexp">/dist/</span>hadoop<span class="hljs-regexp">/common/</span>hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span><span class="hljs-regexp">/hadoop-$&#123;HADOOP_VERSION&#125;.tar.gz | \</span><span class="hljs-regexp">    tar xz -C /u</span>sr<span class="hljs-regexp">/local &amp;&amp; \</span><span class="hljs-regexp">    mv /u</span>sr<span class="hljs-regexp">/local/</span>hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span> <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoopCopy</span></code></pre><p><code>ENV</code>的作用是配置环境变量。配置JAVA和Hadoop环境变量：</p><pre><code class="hljs routeros">ENV <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/java-8-openjdk-amd64 \    <span class="hljs-attribute">HADOOP_INSTALL</span>=/usr/local/hadoop \    <span class="hljs-attribute">NVIDIA_VISIBLE_DEVICES</span>=allENV <span class="hljs-attribute">HADOOP_PREFIX</span>=<span class="hljs-variable">$&#123;HADOOP_INSTALL&#125;</span> \    <span class="hljs-attribute">HADOOP_BIN_DIR</span>=<span class="hljs-variable">$&#123;HADOOP_INSTALL&#125;</span>/bin \    <span class="hljs-attribute">HADOOP_SBIN_DIR</span>=<span class="hljs-variable">$&#123;HADOOP_INSTALL&#125;</span>/sbin \    <span class="hljs-attribute">HADOOP_HDFS_HOME</span>=<span class="hljs-variable">$&#123;HADOOP_INSTALL&#125;</span> \    <span class="hljs-attribute">HADOOP_COMMON_LIB_NATIVE_DIR</span>=<span class="hljs-variable">$&#123;HADOOP_INSTALL&#125;</span>/lib/native \    <span class="hljs-attribute">HADOOP_OPTS</span>=<span class="hljs-string">"-Djava.library.path=<span class="hljs-variable">$&#123;HADOOP_INSTALL&#125;</span>/lib/native"</span>Copy</code></pre><p>设置PATH环境变量：</p><pre><code class="hljs elixir">ENV PATH=<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/nvidia/bin</span><span class="hljs-symbol">:/usr/local/cuda/bin</span><span class="hljs-symbol">:/usr/local/sbin</span><span class="hljs-symbol">:/usr/local/bin</span><span class="hljs-symbol">:/usr/sbin</span><span class="hljs-symbol">:/usr/bin</span><span class="hljs-symbol">:/sbin</span><span class="hljs-symbol">:/bin</span><span class="hljs-symbol">:</span><span class="hljs-variable">$&#123;</span>HADOOP_BIN_DIR&#125;<span class="hljs-symbol">:</span><span class="hljs-variable">$&#123;</span>HADOOP_SBIN_DIR&#125; \LD_LIBRARY_PATH=<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/cuda/extras</span><span class="hljs-regexp">/CUPTI/lib</span><span class="hljs-symbol">:/usr/local/cuda/extras/CUPTI/lib64</span><span class="hljs-symbol">:/usr/local/nvidia/lib</span><span class="hljs-symbol">:/usr/local/nvidia/lib64</span><span class="hljs-symbol">:/usr/local/cuda/lib64</span><span class="hljs-symbol">:/usr/local/cuda/targets/x86_64-linux/lib/stubs</span><span class="hljs-symbol">:</span><span class="hljs-variable">$&#123;</span>JAVA_HOME&#125;/jre/lib/amd64/serverCopy</code></pre><p>完整的Dockerfile如下：</p><pre><code class="hljs crystal">FROM nvidia/<span class="hljs-symbol">cuda:</span><span class="hljs-number">10.1</span>-cudnn7-devel-ubuntu18.<span class="hljs-number">0</span>4ENV LANG=C.UTF-<span class="hljs-number">8</span> LC_ALL=C.UTF-<span class="hljs-number">8</span>ENV HADOOP_VERSION=<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>LABEL HADOOP_VERSION=<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span>ENV MINICONDA_VERSION=<span class="hljs-number">4.5</span>.<span class="hljs-number">4</span>RUN DEBIAN_FRONTEND=noninteractive &amp;&amp; \    apt-get -y update &amp;&amp; \    apt-get -y install build-essential \        wget \        git \        curl \        unzip \        automake \        openjdk-<span class="hljs-number">8</span>-jdk \        openssh-server \        openssh-client \        lsof \        libcupti-dev &amp;&amp; \    apt-get clean &amp;&amp; \    rm -rf /var/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">apt</span>/<span class="hljs-title">lists</span>/*</span>RUN wget --quiet <span class="hljs-symbol">https:</span>/<span class="hljs-regexp">/repo.anaconda.com/miniconda</span><span class="hljs-regexp">/Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh &amp;&amp; /bin</span><span class="hljs-regexp">/bash Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh -b -p /opt</span><span class="hljs-regexp">/miniconda \</span><span class="hljs-regexp">&amp;&amp; rm Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh</span><span class="hljs-regexp">ENV PATH /opt</span><span class="hljs-regexp">/miniconda/bin</span>:$PATH    RUN wget -qO- <span class="hljs-symbol">http:</span>/<span class="hljs-regexp">/archive.apache.org/dist</span><span class="hljs-regexp">/hadoop/common</span><span class="hljs-regexp">/hadoop-$&#123;HADOOP_VERSION&#125;/hadoop</span>-$&#123;HADOOP_VERSION&#125;.tar.gz | \    tar xz -C /usr/local &amp;&amp; \    mv /usr/local/hadoop-$&#123;HADOOP_VERSION&#125; /usr/local/hadoop    ENV JAVA_HOME=<span class="hljs-regexp">/usr/lib</span><span class="hljs-regexp">/jvm/java</span>-<span class="hljs-number">8</span>-openjdk-amd64 \    HADOOP_INSTALL=<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/hadoop \</span><span class="hljs-regexp">    NVIDIA_VISIBLE_DEVICES=all</span><span class="hljs-regexp"></span><span class="hljs-regexp">ENV HADOOP_PREFIX=$&#123;HADOOP_INSTALL&#125; \</span><span class="hljs-regexp">    HADOOP_BIN_DIR=$&#123;HADOOP_INSTALL&#125;/bin</span> \    HADOOP_SBIN_DIR=$&#123;HADOOP_INSTALL&#125;/sbin \    HADOOP_HDFS_HOME=$&#123;HADOOP_INSTALL&#125; \    HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_INSTALL&#125;/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">native</span> \</span>    HADOOP_OPTS=<span class="hljs-string">"-Djava.library.path=$&#123;HADOOP_INSTALL&#125;/lib/native"</span>ENV PATH=<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/nvidia/bin</span>:<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/cuda/bin</span>:<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/sbin:/usr</span><span class="hljs-regexp">/local/bin</span>:<span class="hljs-regexp">/usr/sbin</span>:<span class="hljs-regexp">/usr/bin</span>:<span class="hljs-regexp">/sbin:/bin</span>:$&#123;HADOOP_BIN_DIR&#125;:$&#123;HADOOP_SBIN_DIR&#125;:$PATH \LD_LIBRARY_PATH=<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/cuda/extras</span><span class="hljs-regexp">/CUPTI/lib</span>:<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/cuda/extras</span><span class="hljs-regexp">/CUPTI/lib</span>64:<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/nvidia/lib</span>:<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/nvidia/lib</span>64:<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/cuda/lib</span>64:<span class="hljs-regexp">/usr/local</span><span class="hljs-regexp">/cuda/targets</span><span class="hljs-regexp">/x86_64-linux/lib</span><span class="hljs-regexp">/stubs:$&#123;JAVA_HOME&#125;/jre</span><span class="hljs-regexp">/lib/amd</span>64/serverCopy</code></pre><p>建议先把这一部分进行构建，作为基础镜像，后面要配置其他环境（如安装Pytorch框架登），就不用重复构建这部分，还减少了出错的可能性。这里说一下，启动带CUDA的Docker镜像需要在<code>docker run</code>加上额外的参数<code>--runtime nvidia</code>。</p><p>接下来安装深度学习框架。</p><h2 id="configure-pytorch">Configure PyTorch</h2><p>假设上面的镜像我们命名为xiaoqinfeng/base，那么构建PyTorch的Dockerfile可以像下面这么写：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> xiaoqinfeng/base<span class="hljs-keyword">RUN</span><span class="bash"> pip install -U pip</span><span class="hljs-keyword">RUN</span><span class="bash"> pip install numpy scipy pandas matplotlib tqdm</span><span class="hljs-keyword">RUN</span><span class="bash"> pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.htmlCopy</span></code></pre><p>因为这里我用的CUDA10.1，其他版本的CUDA安装指令可能不太一样，具体可以参考<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">官网</a>。</p><h2 id="configure-tensorflow">Configure Tensorflow</h2><p>如果是安装Tensorflow，那么构建Tensorflow的Dockerfile可以像下面这么写：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> xiaoqinfeng/base<span class="hljs-keyword">RUN</span><span class="bash"> pip install -U pip</span><span class="hljs-keyword">RUN</span><span class="bash"> pip install numpy scipy pandas matplotlib tqdm tensorflow-gpuCopy</span></code></pre><p>这里会自动安装最新版本的Tensorflow2。Tensorflow不同版本对CUDA和cuDNN版本甚至Python版本的支持都不太一样，可以参考<a href="https://www.tensorflow.org/install/source#linux" target="_blank" rel="noopener">官网</a>的说明。</p><h1 id="deep-learning-with-openpai">Deep Learning with OpenPAI</h1><h2 id="what-is-openpai">What is OpenPAI</h2><p>OpenPAI是一个分布式深度学习计算资源管理平台，对于我们用户来说，只需要定义好Docker镜像，然后编写好任务设置，提交到平台之后，平台便会自动分配计算资源来运行任务。</p><p>OpenPAI界面：</p><p><img src="https://i.loli.net/2020/06/24/CDrZjgYR7lNcieh.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p><img src="https://i.loli.net/2020/06/24/FGJpVCbls7YP4aM.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>下面我们来讲讲怎么向OpenPAI平台提交任务。</p><h2 id="submit-jobs-to-openpai">Submit Jobs to OpenPAI</h2><h3 id="pack-code-data-files">Pack Code &amp; Data Files</h3><p>假设你已经完成了代码的编写和测试，你的目录结构可能看起来是这样：</p><pre><code class="hljs css">.├── <span class="hljs-selector-tag">README</span><span class="hljs-selector-class">.md</span>├── <span class="hljs-selector-tag">data</span>│   └── <span class="hljs-selector-tag">dataset</span><span class="hljs-selector-class">.csv</span>├── <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.py</span>└── <span class="hljs-selector-tag">src</span>    ├── <span class="hljs-selector-tag">data</span><span class="hljs-selector-class">.py</span>    └── <span class="hljs-selector-tag">net</span><span class="hljs-selector-class">.pyCopy</span></code></pre><p>因为OpenPAI会创建一个虚拟容器来运行你的代码，所以你的数据和代码必须要以某种方式传送到OpenPAI上的虚拟容器中。我们先来打包，在代码目录下执行<code>tar -cvf files.tar ./</code>。之后，运行<code>python -m http.server &lt;port&gt;</code>。打开浏览器输入<code>&lt;server_ip&gt;:&lt;port&gt;</code>应该就能看到你的文件了：</p><p><img src="https://i.loli.net/2020/06/24/Cbgiw3XKnGYsNHp.png" srcset="/img/loading.gif" /></p><p>由于这个http进程需要一直运行，所以建议使用<code>screen</code>放到后台执行。</p><h3 id="configure-tasks">Configure Tasks</h3><p>像OpenPAI提交任务可以采用网页提交也可以使用VSCode插件，这里我们采用网页提交。登入OpenPAI界面，点击Submit Job：</p><p><img src="https://i.loli.net/2020/06/24/bhXAcJfOmo8RtT2.png" srcset="/img/loading.gif" /></p><p>可以看到提交任务的界面：</p><p><img src="https://i.loli.net/2020/06/24/KME8GuN49gyY1ph.png" srcset="/img/loading.gif" /></p><p>Job name大家可以自己设置。在Command一栏，是执行任务所需的全部命令，首先我们要做的就是将代码数据压缩包下载到容器中并解压：</p><pre><code class="hljs elixir">wget &lt;server_ip&gt;<span class="hljs-symbol">:&lt;port&gt;/files</span>.tartar -xvf files.tarCopy</code></pre><p>然后是运行代码，假设我这里的任务比较简单，只有一行main.py的调用：</p><pre><code class="hljs css"><span class="hljs-selector-tag">python</span> <span class="hljs-selector-tag">main</span><span class="hljs-selector-class">.pyCopy</span></code></pre><p>如果任务的执行比较复杂，也只需把命令填到Command里即可，OpenPAI会自动执行。接下来是设置配置，可以选GPU的数量，内存大小等等：</p><p><img src="https://i.loli.net/2020/06/24/T7ryOxjpJP4FWYR.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>然后是镜像的选择：</p><p><img src="https://i.loli.net/2020/06/24/3n7SpNlsLjBRvgE.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>要注意在本机上构建好镜像之后，需要把镜像重命名为<code>&lt;repository_address&gt;/&lt;image_name&gt;</code>的格式（我们的<code>&lt;repository&gt;</code>是<code>lin-ai-27:5000</code>，假设我的镜像名是<code>xiaoqinfeng/pytorch</code>，那就是改成<code>lin-ai-27:5000/xiaoqinfeng/pytorch</code>），然后执行<code>docker push</code>推送到Docker镜像服务器上才能在OpenPAI上使用。</p><p>提交之后，可以在Jobs界面看到任务的运行情况：</p><p><img src="https://i.loli.net/2020/06/24/ZD4cUgOCIqnyHdB.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><h1 id="misc">Misc</h1><h2 id="store-files-in-containers">Store Files in Containers</h2><p>我们往往需要在程序运行的时候保存文件，如checkpoints等。在OpenPAI上执行程序的话文件是保存在程序中的，如果我们想要在运行完之后把文件复制到本地电脑上呢？这个时候就需要在任务的配置文件里加上复制文件到HDFS的语句。首先确认你的HDFS的URL：如<code>hdfs://172.31.246.52:9000/你的OpenPAI用户名/</code>。</p><p>如果要创建文件夹，则可以使用<code>hdfs dfs -mkdir -p &lt;HDFS URL&gt;+&lt;New Folder&gt;</code>。这里<code>&lt;New Folder&gt;</code>是你要创建的的文件夹的路径，用起来和Linux的<code>mkdir</code>命令其实是差不多的。</p><p>要复制文件（夹）则使用<code>hdfs dfs -cp &lt;Source Dir&gt; &lt;Dest Dir&gt;</code>。</p>]]></content>
    
    
    <categories>
      
      <category>Technical Notes</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Tensorflow</tag>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu20.04LTS 深度学习环境配置 CUDA10.2 + cuDNN7.6.5 + Tensorflow + Pytorch</title>
    <link href="/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA10-2-cuDNN7-6-5-Tensorflow-Pytorch/"/>
    <url>/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA10-2-cuDNN7-6-5-Tensorflow-Pytorch/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>Ubuntu的最新LTS版本也更新到了20.04，在给新机器配置深度学习环境的时候发现比以前容易了许多，特此写一篇Tutorial。这里的安装方法只针对Ubuntu20.04LTS，对于其他版本的系统可能不太适用。</p><h1 id="install-gpu-drivers">Install GPU Drivers</h1><p>这里假设安装系统之后已经做好了必要的配置（安装常用软件依赖、修改国内源等）。Ubuntu20.04中GPU驱动可以直接通过GUI界面安装，十分方便，方法是找到软件与更新 (Software &amp; Updates)，在附加驱动 (additional drivers) 选项卡中选择驱动版本，一般是选择“专有，tested” (proprietary, tested) 那个，之后点Apply Changes，重启。</p><p><img src="https://i.loli.net/2020/06/24/xevtVosYOH7D68I.png" srcset="/img/loading.gif" /></p><h1 id="install-cuda-cudnn">Install CUDA &amp; cuDNN</h1><p>这里选择apt-get的方式安装CUDA：</p><pre><code class="hljs bash">sudo apt-get install nvidia-cuda-toolkit</code></pre><p>输入<code>nvcc --version</code>可以测试是否安装成功，输入<code>nvidia-smi</code>可以看到GPU信息和CUDA版本。</p><p>之后安装cuDNN，进入<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">官网</a>，选择Download cuDNN：</p><p><img src="https://i.loli.net/2020/06/24/VYBRoINFDC9ObA1.png" srcset="/img/loading.gif" /></p><p>会要求登录，如果没有账号的注册一个即可。在这里根据CUDA版本选择适合的cuDNN，我这里是CUDA10.2。我们选择deb包的方式安装，下载下图中圈出来的三个deb包，依次用<code>sudo dpkg -i xxx.deb</code>命令安装。</p><p><img src="https://i.loli.net/2020/06/24/qcDG5t3JY6vfoQM.png" srcset="/img/loading.gif" /></p><h1 id="configure-python">Configure Python</h1><p>为了更好地管理Python包和虚拟环境，我们需要安装Anaconda。使用Anaconda之后，我们可以创建虚拟环境，虚拟环境之间互不干扰。做科学实验我们一般需要安装大量的Python包，有的包之间甚至还有冲突，如果我们把他们都安装在同一个环境下就会难以管理，甚至出冲突。而有了虚拟环境之后，我们可以把不同需求放在不同虚拟环境中，比如深度学习开发放在一个虚拟环境中（安装Tensorflow等），网站开发放在一个虚拟环境中（安装Flask等）。Anaconda默认自带大量的包，不过我们一般会创建新的虚拟环境去安装新的包，所以这里我们选用Miniconda。Miniconda和Anaconda唯一的区别是不会自带大量Python包，这里大家自行选择。Anaconda国内镜像下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/，Miniconda国内镜像下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/。</p><p>比如说我们下载的是<code>Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>，执行<code>bash Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>即可安装。显示一大屏用户协议哪儿按<code>q</code>可以直接跳过，其他选项的默认的输入<code>yes</code>即可。在提示是否需要conda init的时候记得输入<code>yes</code>。</p><p>安装成功之后，重开一个终端，可以看到现在处于<code>base</code>环境中：</p><p><img src="https://i.loli.net/2020/06/24/giml1UKzWuyqTjr.png" srcset="/img/loading.gif" /></p><p>我们先配置一下国内镜像，执行</p><pre><code class="hljs bash">vim ~/.condarc</code></pre><p>然后粘贴下列文本使用清华源：</p><pre><code class="hljs less"><span class="hljs-attribute">channels</span>:  - defaults<span class="hljs-attribute">show_channel_urls</span>: true<span class="hljs-attribute">channel_alias</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda</span><span class="hljs-attribute">default_channels</span>:  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><span class="hljs-attribute">custom_channels</span>:  <span class="hljs-attribute">conda-forge</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">msys2</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">bioconda</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">menpo</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">pytorch</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">simpleitk</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></code></pre><p>之后我们输入<code>conda create -n &lt;your_name&gt;</code>创建一个新的虚拟环境，如果需要指定Python版本，则<code>conda create --n &lt;your_name&gt; python=&lt;python_version&gt;</code>。之后输入<code>conda activate &lt;your_name&gt;</code>进入虚拟环境，如果需要退出，则使用<code>conda deactivate</code>。</p><p>安装常用包：</p><pre><code class="hljs bash">conda install --yes numpy scipy pandas matplotlib tqdm pip jupyter</code></pre><p><code>--yes</code>的作用是手动输入<code>y</code>来确认是否安装，这里列出的是一些最常用的Python包，大家可以根据自己的需求自行调整。<code>conda install</code>为Anaconda中安装Python包的方式。</p><h1 id="install-pytorch">Install PyTorch</h1><p>这里来安装PyTorch环境，推荐使用<code>conda create -n pytorch</code>创建一个专有虚拟环境，然后使用<code>conda install</code>安装常用包。对于安装PyTorch，我们可以使用<code>conda</code>也可以使用<code>pip</code>安装。<code>pip</code>是另外一个安装Python包的工具，由于不检查依赖所以比<code>conda</code>安装速度快，而且包的数量比<code>conda</code>多，使用也更广泛。同样<code>pip</code>也可以使用国内镜像加速下载，详见https://mirrors.tuna.tsinghua.edu.cn/help/pypi/。</p><p>对于CUDA10.2，官方给出的用<code>conda</code>安装PyTorch的命令是：</p><pre><code class="hljs bash">conda install pytorch torchvision cudatoolkit=10.2 -c pytorch</code></pre><p>用<code>pip</code>安装PyTorch的命令是：</p><pre><code class="hljs bash">pip install torch torchvision</code></pre><p>对于其他版本的CUDA安装命令可能不一样，可以去<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">官网</a>查看。</p><p>可以使用以下命令来测试GPU版本的PyTorch是否正常工作：</p><pre><code class="hljs bash">python -c <span class="hljs-string">"import torch; print(torch.cuda.is_available())"</span></code></pre><h1 id="install-tensorflow">Install Tensorflow</h1><p>安装Tensorflow环境同样推荐创建一个专有虚拟环境。对于Tensorflow2的安装，使用<code>pip</code>十分方便，使用</p><pre><code class="hljs bash">pip install tensorflow tensorflow-gpu</code></pre><p>即可。要安装其他版本的Tensorflow可以使用<code>pip install tensorflow==&lt;tf_version&gt; tensorflow-gpu==&lt;tf_version&gt;</code>来指定版本。不过不同版本的Tensorflow要求的CUDA版本都有所不同，可以参考<a href="https://www.tensorflow.org/install/source#linux" target="_blank" rel="noopener">官网</a>的说明。</p><p>可以使用以下命令来测试GPU版本的Tensorflow是否正常工作：</p><pre><code class="hljs bash">python -c <span class="hljs-string">"import tensorflow as tf; tf.config.list_physical_devices('GPU')"</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Technical Notes</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Tensorflow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Anomaly Detection Using Geometric Transformations</title>
    <link href="/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/"/>
    <url>/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文考虑图像数据的异常检测问题。与基于重构的方法不同，本文提出的方法通过对正常图片施加不同的几何变换之后，训练一个多分类器将无监督异常检测问题转化为一个有监督问题。本方法背后的直觉是在训练能够分辨不同变换后的图片之后，分类器一定学得了一些显著的几何特征，这些几何特征是正常类别独有的。</p><h1 id="proposed-method">Proposed Method</h1><h2 id="problem-statement">Problem Statement</h2><p>本文考虑针对图像的异常检测。记<span class="math inline">\(\mathcal X\)</span>为所有自然图像的空间，<span class="math inline">\(X\subseteq\mathcal X\)</span>为正常图像集合。给定数据集<span class="math inline">\(S\subseteq X\)</span>，异常检测的目的是学习一个分类器<span class="math inline">\(h_S(x):\mathcal X\rightarrow\{0,1\}\)</span>，其中<span class="math inline">\(h_S(x)=1\Leftrightarrow x\in X\)</span>。</p><p>为了兼顾查准率和查全率，常用的设置是学习一个打分函数<span class="math inline">\(n_S(x):\mathcal X\rightarrow\mathbb R\)</span>，分数越高代表样本属于<span class="math inline">\(X\)</span>的概率越大。之后，通过设定阈值，便可以构建异常分类器： <span class="math display">\[\begin{align}h_S^\lambda(x)=\begin{cases}1 &amp; n_S(x)\leq\lambda\\0 &amp; n_S(x)&lt;\lambda\end{cases}\end{align}\]</span></p><h2 id="discriminative-learning-of-an-anomaly-scoring-function-using-geometric-transformations">Discriminative Learning of an Anomaly Scoring Function Using Geometric Transformations</h2><p>有初始数据集<span class="math inline">\(S\)</span>，几何变换集合<span class="math inline">\(\mathcal T\)</span>，通过对<span class="math inline">\(S\)</span>中每个样本施加这<span class="math inline">\(|\mathcal T|\)</span>个几何变换得到新数据集记为<span class="math inline">\(S_\mathcal{T}\)</span>，且<span class="math inline">\(S_\mathcal{T}\)</span>中每个样本的标签为变换的序号。之后，在<span class="math inline">\(S_\mathcal{T}\)</span>上训练一个<span class="math inline">\(|\mathcal T|\)</span>分类器。在测试阶段，对测试样本同样施加<span class="math inline">\(|\mathcal T|\)</span>个几何变换，分类器会给出经过<span class="math inline">\(\mathrm{softmax}\)</span>的输出向量，最终的异常分数由经过输出的向量构造的分布对数似然得来。</p><h3 id="creating-and-learning-the-self-labeled-dataset">Creating and Learning the Self-Labeled Dataset</h3><p>设<span class="math inline">\(\mathcal T=\{T_0,T_1,\cdots,T_{k-1}\}\)</span>为几何变换集合，<span class="math inline">\(1\leq i\leq k-1,\space T_i:\mathcal X\rightarrow \mathcal X\)</span>，且<span class="math inline">\(T_0(x)=x\)</span>。<span class="math inline">\(S_\mathcal{T}\)</span>定义为：</p><p><span class="math display">\[S_\mathcal T=\{(T_j(x),j):x\in S,T_j\in\mathcal T\}\]</span> 对于每个<span class="math inline">\(x\in S\)</span>，<span class="math inline">\(j\)</span>为<span class="math inline">\(T_j(x)\)</span>的标签。我们直接学习一个<span class="math inline">\(K\)</span>类分类器<span class="math inline">\(f_\theta\)</span>，来预测输入样本对应的几何变换种类，这相当于是一个图像分类问题。</p><p><img src="https://i.loli.net/2020/06/24/XBFKcPio64u3U1C.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="dirichlet-normality-score">Dirichlet Normality Score</h3><p>接下来要做的是如何定义异常分数，记为<span class="math inline">\(n_S(x)\)</span>，这是文中的一个重要的部分。设几何变换集合<span class="math inline">\(\mathcal T=\{T_0,T_1,\cdots,T_{k-1}\}\)</span>，且<span class="math inline">\(k\)</span>分类器<span class="math inline">\(f_\theta\)</span>在<span class="math inline">\(S_\mathcal{T}\)</span>上完成训练。对于任意一个样本<span class="math inline">\(x\)</span>，令<span class="math inline">\(\mathbf y(x)=\text{softmax}(f_{\theta}(x))\)</span>，即分类器<span class="math inline">\(f_\theta\)</span>输出的<span class="math inline">\(\text{softmax}\)</span>之后的向量。异常分数<span class="math inline">\(n_S(x)\)</span>定义为：</p><p><span class="math display">\[n_S(x)=\sum\limits_{i=0}^{k-1}\log p(\mathbf y(T_i(x))|T_i)\]</span></p><p>该异常分数定义为每个类别上，在几何变换<span class="math inline">\(T_i\)</span>的条件下，输出的<span class="math inline">\(\mathbf y\)</span>的对数似然之和。在文中，作者假设<span class="math inline">\(\mathbf y(T_i(x)|T_i\)</span>服从迪利克雷分布：<span class="math inline">\(\mathbf y(T_i(x))|T_i\sim\text{Dir}(\boldsymbol \alpha_i)\)</span>，其中<span class="math inline">\(\boldsymbol \alpha_i\in\mathbb R^k_+\)</span>，<span class="math inline">\(x\sim p_X(x)\)</span>，<span class="math inline">\(i\sim\text{Uni}(0,k-1)\)</span>，而<span class="math inline">\(p_X(x)\)</span>代表正常样本的真实数据分布。于是：</p><p><span class="math display">\[n_S(x)=\sum_{i=0}^{k-1}\left[\log\Gamma(\sum_{j=0}^{k-1}[\tilde{\boldsymbol\alpha}_i]_j)-\sum_{j=0}^{k-1}\log\Gamma([\tilde{\boldsymbol\alpha}_i]_j)+\sum_{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]_j-1)\log\mathbf y(T_i(x))_j\right]\]</span></p><p>因为<span class="math inline">\(\tilde{\alpha}_i\)</span>相对于<span class="math inline">\(x\)</span>来说是常数，所以可以直接忽略，于是式子简化为： <span class="math display">\[n_S(x)=\sum_{i=0}^{k-1}\sum_{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]_j-1)\log\mathbf y(T_i(x))_j=\sum_{i=0}^{k-1}(\tilde{\boldsymbol \alpha}_i-1)\cdot\log\mathbf y(T_i(x))\]</span></p><p>注意这里的每个<span class="math inline">\(\boldsymbol \alpha_i\)</span>都是一个向量，即对于每个变换<span class="math inline">\(i\)</span>，都对应一个迪利克雷分布，其参数为<span class="math inline">\(\boldsymbol\alpha_i\)</span>；在对训练集进行第<span class="math inline">\(i\)</span>个几何变换之后，我们得到了<span class="math inline">\(\{T_i(x)\}\)</span>，然后分类器<span class="math inline">\(f_\theta(\cdot)\)</span>的输出<span class="math inline">\(\mathbf y(T_i(x))\)</span>相当于迪利克雷分布的观测值，我们需要根据观测值来估计参数<span class="math inline">\(\boldsymbol \alpha_i\)</span>，然后根据这个参数来计算<span class="math inline">\(n_S(x)\)</span>。对于<span class="math inline">\(\boldsymbol\alpha_i\)</span>，可以知道其第<span class="math inline">\(i\)</span>个分量应该是相对比较大的，下面是运行官方代码得到的<span class="math inline">\(\boldsymbol\alpha_i\)</span>的结果（<span class="math inline">\(i=69\)</span>，<span class="math inline">\(i\)</span>从<span class="math inline">\(0\)</span>开始，总共为<span class="math inline">\(72\)</span>维），可以看到第<span class="math inline">\(69\)</span>个分量是最大的。</p><pre><code class="hljs bash">[INFO] value of mle_alpha_t: [ 0.10228925  0.08997199  0.13083569  0.10862965  0.09811163  0.08527119  0.17637901  0.27628416  0.12873376  0.19197053  0.11587154  0.09873095  0.12700618  0.07688542  0.10488203  0.12499191  0.11637607  0.07739511  0.13049147  0.51031647  0.20546597  0.15558449  0.09288609  0.12134945  0.09324992  0.14650162  0.16281216  0.11827823  0.08214853  0.15618336  0.28129761  0.45293697  0.11485838  1.78598954  0.16556983  0.1141158  0.10909459  0.13916602  0.11563799  0.07309986  0.11049714  0.12974086  0.15930642  0.13714361  0.13938356  0.70619553  0.11174039  0.07201538  0.16626109  0.12153727  0.09548811  0.07940956  0.15832209  0.11035474  0.12487912  0.16937875  0.23212662  0.37041831  0.08557451  0.0839439  0.09924258  0.39766872  0.14917286  0.08704662  0.09554555  0.31047109  0.24504759  0.16812463  0.11508187 63.98878807  0.12971073  0.07972932]</code></pre><p>下图也展示了对于每个变换<span class="math inline">\(i\)</span>，<span class="math inline">\(\mathbf y(T_i(x)|T_i\)</span>分布的情况：</p><p><img src="https://i.loli.net/2020/06/23/fLkst4i7Hu6PhQl.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>作者还给出了一种简化的形式，<span class="math inline">\(\hat{n}_S(x)=\frac{1}{k}\sum^{k-1}_{j=0}[\mathbf y(T_j(x))]_j\)</span>。相当于说，对于每个变换<span class="math inline">\(T_i\)</span>分类器都会给出一个<span class="math inline">\(\text{softmax}\)</span>向量，取其第<span class="math inline">\(i\)</span>个分量<span class="math inline">\([\mathbf y(T_j(x))]_j\)</span>，然后把每个变换对应的<span class="math inline">\([\mathbf y(T_j(x))]_j\)</span>加起来。</p><p>整个算法的流程如下：</p><p><img src="https://i.loli.net/2020/06/23/z8MpdeoD6ZGavlN.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>这里结合作者的源代码简单说一下检测阶段的流程。</p><hr /><pre><code class="hljs python"><span class="hljs-keyword">for</span> t_ind <span class="hljs-keyword">in</span> range(transformer.n_transforms):        observed_dirichlet = mdl.predict(transformer.transform_batch(observed_data, [t_ind] * len(observed_data)), batch_size=<span class="hljs-number">1024</span>)</code></pre><p>在训练好模型之后，对于训练集的所有样本，对其进行<span class="math inline">\(K\)</span>个几何变换之后，得到<span class="math inline">\(K\)</span>个样本<span class="math inline">\(\{T_i(x)\}\)</span>，对于所有第<span class="math inline">\(i\)</span>个几何变换对应的样本<span class="math inline">\(\{T_i(x)\}\)</span>，通过分类器<span class="math inline">\(f_\theta\)</span>会给出输出<span class="math inline">\(\mathbf y(T_i(x))\)</span>。这里对应算法中的第<span class="math inline">\(7-8\)</span>行，这个<code>observed_dirichlet</code>就是<span class="math inline">\(S_i\)</span>。</p><hr /><pre><code class="hljs python">log_p_hat_train = np.log(observed_dirichlet).mean(axis=<span class="hljs-number">0</span>)alpha_sum_approx = calc_approx_alpha_sum(observed_dirichlet)alpha_0 = observed_dirichlet.mean(axis=<span class="hljs-number">0</span>) * alpha_sum_approx</code></pre><p>之后这部分主要对应算法中的<span class="math inline">\(9-11\)</span>行。作者把所有的第<span class="math inline">\(i\)</span>个变换，分类器的输出的集合（也就是变量<code>observed_dirichlet</code>）记为<span class="math inline">\(S_i\)</span>，<span class="math inline">\(\bar s\)</span>为<span class="math inline">\(S_i\)</span>的平均，<span class="math inline">\(\bar l\)</span>为<span class="math inline">\(S_i\)</span>对数的平均（变量<code>log_p_hat_train</code>），初始值<span class="math inline">\(\tilde{\alpha}_i\)</span>由<span class="math inline">\(\bar s\frac{(k-1)(-\Psi(1))}{\bar s\cdot\log\bar s-\bar s\cdot\bar l}\)</span>给出（变量<code>alpha_0</code>）。函数<code>calc_approx_alpha_sum</code>实现的是算法中第<span class="math inline">\(11\)</span>行的<span class="math inline">\(\frac{(k-1)(-\Psi(1))}{\bar s\cdot\log\bar s-\bar s\cdot\bar l}\)</span>，代码如下：</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_approx_alpha_sum</span><span class="hljs-params">(observations)</span>:</span>    N = len(observations)    f = np.mean(observations, axis=<span class="hljs-number">0</span>)    <span class="hljs-keyword">return</span> (N * (len(f) - <span class="hljs-number">1</span>) * (-psi(<span class="hljs-number">1</span>))) / (        N * np.sum(f * np.log(f)) - np.sum(f * np.sum(np.log(observations), axis=<span class="hljs-number">0</span>)))</code></pre><hr /><pre><code class="hljs python">mle_alpha_t = fixed_point_dirichlet_mle(alpha_0, log_p_hat_train)</code></pre><p>这里对应算法中的<span class="math inline">\(12-14\)</span>行，即重复<span class="math inline">\(\tilde\alpha_i\leftarrow\Psi^{-1}\left(\Psi(\sum_j[\alpha_i]_j)+\bar l\right)\)</span>来估计<span class="math inline">\(\alpha\)</span>，函数<code>fixed_point_dirichlet_mle</code>代码如下：</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fixed_point_dirichlet_mle</span><span class="hljs-params">(alpha_init, log_p_hat, max_iter=<span class="hljs-number">1000</span>)</span>:</span>    alpha_new = alpha_old = alpha_init    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(max_iter):        alpha_new = inv_psi(psi(np.sum(alpha_old)) + log_p_hat)        <span class="hljs-keyword">if</span> np.sqrt(np.sum((alpha_old - alpha_new) ** <span class="hljs-number">2</span>)) &lt; <span class="hljs-number">1e-9</span>:            <span class="hljs-keyword">break</span>        alpha_old = alpha_new    <span class="hljs-keyword">return</span> alpha_new</code></pre><p><span class="math inline">\(\Psi^{-1}(\cdot)\)</span>是通过数值方法来估计的：</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">inv_psi</span><span class="hljs-params">(y, iters=<span class="hljs-number">5</span>)</span>:</span>    <span class="hljs-comment"># initial estimate</span>    cond = y &gt;= <span class="hljs-number">-2.22</span>    x = cond * (np.exp(y) + <span class="hljs-number">0.5</span>) + (<span class="hljs-number">1</span> - cond) * <span class="hljs-number">-1</span> / (y - psi(<span class="hljs-number">1</span>))    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(iters):        x = x - (psi(x) - y) / polygamma(<span class="hljs-number">1</span>, x)    <span class="hljs-keyword">return</span> x</code></pre><hr /><p>最后，在得到对<span class="math inline">\(\alpha\)</span>的估计之后，可以来计算测试样本的分数了。这里对应的是算法中的第<span class="math inline">\(16\)</span>行。</p><pre><code class="hljs python">x_test_p = mdl.predict(transformer.transform_batch(x_test, [t_ind] * len(x_test)), batch_size=<span class="hljs-number">1024</span>)scores += dirichlet_normality_score(mle_alpha_t, x_test_p)</code></pre><p>函数<code>dirichlet_normality_score</code>代码如下：</p><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dirichlet_normality_score</span><span class="hljs-params">(alpha, p)</span>:</span>    <span class="hljs-keyword">return</span> np.sum((alpha - <span class="hljs-number">1</span>) * np.log(p), axis=<span class="hljs-number">-1</span>)</code></pre><h1 id="experiments">Experiments</h1><h2 id="baselines">Baselines</h2><p>文中用到了如下的Baseline：</p><ul><li><strong>One-class SVM. </strong>单类支持向量机，作者使用了三个变体，分别为<strong>RAW-OC-SVM</strong>——使用原始数据作为输入，<strong>CAE-OC-SVM</strong>——使用一个卷积自编码器来获得低维表示作为输入和<strong>E2E-OC-SVM</strong>——全名为<strong>One-Class Deep Support Vector Data Description</strong>；</li><li><strong>Deep structured energy-based models. </strong></li><li><strong>Deep Autoencoding Gaussian Mixture Model. </strong></li><li><strong>Generative Adversarial Networks. </strong></li></ul><h2 id="datasets">Datasets</h2><p>文中用到了一下几个数据集：</p><ul><li><strong>CIFAR-10</strong></li><li><strong>CIFAR-100</strong></li><li><strong>Fashion-MNIST</strong></li><li><strong>CatsVsDogs</strong></li></ul><p>在实验中所有图片都被归一化到<span class="math inline">\([-1,1]\)</span>的范围。</p><h2 id="experimental-protocol">Experimental Protocol</h2><p>设数据集有<span class="math inline">\(C\)</span>个类，我们会进行<span class="math inline">\(C\)</span>次实验，在第<span class="math inline">\(c\)</span>次实验 (<span class="math inline">\(1\leq c \leq C\)</span>)中我们会将第<span class="math inline">\(c\)</span>个类作为正常样本，而其他类作为异常样本。在训练阶段，训练集只包含正常样本，而在测试阶段则会有正常样本和异常样本。在获得异常分数之后，阈值<span class="math inline">\(\lambda\)</span>则根据ROC曲线下面积选择。</p><p>实验中使用的几何变换基于以下三种基变换：</p><ul><li><strong>Horizontal flip: </strong> 记为<span class="math inline">\(T_b^{flip}(x)\)</span>，<span class="math inline">\(b\in\{T,F\}\)</span>代表是否翻转；</li><li><strong>Translation: </strong> 记为<span class="math inline">\(T_{s_h,s_w}^{trans}(x)\)</span>，其中<span class="math inline">\(s_h,s_w\in\{-1,0,1\}\)</span>。在长宽两个维度上位移分别为<span class="math inline">\(0.25\)</span>高度和<span class="math inline">\(0.25\)</span>宽度，这两个维度发生位移的方向由<span class="math inline">\(s_h\)</span>和<span class="math inline">\(s_w\)</span>决定，当<span class="math inline">\(s_h=s_w=0\)</span>时代表不移动；</li><li><strong>Rotation by multiples 90 degrees: </strong> 记为<span class="math inline">\(T_k^{rot}(x)\)</span>，<span class="math inline">\(k\in\{0,1,2,3\}\)</span>。旋转<span class="math inline">\(k\times90\)</span>度。</li></ul><p>将三种基变换叠加有： <span class="math display">\[\mathcal T=\left\{ T_k^{rot}\circ T_{s_h,s_w}^{trans}\circ T_b^{flip} : \begin{matrix} b &amp;\in \{T,F\}\\ s_h,s_w&amp;\in\{-1,0,1\}\\ k&amp;\in\{0,1,2,3\} \end{matrix} \right\}\]</span> 最终几何变换种数为<span class="math inline">\(2\times3\times3\times4=72\)</span>种。</p><p>分类器模型使用的是<strong>Wide Residual Network</strong>，优化器为Adam，Batch size为128，训练轮数为200。</p><h2 id="results">Results</h2><p>下面是不同方法在不同数据集上的实验结果：</p><p><img src="https://i.loli.net/2020/06/24/gHZohrvz9MGYmxi.png" srcset="/img/loading.gif" /></p><p>评测标准使用的是AUROC。作者关于结果的分析主要有以下几点：</p><ol type="1"><li>在绝大多数情况下，我们的算法都比Baseline要好，而且是越大的数据集效果越好。CatsVsDogs数据集每张图片的大小比其他几个数据集都要大，而Baseline在这个数据集上的结果都在<span class="math inline">\(50\%\)</span>或不到<span class="math inline">\(50\%\)</span>，这基本等同于瞎猜；</li><li>在CIFAR-100数据集里，由于将这100类聚合为了20类，所以存在类内样本差异大的问题。比如在类<span class="math inline">\(5\)</span>、类<span class="math inline">\(7\)</span>和类<span class="math inline">\(13\)</span>上，模型表现就不够好；</li><li>在Fashion-MNIST数据集上几乎所有方法（除了DAGMM）都表现很好。</li></ol><h2 id="on-the-intuition-for-using-geometric-transformations">On the Intuition for Using Geometric Transformations</h2><p>这里作者对所选用的几何变换做了一些解释。实验中选用的三种基本几何变换都是可逆的线性几何变换（且为双射），作者也试过一些复杂的非线性变换，如高斯模糊、锐化、伽马校正等等，但是效果并不好。</p><p>作者认为分类器能够分辨不同变换的能力与最终性能成正比，为了验证这一点，进行了<span class="math inline">\(3\)</span>个实验。从MNIST数据集选择一个数字作为正常样本，几何变换只采用两个，然后选择另一个数字作为异常样本，结果如下：</p><ul><li><strong>Normal digit: 8，Anomaly: 3，Transformations: Identity and horizontal flip. </strong>由于数字<span class="math inline">\(8\)</span>是对称的，所以要让分类器分辨原始的<span class="math inline">\(8\)</span>和翻转之后的<span class="math inline">\(8\)</span>是很难的，AUROC只有<span class="math inline">\(0.646\)</span>；</li><li><strong>Normal digit: 3，Anomaly: 8，Transformations: Identity and horizontal flip. </strong>这里把<span class="math inline">\(3\)</span>作为正常样本，由于<span class="math inline">\(3\)</span>不是对称的，所以两种变换是可以分辨的，AUROC达到了<span class="math inline">\(0.957\)</span>；</li><li><strong>Normal digit: 8，Anomaly: 3，Transformations: Identity and translation by 7 pixels. </strong>同样是把<span class="math inline">\(8\)</span>作为正常样本，但变换用的是平移，AUROC达到了<span class="math inline">\(0.919\)</span>。</li></ul><p>除此之外，作者还设计了一个实验，目的是测试什么样的图像会获得较高的分数<span class="math inline">\(n_S(x)\)</span>。在给定训练好的分类器的情况下，优化输入的图像，目标函数是最大化分数<span class="math inline">\(n_S(x)\)</span>。下图为实验结果：</p><p><img src="https://i.loli.net/2020/06/24/8Pi4EKCRuBXrYgp.png" srcset="/img/loading.gif" /></p><p>在左图中，将数字<span class="math inline">\(3\)</span>作为正常样本训练的分类器、原始输入为数字<span class="math inline">\(0\)</span>的图片时，随着优化的进行，图片慢慢地变得像数字<span class="math inline">\(3\)</span>。在右图中，同样是将数字<span class="math inline">\(3\)</span>作为正常样本训练的分类器，不过原始输入也是数字<span class="math inline">\(3\)</span>，这时图像却没有怎么变化。</p><h1 id="remark">Remark</h1><ul><li>文中提到的在CIFAR100数据的实验上，由于类间差异比较大导致效果较差，那么很自然地，不同的变换样本对应的集簇实际上应当足够分开，集簇内的样本要足够进，这样对于分类器来说才能比较好的分类。不过采用的几何变换并没有针对这一点进行特别设计；</li><li>文中强调了所使用的变换为几何变换，其实除此之外，所使用的变换还都是可以用矩阵表示的可逆的变换。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cross-dataset Time Series Anomaly Detection for Cloud Systems</title>
    <link href="/2020/06/01/Cross-dataset-Time-Series-Anomaly-Detection-for-Cloud-Systems/"/>
    <url>/2020/06/01/Cross-dataset-Time-Series-Anomaly-Detection-for-Cloud-Systems/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文介绍了一种用于云计算平台的时间序列异常检测框架。为了解决标签不足的问题，文中使用了迁移学习的方法，即在有标签的source domain上训练模型，在没有标签的target domain上检测。同时，文中还使用了主动学习的方法来挑选最有价值的无标签样本进行标记。</p><p><a href="https://www.usenix.org/system/files/atc19-zhang-xu.pdf" target="_blank" rel="noopener">📰Get Paper</a></p><h1 id="background">Background</h1><p>针对云计算平台数据的异常检测通常是应用在云监控数据，如KPI、CPU使用率、系统负载等时序数据上。和传统的异常检测不一样的是，时序异常检测往往更难，文中总结了以下几个挑战：</p><ul><li>异常特征的差异性。在不同的云服务系统中，对异常的容忍度是不同的，所以对每个场景或系统组件设置准确的阈值来进行异常检测是十分困难的；</li><li>时间依赖性。该异常检测问题处理的是时间序列数据，而传统的异常检测并不会考虑时间依赖性；</li><li>无监督学习的性能问题。无监督的异常检测方法的性能有限，会带来大量的误报；</li><li>有监督学习需要大量标签。</li></ul><h1 id="proposed-approach">Proposed Approach</h1><p>为了解决上述挑战，文中提出了一个时间序列异常检测框架ATAD (Active Transfer Anomaly Detection)。该框架结合了迁移学习技术和主动学习技术，示意图如下：</p><p><img src="https://i.loli.net/2020/06/25/jOB4rC2gnH9VcQW.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>未标记数据<span class="math inline">\(T_u\)</span>是我们要检测的目标数据 (target domain)，标记数据<span class="math inline">\(T_l\)</span>是我们的源数据 (source domain)，可以是开源数据或者是其他系统的监控数据。</p><h2 id="transfer-learning-component">Transfer Learning Component</h2><p>在应用迁移学习时，我们需要考虑以下几个因素：</p><ul><li>我们处理的是时间序列数据，即在不同的时间点上样本之间不是相互独立的。为了解决这个问题，我们提取了不同的特征，每一个时间点被转换为了高维的特征向量，且每个时间点附近的背景信息被保存在了特征向量之中；</li><li>时间序列的粒度。粗粒度的迁移学习不利于发现异常，本文采用细粒度，即数据点级别的迁移学习；</li><li>迁移学习需要source domain和target domain具有潜在的相似性，所以我们需要对source domain中的样本进行过滤。</li></ul><p><img src="https://i.loli.net/2020/06/25/aM7Qvt6DwGXnThm.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="feature-identification">Feature Identification</h3><p>这一节描述特征工程中用到的特征。在提取特征之前，文中使用了离散傅里叶变换来识别时间序列的周期<span class="math inline">\(p\)</span>，并为后面滑动窗口的大小原则作参考。</p><h4 id="statistical-features">Statistical Features</h4><p>统计特征包含了一些基本的统计信息，如均值、方差等，用到的特征如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/jI9EbCy1XueDViw.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>表中的统计特征都是基于大小等于周期<span class="math inline">\(p\)</span>的滑动窗口的。</p><h4 id="forecasting-error-features">Forecasting Error Features</h4><p>使用预测特征的理由是如果一个数据点偏离预测值很远，那么它很有可能是异常。文中使用了多种时间序列预测模型，如SARIMA、Holt、Holt-Winters、STL等。最终的预测结果使用下式来加权集成： <span class="math display">\[\hat{Y}_t=\sum\limits_{m=1}^{M}\frac{\hat{Y}_{m,t}}{M-1}\left(1-\frac{RMSE_{m,t}}{\sum\limits_{n=1}^M RMSE_{n,t}}\right)\]</span> <span class="math inline">\(M\)</span>代表<span class="math inline">\(M\)</span>个不同模型，<span class="math inline">\(RMSE_{m,t}\)</span>代表模型<span class="math inline">\(m\)</span>在时间<span class="math inline">\(t\)</span>的<span class="math inline">\(RMSE\)</span>，<span class="math inline">\(\hat{Y}_t\)</span>是在时间<span class="math inline">\(t\)</span>的最终预测结果。之后，使用下表中的Metrics来计算不同预测特征：</p><p><img src="https://i.loli.net/2020/06/25/wRmfHj5xFcsLIXp.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>同样的，上述特征都是基于窗口的。</p><h4 id="temporal-features">Temporal Features</h4><p>这一部分是一些时间序列相关特征：</p><p><img src="https://i.loli.net/2020/06/25/mnBrzjfgV716yiR.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>最后，总共提取了37个特征，并且每个特征都进行了正则化。</p><h3 id="the-transfer-between-source-domain-and-target-domain">The Transfer between Source Domain and Target Domain</h3><p>本文结合了基于实例的迁移学习(<strong>Instance-based Transfer Learning</strong>)和基于特征的迁移学习(<strong>Feature-based Transfer Learning</strong>)。</p><p>首先，source domain中的数据差异性是比较大的，所以我们需要选择与target domain相似的样本。</p><p>基于实例的迁移学习(<strong>Instance-based Transfer Learning</strong>)的思想是选择source domain中与target domain相似的样本。对于source domain，在将时间序列<span class="math inline">\(T_l\)</span>转换为特征<span class="math inline">\(F_l\)</span>之后，本文使用<span class="math inline">\(K-means\)</span>算法将<span class="math inline">\(F_l\)</span>分成若干个簇。每个簇<span class="math inline">\(F_l^i, i\in[1,K]\)</span>是<span class="math inline">\(F_l\)</span>的不重叠子集。为了选择合适的样本，我们计算了target domain中的样本和每个簇中心点的欧几里得距离，然后样本会和距离最近的簇<span class="math inline">\(F_l^i\)</span>联系起来。</p><p>之后，为了使source domain和target domain在特征空间的差别更小，作者在每个簇上使用了<strong>CORrelation ALignment</strong> (CORAL) 算法。CORAL是一种领域适应算法 (<strong>Domain Adaption</strong>)，其基本思想是对source domain和target domain进行线性变换使其二阶统计信息（即协方差矩阵）的差别最小化： <span class="math display">\[\min_A\parallel A^\top C^i_lA-C^i_u\parallel_F^2\]</span></p><p>在最后一步，作者在每一个sub source domain <span class="math inline">\(\hat{F}_l^i\)</span>训练了有监督模型（随机森林或SVM），所以最后我们得到了<span class="math inline">\(K\)</span>个基模型。</p><h2 id="active-learning-component">Active Learning Component</h2><p>由于数据的差异性和复杂性太大，仅仅使用迁移学习的技术不足以达到很好的效果。在ATAD中，作者使用了主动学习技术来用较少的成本标注最有价值的样本来提升性能。本文中使用基于<strong>Uncertainty</strong>和<strong>Context Diversity</strong>的主动学习。</p><h3 id="uncertainty">Uncertainty</h3><p>大多数主动学习算法使用不确定性 (Uncertainty) 来作为选择要标记的样本的准则。 <span class="math display">\[Uncertainty=-|Prob(Normal)-Prob(Anomaly)|\]</span> 其中的<span class="math inline">\(Prob\)</span>由基模型给出。</p><h3 id="context-diversity">Context Diversity</h3><p>多样性 (Diversity) 也是一个选择要标记样本的重要参考。如果有两个相似的样本，那么就没有必要将他们都标记。</p><p>时间上相邻的样本往往也是相似的。</p><p>具体的来说，我们对所有样本按照<strong>Uncertainty</strong>排序，然后进行一次扫描，如果当前样本在候选集中某个样本的<strong>Context</strong>之中，我们则忽略当前样本，因为这代表当前样本和候选集中的那个样本是相似的。如果不在<strong>Context</strong>之中，我们则将该样本加入候选集中。</p><p>判断是否在某个样本的<strong>Context</strong>中，如下图所示，直接判断是否落在区间<span class="math inline">\([t-\alpha,t+\alpha]\)</span>中就是了。</p><p><img src="https://i.loli.net/2020/06/25/nci9PvGDEdjky5R.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>主动学习模块的算法流程图如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/RqonKfQS3IWw6Gb.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><h1 id="experiments">Experiments</h1><p>在实验部分，作者试图回答以下问题：</p><ol type="1"><li>ATAD的效果如何？</li><li>迁移学习模块的有效性如何？</li><li>主动学习模块的有效性如何？</li><li>ATAD在基于公开数据时对公司内部数据检测效果如何？</li></ol><h2 id="dataset-and-setup">Dataset and Setup</h2><p>下表是用到的数据集的一些基本信息：</p><p><img src="https://i.loli.net/2020/06/25/HDNGCrYOxezLwaB.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="evaluation-metric">Evaluation Metric</h2><p>评测标准使用的是F1-score： <span class="math display">\[F1=\frac{2\cdot P\cdot R}{P+R}, \space P=\frac{TP}{TP+FP}, \space R=\frac{TP}{TP+FN}\]</span></p><h2 id="results">Results</h2><h3 id="rq1-how-effective-is-atad">RQ1: How effective is ATAD?</h3><p>Baseline包括孤立森林、K-Sigma、S-H-ESD和随机森林。</p><p>最终结果如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/HMneBzlkR7Qg4TN.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>为了评测ATAD利用标签的能力，我们比较了RF在达到和ATAD相似F1 score情况下所需标签的数量，如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/fcoXhCL43yVwYes.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="rq2-how-effective-is-the-transfer-learning-component">RQ2: How effective is the Transfer Learning Component?</h3><p>我们从以下两个方面来探究模型迁移知识的能力：</p><ul><li>使用文中所用到的特征的重要性</li><li>本模型迁移知识的能力</li></ul><p>对于第一点，作者提出传统的方法一般只提取了统计特征，而本文还提取了多种其他特征。作者对提取不同特征进行了比较试验，结果如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/QV2eWzoOxGS6qJd.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>除此之外，作者还展示了不同数据集下前10有效的特征：</p><p><img src="https://i.loli.net/2020/06/25/QXYcP9V3xmJeIq5.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>对于第二点，作者比较了是否使用文中的领域适应算法CORAL，在达到相似F1 score下所需的标签数，如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/JKFf4XngPBdGm3V.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="rq3-how-effective-is-the-active-learning-component">RQ3: How effective is the Active Learning component?</h3><p>为了验证本文所用的主动学习的有效性，作者进行了对比试验。第一个模型 (Supervised model) 使用全部标签但不使用迁移学习训练，第二个 (Naïve) 为只使用主动学习而不使用迁移学习，第三个为本文提出的模型。结果如下图所示，为了达到相似的性能，不同模型需要的标签数。</p><p><img src="https://i.loli.net/2020/06/25/jcXphwBmR8J6SeU.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>下表展示了使用不同主动学习策略 (U - conventional uncertainty method, UCD - 本文使用的方法, random - 随机选择) 进行标记得到的结果：</p><p><img src="https://i.loli.net/2020/06/25/pe24P9gFJfQVrKM.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>同时作者还对不同<span class="math inline">\(\alpha\)</span>的选择进行了实验：</p><p><img src="https://i.loli.net/2020/06/25/ifRhv85IqaVw7gx.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="rq4-how-effective-is-atad-in-detecting-anomalies-in-a-companys-local-dataset-based-on-public-datasets">RQ4: How effective is ATAD in detecting anomalies in a company’s local dataset based on public datasets?</h3><p>这里作者对比了不同方法在微软内部数据集上的结果：</p><p><img src="https://i.loli.net/2020/06/25/5oi3rcGugKXmTha.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Transfer Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</title>
    <link href="/2020/05/06/Learning-Representations-of-Ultrahigh-dimensional-Data-for-Random-Distance-based-Outlier-Detection/"/>
    <url>/2020/05/06/Learning-Representations-of-Ultrahigh-dimensional-Data-for-Random-Distance-based-Outlier-Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文提出了一种针对高维数据异常检测的表示学习方法。文中提出了<strong>RAMODO</strong>框架，一种基于排序的结合表示学习和异常检测的无监督框架。除此之外，基于<strong>RAMODO</strong>，文中还提出了基于此框架的模型<strong>REPEN</strong>。</p><p><a href="https://arxiv.org/pdf/1806.04808" target="_blank" rel="noopener">Paper📰</a></p><h1 id="proposed-method">Proposed Method</h1><h2 id="the-proposed-framework-ramodo">The Proposed Framework: <strong>RAMODO</strong></h2><h3 id="problem-statement">Problem Statement</h3><p>我们的目的是为高维数据学习低维表示，同时在学到的低维表示中能够更好地进行异常检测。设有数据集<span class="math inline">\(\mathcal{X}=\{\mathbf x_1,\mathbf x_2,\cdots, \mathbf x_N\}\)</span> (<span class="math inline">\(\mathbf x_i\in \mathbb{R}^D\)</span>) 和一个基于随机距离的异常检测器<span class="math inline">\(\phi:\mathcal{X}\mapsto \mathbb{R}\)</span>，我们的目标是学习一个表示函数<span class="math inline">\(f:\mathcal{X}\mapsto\mathbb{R}^M (M\ll D)\)</span>使得对于所有异常样本<span class="math inline">\(\mathbf x_i\)</span>和正常样本<span class="math inline">\(\mathbf x_j\)</span>都有<span class="math inline">\(\phi(f(\mathbf x_i))&gt;\phi(f(\mathbf x_j))\)</span>。</p><h3 id="ranking-model-based-representation-learning-framework">Ranking Model-based Representation Learning Framework</h3><p><strong>RAMODO</strong>基于<em>pairwise ranking model</em>。第一步是通过一定的预处理算法（原文中称为<em>outlier thresholding</em>）将数据划分为inlier候选集和outlier候选集；第二步通过随机从inlier候选集采样<span class="math inline">\(n\)</span>个样本生成query set <span class="math inline">\((\mathbf x_i,\cdots,\mathbf x_{i+n-1})\)</span>，从inlier候选集采样一个样本生成<em>positive example</em> <span class="math inline">\((\mathbf x^+)\)</span>，从outlier候选集采样一个样本生成<em>negative example</em> <span class="math inline">\((\mathbf x^-)\)</span>，将三者组合生成 <em>metatriplet</em> <span class="math inline">\(T=(&lt;\mathbf x_i,\cdots,\mathbf x_{i+n-1}&gt;,\mathbf x^+,\mathbf x^-)\)</span>；第三步通过神经网络<span class="math inline">\(f\)</span>学习表示；第四步通过<em>outlier score-based ranking loss</em> <span class="math inline">\(L(\phi(f(\mathbf x^+)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;),\phi(f(\mathbf x^-)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;))\)</span>来进行优化，其中<span class="math inline">\(\phi(\cdot|\cdot)\)</span>为基于距离的异常检测器。</p><p><img src="https://i.loli.net/2020/06/25/4I7fx5ZjBhueUDz.png" srcset="/img/loading.gif" /></p><h2 id="a-ramodo-instance-repen">A <strong>RAMODO</strong> Instance: <strong>REPEN</strong></h2><p><strong>REPEN</strong>为<strong>RAMODO</strong>的实例模型，使用Sp作为异常检测器。</p><h3 id="outlier-thresholding-using-state-of-the-art-detectors-and-cantellis-inequality">Outlier Thresholding Using State-of-the-art Detectors and Cantelli's Inequality</h3><p>第一步使用Sp作为基础获得初始anomaly score：</p><blockquote><p><strong>Definition 1</strong> (<em>Sp-based Outlier Scoring</em>). 给定样本<span class="math inline">\(x_i\)</span>，Sp 以以下方式定义该样本的异常程度： <span class="math display">\[r_i=\frac{1}{m}\sum\limits_{j=1}^m nn\_dist(\mathbf x_i|\mathcal{S}_j)\]</span> 其中<span class="math inline">\(\mathcal S_j\subset \mathcal X\)</span>为数据集随机采样的子集，<span class="math inline">\(m\)</span>为集成大小，<span class="math inline">\(nn_dist(\cdot|\cdot)\)</span>为<span class="math inline">\(\mathcal S_j\)</span>中最近邻居的距离。</p></blockquote><p>接着通过<em>Cantelli's Inequality</em>来定义<em>Pseudo Outlier</em>：</p><blockquote><p><strong>Definition 2 </strong>(<em>Cantelli's Inequality-based Outlier Thresholding</em>). 给定异常分数向量<span class="math inline">\(\mathbf r\in\mathbb R^N\)</span>，更高异常分数代表更高的可能性为异常，设<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\delta^2\)</span>分别为均值和方差，<em>Outlier</em>候选集由以下方式确定： <span class="math display">\[\mathcal{O}=\{\mathbf x_i|r_i \geq \mu + \alpha\delta\}, \space\forall \mathbf x_i\in\mathcal X, \space r_i\in\mathbf r\]</span> 其中<span class="math inline">\(\alpha\geq 0\)</span>为自定义的阈值。</p></blockquote><p><em>Inlier</em>候选集<span class="math inline">\(\mathcal I=\mathcal X\backslash \mathcal O\)</span>。</p><h3 id="triplet-sampling-based-on-outlier-scores">Triplet Sampling Based on Outlier Scores</h3><p>首先，从<span class="math inline">\(\mathcal I\)</span>采样一定数量的样本组成<em>query set</em>，每个样本被采样的概率与其对应的异常分数有关：</p><p><span class="math display">\[p(\mathbf x_i)=\frac{\mathbb Z-r_i}{\sum_{t=1}^{|\mathcal I|}[\mathbb Z-r_t]}\]</span></p><p>其中<span class="math inline">\(\mathbb Z=\sum_{t=1}^{|\mathcal I|}r_t\)</span>。</p><p>之后从<em>inlier set</em>中均匀随机采样一个<em>positive sample</em> <span class="math inline">\(\mathbf x^+\)</span>。最后从<em>outlier set</em>中根据以下概率采样一个<em>negative sample</em> <span class="math inline">\(\mathbf x^-\)</span>： <span class="math display">\[p(\mathbf x_j)=\frac{r_j}{\sum_{t=1}^{|\mathcal O|}r_t}\]</span></p><h3 id="a-shallow-data-representation">A Shallow Data Representation</h3><p>单层神经网络用来获得浅层的表示：</p><blockquote><p><strong>Definition 3 </strong>(<em>Single-layer Fully-connected Representations</em>) 给定输入<span class="math inline">\(x\)</span>， <span class="math display">\[f_\Theta(\mathbf x)=\{\psi(\mathbf w_1^\top\mathbf x),\psi(\mathbf w_2^\top\mathbf x),\cdots,\psi(\mathbf w_M^\top\mathbf x)\}\]</span> 其中<span class="math inline">\(\psi(\cdot)\)</span>为激活函数，<span class="math inline">\(\mathbf w\)</span>为权重矩阵。</p></blockquote><h3 id="ranking-loss-using-random-nearest-neighbor-distance-based-outlier-scores">Ranking Loss Using Random Nearest Neighbor Distance-based Outlier Scores</h3><p>设<span class="math inline">\(\mathcal{Q}=&lt;f_\Theta(\mathbf x_i),\cdots,f_\Theta(\mathbf x_{i+n-1})&gt;\)</span>为<em>query set</em>，给定样本<span class="math inline">\(\mathbf x\)</span>，<strong>REPEN</strong>根据最近邻距离定义了<span class="math inline">\(f_\Theta(\mathbf x)\)</span>的异常程度： <span class="math display">\[\phi(f_\Theta(\mathbf x)|\mathcal{Q})=nn\_dist(f_\Theta(\mathbf x)|\mathcal Q)\]</span> 因此，给定三元组<span class="math inline">\(T=(\mathcal Q,f_\Theta(\mathbf x^+),f_\Theta(\mathbf x^-))\)</span>，我们的目标是学得表示<span class="math inline">\(f(\cdot)\)</span>使得： <span class="math display">\[nn\_dist(f_\Theta(\mathbf x^+)|\mathcal Q)&lt;nn\_dist(f_\Theta(\mathbf x^-)|\mathcal Q)\]</span> 损失函数： <span class="math display">\[J(\Theta;T)=L(\phi(f_\Theta(\mathbf x^+)|\mathcal Q),\phi(f_\Theta(\mathbf x^-)|\mathcal Q))=\\\max\{0, c+nn\_dist(f_\Theta(\mathbf x^+)|\mathcal Q)-nn\_dist(f_\Theta(\mathbf x^-)|\mathcal Q)\}\]</span> 其中<span class="math inline">\(c\)</span>为边界参数。给定一系列三元组，最终优化目标如下： <span class="math display">\[\mathop{\text{arg min}}\limits_{\Theta}\frac{1}{|\mathcal{T}|}\sum\limits_{i=1}^{|\mathcal T|}J(\Theta;T_i)\]</span></p><h3 id="the-algorithm-and-its-time-complexity">The Algorithm and Its Time Complexity</h3><p><img src="https://i.loli.net/2020/06/25/eYtKHBJ7szCgjNa.png" srcset="/img/loading.gif" /></p><h3 id="leveraging-a-few-labeled-outliers-to-improve-triplet-sampling">Leveraging A Few Labeled Outliers to Improve Triplet Sampling</h3><h1 id="experiments">Experiments</h1><h2 id="datasets">Datasets</h2><ul><li>AD：网络广告检测</li><li>LC：肺癌疾病监测</li><li>p53：异常蛋白质活动检测</li><li>R8：文本分类</li><li>News20：文本分类</li><li>URL：异常网址检测</li><li>Webspam：Pascal Large Scale LearningChallenge</li></ul><h2 id="effectiveness-in-real-world-data-with-thousands-to-millions-of-features">Effectiveness in Real-world Data with Thousands to Millions of Features</h2><p>作者分别使用原始特征和<em>REPEN</em>学到的特征进行异常检测，IMP代表性能提升比例，SU代表加速比例。</p><p><img src="https://i.loli.net/2020/06/25/mvUiE1NzyTwOgV8.png" srcset="/img/loading.gif" /></p><h2 id="comparing-to-state-of-the-art-representation-learning-competitors">Comparing to State-of-the-art Representation Learning Competitors</h2><ul><li><strong>AE: </strong>自编码器</li><li><strong>HLLE: </strong> <em>Hessian Locally Linear Embedding</em></li><li><strong>SRP: </strong> <em>Sparse Random Projection</em></li><li><strong>CoP: </strong> <em>Coherent Pursuit</em></li></ul><p><img src="https://i.loli.net/2020/06/25/yQumCRNrHAheJ34.png" srcset="/img/loading.gif" /></p><h2 id="the-capability-of-leveraging-labeled-outliers-as-prior-knowledge">The Capability of Leveraging Labeled Outliers as Prior Knowledge</h2><p><img src="https://i.loli.net/2020/06/25/NOLfKQd2u1JMPtp.png" srcset="/img/loading.gif" /></p><h2 id="sensitivity-test-w.r.t.-the-representation-dimension">Sensitivity Test w.r.t. the Representation Dimension</h2><p><img src="https://i.loli.net/2020/06/25/BoGjY5SEu6vrK3X.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/Rlx7Df9Hvsjp2Eg.png" srcset="/img/loading.gif" /></p><p>文中提到了对于R8、URL、News20这三个数据集在维度<span class="math inline">\(M=1\)</span>的时候表现和其他维度一样好，作者给出的解释是在这几个数据集中异常部分是线性可分的，所以1维就足够了，另一个解释是优化问题。</p><h2 id="scalability-test">Scalability Test</h2><p><img src="https://i.loli.net/2020/06/25/1JfUclWyFYgLdNp.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Representation Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Weakly-supervised Anomaly Detection</title>
    <link href="/2020/03/30/Deep-Weakly-supervised-Anomaly-Detection/"/>
    <url>/2020/03/30/Deep-Weakly-supervised-Anomaly-Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>在文献中，因为标注成本的昂贵，无监督方法占据了异常检测的主要位置。然而，在现实生活中，我们可能会有少量标签，如何利用这部分标签信息就成为了一个问题，作者将其称之为<em>anomaly-informed modeling</em>。作者提出了两点挑战：</p><ol type="1"><li>少量标签可能无法提供所有类型异常的信息；</li><li>大部分无标签数据为正常样本，但其中包含少部分异常（污染）。</li></ol><p>作者提出了基于pairwise relation learning的方法来解决这些问题。文章的主要贡献如下：</p><ol type="1"><li>提出了一种基于pairing-based data augmentation和ordinal regression来进行弱监督异常检测的框架</li><li>基于该框架提出了PReNet，一种基于双流ordinal regression的网络</li><li>从理论和实践角度分析了方法的有效性</li><li>在40个真实数据集上进行了完善的实验</li></ol><h1 id="proposed-method">Proposed Method</h1><h2 id="learning-anomaly-scores-by-predicting-pairwise-relation">Learning Anomaly Scores by Predicting Pairwise Relation</h2><h3 id="problem-formulation">Problem Formulation</h3><p>给定数据集<span class="math inline">\(\mathcal{X}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N,\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，包含两部分，一部分是五标签数据<span class="math inline">\(\mathcal{U}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N\}\)</span>，另一部分是有标签异常数据<span class="math inline">\(\mathcal{A}=\{\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，其中<span class="math inline">\(K\ll N\)</span>。我们的任务目标是学习一个打分函数<span class="math inline">\(\phi:\mathcal{X}\mapsto \mathbb{R}\)</span>，使得对任任意异常样本的打分高于任意正常样本。</p><p>在这个Formulation里，作者将关系学习和异常打分统一了起来。首先，输入的数据集不再是原始样本，而是样本对。样本对包含三种：<em>anomaly-anomaly</em>，<em>anomaly-unlabeled</em>，<em>unlabeled-unlabeled</em>，记为<span class="math inline">\(C_{\{\mathbf{a},\mathbf{a}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>。每一个样本对包含一个标签<span class="math inline">\(y\)</span>，表示该pair对应的异常分数，整个输入数据集<span class="math inline">\(\mathcal{P}=\{\{\mathbf{x}_i,\mathbf{x}_j,y_{ij}\}|\mathbf{x}_i,\mathbf{x}_j\in\mathcal{X} \space\text{and}\space y_{ij}\in\mathbb{N}\}\)</span>。因为有<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}&gt;y_{\{\mathbf a,\mathbf u\}}&gt;y_{\{\mathbf u,\mathbf u\}}\)</span>，所以对关系的学习也是对异常打分的学习。</p><h3 id="the-instantiated-model-prenet">The Instantiated Model: PReNET</h3><p>下图为模型示意图，<strong>Data Augmentation</strong>模块负责产生pair数据，<strong>End-to-End Anomaly Score Learner <span class="math inline">\(\phi\)</span></strong> 模块负责关系学习（异常打分）。</p><p><img src="https://i.loli.net/2020/06/25/6ZF3w9v1Lux5t4Q.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h4 id="data-argumentation-by-pairing">Data Argumentation by Pairing</h4><p>数据的产生分为两步：</p><ol type="1"><li>从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>上随机采样，组成pair；</li><li>对每个pair打上次序(ordinal class feature) 标签<span class="math inline">\(\mathbf{y}\)</span>。</li></ol><p>部分<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>和<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>可能包含异常污染，因为在<span class="math inline">\(\mathcal{U}\)</span>中可能会有未标记的异常样本。</p><h4 id="end-to-end-anomaly-score-learner">End-to-End Anomaly Score Learner</h4><p>令<span class="math inline">\(\mathcal{Z}\in\mathbb{R}^M\)</span>为中间表示空间，那么<strong>Score Learner</strong>可以拆解为特征学习<span class="math inline">\(\psi(\cdot;\Theta_r):\mathcal{X}\mapsto \mathcal{Z}\)</span>和打分函数<span class="math inline">\(\eta((\cdot,\cdot);\Theta_s):(\mathcal{Z},\mathcal{Z})\mapsto\mathbb{R}\)</span>两部分，两部分都由神经网络组成。</p><h4 id="ordinal-regression">Ordinal Regression</h4><p>损失函数定义为： <span class="math display">\[L\left(\phi((\mathbf x_i,\mathbf x_j);\Theta),y_{ij}\right)=|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|\]</span> 采用绝对值而不是均方误差的原因是为了减少异常污染的影响。默认<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}=8\)</span>，<span class="math inline">\(y_{\{\mathbf a,\mathbf u\}}=4\)</span>，<span class="math inline">\(y_{\{\mathbf u,\mathbf u\}}=0\)</span>。最后的优化函数可以写为： <span class="math display">\[\mathop{\text{argmin}}\limits_{\Theta}\frac{1}{|\mathcal{B}|}\sum\limits_{\{\mathbf x_i,\mathbf x_j, y_{ij}\}\in\mathcal{B}}|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|+\lambda R(\Theta)\]</span> <span class="math inline">\(\mathcal{B}\)</span>为一个batch，<span class="math inline">\(R(\Theta)\)</span>为正则项。</p><h3 id="anomaly-detection-using-prenet">Anomaly Detection Using PReNet</h3><h4 id="training-stage">Training Stage</h4><p>训练流程如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/oR6uTL3c7HMpwD4.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><p>为了保证训练样本类别的平衡，<span class="math inline">\(\frac{|\mathcal{B}|}{2}\)</span>的样本采样自<span class="math inline">\(C_{\{\mathbf u,\mathbf u\}}\)</span>，采样自<span class="math inline">\(C_{\{\mathbf a,\mathbf u\}}\)</span>和<span class="math display">\[C_{\{\mathbf a,\mathbf a\}}\]</span>的样本都占<span class="math inline">\(\frac{|\mathcal{B}|}{4}\)</span>。</p><h4 id="anomaly-scoring-stage">Anomaly Scoring Stage</h4><p>在测试阶段，给定测试样本<span class="math inline">\(\mathbf{x}_k\)</span>，先分别从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>采样，然后定义以下<em>anomaly score</em>： <span class="math display">\[s_{\mathbf{x}_k}=\frac{1}{2E}\left[\sum\limits_{i=1}^E\phi((\mathbf a_i,\mathbf x_k);\Theta^*)+\sum\limits_{j=1}^E\phi((\mathbf x_k,\mathbf u_j);\Theta^*)\right]\]</span> <span class="math inline">\(\mathbf a_i\)</span>和<span class="math inline">\(\mathbf u_j\)</span>为随机采样得到的异常样本和正常样本，采样大小<span class="math inline">\(E\)</span>默认为30。</p><h1 id="experiments">Experiments</h1><p>实验部分主要是回答以下四个问题：</p><ol type="1"><li>在有限的标签异常情况下，PReNet能否有效地检测已知和未知的异常；</li><li>在不同数量标签异常的情况下，PReNet的表现如何；</li><li>PReNet对异常污染的鲁棒性如何；</li><li>PReNet不同组件的重要性如何。</li></ol><h2 id="datasets">Datasets</h2><p>实验一共用到了40个数据集，其中12个用来评测算法检测已知的异常的能力（如Table 2所示），28个用来评测算法检测未知的异常的能力（如Table 3所示）。</p><h2 id="competing-methods-and-parameter-settings">Competing Methods and Parameter Settings</h2><p>用到的baseline有以下几个：</p><ul><li>DevNet：同一作者在KDD2019提出的异常检测框架</li><li>Deep support vector data description (DSVDD)：深度支持向量数据描述</li><li>Prototypical network： few-shot classification中的一种模型</li><li>iForest：孤立森林</li></ul><h2 id="performance-evaluation-metrics">Performance Evaluation Metrics</h2><p>用到的Metrics为AUC-ROC和AUC-PR。</p><h2 id="detection-of-known-anomalies">Detection of Known Anomalies</h2><p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）是固定的，下表为实验结果：</p><p><img src="https://i.loli.net/2020/06/25/BfhVE9z8DWipAM6.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><h2 id="detection-of-unknown-anomalies">Detection of Unknown Anomalies</h2><p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）同样是固定的，下表为实验结果：</p><p><img src="https://i.loli.net/2020/06/25/9GM8XTYiSLUn2Ar.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p><img src="https://i.loli.net/2020/06/25/4RxrGZWLqHNnXco.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h2 id="availability-of-known-anomalies">Availability of Known Anomalies</h2><p>本实验主要是研究不同数量标注异常样本的条件下，算法的性能如何。异常污染的比例固定（2%），标注异常的数量从15到120变化。实验结果如下：</p><p><img src="https://i.loli.net/2020/06/25/x4Hf3lU5JO71bvo.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h2 id="further-analysis-of-prenet">Further Analysis of PReNet</h2><h3 id="tolerance-to-anomaly-contamination-in-unlabeled-data">Tolerance to Anomaly Contamination in Unlabeled Data</h3><p>本实验主要研究不同异常污染比例下，算法的性能，即探究算法对异常污染的鲁棒性。标注异常样本的数量恒定（60），异常污染比例在<span class="math inline">\(\{0\%,2\%,5\%,10\%\}\)</span>中变化。实验结果如下所示：</p><p><img src="https://i.loli.net/2020/06/25/TGYCJUs8LlmreNV.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h3 id="ablation-study">Ablation Study</h3><p>这一节是消融实验，分别设置了四个变体：</p><ul><li><strong>BOR: </strong>损失函数替换成了二值回归<em>Binary Ordinal Regression</em>；</li><li><strong>OSNet: </strong>将双流结构简化为单流；</li><li><strong>LDM: </strong>将网络中的隐藏层去除；</li><li><strong>A2H: </strong>加入了额外的隐藏层，并且加入了<span class="math inline">\(\ell_2\)</span>-norm防止过拟合。</li></ul><p><img src="https://i.loli.net/2020/06/25/oR7qlZWfepFT8jK.png" srcset="/img/loading.gif" style="zoom:80%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Discovering Physical Concepts with Neural Networks</title>
    <link href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/"/>
    <url>/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>如题目所示，本文的目的是利用神经网络来发掘物理概念。其思路是从实验数据学到表示，然后用学到的表示来回答物理问题，由此物理概念可以从学到的表示来提取出。作者进行了4个实验：</p><ol type="1"><li>在阻尼振动实验中，模型学到了相关的物理参数；</li><li>在角动量守恒实验中，模型预测了质点的运动；</li><li>给定量子系统的观测数据，模型正确的识别出了量子状态的自由度；</li><li>给定从地球观测的太阳和火星的位置时间序列数据，模型发现了日心说模型。</li></ol><h1 id="preliminaries">Preliminaries</h1><p>作者在附录中对神经网络的基础知识进行了介绍，这里不再赘述，只截取了一些相对前沿的内容。</p><p><img src="https://i.loli.net/2020/06/25/yh5Wj9AQmd6nsFC.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="variational-autoencoders">Variational Autoencoders</h2><p>本文用到的模型基础是VAE：</p><p><img src="https://i.loli.net/2020/06/25/zCnYjVEZHdbqAD3.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="representation-learning">Representation Learning</h3><p><em>Representation learning</em>的主要目标是将数据映射到一个隐向量 (encoder)，为了保证隐向量包含了所有相关信息， 那么应该能够从隐向量还原原数据 (decoder)。传统的Autoencoder是这个思想的最简单实现，而VAE则将AE和<em>Variational Inference</em>结合了起来，是一种经典的生成式模型。现在很多研究关注<em>Disentangled Representation Learning</em>，也就是说我们希望模型能够无监督地学习数据，从中学到有意义的表示。</p><h3 id="boldsymbol-beta-vae"><span class="math inline">\(\boldsymbol \beta\)</span>-VAE</h3><p><span class="math inline">\(\beta\)</span>-VAE是一种特殊的VAE，也是一个经典的<em>Disentangled Representation Learning</em>模型，它和VAE主要的区别是对KL散度一项加上了权重<span class="math inline">\(\beta\)</span>进行调节： <span class="math display">\[C_\beta(x)=-\left[\mathbb{E}_{z\sim p_\phi(z|x)}\log p_\theta(x|z)\right] + \beta D_\text{KL}\left[p_\phi(z|x)\parallel h(z)\right]\]</span> 如果假设<span class="math inline">\(p_\phi(z|x)=\mathcal{N}(\mu,\sigma)\)</span>，那么损失函数可以进行简化： <span class="math display">\[C_\beta(x)=\parallel \hat{x} - x \parallel^2_2-\frac{\beta}{2}\left(\sum\limits_i\log(\sigma_i^2)-\mu_i^2-\sigma_i^2\right)+C\]</span></p><h1 id="network-structure">Network Structure</h1><h2 id="network-structure-scinet">Network Structure: <em>SciNet</em></h2><p>模仿物理学家建模物理问题的过程，作者提出了<em>SciNet</em>，如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/uWd1lOUFxXgQJ7f.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>物理学家在建模物理问题的时候，往往是从一些实验数据出发，根据物理常识提取更加精练的表示，然后用学到的表示来回答物理问题。</p><p>对于单纯的输入输出问题，<em>SciNet</em>可以看作是一个映射，<span class="math inline">\(F:\mathcal{O}\times\mathcal{Q}\rightarrow\mathcal{A}\)</span>。<span class="math inline">\(\mathcal{O}\)</span>是可能的实验数据集合，<span class="math inline">\(\mathcal{Q}\)</span>是可能的问题集合，<span class="math inline">\(\mathcal{A}\)</span>是可能的答案集合。可以将其分为两个步骤：编码过程<span class="math inline">\(E:\mathcal{O}\rightarrow\mathcal{R}\)</span>从实验数据学到表示，解码过程<span class="math inline">\(D:\mathcal{R}\times \mathcal{Q}\rightarrow \mathcal{A}\)</span>根据给定的问题从表示来回答问题。由此，<span class="math inline">\(F(o,q)=D(E(o),q)\)</span>。在实现方面，<em>SciNet</em>采用的是全连接网络。</p><h2 id="training-and-testing-scinet">Training and Testing <em>SciNet</em></h2><p>用来训练的数据形式为<span class="math inline">\((o,q,a_{cor}(o,q))\)</span>，观测<span class="math inline">\(o\)</span>和问题<span class="math inline">\(q\)</span>分别从观测集<span class="math inline">\(\mathcal{O}\)</span>和问题集<span class="math inline">\(\mathcal{Q}\)</span>选出，<span class="math inline">\(a_{cor}(o,q)\)</span>为对应的正确答案。在训练过程中，我们希望准确度尽量高，并且学到<em>minimal uncorrelated representations</em>。为此，作者采用<em>disentangling variational autoencoder</em>作为模型。</p><h1 id="results">Results</h1><p>在文中，作者进行了4个实验来验证模型的有效性。</p><h2 id="damped-pendulum">Damped Pendulum</h2><p>阻尼振动实验：</p><ul><li><p>任务：预测一维阻尼振动在不同时间的位置。</p></li><li><p>物理模型：<span class="math inline">\(-kx-b\dot{x}=m\ddot{x}\)</span>，<span class="math inline">\(k\)</span>为弹性模量，<span class="math inline">\(b\)</span>为阻尼系数，通解为<span class="math inline">\(x(t)=A_0e^{-\frac{b}{2m}t}\cos(\omega t+\delta_0), \space \omega=\sqrt{\frac{k}{m}}\sqrt{1-\frac{b^2}{4mk}}\)</span></p></li><li><p>观测数据：位置时间序列数据<span class="math inline">\(o=[x(t_i)]_{i\in\{1,\cdots,50\}}\in\mathbb{R}^{50}\)</span>，时间间隔相等，质量<span class="math inline">\(m=1\text{kg}\)</span>，振幅<span class="math inline">\(A_0=1\text{m}\)</span>，相位<span class="math inline">\(\delta_0=0\)</span>，弹性模量<span class="math inline">\(k\in[5,10]\text{kg}/\text{s}^2\)</span>，阻尼系数<span class="math inline">\(b\in[0.5,1]\text{kg}/\text{s}\)</span>。</p></li><li><p>问题：预测<span class="math inline">\(q=t_\text{pred}\in\mathbb{R}\)</span></p></li></ul><p><img src="https://i.loli.net/2020/06/25/yWGzxo4eFKmABul.png" srcset="/img/loading.gif" /></p><p>隐变量大小设置为3，结果如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/Q4PKa3pm2htekqd.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>(b)中的三幅图分别是学到的三个隐变量和我们感兴趣的参数<span class="math inline">\(k\)</span>和<span class="math inline">\(b\)</span>的关系图。第一幅图中变量<span class="math inline">\(1\)</span>与<span class="math inline">\(b\)</span>几乎完全线性相关，与<span class="math inline">\(k\)</span>基于线性无关，变量<span class="math inline">\(2\)</span>只和<span class="math inline">\(k\)</span>相关。变量<span class="math inline">\(3\)</span>几乎为一个常数，故不提供额外的信息。由此作者认为<em>SciNet</em>学到了我们关心的两个参数的知识。</p><h2 id="conservation-of-angular-momentum">Conservation of Angular Momentum</h2><p>角动量守恒实验：</p><ul><li>任务：预测一个由长度为<span class="math inline">\(r\)</span>的绳子捆绑着的旋转质点在位置<span class="math inline">\((0,r)\)</span>经一个自由质点撞击后的位置</li><li>物理模型：给定撞击之前的角动量，自由质点撞击之后的速度，旋转质点在撞击之后在时间<span class="math inline">\(t_\text{pred}^\prime\)</span>的位置可以由角动量守恒定律给出：</li></ul><p><span class="math display">\[J=m_\text{rot}r^2\omega-rm_\text{free}(\mathbf{v}_\text{free})_x=m_\text{rot}r^2\omega^\prime-rm_\text{free}(\mathbf{v}^\prime_\text{free})_x=J^\prime\]</span></p><ul><li>观测数据：在撞击之前两个质点的位置数据<span class="math inline">\(o=[(t_i^\text{rot},q_\text{rot}(t_i^\text{rot})),(t_i^\text{free},q_\text{free}(t_i^\text{free}))]_{i\in\{1,\cdots,5\}}\)</span>，质量为固定值，半径<span class="math inline">\(r\)</span>也为固定值。数据添加高斯噪声。</li><li>问题：预测撞击之后自由质点在时间<span class="math inline">\(t_\text{pred}^\prime\)</span>的位置</li></ul><p><img src="https://i.loli.net/2020/06/25/SKfJLxl1QmuzFt9.png" srcset="/img/loading.gif" /></p><p>实验室意图如下：</p><p><img src="https://i.loli.net/2020/06/25/qimk9ZYBe7UPs3z.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>实验结果表明<em>SciNet</em>能够正确预测质点撞击之后的位置，同时对噪音鲁棒。根据(b)，隐变量和角动量存在线性相关关系，作者认为<em>SciNet</em>学到了守恒的动量这一概念。</p><h2 id="representation-of-qubits">Representation of Qubits</h2><p>量子比特实验：</p><ul><li>任务：预测在<span class="math inline">\(n=1,2\)</span>的纯<span class="math inline">\(n\)</span>量子位状态<span class="math inline">\(\psi\in\mathbb{C}^{2^n}\)</span>下任何二进制投影测量<span class="math inline">\(\omega\in\mathbb{C}^{2^n}\)</span>的测量概率。</li><li>物理模型：在执行测量<span class="math inline">\(\omega\in\mathbb{C}^{2^n}\)</span>的状态<span class="math inline">\(\psi\in\mathbb{C}^{2^n}\)</span>下测量0的概率<span class="math inline">\(p(\omega,\psi)\)</span>由<span class="math inline">\(p(\omega,\psi)=|\left&lt;\omega,\psi\right&gt;|^2\)</span>给定</li><li>观测数据：状态<span class="math inline">\(\psi: o=[p(\alpha_i,\psi)]_{i\in\{i,\cdots,n_1\}}\)</span>的操作参数化：表示一组固定的随机二元射影测量值<span class="math inline">\(\mathcal{M}_1=\{\alpha_1,\cdots,\alpha_{n_1}\}\)</span>（一个量子位<span class="math inline">\(n_1 = 10\)</span>，两个量子位<span class="math inline">\(n_1 = 30\)</span>）</li><li>问题：对于固定的一组随机二元射影测量<span class="math inline">\(\mathcal{M}_2=\{\beta_1,\cdots,\beta_{n_2}\}\)</span>，测量<span class="math inline">\(\omega:q=[p(\beta_i,\omega)]_{i\in\{1,\cdots,n_2\}}\)</span>的Operational参数化（一个量子位<span class="math inline">\(n_2 = 10\)</span>，两个量子位<span class="math inline">\(n_2 = 30\)</span>）</li></ul><p><img src="https://i.loli.net/2020/06/25/8lY1LBsQCZUwX2I.png" srcset="/img/loading.gif" /></p><p>实验结果如下：</p><p><img src="https://i.loli.net/2020/06/25/ZTRKfzb63Jrvk5C.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>通过实验发现，<em>SciNet</em>可以在不提供先验物理知识的条件下确定表述状态<span class="math inline">\(\psi\)</span>最小的参数数量。同时，<em>SciNet</em>还能分辨<em>tomographically complete</em>和<em>tomographically incomplete</em>。</p><h2 id="heliocentric-model-of-the-solar-system">Heliocentric Model of the Solar System</h2><p>日心说模型：</p><ul><li>问题：在给定初始条件下预测相对与地球的太阳和火星的角度<span class="math inline">\(\theta_M(t)\)</span>和<span class="math inline">\(\theta_S(t)\)</span></li><li>物理模型：地球和火星围绕太阳以一定角速度做近似圆周运动</li><li>观测数据：给定初始角度，随机选择周周期的哥白尼的观测数据</li></ul><p><img src="https://i.loli.net/2020/06/25/vXl3Ae4RpzibrmY.png" srcset="/img/loading.gif" /></p><p>模型的实现稍有变化，如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/XnsYqcRS6izZGEm.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>这样，对于不同时间都对应一个隐变量<span class="math inline">\(r(t_i)\)</span>，而且隐变量是时间依赖的，对于一个隐变量<span class="math inline">\(r(t_i)\)</span>有一个解码器来输出答案。</p><p><img src="https://i.loli.net/2020/06/25/az3UmkchyFWevP7.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>实验结果表示，<em>SciNet</em>不仅正确预测了太阳和火星相对地球的角度，同时隐变量揭示了火星和地球相对太阳的角度。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transfer Anomaly Detection by Inferring Latent Domain Representations</title>
    <link href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/"/>
    <url>/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>作者提出了一种利用迁移学习提升target domain异常检测性能的算法。文中指出现有的基于迁移学习的异常检测算法需要对每个 target domain 进行单独训练，这样做会带来很大的计算开销。本文通过<em>latent domain vectors</em>来实现无需重新训练的异常检测。<em>latent domain vectors</em>是domain的一种隐含表示，通过该domain中的正常样本得到。在本文中，<em>anomaly score function</em>通过Auto-encoder得到。</p><h1 id="proposed-method">Proposed Method</h1><h2 id="task">Task</h2><p>令<span class="math inline">\(\mathbf{X}_d^+:=\{\mathbf{x}^+_{dn}\}^{N^+_d}_{n=1}\)</span>为第<span class="math inline">\(d\)</span>个domain的异常样本集，<span class="math inline">\(\mathbf{x}_{dn}^+\in\mathbb{R}^M\)</span>为其中第<span class="math inline">\(n\)</span>个样本的<span class="math inline">\(M\)</span>维特征向量，<span class="math inline">\(N^+_d\)</span>为第<span class="math inline">\(d\)</span>个domain异常样本的数量。</p><p>类似的，令<span class="math inline">\(\mathbf{X}_d^-:=\{\mathbf{x}^-_{dn}\}^{N^-_d}_{n=1}\)</span>为第<span class="math inline">\(d\)</span>个domain的正常样本集。我们假设对于每个domain都有<span class="math inline">\(N^+_d\ll N^-_d\)</span>，且特征向量维度都为<span class="math inline">\(M\)</span>。</p><p>假设在 source domain <span class="math inline">\(D_S\)</span>都有正常样本和异常样本，记为<span class="math inline">\(\{\mathbf{X}^+_d\cup\mathbf{X}_d^-\}^{D_S}_{d=1}\)</span>，在 target domain <span class="math inline">\(D_T\)</span>只有正常样本<span class="math inline">\(\{\mathbf{X}_d^-\}^{D_S+D_T}_{d=D_S+1}\)</span>。我们的目标是得到一个对于 target domain 合适的 domain-specific 的异常打分函数。</p><p><img src="https://i.loli.net/2020/06/25/KW2YgScfVZN7Fjz.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="domain-specific-anomaly-score-function">Domain-specific Anomaly Score Function</h2><p>我们基于Auto-encoder定义异常打分函数。对于每个domain，我们假设存在一个<span class="math inline">\(K\)</span>维的隐变量<span class="math inline">\(\mathbf{z}_d\in\mathbb{R}^K\)</span>。对于第<span class="math inline">\(d\)</span>个 domain，异常打分函数定义如下： <span class="math display">\[s_\theta(\mathbf{x}_{dn}|\mathbf{z}_d):=\parallel\mathbf{x}_{dn}-G_{\theta_G}(F_{\theta_F}(\mathbf{x}_{dn},\mathbf{z}_d))\parallel^2\]</span> 其中参数<span class="math inline">\(\theta:=(\theta_G,\theta_F)\)</span>在所有 domain 之间共享。</p><h2 id="models-for-latent-domain-vectors">Models for Latent Domain Vectors</h2><p>隐变量<span class="math inline">\(\mathbf{z}_d\)</span>是无法观测到的，只能通过数据来估计。首先<span class="math inline">\(\mathbf{z}_d\)</span>在<span class="math inline">\(\mathbf{X}_d^-\)</span>条件下的条件分布假设为高斯分布：</p><p><span class="math display">\[q_\theta(\mathbf{z}_d|\mathbf{X}_d^-):=\mathcal{N}(\mathbf{z}_d|\mu_\phi(\mathbf{X}_d^-),\text{diag}(\sigma_\phi^2(\mathbf{X}_d^-)))\]</span> 其中均值<span class="math inline">\(\mu_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K\)</span>和方差<span class="math inline">\(\sigma^2_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K_+\)</span>由神经网络建模，且在所有 domain 之间共享。在<span class="math inline">\(\mathbf{X}_d^-\)</span>给定的时候，我们便能够推断出该 domain 对应的隐变量，</p><p><span class="math inline">\(q_\phi\)</span>的输入为正常样本的集合，故神经网络需要满足<em>permutation invariant</em>。<span class="math inline">\(\tau(\mathbf{X}_d^-)=\rho(\sum_{n=1}^{N_d^-}\eta(\mathbf{x}_{dn}^-))\)</span>，其中<span class="math inline">\(\tau(\mathbf{X}_d^-)\)</span>表示<span class="math inline">\(\mu_\phi(\mathbf{X_d^-})\)</span>或<span class="math inline">\(\ln\sigma_\phi^2(\mathbf{X}_d^-)\)</span>，<span class="math inline">\(\rho\)</span>和<span class="math inline">\(\eta\)</span>为神经网络，</p><h2 id="objective-function">Objective Function</h2><p>目标函数由anomaly score函数和隐变量组成。第<span class="math inline">\(d\)</span>个domain在对应的隐变量<span class="math inline">\(\mathbf{z}_d\)</span>条件下的目标函数为：</p><p><span class="math display">\[L_d(\theta|\mathbf{z}_d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)-\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d))\]</span></p><p>其中<span class="math inline">\(\lambda\geq 0\)</span>为超参数，<span class="math inline">\(f\)</span>为sigmoid函数。公式的第一项表示第<span class="math inline">\(d\)</span>个domain正常样本对应的<em>anomaly score</em>。第二项为可微分的AUC。异常样本的<em>anomaly score</em>应当大于正常样本，所以对任何<span class="math inline">\(\mathbf x_{dm}^+\in\mathbf X_d^+, \mathbf x_{dn}^-\in\mathbf X_d^-\)</span>有<span class="math inline">\(s_\theta(\mathbf x_{dm}^+|\mathbf z_d)&gt;s_\theta(\mathbf x_{dn}^-|\mathbf z_d)\)</span>。第二项<span class="math inline">\(\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d))\)</span>的取值范围是<span class="math inline">\([0,1]\)</span>，当所有的<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>时该项为1，当所有的<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\ll s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>时该项为0，所以最小化该项的相反数相当于鼓励<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>。</p><p>因为隐变量<span class="math inline">\(\mathbf z_d\)</span>包含不确定性，我们应该在目标函数里考虑这一点： <span class="math display">\[\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))\]</span></p><p>第一项是<span class="math inline">\(L_d(\theta|\mathbf z_d)\)</span>关于<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>的期望，第二项是<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>和<span class="math inline">\(p(\mathbf z_d):=\mathcal{N}(\boldsymbol 0,\boldsymbol I)\)</span>的KL散度。第一项可以用<em>monte carlo</em>估计<span class="math inline">\(\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]\approx\frac{1}{L}\sum_{\ell=1}^L L_d(\theta|\mathbf z_d^{(\ell)})\)</span>，除此之外还需要用到<em>reparametrization trick</em>。</p><p>对于第<span class="math inline">\(d\)</span>个target domain，因为没有异常样本（假设），所以<span class="math inline">\(L_d(\theta|\mathbf{z}_d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\)</span>，有： <span class="math display">\[\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z}_d))\]</span></p><p>所以总的损失函数为各domain对应的损失函数之和<span class="math inline">\(\mathcal{L}(\theta,\phi):=\sum_{d=1}^{D_S+D_T}\alpha_d\mathcal{L}_d(\theta,\phi)\)</span>。</p><h2 id="inference">Inference</h2><p>训练好之后，domain-specific的<em>anomaly score</em>可以由下式计算出：</p><p><span class="math display">\[s(\mathbf{x}_{d^\prime}):=\int s_{\theta_*}(\mathbf{x_{d^\prime}}|\mathbf{z}_{d^\prime})q_{\phi_*}(\mathbf{z}_{d^\prime}|\mathbf{X}_{d^\prime}^-)\mathrm{d}\mathbf{z}_{d^\prime}\approx\frac{1}{L}\sum\limits_{\ell=1}^L s_{\theta_*}(\mathbf{x}_{d^\prime}|\mathbf{z}_{d^\prime}^{(\ell)})\]</span></p><h1 id="experiments">Experiments</h1><h2 id="data">Data</h2><p>实验包含五个数据集，第一个是合成数据集。如下图(a)所示，围绕<span class="math inline">\((0,0)\)</span>有<span class="math inline">\(8\)</span>个圈，每个圈包含了一个内圈作为异常样本，第<span class="math inline">\(7\)</span>个圈被选为target domain，其余的为source domain。第二个是MNIST-r，是加入旋转的MNIST，包含6个domain，其中数字“4”被选为异常样本，其余为正常。第三个为Anuran Calls，包含5个domain。第四个是Landmine，主要用在多任务学习中。第五个是IoT，网络流量数据，包含8个domain。</p><p><img src="https://i.loli.net/2020/06/25/6WLAfMwJPuN5Ov9.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h2 id="comparison-methods">Comparison Methods</h2><p>对比的baseline包括NN（普通多层神经网络），NNAUC（加入可微分AUC作为损失函数），AE（普通Autoencoer），AEAUC（加入可微分AUC的AE），OSVM（单类支持向量机），CCSA，TOSVM和OTL。</p><h2 id="results">Results</h2><p>4个真实数据集的结果如下：</p><p><img src="https://i.loli.net/2020/06/25/nfkwTVexRNqyFMY.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2020/06/25/QaMskTZALyeiFI1.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2020/06/25/F7VTyeHMz8mK2uJ.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2020/06/25/B32UmgXcwGhZYk1.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>表5为考虑隐变量不确定性的ablation study。将原来的公式<span class="math inline">\(\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))\)</span>中<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>用迪利克雷分布<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)=\delta(\mathbf z_d-\mu_\phi(\mathbf X_d^-))\)</span>代替并且去掉KL散度。</p><p><img src="https://i.loli.net/2020/06/25/yUHcTBzixsMlY7f.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>表6展示了不同异常比例对效果的影响。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Transfer Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Anomaly Detection with Deviation Networks</title>
    <link href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/"/>
    <url>/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文关注<code>Deep Anomaly Detection</code>，也就是用深度学习的方法来进行异常检测。文中提到现有的<code>Deep Anomaly Detection</code>存在两个弊端：一个是采用深度学习方法来进行特征学习，然后通过下游任务得到<code>Anomaly Score</code>，相比文中End-to-End的<code>Anomaly Score</code>学习，存在优化不充分的风险；另一个是现有的方法主要是无监督学习，无法利用已知的信息（如少量标签）。为此，本文提出了一种端到端的异常检测框架，来解决上述问题。</p><p>本文的主要贡献如下：</p><ul><li>提出了一种端到端的异常检测框架，直接学习<code>Anomaly Score</code>并且可以利用已知信息；</li><li>基于提出的框架，文中提出了一种实例方法 (DevNet)。</li></ul><p><img src="https://i.loli.net/2020/06/25/XT7fqQRWEOuocgy.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="proposed-model">Proposed Model</h1><h2 id="end-to-end-anomaly-score-learning">End-To-End Anomaly Score Learning</h2><h3 id="problem-statement">Problem Statement</h3><p>为了区别于传统的两阶段异常检测（先学习特征表示，然后在学到的特征上定义一个<code>anomaly measure</code>来得到<code>anomaly score</code>），作者对端到端的异常检测问题重新进行形式化。</p><p>给定<span class="math inline">\(N+K\)</span>个样本<span class="math inline">\(\mathcal{X}=\{\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N,\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}\}\)</span>，其中<span class="math inline">\(\boldsymbol x_i\in\mathbb{R}^D\)</span>，无标签样本集<span class="math inline">\(\mathcal{U}=\{\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N\}\)</span>，有标签样本集<span class="math inline">\(\mathcal{K}=\{\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}\}\)</span>，且<span class="math inline">\(K\ll N\)</span>。异常检测的目标是学习一个<code>anomaly scoring function</code><span class="math inline">\(\phi:\mathcal{X}\mapsto\mathbb{R}\)</span>使得<span class="math inline">\(\phi(\boldsymbol x_i)&gt;\phi(\boldsymbol x_j)\)</span>，其中<span class="math inline">\(\boldsymbol x_i\)</span>为异常样本，<span class="math inline">\(\boldsymbol x_j\)</span>为正常样本。</p><h3 id="the-proposed-framework">The Proposed Framework</h3><p>为了解决这个问题，文中提出了一种通用异常检测框架，模型框架如下图所示：</p><p>模型框架如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/ZuE1mb2Ytv6Jdl7.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>主要包含三个部分：</p><ol type="1"><li><em>anomaly scoring network</em>. 图中左边的部分，一个函数<span class="math inline">\(\phi\)</span>，输入样本<span class="math inline">\(\mathbf{x}\)</span>，输出<code>anomaly score</code></li><li><em>reference score generator</em>. 图中右边的部分。只有一个<em>anomaly scoring network</em>并不能进行训练，需要训练的目标。为此加入<em>reference score generator</em>，输入为随机选择的<span class="math inline">\(l\)</span>个正常样本，输出<code>reference score</code>（这<span class="math inline">\(l\)</span>个正常样本<code>anomaly score</code>的均值，记为<span class="math inline">\(\mu_\mathcal{R}\)</span>）</li><li><em>deviation loss</em>. <span class="math inline">\(\phi(\mathbf{x})\)</span>，<span class="math inline">\(\mu_\mathcal{R}\)</span>及对应的标准差<span class="math inline">\(\sigma_\mathcal{R}\)</span>作为<code>deviation loss</code>函数的输入。因为<span class="math inline">\(\mu_\mathcal{R}\)</span>和<span class="math inline">\(\sigma_\mathcal{R}\)</span>对应正常样本集的均值和方差，那么异常样本的<code>anomaly score</code>应该和<span class="math inline">\(\mu_\mathcal{R}\)</span>差别比较大，而正常样本则应该接近<span class="math inline">\(\mu_\mathcal{R}\)</span>。</li></ol><h2 id="deviation-networks">Deviation Networks</h2><p>下面是上述三个部件的具体实现。</p><h3 id="end-to-end-anomaly-scoring-network">End-To-End Anomaly Scoring Network</h3><p>记<span class="math inline">\(\mathcal{Q}\in\mathbb{R}^M\)</span>为中间表示空间，<code>anomaly scoring network</code><span class="math inline">\(\phi(\cdot;\Theta):\mathcal{X}\mapsto\mathbb{R}\)</span>可以定义为数据表示学习<span class="math inline">\(\psi(\cdot;\Theta_t):\mathcal{X}\mapsto\mathcal{Q}\)</span>和异常分数学习<span class="math inline">\(\eta(\cdot;\Theta_s):\mathcal{Q}\mapsto\mathbb{R}\)</span>两阶段的组合，其中<span class="math inline">\(\Theta=\{\Theta_t,\Theta_s\}\)</span>。</p><p><span class="math inline">\(\psi(\cdot;\Theta_t)\)</span>可以用一个<span class="math inline">\(H\)</span>层神经网络来实现： <span class="math display">\[\mathrm{q}=\psi(\mathbf{x};\Theta_t)\]</span> 其中<span class="math inline">\(\mathbf{x}\in\mathcal{X}\)</span>，<span class="math inline">\(\mathrm{q}\in\mathcal{Q}\)</span>。</p><p><span class="math inline">\(\eta(\cdot;\Theta_s)\)</span>可以用一个单层的神经网络来实现： <span class="math display">\[\eta(\mathrm q;\Theta_s)=\sum\limits_{i=1}^M w_i^oq_i+w_{M+1}^o\]</span> 其中<span class="math inline">\(\mathrm q\in\mathcal Q\)</span>，<span class="math inline">\(\Theta_s=\{\mathbf{w}^o\}\)</span>。</p><p>所以有： <span class="math display">\[\phi(\mathbf{x};\Theta)=\eta(\psi(\mathbf{x};\Theta_t);\Theta_s)\]</span></p><h3 id="gaussian-prior-based-reference-scores">Gaussian Prior-based Reference Scores</h3><p>有两种方法来获得<span class="math inline">\(\mu_\mathcal{R}\)</span>，一种是data-driven，一种是prior-driven。如果是data-driven的话则采用另一个神经网络，文中表示为了更好的解释性和计算效率，所以采用的是prior-driven。 <span class="math display">\[\begin{align}r_1,r_2,\cdots,r_l\sim \mathcal{N}(\mu,\sigma^2),\\\mu_\mathcal{R}=\frac{1}{l}\sum\limits_{i=1}^l r_i\end{align}\]</span> 在文中，采用的prior是标准高斯分布。</p><h2 id="z-score-based-deviation-loss">Z-Score Based Deviation Loss</h2><p><em>anomaly scoring network</em>的优化目标可以定义为Z-Score的方式： <span class="math display">\[dev(\boldsymbol x)=\frac{\phi(\boldsymbol x;\Theta)-\mu_{\mathcal{R}}}{\sigma_{\mathcal{R}}}\]</span> <span class="math inline">\(dev(\boldsymbol x)\)</span>可以看作是样本偏离标准的程度，而我们肯定希望异常样本偏离标准越大，正常样本越接近标准。文中采用的损失函数是<code>Contrastive Loss</code>： <span class="math display">\[L(\phi(\boldsymbol x;\Theta),\mu_\mathcal{R},\sigma_\mathcal{R})=(1-y)|dev(\boldsymbol x)| + y \max(0, a - dev(\boldsymbol x))\]</span> <code>Contrastive Loss</code>的直观解释可以看下图：</p><p><img src="https://i.loli.net/2020/06/25/aPbSipCsk2JwNcD.png" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p>对于负例（正常），优化过程将他们尽量向原点靠近，对于正例（异常），优化过程将他们拉向边界。</p><h2 id="the-devnet-algorithm">The DevNet Algorithm</h2><p><code>DevNet</code>的算法流程图如下：</p><p><img src="https://i.loli.net/2020/06/25/km9H5DoNRbOQ784.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="interpretability-of-anomaly-scores">Interpretability of Anomaly Scores</h2><p>因为<em>reference score generator</em>选择的是确定的高斯分布，于是可以用概率论给出一些解释性。作者给出了一个结论，</p><blockquote><p><strong>PROPOSITION</strong>： 设<span class="math inline">\(\boldsymbol x\in\mathcal{X}\)</span>，<span class="math inline">\(z_p\)</span>为<span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>的分位数，那么<span class="math inline">\(\phi(\boldsymbol x)\)</span>在区间<span class="math inline">\(\mu\pm z_p\sigma\)</span>的概率为<span class="math inline">\(2(1-p)\)</span>。</p></blockquote><p>例如，假设<span class="math inline">\(p=0.95\)</span>，那么<span class="math inline">\(z_{0.95}=1.96\)</span>，表示异常分数高于1.96的样本将以0.95的置信度为异常。</p><h1 id="experiment">Experiment</h1><p>实验用到了9个数据集，4个Baseline (REPEN，DSVDD，FSNET，iForest)，以及ROC和PR曲线两种评测标准。</p><h2 id="effectiveness-in-real-world-data-sets">Effectiveness in Real-world Data Sets</h2><h3 id="experiment-settings">Experiment Settings</h3><p>这一个实验主要是为了验证算法在真实场景下的效果，即大量无标签数据和极少量标签数据。训练集包含两部分，一部分是无标签数据<span class="math inline">\(\mathcal{U}\)</span>,包含<span class="math inline">\(2\%\)</span>的异常样本，另一部分是有标签数据<span class="math inline">\(\mathcal{K}\)</span>，由随机采样<span class="math inline">\(0.005\%-1\%\)</span>的训练数据和<span class="math inline">\(0.08\%-6\%\)</span>的异常样本组成。</p><h3 id="findings">Findings</h3><p>实验结果如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/DKqxJROngML8IS2.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>从结果上看来，本文提出的方法在所有数据集上都比Baseline好，说明<code>DevNet</code>端到端直接优化<code>Anomaly Score</code>的方式是有效的。</p><h2 id="data-efficiency">Data Efficiency</h2><h3 id="experiment-settings-1">Experiment Settings</h3><p>这一个实验主要是为了探究基于深度的异常检测方法的<em>data efficiency</em>。和上一个实验一样，无标签数据集包含<span class="math inline">\(2\%\)</span>的异常，而有标签的异常数量从<span class="math inline">\(5\)</span>到<span class="math inline">\(120\)</span>不等。本实验试图回答以下两个问题：</p><ul><li><code>DevNet</code>的<em>data efficiency</em>如何？</li><li>基于深度的方法在多大程度上能够利用标签信息？</li></ul><h3 id="findings-1">Findings</h3><p>在几个基于深度的Baseline中，<code>DevNet</code>的效果是最好的，同时在有标签异常非常有限的情况下，<code>DevNet</code>也能很好的利用标签信息，达到更好的效果。</p><p><img src="https://i.loli.net/2020/06/25/iIWGBPosKCuxbRF.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="robustness-w.r.t.-anomaly-contamination">Robustness w.r.t. Anomaly Contamination</h2><h3 id="experiment-settings-2">Experiment Settings</h3><p>在第一个实验中，无标签数据集<span class="math inline">\(\mathcal{U}\)</span>包含的是固定的异常比例<span class="math inline">\(2\%\)</span>，而在这个实验中，作者测试了从<span class="math inline">\(0\%\)</span>到<span class="math inline">\(20\%\)</span>之间不同异常比例来测试算法的鲁棒性（即使<span class="math inline">\(\mathcal{U}\)</span>中包含异常，由于没有标签，在训练的时候仍然假设都为正常来进行训练）。本实验试图回答以下问题：</p><ul><li>基于深度的异常检测方法的鲁棒性如何？</li><li>当训练集中异常污染的比例较高的时候基于深度的方法能否打败无监督的方法？</li></ul><h3 id="findings-2">Findings</h3><p>下图为实验结果：</p><p><img src="https://i.loli.net/2020/06/25/JCnIjLOc84RFP2V.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>从结果上来看，<code>DevNet</code>比其他基于深度的方法鲁棒性更好，同时在高异常污染的情况下仍然比纯无监督方法效果要好。</p><h2 id="ablation-study">Ablation Study</h2><p>本实验设置了<code>DevNet</code>的三个变体（默认的<code>DevNet-Def</code>为单层隐层加上一个输出层）来进行消融实验，分别是：</p><ul><li><code>DevNet-Rep</code>，去掉了<em>anomaly scoring network</em>网络的输出层，对应<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>；</li><li><code>DevNet-Linear</code>，去掉了网络中的非线性层，对应<em>learning of non-linear features</em>；</li><li><code>DevNet-3HL</code>，隐层数量为3层。</li></ul><p>对比结果如下：</p><p><img src="https://i.loli.net/2020/06/25/5LcyAwGMB8gb2UP.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>通过实验可以发现，<code>DevNet-Rep</code>说明了<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>的有效性，而<code>DevNet-Linear</code>说明了<em>learning of non-linear features</em>的重要性。<code>DevNet-3HL</code>说明了加深网络并不总能带来性能的提升。</p><h2 id="scalability-test">Scalability Test</h2><p>这一个实验使用合成的数据来测试算法对大规模数据的处理能力，分别从<em>Data Size</em>和<em>Data Dimensionality</em>两方面来测试。结果如下：</p><p><img src="https://i.loli.net/2020/06/25/5gbPdJkB47e3FsV.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>可以看出，<code>DevNet</code>对<em>Data Size</em>并不敏感，同时，面对高维数据，<code>DevNet</code>也没有表现出劣势。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Geant4 安装教程与调试环境配置</title>
    <link href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    <url>/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>Geant4安装的教程很多，版本都很旧了，这里写一个新版本（10.6）基于Ubuntu的安装教程，并且开启CLion IDE调试。</p><h1 id="step-1-download-packages">Step 1: Download Packages</h1><p>首先进入官网(<a href="http://geant4.web.cern.ch/support/download" target="_blank" rel="noopener">http://geant4.web.cern.ch/support/download</a>)下载源代码（推荐tar.gz格式）及数据文件，解压。新建一个文件夹专门用来放<code>Geant4</code>相关文件，新建data，source，build文件夹，将Geant4的文件复制进来并按如下结构组织：</p><pre><code class="hljs bash">.├── build├── data│   ├── G4ABLA3.1│   ├── G4EMLOW7.9│   ├── G4ENSDFSTATE2.2│   ├── G4INCL1.0│   ├── G4NDL4.6│   ├── G4PARTICLEXS2.1│   ├── G4PII1.3│   ├── G4SAIDDATA2.0│   ├── G4TENDL1.3.2│   ├── PhotonEvaporation5.5│   ├── RadioactiveDecay5.4│   └── RealSurface2.1.1└── <span class="hljs-built_in">source</span>    └── geant4.10.06</code></pre><p><img src="https://i.loli.net/2020/06/25/OuZaAJ3WyEYwsG7.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/R5Tmk68AhPbaDHY.png" srcset="/img/loading.gif" /></p><h1 id="step-2-install-dependencies">Step 2: Install Dependencies</h1><p>安装编译所需环境：</p><pre><code class="hljs bash">sudo apt-get install build-essential cmake</code></pre><p>安装相关依赖：</p><pre><code class="hljs bash">sudo apt-get install libgl1-mesa-dev libglu1-mesa-dev libxt-dev libxmu-dev libxi-dev zlib1g-dev libgl2ps-dev libexpat1-dev libxerces-c-dev</code></pre><p>如果要用到QT需要单独安装QT。</p><h1 id="step-3-compile">Step 3: Compile</h1><p>进入build文件夹，用cmake命令：</p><pre><code class="hljs bash">cmake ../<span class="hljs-built_in">source</span>/geant4.10.06/ -DCMAKE_BUILD_TYPE=DEBUG -DGEANT4_USE_GDML=ON -DGEANT4_USE_OPENGL_X11=ON -DGEANT4_USE_RAYTRACER_X11=ON -DGEANT4_BUILD_MULTITHREADED=ON</code></pre><p>其中<code>../source/geant4.10.06/</code>替换成换成（如果版本不一样）你自己的Geant4源代码所在目录，需要QT则加上<code>-DGEANT4_USE_QT=ON</code>。如果不需要调试则把<code>-DCMAKE_BUILD_TYPE=DEBUG</code>改成<code>-DCMAKE_BUILD_TYPE=RELEASE</code>。<code>-DGEANT4_BUILD_MULTITHREADED=ON</code>是多线程，视情况开启。</p><p>完成之后开始编译：</p><pre><code class="hljs bash">make -jX</code></pre><p><code>-jX</code>为多线程编译，如<code>-j8</code>。</p><p>编译完成之后进行安装：</p><pre><code class="hljs bash">sudo make install</code></pre><h1 id="step-4-configure">Step 4: Configure</h1><p>安装的默认路径在<code>/usr/local/share/Geant4-10.6.0</code>，将下载的数据文件复制到该文件夹：</p><pre><code class="hljs bash">sudo cp -r ./data/ /usr/<span class="hljs-built_in">local</span>/share/Geant4-10.6.0/</code></pre><p>之后，在<code>~/.bashrc</code>里添加<code>/usr/local/share/Geant4-10.6.0/geant4make/geant4make.sh</code>，如果你的版本和我的不一样，相应修改即可。</p><h1 id="step-5-clion-configuration">Step 5: CLion Configuration</h1><p>最后我们来配置CLion环境，配好之后可以在IDE中编写<code>Geant4</code>代码，还可以断点调试，非常方便。安装CLion的过程这里省略，打开一个<code>Geant4</code>自带的例子或者自己新建一个项目，打开<code>Edit Configurations</code>。</p><p><img src="https://i.loli.net/2020/06/25/W1xXHUqIvofyKQk.png" srcset="/img/loading.gif" /></p><p>随便打开一个终端，输入一下命令获取环境变量：</p><pre><code class="hljs bash">env | grep G4</code></pre><p>在<code>Environment variables</code>填入刚才获取的环境变量（复制之后按一下粘贴就可以了），然后把<code>Working directory</code>设置成当前文件夹。</p><p><img src="https://i.loli.net/2020/06/25/HrslFTOau51y8tX.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/5UKaCgvnWjYmNTG.png" srcset="/img/loading.gif" /></p><p>现在就大功告成了！</p>]]></content>
    
    
    <categories>
      
      <category>Technical Notes</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Geant4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Complementary Set Variational Autoencoder for Supervised Anomaly Detection</title>
    <link href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/"/>
    <url>/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>对于异常检测问题，异常的模式是多种多样的。有监督模型能够较好地处理训练集中出现过的模式，无监督模型能够处理训练集中未出现过的模式，但对于训练集中出现过的异常模型并没有学习。本文提出了一种既能学习训练集中出现过的异常模式，同时能处理未出现过的异常模式的方法。</p><h1 id="proposed-model">Proposed Model</h1><h2 id="conventional-vae">Conventional VAE</h2><p>首先回顾一下原始的VAE。</p><p>原始VAE中的损失函数为： <span class="math display">\[\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})=\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}\parallel p(\boldsymbol{z}))]\]</span> 原文中作者证明了<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\leq\log p(\boldsymbol{x};\boldsymbol{\theta})\)</span>，所以<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\)</span>可以看作是数据分布<span class="math inline">\(p(\boldsymbol{x})\)</span>对数似然的一个下界。<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\)</span>又被称为证据下界 (ELBO)。<span class="math inline">\(\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]\)</span>中的期望一般用蒙特卡洛来进行估计： <span class="math display">\[\begin{align}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq&amp; \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})\parallel p(\boldsymbol{z})],\\\boldsymbol{z}^{(l)}&amp;\sim q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}), \space l\in\{1,2,\cdots,L\}\end{align}\]</span> 对于隐变量<span class="math inline">\(\boldsymbol{z}\)</span>，一般假设先验服从标准高斯分布，后验服从均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^2\)</span>的高斯分布，故KL散度能直接写出解析式： <span class="math display">\[\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-C(-\frac{1}{2}-\log\sigma+\frac{1}{2}\sigma^2+\frac{1}{2}\mu^2)\]</span> 使用VAE来做异常检测通常是在正常数据上进行训练，在检测阶段，如果是异常样本，那么VAE不能很好地重构它，这样会导致较大的重构误差。</p><h2 id="prior-distribution-for-anomalies">Prior Distribution for Anomalies</h2><p><img src="https://i.loli.net/2020/06/25/vrxAzRVCtaE3oLc.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>在原始VAE异常检测中，无论输入样本<span class="math inline">\(\boldsymbol{x}\)</span>是否异常，VAE都会使对应编码的后验<span class="math inline">\(p(\boldsymbol{z}|\boldsymbol{x})\)</span>服从高斯分布，且施加标准高斯分布的约束。在本文中，作者对异常和正常样本对应的隐变量的先验分布做了不同假设。首先，正常先验依然是标准高斯分布，记为<span class="math inline">\(p_n(\boldsymbol{z})\)</span>。而对于异常先验，作者认为异常即为“不正常”，和正常是补集的关系。作者在文中定义异常先验分布<span class="math inline">\(p_a(\boldsymbol{z})\)</span>为： <span class="math display">\[p_a(\boldsymbol{z})=\frac{1}{Y^\prime}(\max\limits_{\boldsymbol{z}^\prime}p_n(\boldsymbol{z}^\prime)-p_n(\boldsymbol{z}))\]</span></p><p>其中<span class="math inline">\(Y^\prime\)</span>为使<span class="math inline">\(p_a(\boldsymbol{z})\)</span>成为一个概率分布的调节因子。实际上，<span class="math inline">\(Y^\prime\)</span>往往会成为无限大，因为<span class="math inline">\(p(\boldsymbol z)\)</span>在整个定义域上都有定义。为了解决这个问题，作者加入了<span class="math inline">\(p_w(\boldsymbol z)\)</span>，一个在每个维度都足够宽的辅助分布：</p><p><span class="math display">\[p_a(\boldsymbol z)=\frac{1}{Y}p_w(\boldsymbol z)\left(\max\limits_{\boldsymbol z^\prime}p_n(\boldsymbol z^\prime)-p_n(\boldsymbol z)\right)\]</span></p><p>其中<span class="math inline">\(Y\)</span>为有限的常数。在文中<span class="math inline">\(p_n(\boldsymbol z)\)</span>和<span class="math inline">\(p_w(\boldsymbol z)\)</span>都为高斯分布，那么<span class="math inline">\(p_a(\boldsymbol z)\)</span>的具体形式为：</p><p><span class="math display">\[p_a(\boldsymbol z)=\frac{1}{Y}\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\{\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)-\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol 1)\}\]</span></p><p>其中：</p><p><span class="math display">\[\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)=\frac{1}{\sqrt{2\pi}}\]</span></p><p><span class="math display">\[Y=\int_{-\infty}^{\infty}p_a(\boldsymbol z)\mathrm{d}\boldsymbol z=\frac{1}{\sqrt{2\pi}}\left\{1-\frac{1}{\boldsymbol s^2+1}\right\}\]</span></p><p><span class="math inline">\(\boldsymbol s^2\)</span>为超参数，控制分布的宽度。用文中的先验替换VAE原始的KL散度，可写为：</p><p><span class="math display">\[\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]=\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\frac{\mathcal N(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)}{\frac{1}{Y}\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\left\{\frac{1}{2\pi}-\mathcal N(\boldsymbol z;\boldsymbol0,\boldsymbol 1)\right\}}\mathrm{d}\boldsymbol z\]</span></p><p>展开后：</p><p><span class="math display">\[\begin{align}\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;=\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)\mathrm{d}\boldsymbol z\\&amp;+\log Y\\&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\mathrm{d}\boldsymbol z\\&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\left\{\frac{1}{\sqrt{2\pi}}-\mathcal{N}(\boldsymbol z;\boldsymbol 0, \boldsymbol 1)\right\}\mathrm{d}\boldsymbol z\end{align}\]</span></p><p>使用泰勒展开，<span class="math inline">\(\log (x+\frac{1}{2\pi})\simeq-\log 2\pi+2\pi x\)</span>，KL散度可以用下式估计：</p><p><span class="math display">\[\begin{align}\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;\simeq\sqrt{\frac{2\pi}{\boldsymbol\sigma^2+1}}\exp\left(\frac{-\boldsymbol\mu^2}{2(\boldsymbol\sigma^2+1)}\right)\\&amp;+\frac{\boldsymbol\mu^2+\boldsymbol\sigma^2}{2\boldsymbol s^2}-\log\boldsymbol\sigma+\log\boldsymbol s+\log\left(\sqrt{\boldsymbol s^2+1}-1\right)\\&amp;-\frac{\log(\boldsymbol s^2+1)}{2}+\frac{\log(2\pi)-1}{2}\end{align}\]</span></p><p>下图为一维时<span class="math inline">\(p_n(\boldsymbol z)\)</span>和<span class="math inline">\(p_a(\boldsymbol z)\)</span>的示例：</p><p><img src="https://i.loli.net/2020/06/25/QHXo24cKj9uRwzW.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="implementation-of-proposed-method">Implementation of proposed method</h3><p>文中使用编码器输出的分布<span class="math inline">\(\mathcal{N}(\boldsymbol z;\boldsymbol \mu, \boldsymbol \sigma^2)\)</span>与标准正态分布之间的KL散度来作为异常分数。在每一轮的训练过程中，加入一轮使用Anomaly Prior的训练。</p><p><img src="https://i.loli.net/2020/06/25/wrmzADXtsyuJ6EZ.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="experiments">Experiments</h1><h2 id="mnist">MNIST</h2><p>作者设计了两个Task：</p><ol type="1"><li>Task 1. <span class="math inline">\(N\)</span> vs. <span class="math inline">\(\bar{N}\)</span>. 将手写数字中的一个作为已知异常，其他作为正常，并加入均匀分布作为未知的异常。</li><li>Task 2. 手写数字被分为3组：已知异常，正常，未知异常。</li></ol><p>细节如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/ifcIxr9zOpEhksA.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>在实现上，使用Adam优化器，<code>batch_size</code>为100，<code>epochs</code>为200。<code>Encoder</code>和<code>Decoder</code>都由三层感知机组成，超参数<span class="math inline">\(s^2\)</span>设置为400。评测标准使用AUC (area under the receiver characteristic curve)。</p><p>下表为实验结果：</p><p><img src="https://i.loli.net/2020/06/25/YTpO98y1ZPmAK3g.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>VAE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE</title>
    <link href="/2020/01/06/Unsupervised-Anomaly-Detection-for-Intricate-KPIs-via-Adversarial-Training-of-VAE/"/>
    <url>/2020/01/06/Unsupervised-Anomaly-Detection-for-Intricate-KPIs-via-Adversarial-Training-of-VAE/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/8737430/" target="_blank" rel="noopener">论文📃</a></p><p><a href="https://github.com/yantijin/Buzz" target="_blank" rel="noopener">代码📥</a></p><p>本文介绍了一种利用对抗训练来进行时间序列异常检测的方法<em>Buzz</em>。作者认为在现实中复杂的KPI数据大量存在，这种数据通常带有非高斯分布的噪声，同时数据分布复杂，导致一般的生成式模型无法对数据进行很好的建模，所以作者提出了基于对抗训练的模型。在文中，作者的创新点主要有三个：</p><ol type="1"><li>为了处理复杂数据，将数据空间分为多个子空间，在每个子空间上进行距离的度量；</li><li>采用<em>Wasserstein</em>距离度量模型建模的分布和真实分布之间的距离；</li><li>建立了基于对抗训练的<em>Buzz</em>的损失函数和VAE之间的关系。</li></ol><p><img src="https://i.loli.net/2020/06/25/FwHi7y56njz8R4m.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="background">Background</h1><h2 id="anomaly-detection">Anomaly Detection</h2><p>对于任意时间<span class="math inline">\(t\)</span>，给定历史观察值<span class="math inline">\(x_{t-T+1},\cdots,x_t\)</span>，确定异常是否发生(记为<span class="math inline">\(y_t=1\)</span>)。通常来收异常检测算法给出的是发生异常的可能性，如<span class="math inline">\(p(y_t=1|x_{t-T+1},\cdots,x_t)\)</span>。</p><h2 id="vae">VAE</h2><table><thead><tr class="header"><th><strong>Model</strong></th><th><strong>Latent</strong></th><th><strong>Data</strong></th></tr></thead><tbody><tr class="odd"><td><strong><em>Auto-encoder (AE)</em></strong></td><td><code>None</code></td><td><code>L1Loss</code></td></tr><tr class="even"><td><strong><em>Variational Auto-encoder (VAE)</em></strong></td><td><code>KL Divergence</code></td><td><code>Log Likelihood</code></td></tr><tr class="odd"><td><strong><em>Adversarial Auto-encoder (AAE)</em></strong></td><td><code>Discriminator</code></td><td><code>L1Loss</code></td></tr><tr class="even"><td><strong><em>Wasserstein Auto-encoder (WAE)</em></strong></td><td><code>MaxMeanDiscrepancy</code> or <code>Discriminator</code></td><td><code>L1Loss</code></td></tr><tr class="odd"><td><strong><em>AlphaGAN</em></strong></td><td><code>Discriminator</code></td><td><code>Discriminator</code>+<code>L1Loss</code></td></tr></tbody></table><h2 id="gan-and-wgan-gp">GAN and WGAN-GP</h2><p>原始GAN等价于优化： <span class="math display">\[\mathbb{E}_{x\sim P_r}\log{\frac{P_r(x)}{\frac{1}{2}\left[P_r(x)+P_g(x)\right]}}+\mathbb{E}_{x\sim P_g}\log{\frac{P_g(x)}{\frac{1}{2}\left[P_r(x)+P_g(x)\right]}}\]</span> 根据KL散度和JS散度的定义： <span class="math display">\[\text{KL}(P_1\parallel P_2)=\mathbb{E}_{x\sim P_1}\log{\frac{P_1}{P_2}}\]</span></p><p><span class="math display">\[\text{JS}(P_1\parallel P_2)=\frac{1}{2}\text{KL}(P_1\parallel \frac{P_1+P_2}{2})+\frac{1}{2}\text{KL}(P_2\parallel \frac{P_1+P_2}{2})\]</span></p><p>可重写为： <span class="math display">\[2\text{JS}(P_r\parallel P_g)-2\log 2\]</span> 当<span class="math inline">\(P_r\)</span>与<span class="math inline">\(P_g\)</span>的支撑集（support）是高维空间中的低维流形（manifold）时，<span class="math inline">\(P_r\)</span>与<span class="math inline">\(P_g\)</span>重叠部分测度（measure）为0的概率为1。</p><ul><li>支撑集（support）其实就是函数的非零部分子集，比如<code>ReLU</code>函数的支撑集就是[公式]，一个概率分布的支撑集就是所有概率密度非零部分的集合。</li><li>流形（manifold）是高维空间中曲线、曲面概念的拓广，我们可以在低维上直观理解这个概念，比如我们说三维空间中的一个曲面是一个二维流形，因为它的本质维度（intrinsic dimension）只有2，一个点在这个二维流形上移动只有两个方向的自由度。同理，三维空间或者二维空间中的一条曲线都是一个一维流形。</li><li>测度（measure）是高维空间中长度、面积、体积概念的拓广，可以理解为“超体积”。</li></ul><p><em>Wasserstein</em>距离定义如下： <span class="math display">\[W(P_r,P_g)=\inf\limits_{\gamma\sim\prod(P_r,P_g)}\mathbb{E}_{(x,y)\sim \gamma}\left[\parallel x-y\parallel\right]\]</span> 下确界<span class="math inline">\(\inf\)</span>没法直接求解，不过根据相关定理其等价于： <span class="math display">\[W(P_r,P_g)=\frac{1}{K}\sup\limits_{\parallel f\parallel_L\leq K}\mathbb{E}_{x\sim P_r}[f(x)]-\mathbb{E}_{x\sim P_g}[f(x)]\]</span> <em>Lipschitz</em>连续是指存在一个常数<span class="math inline">\(K\geq 0\)</span>使得定义域内任意两个元素<span class="math inline">\(x_1\)</span>和<span class="math inline">\(x_2\)</span>都满足： <span class="math display">\[|f(x_1)-f(x_2)|\leq K|x_1-x_2|\]</span> WAN的损失函数： <span class="math display">\[\mathcal{L}=\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_g}\left[D({\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_r}\left[D(\mathbf{x})\right]\]</span> WGAN-GP的损失函数为： <span class="math display">\[\mathcal{L}=\mathop{\mathbb{E}}\limits_{\tilde{\mathbf{x}}\sim\mathbb{P}_g}\left[D(\tilde{\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_r}\left[D(\mathbf{x})\right] + \lambda\mathop{\mathbb{E}}\limits_{\hat{\mathbf{x}}\sim\mathbb{P}_{\hat{\mathbf{x}}}}\left[(\parallel\nabla_{\hat{\mathbf{x}}}D(\hat{\mathbf{x}})\parallel_2-1)^2\right]\]</span></p><h1 id="proposed-method">Proposed Method</h1><p>下图为<em>Buzz</em>的总体流程：</p><p><img src="https://i.loli.net/2020/06/25/aGPdzq8TVS6fetJ.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>数据会首先进行一些预处理，之后进行训练。在检测阶段会根据异常分数来判定异常。</p><h2 id="motivation">Motivation</h2><p>在文中，最关键的两个创新点分别是<em>Wasserstein</em>距离和对数据分布进行分区的方法。</p><ul><li><p>在使用距离度量方面， 因为<em>Wasserstein</em>在WGAN中取得了很好的效果，是一种鲁棒的距离度量，所以作者在文中采用了<em>Wasserstein</em>距离来衡量生成的分布和真实的分布之间的距离，并由此引入了对抗训练；</p></li><li><p>在分区方法方面，作者认为原始数据过于复杂，所以将数据空间<span class="math inline">\(\mathcal{X}\)</span>进行划分，然后在每个子空间上使用<em>Wasserstein</em>度量距离，而总体的距离由每个分区的距离的期望求得。</p></li></ul><p>作者还发现，当划分地越来越细时，总体距离接近于特定形式的VAE的重构误差项。</p><p><img src="https://i.loli.net/2020/06/25/knzq4yPaBrXWgeI.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="network-structure">Network Structure</h2><p>下图为模型的网络结构：</p><p><img src="https://i.loli.net/2020/06/25/Qrgpuh4Iw7YTlny.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="training">Training</h2><h3 id="objective-function">Objective Function</h3><p>先定义一些符号：</p><ul><li><span class="math inline">\(b\)</span>和<span class="math inline">\(s\)</span>分别为Batch的大小和邻居的大小，数据集按<span class="math inline">\(s\)</span>进行切分，然后随机打乱，每个Batch包含<span class="math inline">\(b\)</span>个<span class="math inline">\(s\)</span>，之后<span class="math inline">\(s/=2,b*=2\)</span>；</li><li><span class="math inline">\(\mathcal{W}=\{w_1,w_2,\cdots,w_b\}\)</span>为一个Batch，且满足每个<span class="math inline">\(w_i\)</span>是<span class="math inline">\(s\)</span>的倍数；</li><li><span class="math inline">\(w\in\mathcal{W}\)</span>的邻域集(neighborhood set)为一个时间上的partition，记为<span class="math inline">\(\{w,w+1,\cdots,w+s-1\}\)</span></li><li><span class="math inline">\(\mathbf{x}^{(w)},\mathbf{x}^{(w+1)},\cdots,\mathbf{x}^{(w+s-1)}\)</span>为在空间<span class="math inline">\(\mathcal{X}\)</span>上的一个partition，记为<span class="math inline">\(S_w\)</span>，其中<span class="math inline">\(\mathbf{x}^{(w)}\)</span>表示以<span class="math inline">\(w\)</span>为结尾的时间窗口。</li></ul><p><em>Buzz</em>的损失函数和WGAN-GP类似，但做了一些改进，由下面四部分组成，下面分别解释。</p><p>第一个是每一个partition的<span class="math inline">\(\mathbf{z}\)</span>后验的KL散度： <span class="math display">\[\mathcal{K} = \frac{1}{bs}\sum\limits_{w\in\mathcal{W}}\sum\limits_{i=1}^{s-1}\text{KL}\left[q_\phi(\mathbf{z}|\mathbf{x})\parallel\mathcal{N}(\mathbf{0},\mathbf{1})\right]\]</span></p><p>第二个在训练时是一个常数： <span class="math display">\[Z(\lambda) = \frac{\Gamma(W)}{\Gamma(\frac{W}{2})}2\pi^{\frac{W}{2}}\lambda^{-W}\]</span></p><p>其中<span class="math inline">\(\Gamma\)</span>是<em>Gamma</em>函数。</p><p>第三个是<em>Wasserstein</em>距离： <span class="math display">\[\mathcal{T}(F,w)=\frac{1}{bs}\sum\limits_{i=1}^{s-1}\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x}^{w+i})}\left[F(\mathbf{x}^{(w+i)})-F(G(\mathbf{z}))\right]\]</span></p><p>第四个是<em>Gradient Penalty</em>：</p><p><span class="math display">\[\mathcal{R}(F,w)=\frac{1}{bs}\sum\limits_{i=1}^{s-1}\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x}^{(w+i)})}\left[\mathbb{E}_{\varepsilon\sim[0,1]}(\parallel\nabla_{\hat{\mathbf{x}}}(\hat{\mathbf{x}})\parallel-\mathbf{1})^2\right]\]</span></p><p>其中<span class="math inline">\(\hat{\mathbf{x}}=\varepsilon \mathbf{x}^{w+i}+(1-\varepsilon)G(\mathbf{z})\)</span>为生成数据与真实数据的插值。</p><blockquote><p>原始的WGAN-GP的损失函数为： <span class="math display">\[L=\mathop{\mathbb{E}}\limits_{\tilde{\mathbf{x}}\sim\mathbb{P}_g}\left[D(\tilde{\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_r}\left[D(\mathbf{x})\right] + \lambda\mathop{\mathbb{E}}\limits_{\hat{\mathbf{x}}\sim\mathbb{P}_{\hat{\mathbf{x}}}}\left[(\parallel\nabla_{\hat{\mathbf{x}}}D(\hat{\mathbf{x}})\parallel_2-1)^2\right]\]</span> 其中<span class="math inline">\(\mathbb{P}_g\)</span>为生成器的分布，<span class="math inline">\(\mathbb{P}_r\)</span>为真实分布，<span class="math inline">\(\mathbb{P}_{\hat{\mathbf{x}}}\)</span>为真实数据和生成数据插值得到的分布。</p></blockquote><p><span class="math display">\[\hat{\mathcal{L}}_{Buzz}=-\lambda\sup\limits_F\left[\sum\limits_{w\in\mathcal{W}}(\left|\mathcal{T}(F,w)\right|-\eta\mathcal{R}(F,w))\right]-\mathcal{K}-\log Z(\lambda)\]</span></p><h3 id="training-procedure">Training Procedure</h3><p><em>Buzz</em>的训练过程与WGAN-GP类似，</p><h2 id="detection">Detection</h2><p>文中假设解码器的输出服从如下分布：</p><p><span class="math display">\[p_\theta(\mathbf{x}|\mathbf{z})=\frac{1}{Z(\lambda)}\exp\{-\lambda\parallel\mathbf{x}-G(\mathbf{z})\parallel\}\]</span></p><p>作者定义异常分数为： <span class="math display">\[\mathcal{S}=\log p_\theta(\mathbf{x})-\log p_\theta(\bar{\mathbf{x}})\]</span> 其中<span class="math inline">\(\bar{\mathbf{x}}\)</span>为经过MCMC填充后的样本。</p><p>异常分数也可以展开为： <span class="math display">\[\log\frac{1}{L}\sum\limits_{l=1}^L\left[\frac{p_\theta(\mathbf{x}|\mathbf{z^{(l)}})p_\theta(\mathbf{z}^{(l)})}{q_\phi(\mathbf{z}^{(l)}|\bar{\mathbf{x}})}\right]-\log\frac{1}{L}\sum\limits_{l=1}^L\left[\frac{p_\theta(\bar{\mathbf{x}}|\mathbf{z}^{(l)})p_\theta(\mathbf{z}^{(l)})}{q_\phi(\mathbf{z}^{(l)}|\bar{\mathbf{x}})}\right]\]</span></p><p>最终算法流程图为：</p><p><img src="https://i.loli.net/2020/06/25/Y3NLjRiaWO5eHGT.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h2 id="theoretical-analysis">Theoretical Analysis</h2><p>在理论分析中，作者主要是想建立<span class="math inline">\(\mathcal{L}_{Buzz}\)</span>和VAE的损失函数<span class="math inline">\(\mathcal{L}_{vae}\)</span>之间的联系，损失函数<span class="math inline">\(\mathcal{\hat{L}}_{Buzz}\)</span>为：</p><p><span class="math display">\[\hat{\mathcal{L}}_{Buzz}=-\lambda\sup\limits_F\left[\sum\limits_{w\in\mathcal{W}}(\left|\mathcal{T}(F,w)\right|-\eta\mathcal{R}(F,w))\right]-\mathcal{K}-\log Z(\lambda)\]</span></p><p>为了便于分析，去掉<em>Gradient Penalty</em>的部分，公式可简化为：</p><p><span class="math display">\[\mathcal{L}_{Buzz}=-\lambda\mathbb{E}_{p(w)}W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]-\mathcal{K}-\log Z(\lambda)\]</span></p><p>实际上<span class="math inline">\(Z(\lambda)=\mathfrak{S}_W\Gamma(W)\lambda^{-W}\)</span>，其中<span class="math inline">\(\mathfrak{S}_W\)</span>为<span class="math inline">\(W\)</span>维单位球的表面积。</p><blockquote><p><span class="math inline">\(n\)</span>维空间单位球表面积公式： <span class="math display">\[\omega_n=\frac{2\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\]</span></p></blockquote><p>而<span class="math inline">\(W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]\)</span>为<em>Wasserstein</em>距离： <span class="math display">\[W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]=\sup\limits_{Lip(f)\leq 1}\left\{\int_\mathcal{X}f(\mathbf{x})p(\mathbf{x}|w)\mathrm{d}\mathbf{x}-\int_\mathcal{X}f(\mathbf{y})p(\mathbf{y}|w)\mathrm{d}\mathbf{y}\right\}\]</span></p><h3 id="lemma-1">Lemma 1</h3><p>通过设定具体形式的后验分布，VAE的损失函数可以写为：</p><blockquote><p>设<span class="math inline">\(\mathbf{x}\)</span>的后验分布<span class="math inline">\(p(\mathbf{x}|\mathbf{z})=\frac{1}{Z(\lambda)}\exp\{-\lambda\parallel\mathbf{x}-G(\mathbf{z})\parallel\}\)</span>，那么VAE的损失函数为： <span class="math display">\[\mathcal{L}_{vae}=\lambda\mathbb{E}_{p(w)}\left[\mathbb{E}_{p(\mathbf{x}|w)}\mathbb{E}_{p_G(\mathbf{y}|\mathbf{x})}-\parallel\mathbf{x}-\mathbf{y}\parallel\right]-\mathcal{K}-\log{Z(\lambda)}\]</span></p></blockquote><p>后验分布实际上是一个Laplace分布：</p><blockquote><p><strong>Laplace Distribution</strong>: <span class="math display">\[f(x|\theta,\lambda)=\frac{1}{2\lambda}\exp{\left(-\frac{|x-\theta|}{\lambda}\right)}\]</span></p></blockquote><p>可以直接把后验分布带入VAE的损失函数就得到了。</p><h3 id="lemma-2">Lemma 2</h3><p><span class="math inline">\(S_w\)</span>定义为数据空间<span class="math inline">\(\mathcal{X}\)</span>的一个partition，而<span class="math inline">\(S=\{(\mathbf{x}_1,\mathbf{x}_2)| \exists w, \mathbf{x}_1\in S_w,\mathbf{x}_2\in S_w\}\)</span>。</p><blockquote><p>当<span class="math inline">\(G,\phi,\lambda\)</span>固定时，<span class="math inline">\(S\downarrow\)</span>有<span class="math inline">\(\mathcal{L}_{Buzz}\downarrow\)</span></p></blockquote><h3 id="lemma-3">Lemma 3</h3><blockquote><p><span class="math inline">\(\max\mathcal{L}_{Buzz}\geq\max{\mathcal{L}_{vae}}\)</span>，同时，当<span class="math inline">\(S\downarrow\text{diag}{\mathcal{X}}\)</span>时<span class="math inline">\(\max\mathcal{L}_{Buzz}\downarrow\max\mathcal{L}_{vae}\)</span></p></blockquote><p><img src="https://i.loli.net/2020/06/25/XM2xwBbOLzikhoc.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="lemma-4">Lemma 4</h3><blockquote><p>令<span class="math inline">\(p^\prime_G(\mathbf{y}|\mathbf{x})\)</span>表示<span class="math inline">\(\mathbb{E}_{q_{\phi^\prime}}\left[p_G(\mathbf{y}|\mathbf{z})\right]\)</span>。如果<span class="math inline">\((G,\phi,\lambda)\)</span>是一个解，那么存在<span class="math inline">\((G,\phi^\prime,\lambda)\)</span>使得： <span class="math display">\[\mathbb{E}_{p(\mathbf{x}|w)}\mathbb{E}_{p_G^\prime(\mathbf{y}|\mathbf{x})}\parallel\mathbf{x}-\mathbf{y}\parallel=W^1\left[P(\mathbf{x}|w)\parallel P_G(\mathbf{y}|w)\right]\]</span> 此时<span class="math inline">\(\mathcal{L}_{Buzz}-\mathcal{L}_{vae}^\prime=\mathcal{K}^\prime-\mathcal{K}\)</span>，其中<span class="math inline">\(\mathcal{L}^\prime,\mathcal{K}^\prime\)</span>分别为<span class="math inline">\((G,\phi^\prime,\lambda)\)</span>时的<span class="math inline">\(\mathcal{L}\)</span>和<span class="math inline">\(\mathcal{K}\)</span>。</p></blockquote><p><span class="math display">\[\mathcal{L}^\dagger_{Buzz}=\mathbb{E}_{p(\mathbf{x})}\left[\mathbb{E}_{q_{\phi^\prime}(\mathbf{z}|\mathbf{x})}\log_{p_\theta}(\mathbf{x}|\mathbf{z})\right]-\min\limits_{\bar{\phi}\sim\phi^\prime}\bar{\mathcal{K}}\]</span></p><h3 id="lemma-5">Lemma 5</h3><p>这里主要是想证明</p><blockquote><p>对于固定的<span class="math inline">\(w\)</span>，令： <span class="math display">\[\mathcal{F}=\{f|Lip(f)\leq 1\}, \space \mathcal{F}^*=\left\{f|_{S_w}\bigg|Lip(f|_{S_w})\leq 1\right\}\]</span> 有<span class="math inline">\(\sup_{f\in\mathcal{F}}\mathcal{T}(f)=\sup_{f|_{S_w}\in\mathcal{F}^*}\mathcal{T}^*\left(f|_{S_w}\right)\)</span>。</p></blockquote><h3 id="theorem-6">Theorem 6</h3><blockquote><p><span class="math inline">\(\mathcal{L}_{Buzz}\)</span>的对偶形式为： <span class="math display">\[\mathcal{L}_{Buzz}=-\lambda\sup\limits_{Lip(F;S)\leq 1}\mathbb{E}_{p(w)}\mathcal{T}^*(F)-\mathcal{K}-\log Z(\lambda)\]</span></p></blockquote><p>近似的<span class="math inline">\(\mathcal{L}_{Buzz}\)</span>的对偶形式为： <span class="math display">\[\bar{\mathcal{L}}_{Buzz}=-\lambda\sup\limits_{Lip(F;S)\leq 1}\mathbb{E}_{p(w)}\mathcal{T}(F)-\mathcal{K}-\log Z(\lambda)\]</span></p><h1 id="experiment">Experiment</h1><p><img src="https://i.loli.net/2020/06/25/GEi3zfeAgXVsvBN.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/7EkhRGNvIsSmwn6.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/yTIhoZb1kVOf4Mv.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>VAE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Variational Approaches for Auto-Encoding Generative Adversarial Networks</title>
    <link href="/2019/11/02/Variational-Approaches-for-Auto-Encoding-Generative-Adversarial-Networks/"/>
    <url>/2019/11/02/Variational-Approaches-for-Auto-Encoding-Generative-Adversarial-Networks/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>本文揭示了对抗生成网络（Generative Adversarial Networks, GAN）和变分自编码器（Variational Auto-encoders, VAE）之间的联系，并据此提出了一种将两者结合的新模型。文中主要是将不可解的似然函数和未知的后验分布用一个非确定的分布（Immplicit Distribution）替代，并加入判别器来使得该分布逼近真实的分布。通过这个方法，作者将VAE中的损失函数进行了替换，变成了GAN中的“生成-判别”模式。</p><p><a href="https://arxiv.org/abs/1706.04987" target="_blank" rel="noopener">原文</a></p><h1 id="contribution">Contribution</h1><p>本文有如下贡献：</p><ul><li>本文提出变分推断（Variational Inference）也能通过对非确定分布的估计应用在GAN中；</li><li>基于似然的模型（Likelihood-based Models）和非似然模型（Likelihood-free Models）能够通过对抗学习结合起来；</li><li>作者根据文中提出的新观点修改了VAE的损失函数，将其称之为Auto-encoding GAN (<span class="math inline">\(\alpha\)</span>-GAN)，并提出了对应的实用的改进；</li><li>本文与众多State-of-Art模型进行了对比</li></ul><h1 id="methodology">Methodology</h1><h2 id="overcoming-intractability-in-generative-models">Overcoming Intractability in Generative Models</h2><h3 id="latent-variable-models">Latent Variable Models</h3><p>隐变量模型通过隐变量的形式描述了数据的产生过程。最简单的形式是假设隐变量<span class="math inline">\(\mathbf{z}\)</span>服从一个先验分布<span class="math inline">\(\mathbf{z}\sim p(\mathbf{z})\)</span>，而数据<span class="math inline">\(\mathbf{x}\)</span>从条件分布<span class="math inline">\(p(\mathbf{x}|\mathbf{z})\)</span>抽样产生。通常来说描述<span class="math inline">\(p(\mathbf{x}|\mathbf{z})\)</span>的模型称为生成器<span class="math inline">\(\mathcal{G}_\theta(\mathbf{z})\)</span>，带有可优化的参数<span class="math inline">\(\theta\)</span>，而<span class="math inline">\(\mathbf{z}\)</span>通常假设为正态分布<span class="math inline">\(\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\)</span>。</p><p>文中区分了两种隐变量模型，一种是<em>Implicit Latent Variable Models</em>，一种是<em>Prescribed Latent Variable Models</em>。文中的描述不太清楚，个人认为两者的区别是前者图模型中的<span class="math inline">\(\mathbf{x}\)</span>不是一个随机变量，在优化的时候需要用一个刻画生成的<span class="math inline">\(\mathbf{x}\)</span>和真实的<span class="math inline">\(\mathbf{x}\)</span>的差别的函数<span class="math inline">\(\delta(\mathbf{x}-\mathcal{G}_\theta(\mathbf{z}))\)</span>，而后者图模型中<span class="math inline">\(\mathbf{x}\)</span>是一个随机变量，这样可以写出似然函数用极大似然估计。</p><p>无论是GAN还是VAE都需要通过边缘分布<span class="math inline">\(p_\theta(\mathbf{x})\)</span>来刻画建模的好坏，比如说根据<span class="math inline">\(p_\theta(\mathbf{x})\)</span>与真实分布<span class="math inline">\(p^*_\theta(\mathbf{x})\)</span>之间的KL散度<span class="math inline">\(\text{KL}\left[p_\theta(\mathbf{x})\parallel p_\theta^*(\mathbf{x})\right]\)</span>。但通常情况下<span class="math inline">\(p_\theta(\mathbf{x})\)</span>都是不可解的，而GAN和VAE通过不同的途径解决了这个问题。</p><h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3><p>GAN没有直接计算<span class="math inline">\(p_\theta(\mathbf{x})\)</span>，而是使用了一个判别器来判别样本是从<span class="math inline">\(p_\theta(\mathbf{x})\)</span>还是<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>采样得到的，如果判别器无法进行区分，那我们认为此时<span class="math inline">\(p_\theta(\mathbf{x})\approx p_\theta^*(\mathbf{x})\)</span>。</p><p>令随机变量<span class="math inline">\(y\in\{0,1\}\)</span>，<span class="math inline">\(y=1\)</span>表示样本<span class="math inline">\(\mathbf{x}\)</span>来自真实分布，<span class="math inline">\(y=0\)</span>表示样本<span class="math inline">\(\mathbf{x}\)</span>来自生成分布，而判别器的输出为<span class="math inline">\(\mathbf{x}\)</span>来自真实分布的概率<span class="math inline">\(\mathcal{D}_\phi(\mathbf{x})=p(y=1|\mathbf{x})\)</span>。GAN通过对来自真实分布和生成分布的样本求二元交叉熵来作为判别器损失函数： <span class="math display">\[\textbf{Discriminator Loss: }\mathbb{E}_{p^*(\mathbf{x})}[-\log\mathcal{D}_\phi(\mathbf{x})]+\mathbb{E}_{p_\theta(\mathbf{x})}[-\log(1-\mathcal{D}_\phi(\mathbf{x}))]\]</span></p><p>生成器将最大化判别器对生成样本判定为真的概率作为损失函数，同时还有一个等价的但在实践中表现更好的替代版本：</p><p><span class="math display">\[\textbf{Generator Loss: }\mathbb{E}_{p_\theta(\mathbf{x})}[\log(1-\mathcal{D}_\phi(\mathbf{x}))];\textbf{ Alternative Loss: }\mathbf{E}_{p_\theta(\mathbf{x})}[-\log\mathcal{D}_\phi(\mathbf{x})]\]</span></p><h3 id="the-density-ratio-trick">The Density Ratio Trick</h3><p>令<span class="math inline">\(p^*(\mathbf{x})=p(\mathbf{x}|y=1)\)</span>，<span class="math inline">\(p_\theta(\mathbf{x})=p(\mathbf{x}|y=0)\)</span>。定义<em>Density Ratio</em> <span class="math inline">\(r_\phi(\mathbf{x})\)</span>为真实分布和生成分布之间的比例：</p><p><span class="math display">\[r_\phi(\mathbf{x})=\frac{p^*(\mathbf{x})}{p_\theta(\mathbf{x})}=\frac{p(\mathbf{x}|y=1)}{p(\mathbf{x}|y=0)}=\frac{p(y=1|\mathbf{x})}{p(y=0|\mathbf{x})}=\frac{\mathcal{D}_\phi(\mathbf{x})}{1-\mathcal{D}_\phi(\mathbf{x})}\]</span></p><p>上式表明了<em>Density Ratio</em>的计算可以仅通过从两个分布上采样得到的样本加上一个二分类器<span class="math inline">\(\mathcal{D}_\phi(\mathbf{x})\)</span>实现（假设<span class="math inline">\(p(y=0)=p(y=1)\)</span>）。更深入的说，对于不可解的分布<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>，我们可以通过计算<em>Density Ratio</em>来了解我们近似的分布<span class="math inline">\(p_\theta(\mathbf{x})\)</span>和真实的<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>之间的相对性。而且我们只需要能够在两个分布上进行采样，并且训练一个判别器即可。因为判别器是一个普通的分类器，所以大量的主流分类器都可以使用。</p><h3 id="variational-inference">Variational Inference</h3><p>现在来看VAE，另一种解决不可解分布的方法是近似。<em>Variational Inference</em>通过引入一个变分分布<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>推出了不可解的<span class="math inline">\(\mathbf{x}\)</span>的对数似然的下界（常被称为证据下界ELBO）：</p><p><span class="math display">\[\log p_\theta(\mathbf{x})=\log\int p_\theta(\mathbb{x}|\mathbb{z})p(\mathbf{z})\text{d}\mathbf{z}\geq \mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log p_\theta(\mathbf{x}|\mathbf{z})\right]-\text{KL}\left[q_\eta(\mathbf{z}|\mathbf{x})\parallel p(\mathbf{z})\right]=\mathcal{F}(\boldsymbol{\theta}, \boldsymbol{\eta})\]</span></p><p>VAE是<em>Variational Inference</em>的一种实现，变分分布通过一个神经网络进行建模，并且建立起了完整的可优化的模型。</p><h3 id="synthetic-likelihood">Synthetic Likelihood</h3><p>当似然函数未知（GAN中没有显式的似然函数，而VAE中有）的时候，<em>Variational Inference</em>便无法直接使用。对于没有显式的似然函数的情况，以VAE的ELBO的第一项为例，假设<span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\)</span>分布的具体形式未知，我们只有从<span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\)</span>采样得到的样本，如何计算<span class="math inline">\(\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]\)</span>呢？一个方法是乘以<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>再除以<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>：</p><p><span class="math display">\[\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]=\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{p_\theta(\mathbf{x}|\mathbf{z})}{p^*(\mathbf{x})}\right]+\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p^*(\mathbf{x})]\]</span></p><p>公式(5)中的第一项包括了合成似然<span class="math inline">\(R(\theta)=\frac{p_\theta(\mathbf{x}|\mathbf{z})}{p^*(\mathbf{x})}\)</span>，优化<span class="math inline">\(R(\theta)\)</span>相当于优化<span class="math inline">\(\log p_\theta(\mathbf{x}|\mathbf{z})\)</span>。第二项与生成网络的参数<span class="math inline">\(\theta\)</span>无关，所以在优化的时候可以忽略。</p><h2 id="a-fusion-of-variational-and-adversarial-learning">A Fusion of Variational and Adversarial Learning</h2><p>GAN和VAE分别从不同的角度解决了生成模型的推断问题，我们下面从VAE出发，考虑将两者结合起来。</p><h3 id="implicit-variational-distributions">Implicit Variational Distributions</h3><p>变分推断<strong>Variational Inference</strong>的主要任务就是确定<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>，通常的做法如<strong>Mean-field Variational Inference</strong>会假设一个简单的分布，如高斯分布。在本文中不对<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>的形式作假设，仅假设其为一个隐含的分布。运用上文提到的<em>Density Ratio Trick</em>，我们可以将VAE损失函数中的第二项改写为：</p><p><span class="math display">\[-\text{KL}[q_\eta(\mathbf{z}|\mathbf{x})\parallel p(\mathbf{z})]=\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{p(\mathbf{z})}{q_\eta(\mathbf{z}|\mathbf{x})}\right]\approx\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}{1-\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}\right]\]</span></p><p>文中引入了一个隐变量分类器（Latent Classifier）<span class="math inline">\(\mathcal{C}_{\boldsymbol{\omega}}(\mathbf{z})\)</span>，用来判别<span class="math inline">\(\mathbf{z}\)</span>是从编码网络还是从标准高斯分布中采样得到的（猜测这样做的好处是不用再对<span class="math inline">\(\mathbf{z}\)</span>的后验做高斯分布的假设了，也不需要在变分网络输出形成的高斯分布上采样得到<span class="math inline">\(\mathbf{z}\)</span>了，这样重参数技巧也省了）。具体实现上，期望可以用蒙特卡洛方法（采样多次取均值）进行计算。</p><h3 id="likelihood-choice">Likelihood Choice</h3><p>对于VAE损失函数第一项，对应生成网络，我们可以选择对<span class="math inline">\(p(\mathbf{x}|\mathbf{z})\)</span>分布的具体形式做假设， 这样对应<em>Likelihood-based</em>的情况。文中选择的是<em>Zero-mean Laplace Distribution</em> <span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\propto\exp(-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1)\)</span>（不就是<span class="math inline">\(L_1\)</span> Loss吗？？？）。</p><p>对于<em>Likelihood-free</em>的情况，可以继续使用上面提到的<em>Density Ratio Trick</em>，这时需要加一个一个判别器。</p><p><span class="math display">\[\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1\right]\space\space\text{  or  }\space\space\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}{1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}\right]\]</span></p><p>对于两种选择，前者对应VAE，好处是不会出现模式崩溃的情况，后者对应GAN，容易出现模式崩溃的情况，但是可以使用对抗学习的方式（这是优点？？？），本文选择两种都用（我全都要.jpg）。</p><h3 id="hybrid-loss-functions">Hybrid Loss Functions</h3><p>将前面的讨论结合起来，最后的损失函数就是：</p><p><span class="math display">\[\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\eta})=\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1+\log\frac{\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}{1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}+\log\frac{\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}{1-\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}\right]\]</span></p><p>最后模型包含四个网络：生成网络<span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\)</span>、推断网络<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>以及两个判别器<span class="math inline">\(\mathcal{C}_{\boldsymbol{\omega}}\)</span>和<span class="math inline">\(\mathcal{D}_\phi\)</span>，作者将其命名为<span class="math inline">\(\alpha\)</span>-GAN。</p><p>算法流程如下：</p><p><img src="https://i.loli.net/2020/06/25/hkIY9xuDWl4GbLz.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="improved-techniques">Improved Techniques</h3><p>作者为了改进模型的稳定性和效率，将生成器的Loss中的<span class="math inline">\(-\log(1-\mathcal{D}_\phi)\)</span>修改为了<span class="math inline">\(\log\mathcal{D}_\phi-\log(1-\mathcal{D}_\phi)\)</span>，并声称这样能提供非饱和（Non-saturating）的梯度：</p><p><span class="math display">\[\textbf{Generator Loss: } \mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1-\log\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))+\log(1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z})))\right]\]</span></p><p>作者认为在生成器损失函数中加入<span class="math inline">\(\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1\)</span>能够在一定程度防止模式崩溃。</p><p>除此之外，作者发现将真实样本（原文是The Samples）作为生成的样本输入到判别器中能够提升性能。作者给出的解释是根据Jensen不等式：<span class="math inline">\(\log p_\theta(\mathbf{x})=\log\int p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})\text{d}\mathbf{z}\geq \mathbb{E}_{p(\mathbf{z})}[\log p_\theta(\mathbf{x}|\mathbf{z})]\)</span>，</p><p>[TODO]</p><h2 id="related-work">Related Work</h2><p>[TODO]</p><p><img src="https://i.loli.net/2020/06/25/TKt3yUwPDQm4MeV.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p><img src="https://i.loli.net/2020/06/25/5jg9wNMoXuTzeVY.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><h1 id="experiments">Experiments</h1><p>[TODO]</p><h2 id="metrics">Metrics</h2><p>本文使用了几种不同的评测生成模型的方法：</p><ul><li><strong>Inception Score: </strong></li><li><strong>Multi-scale Structural Similarity (MS-SSIM): </strong></li><li><strong>Independent Wasserstein Critic: </strong></li></ul><h2 id="results-on-colormnist">Results on ColorMNIST</h2><p><img src="https://i.loli.net/2020/06/25/QRBKHn51fxO2WUq.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2020/06/25/kM4lrsBVSDKaJwW.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><h2 id="results-on-celeba">Results on CelebA</h2><p><img src="https://i.loli.net/2020/06/25/mzdjY67uH9pZ1ny.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h2 id="results-on-cifar-10">Results on CIFAR-10</h2><p><img src="https://i.loli.net/2020/06/25/nDGgM6AHVyKwIam.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p><img src="https://i.loli.net/2020/06/25/K1RQdMyj8zbqlCJ.png" srcset="/img/loading.gif" style="zoom:50%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>GAN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Variational Inference</tag>
      
      <tag>VAE</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anomaly Detection in Streams with Extreme Value Theory</title>
    <link href="/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/"/>
    <url>/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文基于<strong>Extreme Value Theory</strong>提出了一种不需要手动设置阈值也不需要对数据分布作任何假设的时间序列异常检测方法。除此之外，本方法可以用在通用的自动阈值选择的场合中。</p><p><a href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-in-streams-with-extreme-value-theory" target="_blank" rel="noopener">原文</a></p><h1 id="background">Background</h1><p>在很多情况下我们需要进行阈值的选择。阈值的选择可以通过实验的方法或者对数据分布进行假设的方法来得到，不过这样做通常不准确。借助<strong>Extreme Value Theory</strong>我们可以在不需要对原始数据的分布作很强的假设的情况下，推断我们想要的极端事件的分布（在异常检测中就是异常值）。</p><p>下面给出一些数学符号，<span class="math inline">\(X\)</span>为随机变量，<span class="math inline">\(F\)</span>为累积分布函数，即<span class="math inline">\(F(x)=\mathbb{P}(X\leq x)\)</span>。记<span class="math inline">\(F\)</span>的“末尾”分布<span class="math inline">\(\bar{F}(x)=1-F(x)=\mathbb{P}(X&gt;x)\)</span>。对于一个随机变量<span class="math inline">\(X\)</span>和给定的概率<span class="math inline">\(q\)</span>，记<span class="math inline">\(z_q\)</span>为在<span class="math inline">\(1-q\)</span>水平的分位数，即<span class="math inline">\(z_q\)</span>为满足<span class="math inline">\(\mathbb{P}(X\leq z_q)\geq 1-q\)</span>最小的值。</p><h2 id="extreme-value-distributions">Extreme Value Distributions</h2><p><strong>Extreme Value Theory</strong>主要是为了找出极端事件发生的规律，有学者证明，在很弱的条件下，所有极端事件都服从一个特定的分布，而不管原始分布如何。具体形式如下：</p><p><span class="math display">\[G_\gamma:x\mapsto \exp(-(1+\gamma x)^{-\frac{1}{\gamma}}), \space\space\space\space\space\gamma\in\mathbb{R}, \space\space\space\space\space 1+\gamma x&gt;0\]</span></p><p>其中<span class="math inline">\(\gamma\)</span>称为<strong>Extreme Value Index</strong>，由原始分布决定。</p><p>更严谨的说法是Fisher-Tippett-Gnedenko定理（极值理论第一定理）：</p><blockquote><p><strong>THEOREM: </strong>(Fisher-Tippett-Gnedenko). 令<span class="math inline">\(X_1,X_2,\cdots,X_n,\cdots\)</span>为独立同分布的随机变量序列，<span class="math inline">\(M_n=\max \{X_1,\cdots,X_n\}\)</span>。如果实数对序列<span class="math inline">\((a_n,b_n)\)</span>存在且满足<span class="math inline">\(a_n&gt;0\)</span>和<span class="math inline">\(\lim\limits_{n\rightarrow \infty}P\left(\frac{M_n-b_n}{a_n}\leq x\right)=F(x)\)</span>，其中<span class="math inline">\(F\)</span>为非退化分布函数，那么<span class="math inline">\(F\)</span>属于Gumbel、Fréchet或Weibull分布族（或总称Generalized Extreme Value Distribution）中的一种。</p></blockquote><p>这是一个反直觉的结论，但是想到当事件发生变得极端时，即<span class="math inline">\(\mathbb{P}(X&gt;x)\rightarrow 0\)</span>，<span class="math inline">\(\bar{F}(x)=\mathbb{P}(X&gt;x)\)</span>分布的形状其实并没有很多种选择。Table 1展示了几种不同分布对应的<span class="math inline">\(\gamma\)</span>：</p><p><img src="https://i.loli.net/2020/06/24/jyhoWZGc2gFTrJv.png" srcset="/img/loading.gif" /></p><p>Figure 1展示了几种不同<span class="math inline">\(\gamma\)</span>情况下的“末尾”分布：</p><p><img src="https://i.loli.net/2020/06/24/4rmZL1AMcBJ2Vzq.png" srcset="/img/loading.gif" /></p><h2 id="power-of-evt">Power of EVT</h2><p>根据<strong>Extreme Value Theory</strong>，我们可以在原始分布未知的情况下计算极端事件的概率。但是<span class="math inline">\(\bar{G}_\gamma\)</span>分布中参数<span class="math inline">\(\gamma\)</span>是未知的，我们需要一种高效的方法来进行估计。<strong>The Peaks-Over-Threshold</strong> (POT) 方法是本文介绍的一种方法。</p><p><img src="https://i.loli.net/2020/06/24/hX2T1IkMAqfioZl.png" srcset="/img/loading.gif" /></p><h2 id="peaks-over-threshold-approach">Peaks-Over-Threshold Approach</h2><p>POT方法依赖于Pickands-Balkema-De Haan定理（极值理论第二定理），维基百科版：</p><blockquote><p>考虑一个未知分布<span class="math inline">\(F\)</span>和随机变量<span class="math inline">\(X\)</span>，我们的目标是估计<span class="math inline">\(X\)</span>在超过确定阈值<span class="math inline">\(u\)</span>下的条件分布<span class="math inline">\(F_u\)</span>，定义为： <span class="math display">\[F_u(y)=P(X-u\leq y|X&gt;u)=\frac{F(u+y)-F(u)}{1-F(u)}\]</span> 其中<span class="math inline">\(0\leq y\leq x_F-u\)</span>，<span class="math inline">\(x_F\)</span>为<span class="math inline">\(F\)</span>的右端点。<span class="math inline">\(F_u\)</span>描述了超过特征阈值<span class="math inline">\(u\)</span>的分布，称为<strong>Conditional Excess Distribution Function</strong>。</p><p><strong>STATEMENT: </strong>(Pickands-Balkema-De Haan). 设<span class="math inline">\((X_1,X_2,\cdots)\)</span>为独立同分布随机变量序列，<span class="math inline">\(F_u\)</span>为相应的Conditional Excess Distribution Function。对于一大类的<span class="math inline">\(F\)</span>和很大的<span class="math inline">\(u\)</span>，<span class="math inline">\(F_u\)</span>能够很好的被Generalized Pareto Distribution所拟合： <span class="math display">\[F_u(y)\rightarrow G_{k,\sigma}(y),\space\space \text{as } u\rightarrow \infty\]</span> 其中： <span class="math display">\[G_{k,\sigma}(y)=\begin{cases}1-(1+ky/\sigma)^{-1/k}, &amp;\text{if }k\neq 0\\1-e^{-y/\sigma}, &amp;\text{if }k=0\end{cases}\]</span> 当<span class="math inline">\(k\geq 0\)</span>时<span class="math inline">\(\sigma&gt;0, y\geq 0\)</span>，<span class="math inline">\(k&lt;0\)</span>时<span class="math inline">\(0\leq y\leq -\sigma/k\)</span>。</p></blockquote><p>论文中给出的定理如下：</p><blockquote><p><strong>THEOREM: </strong>(Pickands-Balkema-De Haan). 累积概率密度函数<span class="math inline">\(F\in\mathcal{D}_\gamma\)</span>当且仅当函数<span class="math inline">\(\sigma\)</span>存在时，对所有<span class="math inline">\(x\in\mathbb{R}\)</span>在<span class="math inline">\(1+\gamma x&gt;0\)</span>的条件下有： <span class="math display">\[\frac{\bar{F}(t+\sigma(t)x)}{\bar{F}(t)}\mathop{\rightarrow}\limits_{t\rightarrow\tau}(1+\gamma x)^{-\frac{1}{\gamma}}\]</span></p></blockquote><p>上式可以写成如下形式： <span class="math display">\[\bar{F}_t(x)=\mathbb{P}(X-t&gt;x|X&gt;t)\mathop{\sim}\limits_{t\rightarrow\tau}\left(1+\frac{\gamma x}{\sigma(t)}\right)^{-\frac{1}{\gamma}}\]</span> 该式表明<span class="math inline">\(X\)</span>超过阈值<span class="math inline">\(t\)</span>的概率（写为<span class="math inline">\(X-t\)</span>）服从<strong>Generalized Pareto Distribution</strong> (GPD)，参数为<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\sigma\)</span>。POT主要是拟合GPD而不是EVT分布。</p><p>如果我们要估计参数<span class="math inline">\(\hat{\gamma}\)</span>和<span class="math inline">\(\hat{\sigma}\)</span>，分位数可以通过下式计算得到： <span class="math display">\[z_q\simeq t+\frac{\hat{\sigma}}{\hat{\gamma}}\left(\left(\frac{qn}{N_t}\right)^{-\hat{\gamma}}-1\right)\]</span></p><p>其中<span class="math inline">\(t\)</span>是一个“很高”的阈值，<span class="math inline">\(q\)</span>是给定的概率值，<span class="math inline">\(n\)</span>是所有观测样本的数量，<span class="math inline">\(N_t\)</span>是peaks的数量，即<span class="math inline">\(X_i&gt;t\)</span>的数量。为了进行高效的参数估计，文中使用了极大似然估计。</p><h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2><p>设<span class="math inline">\(X_1,\cdots,X_n\)</span>为独立同分布的随机变量，概率密度函数记为<span class="math inline">\(f_\theta\)</span>，<span class="math inline">\(\theta\)</span>为分布中的参数，那么似然函数可以写为：</p><p><span class="math display">\[\mathcal{L}(X_1,\cdots,X_n;\theta)=\prod\limits_{i=1}^n f_\theta(X_i)\]</span></p><p>在极大似然估计中，我们需要找到合适的参数使得似然函数最大化。在我们的问题中，似然函数如下： <span class="math display">\[\log\mathcal{L}(\gamma,\sigma)=-N_t\log\sigma-\left(1+\frac{1}{\gamma}\right)\sum\limits_{i=1}^{N_t}\log\left(1+\frac{\gamma}{\sigma}Y_i\right)\]</span> 其中<span class="math inline">\(Y_i&gt;0\)</span>表示<span class="math inline">\(X_i\)</span>超过阈值<span class="math inline">\(t\)</span>的部分。</p><p>文中使用了<strong>Grimshaw's Trick</strong>来将含两个参数的优化问题转换为只含一个参数的优化问题。记<span class="math inline">\(\ell(\gamma,\sigma)=\log\mathcal{L}(\gamma,\sigma)\)</span>，对于所有极值来说有<span class="math inline">\(\nabla \ell(\gamma, \sigma)=0\)</span>。Grimshaw's Trick表明对于满足<span class="math inline">\(\nabla \ell(\gamma, \sigma)=0\)</span>的一对<span class="math inline">\((\gamma^*,\sigma^*)\)</span>，<span class="math inline">\(x^*=\frac{\gamma^*}{\sigma^*}\)</span>为等式<span class="math inline">\(u(X)v(X)=1\)</span>的解，其中： <span class="math display">\[\begin{align}u(x)&amp;=\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\frac{1}{1+xY_i}\\v(x)&amp;=1+\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\log(1+xY_i)\end{align}\]</span> 在找到满足该等式的解<span class="math inline">\(x^*\)</span>后，我们可以得到<span class="math inline">\(\gamma^*=v(x^*)-1\)</span>和<span class="math inline">\(\sigma^*=\gamma^*/x^*\)</span>，于是问题就变成了如何寻找方程的所有根。</p><p>因为<span class="math inline">\(\log\)</span>的存在，所以有<span class="math inline">\(1+xY_i&gt;0\)</span>。而<span class="math inline">\(Y_i\)</span>是正数，所以<span class="math inline">\(x^*\)</span>的范围一定在<span class="math inline">\(\left(-\frac{1}{Y^M},+\infty\right)\)</span>，其中<span class="math inline">\(Y^M=\max Y_i\)</span>。</p><p>Grimshaw（作者参考的一篇<a href="https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485040" target="_blank" rel="noopener">论文</a>）还给出了一个上界： <span class="math display">\[x^*_{\text{max}}=2\frac{\bar{Y}-Y^m}{(Y^m)^2}\]</span> 其中<span class="math inline">\(Y^m=\min Y_i\)</span>，<span class="math inline">\(\bar{Y}\)</span>为<span class="math inline">\(Y_i\)</span>的均值。详细的优化方法会在下文讨论。</p><p>背景部分到此结束，接下来的部分就是作者提出的新方法。</p><h1 id="methodology">Methodology</h1><p>Extreme Value Theory给出了在对原始分布未知的情况下估计使得<span class="math inline">\(\mathbb{P}(X&gt;z_q)&lt;q\)</span>的<span class="math inline">\(z_q\)</span>的方法。</p><p>本文据此提出了时间序列流的异常检测方法。首先根据已知的观测值<span class="math inline">\(X_1,\cdots,X_n\)</span>得到阈值<span class="math inline">\(z_q\)</span>，然后根据数据的特性运用两种不同方法来更新<span class="math inline">\(z_q\)</span>。对于平稳时间序列，使用SPOT；对于非平稳时间序列，使用DSPOT。</p><h2 id="initialization-step">Initialization Step</h2><p>在进行异常检测之前，需要根据已有的观测数据进行<span class="math inline">\(z_q\)</span>的估计。给定<span class="math inline">\(n\)</span>个观测值<span class="math inline">\(X_1,\cdots,X_n\)</span>和一个固定的概率值<span class="math inline">\(q\)</span>，我们的目标是估计阈值<span class="math inline">\(z_q\)</span>使得<span class="math inline">\(\mathbb{P}(X&gt;z_q)&lt;q\)</span>。其主要流程是首先设定一个较大的阈值<span class="math inline">\(t\)</span>，然后通过拟合GPD分布来计算<span class="math inline">\(z_q\)</span>。过程如下图所示：</p><p><img src="https://i.loli.net/2020/06/24/fzeC8vuDtA6mEdl.png" srcset="/img/loading.gif" /></p><p>算法流程如下所示：</p><p><img src="https://i.loli.net/2020/06/24/AEQpnZPiW3C4mr7.png" srcset="/img/loading.gif" /></p><p><span class="math inline">\(Y_t\)</span>代表大于<span class="math inline">\(t\)</span>的观测值的集合，GPD分布的拟合使用了前文提到的Grimshaw's Trick。</p><h2 id="finding-anomalies-in-a-stream">Finding Anomalies in a Stream</h2><p>通过Initialization Step使用POT算法得到的<span class="math inline">\(z_q\)</span>，我们定义其为"Normality Bound"，用于后面的检测。在后面的步骤中，我们会根据新得到的观测值来更新<span class="math inline">\(z_q\)</span>。</p><h3 id="stationary-case">Stationary Case</h3><p>我们首先来讨论时间序列没有时间依赖性的情况（<span class="math inline">\(X_1,\cdots,X_n\)</span>之间独立同分布）。通过POT算法对所有观测值得到<span class="math inline">\(z_q\)</span>之后，Streaming POT (SPOT) 算法会检查<span class="math inline">\(X_n\)</span>之后的值（数据流场景，<span class="math inline">\(X_1,\cdots,X_n\)</span>是历史数据，还会有新的数据进来），如果大于<span class="math inline">\(z_q\)</span>，则将<span class="math inline">\(X_i\)</span>加入异常点集合中；如果大于<span class="math inline">\(t\)</span>但小于<span class="math inline">\(z_q\)</span>，则将<span class="math inline">\(X_i\)</span>加入观测值集合中，更新<span class="math inline">\(z_q\)</span>；其他情况我们<span class="math inline">\(X_i\)</span>是正常情况。算法流程图如下：</p><p><img src="https://i.loli.net/2020/06/24/h5yKnlCAYxbHu2R.png" srcset="/img/loading.gif" /></p><h3 id="drifting-case">Drifting Case</h3><p>SPOT算法只适用于平稳分布的情况，但在现实生活中这样的假设过强了。于是作者提出了能处理时间依赖性的Streaming POT with Drift (DSPOT) 算法。</p><p><img src="https://i.loli.net/2020/06/25/O49XwQvVGH7k1ri.png" srcset="/img/loading.gif" /></p><p>在DSPOT中，我们不使用<span class="math inline">\(X_i\)</span>的绝对值，而是用相对值<span class="math inline">\(X^\prime_i=X_i-M_i\)</span>，其中<span class="math inline">\(M_i\)</span>是<span class="math inline">\(i\)</span>时刻的局部特征，如Figure 4所示。最简单的实现是使用局部均值，即<span class="math inline">\(M_i=(1/d)\cdot\sum\limits_{k=1}^d X_{i-k}^*\)</span>，<span class="math inline">\(X_{i-1}^*,\cdots,X_{i-d}^*\)</span>是长度为<span class="math inline">\(d\)</span>的窗口。我们假设<span class="math inline">\(X^\prime_i\)</span>服从平稳分布的假设。</p><p>算法流程图如下所示：</p><p><img src="https://i.loli.net/2020/06/25/P6hOsD9dnNIHvUV.png" srcset="/img/loading.gif" /></p><h2 id="numerical-optimization">Numerical Optimization</h2><p>现在剩下的问题就是优化了，前文已经提到对GPD的拟合已经被优化成一个参数的优化问题，下面将会详细讨论优化算法。</p><h3 id="reduction-of-the-optimal-parameters-search">Reduction of the Optimal Parameters Search</h3><p>前文已经得到了一个初步的<span class="math inline">\(x^*\)</span>的Bound，即<span class="math inline">\(x^*&gt;-\frac{1}{Y^M}\)</span>和<span class="math inline">\(x^*\leq 2\frac{\bar{Y}-Y^m}{(Y^m)^2}\)</span>，下面将给出一个更严格的Bound。</p><blockquote><p><strong>PROPOSITION: </strong>如果<span class="math inline">\(x^*\)</span>是<span class="math inline">\(u(x)v(x)=1\)</span>的解，那么： <span class="math display">\[x^*\leq 0 \text{ or } x^*\geq 2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m}\]</span></p></blockquote><p>证明见论文原文。</p><p>这样<span class="math inline">\(x^*\)</span>的范围就进一步缩小了，于是有<span class="math inline">\(u(x)v(X)=1\)</span>的解<span class="math inline">\(x^*\)</span>在以下范围之内： <span class="math display">\[\left(-\frac{1}{Y^M},0\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]\]</span></p><h3 id="how-can-we-maximize-the-likelihood-function">How Can We Maximize the Likelihood Function?</h3><p>接下来是优化的具体实现问题。文中首先设定了一个很小的值<span class="math inline">\(\epsilon&gt;0\space(\sim 10^{-8})\)</span>，然后在下面的范围内寻找函数<span class="math inline">\(w:x\mapsto u(x)v(x)-1\)</span>的根： <span class="math display">\[\left[-\frac{1}{Y^M}+\epsilon,-\epsilon\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]\]</span> 作者没有使用现有的寻找函数根的算法，而是转换为如下优化问题： <span class="math display">\[\min\limits_{x_1,\cdots,x_k\in I}\sum\limits_{i=1}^k w(x_k)^2\]</span> 其中<span class="math inline">\(I\)</span>就是<span class="math inline">\(x^*\)</span>的Bound。该问题是一个很典型的优化问题，可以被很多成熟的算法所解决。</p><h3 id="initial-threshold">Initial Threshold</h3><p>在算法的Initialization Step，需要事先设定一个阈值<span class="math inline">\(t\)</span>，如果设定的太大，那么<span class="math inline">\(Y_t\)</span>的数量就会很少。作者给出的建议是保证<span class="math inline">\(t&lt;z_q\)</span>，即<span class="math inline">\(t\)</span>对应的概率值应该小于<span class="math inline">\(1-q\)</span>。</p><h1 id="experiments">Experiments</h1><p>在实验部分，作者在合成数据和真实数据上试验了SPOT算法和DSPOT算法的有效性。</p><h2 id="dspot-reliability">(D)SPOT Reliability</h2><p>作者首先在合成数据上验证SPOT的有效性。具体做法是使用高斯分布生成数据（高斯分布的分位数能够直接计算），然后将SPOT得出的<span class="math inline">\(z_q\)</span>和理论值进行对比。误差定义如下： <span class="math display">\[\text{error rate}=\left|\frac{z^{\text{SPOT}}-z^{\text{th}}}{z^{\text{th}}}\right|\]</span> 下图是采用不同数量观测值的结果：</p><p><img src="https://i.loli.net/2020/06/25/GXlu2MAJaoqxyd4.png" srcset="/img/loading.gif" /></p><h2 id="finding-anomalies-with-spot">Finding Anomalies with SPOT</h2><p>在这一节作者在真实数据集上进行了实验以验证SPOT算法的有效性，结果如下图：</p><p><img src="https://i.loli.net/2020/06/25/wTkZxKarFVDOlp6.png" srcset="/img/loading.gif" /></p><p>在文中作者说算法的True Positive达到了<span class="math inline">\(86\%\)</span>，False Positive小于<span class="math inline">\(4\%\)</span>。</p><p><img src="https://i.loli.net/2020/06/25/RcUnwtHud7DjNXv.png" srcset="/img/loading.gif" /></p><h2 id="finding-anomalies-with-dspot">Finding Anomalies with DSPOT</h2><p>在这一节作者使用DSPOT在真实数据集上进行了实验。窗口大小<span class="math inline">\(d=450\)</span>，预设的风险概率值<span class="math inline">\(q=10^{-3}\)</span>。结果如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/lIxqpnGtL7feVKs.png" srcset="/img/loading.gif" /></p><p>在图中可以看出在<span class="math inline">\(8000\)</span> Minutes之后上界显著提高，作者分析了原因，认为是因为超过阈值<span class="math inline">\(t\)</span>的点<span class="math inline">\(Y_t\)</span>的存储是全局的，在前<span class="math inline">\(8000\)</span> Minutes算法存储了很多较高的<span class="math inline">\(Y_t\)</span>值，而在<span class="math inline">\(8000\)</span> Minutes之后，真实数据的趋势开始下降，但算法仍是根据全局的<span class="math inline">\(Y_t\)</span>来进行<span class="math inline">\(z_q\)</span>的计算（这一段没有特别明白）。作者给出的修正方法是只保存固定数量的Peaks。</p><p>下图是作者在股票数据上得到的实验结果：</p><p><img src="https://i.loli.net/2020/06/25/VeEo8OPbzyUxXrR.png" srcset="/img/loading.gif" /></p><h2 id="performances">Performances</h2><p>作者还验证了算法的时间效率。</p><p><img src="https://i.loli.net/2020/06/25/Egh7CxsU2TtL6az.png" srcset="/img/loading.gif" /></p><p>表中T代表的是每个Iteration的时间，M代表的是Peaks的比例，"bi-"前缀代表的是同时计算上界和下界。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Introduction to Variational Autoencoders</title>
    <link href="/2019/10/22/An-Introduction-to-Variational-Autoencoders/"/>
    <url>/2019/10/22/An-Introduction-to-Variational-Autoencoders/</url>
    
    <content type="html"><![CDATA[<h1 id="deep-generative-models">Deep Generative Models</h1><p>生成模型是指一系列用于随机生成可观测数据的模型。假设在一个高维空间<span class="math inline">\(\mathcal{X}\)</span>中，存在一个随机向量<span class="math inline">\(\mathbf{X}\)</span>服从一个未知的分布<span class="math inline">\(p_r(x),x\in \mathcal{X}\)</span>。生成模型就是根据一些可观测的样本<span class="math inline">\(x^{(1)},x^{(2)},\cdots,x^{(N)}\)</span>来学习一个参数化的模型<span class="math inline">\(p_\theta(x)\)</span>来近似未知分布<span class="math inline">\(p_r(x)\)</span>。</p><p>生成模型主要用于密度估计和样本生成。</p><hr /><p>密度估计即给定一组数据<span class="math inline">\(\mathcal{D}=\{x^{(i)}\},1\leq i\leq N\)</span>，假设他们都是从相同的概率密度函数<span class="math inline">\(p_r(x)\)</span>独立产生的。密度估计就是根据数据集<span class="math inline">\(\mathcal{D}\)</span>来估计其概率密度函数<span class="math inline">\(p_r(x)\)</span>。</p><p>如果将生成模型用于监督学习，那么就是输出标签的条件概率分布<span class="math inline">\(p(y|x)\)</span>，根据贝叶斯公式：</p><p><span class="math display">\[p(y|x)=\frac{p(x,y)}{\sum_y p(x,y)}\]</span></p><p>问题就变为了联合概率<span class="math inline">\(p(x,y)\)</span>的密度估计问题。</p><hr /><p>样本生成即根据给定的概率分布<span class="math inline">\(p_\theta(x)\)</span>生成一些服从这个分布的样本，即采样。在含隐变量的生成模型中，生成<span class="math inline">\(x\)</span>的过程一般包含两步：</p><ol type="1"><li>根据隐变量的分布<span class="math inline">\(p_\theta(z)\)</span>采样得到<span class="math inline">\(z\)</span>；</li><li>根据条件分布<span class="math inline">\(p_\theta(x|z;\theta)\)</span>进行采样得到<span class="math inline">\(x\)</span>。</li></ol><p>所以在生成模型中的重点是估计条件分布<span class="math inline">\(p(x|z;\theta)\)</span>。</p><h1 id="parameter-estimation-for-hidden-variable-with-em-algorithm">Parameter Estimation for Hidden Variable with EM Algorithm</h1><p>如果图模型中存在隐变量，就需要使用EM算法进行参数估计。</p><p>在一个包含隐变量的图模型中，令<span class="math inline">\(\mathbf{X}\)</span>为可观测变量集合，<span class="math inline">\(\mathbf{Z}\)</span>为隐变量集合，则一个样本<span class="math inline">\(x\)</span>的边际似然函数为：</p><p><span class="math display">\[p(x;\theta)=\sum_z p(x,z;\theta)\]</span></p><p>给定包含<span class="math inline">\(N\)</span>个训练样本的训练集<span class="math inline">\(\mathcal{D}=\{x^{(n)}\},1\leq i\leq N\)</span>，则训练集的对数边际似然为：</p><p><span class="math display">\[\begin{align}\mathcal{L}(\mathcal{D};\theta)&amp;=\frac{1}{N}\sum_{n=1}^N \log p(x^{(n)};\theta)\\&amp;=\frac{1}{N}\sum_{n=1}^N \log \sum_z p(x^{(n)},z;\theta)\end{align}\]</span></p><hr /><p>这时，只要最大化整个训练集的对数边际似然<span class="math inline">\(\mathcal{L}(\mathcal{D};\theta)\)</span>，即可估计出最优的参数<span class="math inline">\(\theta^*\)</span>。不过在计算梯度的时候，需要在对数函数内部进行求和或积分计算。为了更好的计算<span class="math inline">\(\log p(x;\theta)\)</span>，我们引入一个额外的变分函数<span class="math inline">\(q(z)\)</span>，<span class="math inline">\(q(z)\)</span>为定义在隐变量<span class="math inline">\(z\)</span>上的分布。样本<span class="math inline">\(x\)</span>的对数边际似然函数为：</p><p><span class="math display">\[\begin{align}\log p(x;\theta)&amp;=\log \sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\\&amp;\geq\sum_z q(z)\log \frac{p(x,z;\theta)}{q(z)}\\&amp;\triangleq ELBO(q,x;\theta)\end{align}\]</span></p><p>其中<span class="math inline">\(ELBO(q,x;\theta)\)</span>为对数边际似然函数<span class="math inline">\(\log p(x;\theta)\)</span>的下界，称为证据下界。公式中使用了Jensen不等式(即对于凹函数<span class="math inline">\(g\)</span>，有<span class="math inline">\(g(\mathbb{E}[x])\geq\mathbb{E}[g(X)]\)</span>)。在这里，<span class="math inline">\(\frac{p(x,z;\theta)}{q(z)}\)</span>可视为<span class="math inline">\(q(z)\)</span>的函数，记为<span class="math inline">\(f(q(z))\)</span>，那么<span class="math inline">\(f(q(z))\)</span>的期望即<span class="math inline">\(\mathbb{E}[f(q(z))]=\sum_z q(z)f(q(z))=\sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\)</span>。而根据Jensen不等式，有<span class="math inline">\(g(\mathbb{E}[f(q(z))])\geq\mathbb{E}[g(f(q(z)))]\Leftrightarrow g(\sum_z q(z)\frac{p(x,z;\theta)}{q(z)})\geq \sum_z q(z)g(\frac{p(x,z;\theta)}{q(z)})\)</span>，在这里<span class="math inline">\(g\)</span>就是对数函数。</p><hr /><p>根据Jensen不等式取等的条件：<span class="math inline">\(\frac{p(x,z;\theta)}{q(z)}=c\)</span>，<span class="math inline">\(c\)</span>为常数，有：</p><p><span class="math display">\[\begin{align}\sum_z p(x,z;\theta)&amp;=c\sum_z q(z)\\\Leftrightarrow\sum_z p(x,z;\theta)&amp;=c\cdot1\end{align}\]</span></p><p>因此：</p><p><span class="math display">\[\begin{align}q(z)&amp;=\frac{p(x,z;\theta)}{\sum_z p(x,z;\theta)}\\&amp;=\frac{p(x,z;\theta)}{p(x;\theta)}\\&amp;=p(z|x;\theta)\end{align}\]</span></p><p>所以，当且仅当<span class="math inline">\(q(z)=p(z|x;\theta)\)</span>时，<span class="math inline">\(\log p(x;\theta)=ELBO(q,x;\theta)\)</span>。</p><hr /><p>于是最大化对数边际似然函数<span class="math inline">\(\log p(x;\theta)\)</span>的过程可以分解为两个步骤：</p><ol type="1"><li>先找到近似分布<span class="math inline">\(q(z)\)</span>使得<span class="math inline">\(\log p(x;\theta)=ELBO(q,x;\theta)\)</span>；</li><li>再寻找参数<span class="math inline">\(\theta\)</span>最大化<span class="math inline">\(ELBO(q,x;\theta)\)</span>。</li></ol><p>这就是期望最大化(Expectation-Maximum,EM)算法。</p><hr /><p>EM算法通过迭代的方法，不断重复直到收敛到某个局部最优解。在第<span class="math inline">\(t\)</span>步更新时，E步和M步分别为：</p><ol type="1"><li><p>E步：固定参数<span class="math inline">\(\theta_t\)</span>，找到一个分布使<span class="math inline">\(ELBO(q,x;\theta_t)\)</span>最大，即等于<span class="math inline">\(\log p(x;\theta_t)\)</span>：<span class="math inline">\(q_{t+1}(z)=\text{arg}_q \max ELBO(q,x;\theta_t)\)</span>；</p></li><li><p>M步：固定<span class="math inline">\(q_{t+1}(z)\)</span>，找到一组参数使得证据下界最大，即：<span class="math inline">\(\theta_{t+1}=\text{arg}_\theta\max ELBO(q_{t+1},x;\theta)\)</span>。</p></li></ol><hr /><p>对数边际似然也可以通过信息论的视角来进行分解：</p><p><span class="math display">\[\begin{align}\log p(x;\theta)&amp;=\sum_z q(z)\log p(x;\theta)\\&amp;=\sum_z q(z)(\log p(x,z;\theta)-\log p(z|x;\theta))\\&amp;=\sum_z q(z)\log\frac{p(x,z;\theta)}{q(z)}-\sum_z q(z)\log\frac{p(z|x;\theta)}{q(z)}\\&amp;=ELBO(q,x;\theta)+D_{KL}(q(z)\parallel p(z|x;\theta))\end{align}\]</span></p><p>其中<span class="math inline">\(D_{KL}(q(z)\parallel p(z|x;\theta))\)</span></p><h1 id="generative-model-with-hidden-variable">Generative Model with Hidden Variable</h1><p>假设一个生成模型包含不可观测的隐变量，其中可观测变量<span class="math inline">\(x\)</span>为一个高维空间中的随机向量，而不可观测的隐变量<span class="math inline">\(z\)</span>为一个相对低维空间中的随机向量。</p><p>这个生成模型的联合概率密度函数可以表达为：</p><p><span class="math display">\[p(x,z;\theta)=p(x|z;\theta)p(z;\theta)\]</span></p><p>其中<span class="math inline">\(p(z;\theta)\)</span>为隐变量<span class="math inline">\(z\)</span>的先验概率分布；<span class="math inline">\(p(x|z;\theta)\)</span>为已知<span class="math inline">\(z\)</span>条件下<span class="math inline">\(x\)</span>的概率分布。通常情况下，我们可以假设<span class="math inline">\(p(z;\theta)\)</span>和<span class="math inline">\(p(x|z;\theta)\)</span>服从某种带参的分布族，其形式已知，而参数可以通过最大似然来进行估计。</p><p>给定一个样本<span class="math inline">\(x\)</span>，其对数边际似然<span class="math inline">\(\log p(x;\theta)\)</span>可以分解为：</p><p><span class="math display">\[\log p(x;\theta)=ELBO(q,x;\theta,\phi)+D_{KL}(q(z;\phi)\parallel p(z|x;\theta))\]</span></p><p>其中<span class="math inline">\(q(z;\phi)\)</span>为额外引入的变分密度函数，<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>为证据下界：</p><p><span class="math display">\[ELBO(q,x;\theta,\phi)=\mathbb{E}_{z\sim q(z;\phi)}[\log{\frac{p(x,z;\theta)}{q(z;\phi)}}]\]</span></p><p>最大化<span class="math inline">\(\log p(x;\theta)\)</span>可以用EM算法来求解：</p><ul><li><strong>E-step:</strong> 寻找一个密度函数<span class="math inline">\(q(z;\phi)\)</span>使其等于或接近于后验密度函数<span class="math inline">\(p(z|x;\theta)\)</span>;</li><li><strong>M-step:</strong> 保持<span class="math inline">\(q(z;\phi)\)</span>固定，寻找<span class="math inline">\(\theta\)</span>来最大化<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>。</li></ul><p>在EM算法的每次迭代中，理论上最优的<span class="math inline">\(q(z;\phi)\)</span>为隐变量的后验概率密度函数<span class="math inline">\(p(z|x;\theta)\)</span>：</p><p><span class="math display">\[p(z|x;\theta)=\frac{p(x|z;\theta)p(z;\theta)}{\int_z p(x|z;\theta)p(z;\theta)\text{d}z}\]</span></p><p>后验密度函数<span class="math inline">\(p(z|x;\theta)\)</span>的计算是一个统计推断的问题，在一般情况下<span class="math inline">\(p(x|z;\theta)\)</span>也比较难以计算。</p><h1 id="variational-autoencoder">Variational Autoencoder</h1><p>变分自编码器(Variational Autoencoder, VAE)的主要思想是利用神经网络来分别建模两个复杂的条件概率密度函数：</p><ol type="1"><li>用神经网络来产生变分分布<span class="math inline">\(q(z;\phi)\)</span>，称为推断网络。推断网络的输入为<span class="math inline">\(x\)</span>，输出为变分分布<span class="math inline">\(q(z|x;\phi)\)</span>；</li><li>用神经网络来产生概率分布<span class="math inline">\(p(x|z;\theta)\)</span>，称为生成网络。生成网络的输入为<span class="math inline">\(z\)</span>，输出为概率分布<span class="math inline">\(p(x|z;\theta)\)</span>。</li></ol><p><img src="https://i.loli.net/2020/06/24/B1d9UtTzNfjG6e2.png" srcset="/img/loading.gif" /></p><p>VAE的图模型如下图所示：</p><p><img src="https://i.loli.net/2020/06/24/GAhy281seQ3tbZT.png" srcset="/img/loading.gif" /></p><h2 id="variational-network">Variational Network</h2><p>假设<span class="math inline">\(q(z|x;\phi)\)</span>是服从对角化协方差的高斯分布：</p><p><span class="math display">\[q(z|x;\phi)=\mathcal{N}(z;\mu_I,\sigma^2_I I)\]</span></p><p>其中<span class="math inline">\(\mu_I\)</span>和<span class="math inline">\(\sigma_I^2\)</span>是高斯分布的均值和方差，可以通过推断网络<span class="math inline">\(f_I(x;\phi)\)</span>来预测：</p><p><span class="math display">\[\left[\begin{matrix}\mu_I\\\sigma_I\end{matrix}\right]=f_I(x;\phi)\]</span> 推断网络<span class="math inline">\(f_I(x;\phi)\)</span>可以是一般的全连接网络或卷积网络，比如一个两层的神经网络：</p><p><span class="math display">\[\begin{align}h&amp;=\sigma(W^{(1)}x+b^{(1)})\\\mu_I&amp;=W^{(2)}h+b^{(2)}\\\sigma_I&amp;=\text{softplus}(W^{(3)}h+b^{(3)})\end{align}\]</span></p><p>其中所有网络参数<span class="math inline">\(\{W^{(1)},W^{(2)},W^{(3)},b^{(1)},b^{(2)},b^{(3)}\}\)</span>即对应了变分参数<span class="math inline">\(\phi\)</span>。</p><hr /><p>推断网络的目标是使得<span class="math inline">\(q(z|x;\phi)\)</span>来尽可能接近真实的后验<span class="math inline">\(p(z|x;\theta)\)</span>，需要找到变分参数<span class="math inline">\(\phi^*\)</span>来最小化两个分布的KL散度：</p><p><span class="math display">\[\phi^*=\text{arg}_\phi\min{D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))}\]</span></p><p>由于<span class="math inline">\(p(z|x;\theta)\)</span>未知，故KL散度无法直接计算，不过由于<span class="math inline">\(D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))=\log p(x;\theta)-ELBO(q,x;\theta,\phi)\)</span>，所以可以直接最大化证据下界，有：</p><p><span class="math display">\[\phi^*=\text{arg}_\phi\max{ELBO(q,x;\theta,\phi)}\]</span></p><h2 id="generative-network">Generative Network</h2><p>生成模型的联合分布可以分解为两部分：隐变量<span class="math inline">\(z\)</span>的先验分布<span class="math inline">\(p(z;\theta)\)</span>和条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>。为简单起见，一般假设隐变量<span class="math inline">\(z\)</span>的先验分布为标准正态分布<span class="math inline">\(\mathcal{N}(z|0,I)\)</span>，隐变量每一维之间都是独立的。条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>可以通过生成网络来建模，我们同样用参数化的分布族来表示条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>，这些分布族的函数可以用生成网络计算得到。根据变量<span class="math inline">\(x\)</span>的类型不同，可以假设<span class="math inline">\(p(x|z;\theta)\)</span>服从不同的分布族。如果<span class="math inline">\(x\in\{0,1\}^d\)</span>是<span class="math inline">\(d\)</span>维的二值向量，可以假设<span class="math inline">\(\log p(x|z;\theta)\)</span>服从多变量的伯努利分布，即：</p><p><span class="math display">\[\begin{align}p(x|z;\theta)&amp;=\prod\limits_{i=1}^d p(x_i|z;\theta)\\&amp;=\prod\limits_{i=1}^d \gamma_i^{x_i}(1-\gamma_i)^{(1-x_i)}\end{align}\]</span></p><p>如果<span class="math inline">\(x\in\mathbb{R}^d\)</span>是<span class="math inline">\(d\)</span>维的连续向量，可以假设<span class="math inline">\(p(x|z;\theta)\)</span>服从对角化协方差的高斯分布，即：</p><p><span class="math display">\[p(x|z;\theta)=\mathcal{N}(x;\mu_G,\sigma_G^2 I)\]</span></p><hr /><p>生成网络的目标是找到一组<span class="math inline">\(\theta^*\)</span>最大化证据下界<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>：</p><p><span class="math display">\[\theta^*=\text{arg}_\theta\max ELBO(q,x;\theta,\phi)\]</span></p><h2 id="model-combination">Model Combination</h2><p>推断网络和生成网络的目标都是最大化证据下界因此总的目标函数为：</p><p><span class="math display">\[\begin{align}\max_{\theta,\phi}ELBO(q,x;\theta,\phi)&amp;=\max_{\theta,\phi}\mathbb{E}_{z\sim q(z;\phi)}[\log\frac{p(x|z;\theta)p(z;\theta)}{q(z;\theta)}]\\&amp;=\max_{\theta,\phi}\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]-D_{KL}(q(z|x;\phi)\parallel p(z;\theta))\end{align}\]</span></p><p>其中先验分布<span class="math inline">\(p(z;\theta)=\mathcal{N}(z|0,I)\)</span>。</p><p>公式中<span class="math inline">\(\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]\)</span>一般通过采样的方式进行计算，最后取平均值。</p><h2 id="model-training">Model Training</h2><p>给定数据集<span class="math inline">\(\mathcal{D}\)</span>，包含<span class="math inline">\(N\)</span>个从未知数据分布中抽取的独立同分布样本<span class="math inline">\(x^{(1)},x^{(2)},\cdots,x^{(N)}\)</span>。变分自编码器的目标函数为：</p><p><span class="math display">\[\mathcal{J}(\phi,\theta|\mathcal{D})=\sum\limits_{n=1}^N(\frac{1}{M}\sum\limits_{m=1}^M\log p(x^{(n)}|z^{(n,m)};\theta)-D_{KL}(q(z|x^{(n)};\phi)\parallel\mathcal{N}(z;0,I)))\]</span></p><p>如果采用随机梯度下降法，每次从数据集中采一个样本<span class="math inline">\(x\)</span>，然后根据<span class="math inline">\(q(z|x;\phi)\)</span>采一个隐变量<span class="math inline">\(z\)</span>，则目标函数变为：</p><p><span class="math display">\[\mathcal{J}(\phi,\theta|x)=\log p(x|z;\theta)-D_{KL}(q(z|x;\phi)\parallel\mathcal{N}(z;0,I))\]</span></p><p>假设<span class="math inline">\(q(z|x;\phi)\)</span>是正态分布，KL散度可直接算出：</p><p><span class="math display">\[D_{KL}(\mathcal{N}(\mu_1,\Sigma_1)\parallel\mathcal(\mu_2,\Sigma_2))\\=\frac{1}{2}(\text{tr}(\sigma_I^2 I)+\mu_I^T\mu_I-d-\log(|\sigma_I^2 I|))\]</span></p><hr /><p>再参数化是将一个参数为<span class="math inline">\(u\)</span>的函数<span class="math inline">\(f(u)\)</span>，通过一个函数<span class="math inline">\(u=g(v)\)</span>，转换为参数为<span class="math inline">\(v\)</span>的函数<span class="math inline">\(\hat{f}(v)=f(g(v))\)</span>。在变分自编码器中，一个问题是如何求随机变量<span class="math inline">\(z\)</span>关于<span class="math inline">\(\phi\)</span>的导数。但由于是采样的方式，无法直接刻画<span class="math inline">\(z\)</span>和<span class="math inline">\(\phi\)</span>之间的函数关系，因此也无法计算导数。</p><p>如果<span class="math inline">\(z\sim q(z|x;\phi)\)</span>的随机性独立于参数<span class="math inline">\(\phi\)</span>，我们可以通过再参数化的方法来计算导数。假设<span class="math inline">\(q(z|x;\phi)\)</span>为正态分布<span class="math inline">\(\mathcal{N}(\mu_I,\sigma^2_I I)\)</span>，其中<span class="math inline">\(\mu_I\)</span>和<span class="math inline">\(\sigma_I\)</span>是推断网络<span class="math inline">\(f_I(x;\phi)\)</span>的输出。我们可以通过下面的方式采样<span class="math inline">\(z\)</span>：</p><p><span class="math display">\[z=\mu_I+\sigma_I\odot \varepsilon\]</span></p><p>其中<span class="math inline">\(\varepsilon\sim\mathcal{N}(0,I)\)</span>。这样<span class="math inline">\(z\)</span>和<span class="math inline">\(\mu_I,\sigma_I\)</span>的关系从采样关系变为函数关系。</p><hr /><p>如果进一步假设<span class="math inline">\(p(x|z;\theta)\)</span>服从高斯分布<span class="math inline">\(\mathcal{N}(x|\mu_G,I)\)</span>，其中<span class="math inline">\(\mu_G=f_G(z;\theta)\)</span>是生成网络的输出，则目标函数可以简化为：</p><p><span class="math display">\[\mathcal{J}(\phi,\theta|x)=-\parallel x-\mu_G\parallel^2+D_{KL}(\mathcal{N}(\mu_I,\sigma_I)\parallel\mathcal{N}(0,I))\]</span></p><p>其中第一项可以近似看作是输入<span class="math inline">\(x\)</span>的重构正确性，第二项可以看作是正则化项。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Variational Inference</tag>
      
      <tag>VAE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Recurrent Neural Networks for Multivariate Time Series with Missing Values</title>
    <link href="/2019/10/18/Recurrent-Neural-Networks-for-Multivariate-Time-Series-with-Missing-Values/"/>
    <url>/2019/10/18/Recurrent-Neural-Networks-for-Multivariate-Time-Series-with-Missing-Values/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>文中提出了一种可以处理带缺失值多为时间序列的GRU模型：<strong>GRU-D</strong>。本模型不仅可以捕捉时间序列中的长期依赖模式，并且还能利用时间序列中的缺失模式来达到更好的时间序列预测效果。</p><p><a href="https://www.nature.com/articles/s41598-018-24271-9" target="_blank" rel="noopener">原文</a></p><h1 id="methodology">Methodology</h1><h2 id="notations">Notations</h2><p>记包含<span class="math inline">\(D\)</span>个变量的多变量时间序列为<span class="math inline">\(X=(x_1,x_2,\cdots,x_T)^T\in\mathbb{R}^{T\times D}\)</span>，其中对于每个<span class="math inline">\(t\in\{1,2,\cdots,T\},x_t\in\mathbb{R}^D\)</span>表示时间序列在时间<span class="math inline">\(t\)</span>的观测值，<span class="math inline">\(x_t^d\)</span>表示<span class="math inline">\(x_t\)</span>的第<span class="math inline">\(d\)</span>个成分。记<span class="math inline">\(s_t\in\mathbb{R}\)</span>为<span class="math inline">\(t\)</span>时刻的时间戳，并假设第一个观测值的时间戳为<span class="math inline">\(0\)</span>。对于包含缺失值的时间序列，我们用<strong>Masking Vector</strong> <span class="math inline">\(m_t\in\{0,1\}\)</span>进行标记，同时对每个<span class="math inline">\(x_t^d\)</span>维护距离上一个观测值的<strong>Time Interval</strong> <span class="math inline">\(\delta_t^d\in\mathbb{R}\)</span>，公式如下： <span class="math display">\[m_t^d=\begin{cases}1, &amp;\text{if }x_t^d\text{ is observed}\\0, &amp;\text{otherwise}\end{cases}\]</span></p><p><span class="math display">\[\delta_t^d=\begin{cases}s_t-s_{t-1}+\delta_{t-1}^d, &amp;t&gt;1,m_{t-1}^d=0\\s_t-s_{t-1}, &amp;t&gt;1, m_{t-1}^d=1\\0, &amp;t=1\end{cases}\]</span></p><p>下图是一些示例：</p><p><img src="https://i.loli.net/2020/06/25/C4FKQw2AZ9xkalo.png" srcset="/img/loading.gif" /></p><p>在本文中，我们主要关注时间序列的分类问题，即给定数据集<span class="math inline">\(\mathcal{D}=\{(X_n,s_n,M_n)\}_{n=1}^N\)</span>，我们要对每个样本的类别进行预测<span class="math inline">\(l_n\in\{1,\cdots,L\}\)</span>。</p><h2 id="gru-rnn-for-time-series-classification">GRU-RNN for Time Series Classification</h2><p>GRU是一种改进版本的RNN，其最大不同是加入了门控机制。GRU单元的结构如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/wpKQsxEklizVTtm.png" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p>GRU包含了重置门和更新门，其中重置门<span class="math inline">\(R_t\)</span>负责控制上一时间的隐状态<span class="math inline">\(h_{t-1}\)</span>有多少部分需要保留，而更新门则决定由<span class="math inline">\(R_t\)</span>计算出来的候选隐状态<span class="math inline">\(\tilde{h}_t\)</span>有多少部分需要保留。最后当前时间的隐状态由<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(\tilde{h}_t\)</span>共同算出。GRU的状态更新公式如下： <span class="math display">\[\begin{align}R_t&amp;=\sigma(W_rx_t+U_rh_{t-1}+b_r)\\Z_t&amp;=\sigma(W_zx_t+U_zh_{t-1}+b_z)\\\tilde{h}_t&amp;=\text{tanh}(Wx_t+U(R_t\odot h_{t-1})+b)\\h_t&amp;=(1-Z_t)\odot h_{t-1}+Z_t\odot \tilde{h}_t\end{align}\]</span> 文中提出了一些处理缺失值的简单方法：</p><ol type="1"><li>直接用均值替代：<span class="math inline">\(x_t^d\leftarrow m_t^dx_t^d+(1-m_t^d)\tilde{x}^d\)</span>，其中<span class="math inline">\(\tilde{x}^d=\frac{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d x_{t,n}^d}{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d\tilde{x}^d}\)</span>。这种方法称为<strong>GRU-Mean</strong>；</li><li>用上一个观测值替代：<span class="math inline">\(x_t^d\leftarrow m_t^d x_t^d+(1-m_t^d)x_{t^\prime}^d\)</span>。这种方法称为<strong>GRU-Forward</strong>；</li><li>不填充，将是否缺失，距离上一个观测值的时间作为额外信息输入：<span class="math inline">\(x_t^{(n)}\leftarrow[x_t^{(n)};m_t^{(n)};\delta_t^{(n)}]\)</span>。这种方法称为<strong>GRU-Simple</strong>。</li></ol><h3 id="gru-d-model-with-trainable-decays">GRU-D: Model with Trainable Decays</h3><p>文中提出了时间序列缺失值的两个性质：一个是在上一个观测值距离很远的情况下缺失值倾向于接近一个默认的值，第二个是缺失值的影响会随着时间减弱。为了体现上述两点，文中提出了GPU-D模型，模型框架如下：</p><p><img src="https://i.loli.net/2020/06/25/aXbS4ADkLfeHPCR.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>在模型中，<strong>Decay Rates</strong>被设定为一个带参数的函数和GRU一起训练： <span class="math display">\[\gamma_t=\exp\{-\max(0,W_\gamma\delta_t+b_\gamma)\}\]</span></p><p><span class="math display">\[\hat{x}_t^d=m_t^dx_t^d+(1-m_t^d)(\gamma_{x_t}^dx_{t^\prime}^d+(1-\gamma_{x_t}^d)\tilde{x}^d)\]</span> 其中<span class="math inline">\(x_{t^\prime}^d\)</span>是第<span class="math inline">\(d\)</span>个变量的上一个观测值，<span class="math inline">\(\tilde{x}^d\)</span>是第<span class="math inline">\(d\)</span>个变量的经验均值。这样<span class="math inline">\(\hat{x}_t^d\)</span>就代表经过<strong>Input Decay</strong>的输入。</p><p>文中提到只用<strong>Input Decay</strong>是不够的，除此之外作者还使用了<strong>Hidden State Decay</strong>，即对<span class="math inline">\(h_{t-1}\)</span>进行Decay，公式如下： <span class="math display">\[\hat{h}_{t-1}=\gamma_{h_t}\odot h_{t-1}\]</span> 用Decay之后的<span class="math inline">\(\hat{x}_t\)</span>和<span class="math inline">\(\hat{h}_{t-1}\)</span>替换原始的GRU公式就得到了GRU-D模型： <span class="math display">\[\begin{align}R_t&amp;=\sigma(W_r\hat{x}_t-U_r\hat{h}_{t-1}+V_rm_t+b_r)\\Z_t&amp;=\sigma(W_z\hat{x}_t+U_z\hat{h}_{t-1}+V_zm_t+b_z)\\\tilde{h}_t&amp;=\text{tanh}(W\hat{x}_t+U(R_t\odot \hat{h}_{t-1})+Vm_t+b)\\h_t&amp;=(1-z_t)\odot \hat{h}_{t-1}+z_t\odot\tilde{h}_t\end{align}\]</span></p><h1 id="experiments">Experiments</h1><h2 id="baseline-imputation-methods">Baseline Imputation Methods</h2><p>下图为文中比较中用到的Baseline：</p><p><img src="https://i.loli.net/2020/06/25/BFQdwMOXLc15mnl.png" srcset="/img/loading.gif" /></p><h2 id="baseline-prediction-methods">Baseline Prediction Methods</h2><p>下图为文中用到的用来预测的Baseline：</p><p><img src="https://i.loli.net/2020/06/25/qFxDRBLvNpAIjMs.png" srcset="/img/loading.gif" /></p><h2 id="results">Results</h2><p>文中用到的数据集如下：</p><ul><li><em>Gesture phase segmentation dataset (Gesture)</em>.</li><li><em>PhysioNet Challenge 2012 dataset (PhysioNet)</em>.</li><li><em>MIMIC-Ⅲ dataset (MIMIC-Ⅲ)</em>.</li></ul><p>下图展示了不同方法在人工合成数据集上的表现：</p><p><img src="https://i.loli.net/2020/06/25/6GVYuxoFP5yHAjf.png" srcset="/img/loading.gif" /></p><p>下表展示了不同模型在预测任务表现的对比：</p><p><img src="https://i.loli.net/2020/06/25/qTZsgtGewh19Y8V.png" srcset="/img/loading.gif" style="zoom: 67%;" /></p><p>下表展示了不同方法在MIMIC-Ⅲ和PhysioNet数据集上的多任务表现：</p><p><img src="https://i.loli.net/2020/06/25/dQiqebwYTCVfm5W.png" srcset="/img/loading.gif" /></p><p>下图分别展示了模型学到的<strong>Input Decay</strong>和<strong>Hidden State Decay</strong>：</p><p><img src="https://i.loli.net/2020/06/25/2F7MrgOXA9nuZqC.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>RNN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</title>
    <link href="/2019/10/18/Robust-Anomaly-Detection-for-Multivariate-Time-Series-through-Stochastic-Recurrent-Neural-Network/"/>
    <url>/2019/10/18/Robust-Anomaly-Detection-for-Multivariate-Time-Series-through-Stochastic-Recurrent-Neural-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>本文提出了<em>OmniAnomaly</em>：一种针对多变量时间序列的随机循环神经网络异常检测算法。该模型运用了一系列技术来捕捉多变量时间序列的正常模式，并在检测阶段基于重构误差来检测异常，同时本文还提供了一定的理论解释。</p><p><a href="https://www.kdd.org/kdd2019/accepted-papers/view/robust-anomaly-detection-for-multivariate-time-series-through-stochastic-re" target="_blank" rel="noopener">原文</a></p><h1 id="contribution">Contribution</h1><ol type="1"><li>提出了<em>OmniAnomaly</em>，一种基于随机循环神经网络的多变量时间序列异常检测算法；</li><li>提出了针对多变量时间序列异常检测的解释方法；</li><li>通过实验证明了<em>OmniAnomaly</em>中所用的关键技术的有效性，包括GRU，planar NF, stochastic variable connection和adjusted Peaks-Over-Threshold method；</li><li>通过大量的实验我们证明了<em>OmniAnomaly</em>的有效性；</li><li>发布了代码和数据集。</li></ol><h1 id="background">Background</h1><h2 id="linear-gaussian-state-space-model">Linear Gaussian State Space Model</h2><p>状态空间模型（State Space Model, SSM）的概念来自于控制理论，在这里我们主要讨论其在时间序列中的应用。其大概思想是我们认为时间序列在时刻<span class="math inline">\(t\)</span>的观测值<span class="math inline">\(z_t\)</span>是一个隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>的条件分布<span class="math inline">\(p(z_t|\boldsymbol{l}_t)\)</span>，而这个隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>刻画了时间序列的内在规律，同时隐含状态会随着时间更新，即服从条件分布<span class="math inline">\(p(\boldsymbol{l}_t|\boldsymbol{l}_{t-1})\)</span>。</p><p>在线性状态空间模型（Linear State Space Model）中我们以如下的方式刻画隐含状态的更新： <span class="math display">\[\boldsymbol{l}_t=\boldsymbol{F}_t\boldsymbol{l}_{t-1}+\boldsymbol{g}_t\varepsilon_t, \space\space\space\varepsilon_t\sim\mathcal{N}(0,1)\]</span> <span class="math inline">\(\boldsymbol{F}_t\)</span>为确定的状态转移矩阵，而<span class="math inline">\(\boldsymbol{g}_t\varepsilon_t\)</span>则表示了状态转移的随机性。</p><p>观测值<span class="math inline">\(z_t\)</span>从隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>计算而来： <span class="math display">\[\begin{align}z_t&amp;=y_t+\sigma_t\epsilon_t,\\y_t&amp;=\boldsymbol{a}_t^\top\boldsymbol{l}_{t-1}+b_t,\\\epsilon_t&amp;\sim\mathcal{N}(0,1)\end{align}\]</span> 其中<span class="math inline">\(\boldsymbol{a}_t\in\mathbb{R}^L,\sigma_t\in \mathbb{R},b_t\in\mathbb{R}\)</span>都是额外的参数。初始状态<span class="math inline">\(\boldsymbol{l}_0\)</span>则从一个独立的高斯分布得来，即<span class="math inline">\(\boldsymbol{l}_0\sim N(\boldsymbol\mu_0,\text{diag}(\boldsymbol{\sigma}_0^2))\)</span>。</p><p>令参数集合<span class="math inline">\(\Theta_t=(\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0,\boldsymbol{F}_t,\boldsymbol{g}_t,\boldsymbol{a}_t,b_t,\sigma_t),\forall t&gt;0\)</span>，一般来说参数集合不会随着时间变化，即每个时刻<span class="math inline">\(t\)</span>共享同样的参数<span class="math inline">\(\Theta_t=\Theta,\forall t&gt;0\)</span>。对参数的估计可以采用极大似然估计： <span class="math display">\[\begin{align}\Theta^*_{1:T}&amp;=\arg\max_{\Theta_{1:T}}p(z_{1:T}|\Theta_{1:T}),\\\end{align}\]</span> 其中： <span class="math display">\[\begin{align}p(z_{1:T}|\Theta_{1:T})&amp;=p(z_1|\Theta_1)\prod\limits_{t=2}^T p(z_t|z_{1:t-1},\Theta_{1:t})\\&amp;=\int p(\boldsymbol{l}_0)\left[\prod\limits_{t=1}^T p(z_t|\boldsymbol{l}_t)p(\boldsymbol{l}_t|\boldsymbol{l}_{t-1})\right]\mathrm{d}\boldsymbol{l}_{0:T}\end{align}\]</span></p><h2 id="planar-normalizing-flow">Planar Normalizing Flow</h2><h3 id="normalizing-flows">Normalizing Flows</h3><p>VAE采用一个变分分布<span class="math inline">\(q_\phi(z|x)\)</span>来近似真实的后验分布<span class="math inline">\(p(z|x)\)</span>，并推导出<span class="math inline">\(\log p_\theta(x)\)</span>的下界（称为ELBO）来作为优化目标函数： <span class="math display">\[\begin{align}\log p_\theta(x)&amp;=\log \int p_\theta(x|z)p(z)\mathrm{d}z\\&amp;=\log\int\frac{q_\phi(z|x)}{q_\phi(z|x)}p_\theta(x|z)p(z)\mathrm{d}z\\&amp;\geq-D_{KL}[q_\phi(z|x)\parallel p(z)]+\mathbb{E}_q[\log p_\theta(x|z)]\end{align}\]</span> <span class="math inline">\(\log p_\theta(x)\)</span>与ELBO取等的条件是<span class="math inline">\(D_{KL}[q_\phi(z|x)\parallel p(z)]\)</span>，表明变分分布完全匹配了真实的后验分布。但在实际应用中，真实的后验分布可能会非常复杂，而我们的变分分布通常是一个确定的较为简单的分布，如高斯分布。这样变分分布可能很难对真实后验分布得到一个很好的拟合。</p><p>一个解决方案是使用标准化流（Normalizing Flows）。标准化流是从一个相对简单的分布出发，执行一系列可逆的映射，将原始简单的分布转化为一个复杂的分布。</p><p>首先考虑一个光滑的、可逆的映射<span class="math inline">\(f:\mathbb{R}^d\mapsto \mathbb{R}^d\)</span>，记<span class="math inline">\(g=f^{-1}\)</span>，那么<span class="math inline">\(g\circ f(\mathbf{z})=\mathbf{z}\)</span>。令<span class="math inline">\(\mathbf{z}^\prime=f(\mathbf{z})\)</span>，那么<span class="math inline">\(\mathbf{z}^\prime\)</span>的分布为： <span class="math display">\[q(\mathbf{z}^\prime)=q(\mathbf{z})\left|\text{det}\frac{\partial f^{-1}}{\partial \mathbf{z}^\prime}\right|=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}\]</span> 式中<span class="math inline">\(q(\mathbf{z}^\prime)=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}\)</span>说明了<span class="math inline">\(\mathbf{z}^\prime\)</span>的分布等于<span class="math inline">\(\mathbf{z}\)</span>的分布乘上<span class="math inline">\(f\)</span>的Jacobian矩阵的行列式的倒数。那么对于映射多次的情况： <span class="math display">\[\mathbf{z}_K=f_K\circ\cdots\circ f_2\circ f_1(\mathbf{z}_0)\]</span> <span class="math inline">\(\mathbf{z}_K\)</span>的分布可以通过链式计算得到： <span class="math display">\[\ln q_K(\mathbf{z}_K)=\ln q_0(\mathbf{z}_0)-\sum\limits_{k=1}^K\ln\left|\text{det}\frac{\partial f_k}{\partial \mathbf{z}_{k-1}}\right|\]</span></p><h3 id="planar-flows">Planar Flows</h3><p>考虑一个变换族： <span class="math display">\[f(\mathbf{z})=\mathbf{z}+\mathbf{u}h(\mathbf{w}^\top\mathbf{z}+b)\]</span> 其中<span class="math inline">\(\lambda=\{\mathbf{w}\in \mathbb{R}^d,\mathbf{u}\in\mathbb{R}^d,b\in\mathbb{R}\}\)</span>为参数集合，<span class="math inline">\(h(\cdot)\)</span>为元素级的非线性函数（如各种激活函数）。令<span class="math inline">\(\psi(\mathbf{z})=h^\prime(\mathbf{w}^\top\mathbf{z}+b)\mathbf{w}\)</span>，则<span class="math inline">\(f\)</span>的Jacobian矩阵行列式绝对值等于： <span class="math display">\[\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|=\left|\text{det}(\mathbf{I}+\mathbf{u}\psi(\mathbf{z})^\top)\right|=\left|1+\mathbf{u}^\top\psi(\mathbf{z})\right|\]</span> 但是<span class="math inline">\(f\)</span>并不保证总是可逆的，如<span class="math inline">\(h(x)=\tanh(x)\)</span>时，<span class="math inline">\(f\)</span>可逆的条件是<span class="math inline">\(\mathbf{w}^\top \mathbf{u}\geq-1\)</span>。</p><p>下面讨论如何保证可逆的条件。考虑将<span class="math inline">\(\mathbf{z}\)</span>分解为<span class="math inline">\(\mathbf{z}=\mathbf{z}_\bot+\mathbf{z}_\parallel\)</span>，其中<span class="math inline">\(\mathbf{z}_\bot\)</span>与<span class="math inline">\(\mathbf{w}\)</span>正交，<span class="math inline">\(\mathbf{z}_\parallel\)</span>与<span class="math inline">\(\mathbf{w}\)</span>平行，那么： <span class="math display">\[f(z)=\mathbf{z}_\bot+\mathbf{z}_\parallel+\mathbf{u}h(\mathbf{w}^\top \mathbf{z}_\parallel +b)\]</span> 实际上得到<span class="math inline">\(\mathbf{z}_\parallel\)</span>之后可以很容易的得到<span class="math inline">\(\mathbf{z}_\bot\)</span>，令<span class="math inline">\(\mathbf{y}=f(\mathbf{z})\)</span>，有： <span class="math display">\[\mathbf{z}_\bot=\mathbf{y}-\mathbf{z}_\parallel-\mathbf{u}h(\mathbf{w}^\top\mathbf{z}_\parallel+b)\]</span> 而<span class="math inline">\(\mathbf{z}_\parallel\)</span>与<span class="math inline">\(\mathbf{w}\)</span>平行，易知<span class="math inline">\(\mathbf{z}_\parallel=\alpha\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}\)</span>，其中<span class="math inline">\(\alpha\in\mathbb{R}\)</span>。</p><p>对式(16)两边同时乘以<span class="math inline">\(\mathbf{w}^\top\)</span>可得： <span class="math display">\[\mathbf{w}^\top f(\mathbf{z})=\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\]</span> 当<span class="math inline">\(\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\)</span>对于<span class="math inline">\(\alpha\)</span>是非递减函数的时候，<span class="math inline">\(f\)</span>是可逆的。因为<span class="math inline">\(\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\)</span>是非递减函数时有<span class="math inline">\(1+\mathbf{w}^\top\mathbf{u}h^\prime(\alpha+b)\geq 0\equiv \mathbf{w}^\top \mathbf{u}\geq -\frac{1}{h^\prime(\alpha + b)}\)</span>，而<span class="math inline">\(0\leq h^\prime(\alpha + b) \leq 1\)</span>（<span class="math inline">\(\tanh\)</span>函数的性质），所以总是有<span class="math inline">\(\mathbf{w}^\top \mathbf{u}\geq-1\)</span>。</p><p>对于任意一个<span class="math inline">\(\mathbf{u}\)</span>，我们可以通过特定的方式构造一个<span class="math inline">\(\hat{\mathbf{u}}\)</span>使得<span class="math inline">\(\mathbf{w}^\top\hat{\mathbf{u}}&gt;-1\)</span>，即令<span class="math inline">\(\hat{\mathbf{u}}(\mathbf{w},\mathbf{u})=\mathbf{u}+[m(\mathbf{w}^\top\mathbf{u})-(\mathbf{w}^\top\mathbf{u})]\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}\)</span>，其中<span class="math inline">\(m(x)=-1+\log(1+e^x)\)</span>。</p><p><img src="https://i.loli.net/2020/06/25/uPyplhWBazROEw4.png" srcset="/img/loading.gif" /></p><h1 id="methodology">Methodology</h1><h2 id="problem-statement">Problem Statement</h2><p>本文针对的是多变量时间序列<span class="math inline">\(x=\{x_1,x_2,\cdots,x_N\}\in R^{M\times N}\)</span>，<span class="math inline">\(N\)</span>为时间长度，其中某一时刻的观测值<span class="math inline">\(x_t\in R^M\)</span>为一个<span class="math inline">\(M\)</span>维的向量。作者使用<span class="math inline">\(x_{t-T:t}\in R^{M\times(T+1)}\)</span>来表示<span class="math inline">\(t-T\)</span>到<span class="math inline">\(t\)</span>之间的时间序列。</p><p><img src="https://i.loli.net/2020/06/25/4eHhs82uOzI5tG3.png" srcset="/img/loading.gif" /></p><h2 id="overall-structure">Overall Structure</h2><p>算法的总体框架如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/wd8maAoVb3Fk9vP.png" srcset="/img/loading.gif" /></p><p>预处理模块主要是对数据进行标准化以及窗口切分。训练模块则根据输入的数据对正常模式进行捕捉，输出异常分数。在线检测模块则会定期执行。</p><h2 id="network-architecture">Network Architecture</h2><p>模型的总体结构如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/Lp7D81EvxVsywXQ.png" srcset="/img/loading.gif" /></p><p>在qnet中，首先GRU被用来建模样本的时间依赖关系，之后VAE将样本<span class="math inline">\(\mathbf{x}\)</span>映射到隐空间<span class="math inline">\(\mathbf{z}\)</span>。文中使用了Linear Gaussian State Space Model来建模隐变量之间的时间依赖关系。除此之外，作者还使用了Planar Normalizing Flow来将隐变量映射到复杂的非高斯分布。在pnet中，隐变量<span class="math inline">\(\mathbf{z}_{t-T:t}\)</span>被用来重建<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>，直观上来说，对样本的好的隐变量表示可以带来更好的重构效果。</p><p>从细节上来说，在时间<span class="math inline">\(t\)</span>，qnet的输入为<span class="math inline">\(\mathbf{x}_t\)</span>和<span class="math inline">\(\mathbf{e}_{t-1}\)</span>，两者经过GRU Cell之后会产生<span class="math inline">\(t\)</span>时间的<span class="math inline">\(\mathbf{e_t}\)</span>。<span class="math inline">\(\mathbf{e}_t\)</span>是GRU捕捉时间依赖性的关键，可以认为它包含了<span class="math inline">\(\mathbf{x}_{1:t}\)</span>的信息。之后<span class="math inline">\(\mathbf{e}_t\)</span>会和<span class="math inline">\(\mathbf{z}_{t-1}\)</span>进行拼接，进入标准的VAE变分网络结构，通过网络输出的参数<span class="math inline">\(\mu_{z_t},\sigma_{z_t}\)</span>采样得到隐变量<span class="math inline">\(\mathbf{z}_t^0\)</span>，此时隐变量可以说捕捉了时间依赖性。</p><p>网络中涉及到的公式如下所示：</p><p><span class="math display">\[\begin{align}e_t&amp;=(1-c_t^e)\circ\text{tanh}(w^ex_t+u^e(r_t^e\circ e_{t-1})+b^e)+c_t^e\circ e_{t-1}\\\mu_{z_t}&amp;=w^{\mu_z}h^\phi([z_{t-1},e_t])+b^{\mu_z}\\\sigma_{z_t}&amp;=\text{softplus}(w^{\sigma_z}h^\phi([z_{t-1},e_t])+b^{\sigma_z})+\epsilon^{\sigma_z}\end{align}\]</span></p><p>其中<span class="math inline">\(r_t^e=\text{sigmoid}(\mathbf{w}^{r^e}\mathbf{x}_t+\mathbf{u}^{r^e}\mathbf{e}_{t-1}+b^{r^e})\)</span>是GRU中的重置门，<span class="math inline">\(c_t^e=\text{sigmoid}(\mathbf{w}^{c^e}\mathbf{x}_t+\mathbf{u}^{c^e}\mathbf{e}_{t-1}+b^{c^e})\)</span>是GRU中的更新门。</p><p>此时<span class="math inline">\(\mathbf{z}_t^0\)</span>服从高斯分布，为了拟合复杂的后验分布，我们使用Planar Normalizing Flow来对<span class="math inline">\(\mathbf{z}_t^0\)</span>进行变换，最后得到经<span class="math inline">\(K\)</span>次变换后的随机变量<span class="math inline">\(\mathbf{z}_t^K\)</span>。</p><p>在时间<span class="math inline">\(t\)</span>，pnet试图通过<span class="math inline">\(\mathbf{z}_t^K\)</span>来重构<span class="math inline">\(\mathbf{x}_t\)</span>。首先<span class="math inline">\(\mathbf{z}\)</span>空间中的变量会根据Linear Gaussian State Space Model来进行“连接“，公式为<span class="math inline">\(\mathbf{z}_t=\mathbf{O}_\theta(\mathbf{T}_\theta\mathbf{z}_{t-1}+\mathbf{v}_t)+\boldsymbol{\epsilon}_t\)</span>，其中<span class="math inline">\(\mathbf{O}_\theta\)</span>和<span class="math inline">\(\mathbf{T}_\theta\)</span>为状态转移矩阵，<span class="math inline">\(\mathbf{v}_t\)</span>和<span class="math inline">\(\boldsymbol{\epsilon}_t\)</span>为随机噪声。之后<span class="math inline">\(\mathbf{z}_t\)</span>和<span class="math inline">\(\mathbf{d}_{t-1}\)</span>会作为GRU的输入，产生<span class="math inline">\(\mathbf{d}_t\)</span>。之后<span class="math inline">\(\mathbf{d}_t\)</span>会经过标准VAE中的生成网络，通过网络输出的高斯分布参数<span class="math inline">\(\mu_{x_t},\sigma_{x_t}\)</span>采样得到重构后的样本<span class="math inline">\(\mathbf{x}^\prime_t\)</span>。pnet中涉及到的公式如下所示： <span class="math display">\[\begin{align}d_t&amp;=(1-c_t^d)\circ\text{tanh}(w^dz_t+u^d(r_t^d\circ d_{t-1})+b^d)+c_t^d\circ d_{t-1}\\\mu_{x_t}&amp;=w^{\mu_x}h^\theta(d_t)+b^{\mu_x}\\\sigma_{x_t}&amp;=\text{softplus}(w^{\sigma_x}h^\theta(d_t)+b^{\sigma_x})+\epsilon^{\sigma_x}\end{align}\]</span></p><p>其中<span class="math inline">\(r_t^d=\text{sigmoid}(\mathbf{w}^{r^d}\mathbf{x}_t+\mathbf{u}^{r^d}\mathbf{d}_{t-1}+b^{r^d})\)</span>是GRU中的重置门，<span class="math inline">\(c_t^d=\text{sigmoid}(\mathbf{w}^{c^d}\mathbf{x}_t+\mathbf{u}^{c^d}\mathbf{d}_{t-1}+b^{c^d})\)</span>是GRU中的更新门。</p><h2 id="offline-model-training">Offline Model Training</h2><p>和传统VAE类似，模型的训练可以通过优化ELBO来完成。记长度为<span class="math inline">\(T+1\)</span>的输入序列为<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>，隐空间变量采样次数为<span class="math inline">\(L\)</span>，第<span class="math inline">\(l\)</span>个隐空间变量为<span class="math inline">\(\mathbf{l}^{(l)}_{t-T:t}\)</span>，损失函数可以写成如下形式：</p><p><span class="math display">\[\tilde{\mathcal{L}}(\mathbf{x}_{t-T:t})\approx\frac{1}{L}\sum_{t=1}^L[\log(p_\theta(\mathbf{x}_{t-T:t}|\mathbf{z}_{t-T:t}^{(l)}))+\log(p_\theta(\mathbf{z}_{t-T:t}^{(l)}))-\log(q_\phi(\mathbf{z}_{t-T:t}^|\mathbf{x}_{t-T:t}))]\]</span></p><p>第一项<span class="math inline">\(\log(p_\theta(\mathbf{x}_{t-T:t}|\mathbf{z}_{t-T:t}^{(l)}))\)</span>可以看作是重构误差；第二项<span class="math inline">\(\log(p_\theta(\mathbf{z}_{t-T:t}))=\sum_{i=t-T}^t \log(p_\theta(\mathbf{z}_i|\mathbf{z}_{i-1}))\)</span>通过Linear Gaussian State Space Model计算；第三项<span class="math inline">\(-\log(q_\phi(\mathbf{z}_{t-T:t}|\mathbf{x}_{t-T:t}))=-\sum_{i=t-T}^t\log(q_\phi(\mathbf{z}_i|\mathbf{z}_{i-1},\mathbf{x}_{t-T:i}))\)</span>为隐变量<span class="math inline">\(\mathbf{z}\)</span>后验分布的估计，同时<span class="math inline">\(\mathbf{z}_i\)</span>是经Planar Normalizing Flow转换过的。</p><h2 id="online-detection">Online Detection</h2><p>在训练好模型之后，就可以进行异常检测了。在时间<span class="math inline">\(t\)</span>，我们通过根据长度为<span class="math inline">\(T+1\)</span>的序列<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>来重构<span class="math inline">\(\mathbf{x}_t\)</span>，并根据重构概率<span class="math inline">\(\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))\)</span>来判定异常。定义<span class="math inline">\(\mathbf{x}_t\)</span>对应的异常分数<span class="math inline">\(S_t=\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))\)</span>，高异常分数代表样本<span class="math inline">\(\mathbf{x}_t\)</span>能够以大概率重构（因为模型是用正常样本训练，可以认为模型建模的是正常样本的分布，重构概率高就代表符合正常分布）。给定阈值之后便可根据异常分数来进行异常的判定。</p><h2 id="automatic-threshold-selection">Automatic Threshold Selection</h2><p>在异常检测阶段，需要根据设定的阈值和每个样本的异常分数来判断该样本是否为异常，所以阈值的选择十分重要。文中用到了一种根据<strong>Extreme Value Theory</strong>自动选择阈值的算法。对于一个分布，其中的极端事件往往位于分布的末尾，而Extreme Value Theory第一定理给出不管原始分布如何，这些极端事件的分布服从一个带参的分布族。因此，可以在对数据分布未知的情况下估计极端事件的分布。</p><p>除了Extreme Value Theory第一定理之外，Extreme Value Theory第二定理给出随机变量大于特定阈值<span class="math inline">\(t\)</span>的分布可以用Generalized Pareto Distribution来描述。作者使用了基于Extreme Value Theory第二定理的Peaks-Over-Threshold算法来进行阈值的选择。因为Extreme Value Theory第二定理给出随机变量大于特定阈值<span class="math inline">\(t\)</span>的分布，而在本文的场景中我们需要刻画的异常点的分布应该是小于一个给定阈值的分布，所以需要修改一下公式。</p><p>对于给定的数据，模型会给出对应的异常分数序列<span class="math inline">\(\{S_1,S_2,\cdots,S_{N^\prime}\}\)</span>，给定预先设定的阈值<span class="math inline">\(th\)</span>，<span class="math inline">\(S_i\)</span>极端部分（即小于<span class="math inline">\(th\)</span>的部分）的分布符合Generalized Pareto Distribution，公式如下： <span class="math display">\[\bar{F}(s)=P(th-S&gt;s|S&lt;th)\sim(1+\frac{\gamma s}{\beta})^{-\frac{1}{\gamma}}\]</span></p><p>其中<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>为分布的形状参数，本文使用极大似然估计来对参数进行估计。设参数的估计值分别为<span class="math inline">\(\hat{\gamma}\)</span>和<span class="math inline">\(\hat{\beta}\)</span>，最终的阈值<span class="math inline">\(th_F\)</span>由拟合得到的分布的分位数确定：</p><p><span class="math display">\[th_F\simeq th-\frac{\hat{\beta}}{\hat{\gamma}}((\frac{qN^\prime}{N^\prime_{th}})^{-\hat{\gamma}}-1)\]</span></p><p>其中<span class="math inline">\(q\)</span>为期望<span class="math inline">\(S&lt;th\)</span>的概率，<span class="math inline">\(N^\prime\)</span>为观测值的数量，<span class="math inline">\(N^\prime_{th}\)</span>为<span class="math inline">\(S_i&lt;th\)</span>的个数。</p><h2 id="anomaly-interpretation">Anomaly Interpretation</h2><p><span class="math display">\[\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))=\sum_{i=1}^M\log(p_\theta(x_t^i|\mathbf{z}_{t-T:t}))\]</span></p><h1 id="experiments">Experiments</h1><h2 id="datasets-and-metrics">Datasets and Metrics</h2><h2 id="overall-performance">Overall Performance</h2><p><img src="https://i.loli.net/2020/06/25/yYLiWfXBDQklbPU.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/HlORKBEu7ij6Vor.png" srcset="/img/loading.gif" /></p><h2 id="effects-of-major-techniques">Effects of Major Techniques</h2><p><img src="https://i.loli.net/2020/06/25/eKXGJ9ZlSmq8r4Q.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/dlibhyOBXNmPrkw.png" srcset="/img/loading.gif" /></p><h2 id="visualization-on-z-space-representations">Visualization on Z-Space Representations</h2><p><img src="https://i.loli.net/2020/06/25/MQXv5pZAejgVJ9s.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/jCkovq2lWrP6KXy.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/25/JSZuUkzNyci41DA.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>VAE</tag>
      
      <tag>RNN</tag>
      
      <tag>Flow-based Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAIN: Missing Data Imputation using Generative Adversarial Nets</title>
    <link href="/2019/10/16/GAIN-Missing-Data-Imputation-using-Generative-Adversarial-Nets/"/>
    <url>/2019/10/16/GAIN-Missing-Data-Imputation-using-Generative-Adversarial-Nets/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>本文基于GAN提出了一种时间序列缺失值填充（Time Series Imputation）的方法。其主要的思路为生成器<span class="math inline">\(G\)</span>从隐空间<span class="math inline">\(Z\)</span>生成完整的样本，而判别器<span class="math inline">\(D\)</span>则输出样本中不同部分为真实的概率。除此之外，作者提出了使用Hint Vector来揭示原始数据中缺失部分的信息，来优化训练过程。</p><p><a href="https://arxiv.org/abs/1806.02920" target="_blank" rel="noopener">原文</a></p><h1 id="methodology">Methodology</h1><h2 id="problem-formulation">Problem Formulation</h2><p>考虑一个<span class="math inline">\(d\)</span>维的空间<span class="math inline">\(\mathcal{X}=\mathcal{X}_1\times \cdots\times \mathcal{X}_d\)</span>，设<span class="math inline">\(\mathbf{X}=(X_1,\cdots,X_d)\)</span>维空间<span class="math inline">\(\mathcal{X}\)</span>上的随机向量（即理想的完整的时间序列），记其分布为<span class="math inline">\(P(\mathbf{X})\)</span>。设<span class="math inline">\(\mathbf{M}=(M_1,\cdots,M_d)\)</span>为Mask向量表示<span class="math inline">\(\mathbf{X}\)</span>中被观察到的部分。（即标识时间序列哪些部分有缺失），取值为<span class="math inline">\(\{0,1\}^d\)</span>。</p><p>对于每一个<span class="math inline">\(i\in\{1,\cdots,d\}\)</span>，我们定义一个新空间<span class="math inline">\(\tilde{\mathcal{X}}=\mathcal{X}\cup\{*\}\)</span>，其中<span class="math inline">\(*\)</span>表示不属于任意<span class="math inline">\(\mathcal{X}_i\)</span>的一个点。令<span class="math inline">\(\tilde{\mathcal{X}}=\tilde{\mathcal{X}_1}\times\cdots\times\tilde{\mathcal{X}_d}\)</span>，同时定义一个新的随机变量（即我们观测到的含有缺失值的时间序列）<span class="math inline">\(\tilde{\mathbf{X}}=(\tilde{X}_1,\cdots,\tilde{X}_d)\in \tilde{\mathcal{X}}\)</span>： <span class="math display">\[\tilde{X}_i=\begin{cases}X_i,&amp;\text{if } M_i=1\\*,&amp;\text{otherwise}\end{cases}\]</span> 假设数据集的形式为<span class="math inline">\(\mathcal{D}=\{(\tilde{x}^i,m^i)\}^n_{i=1}\)</span>，我们的任务是从<span class="math inline">\(P(\mathbf{X}|\tilde{\mathbf{X}}=\tilde{x}^i)\)</span>上采样来对缺失值进行填充。</p><h2 id="model-architecture">Model Architecture</h2><p>模型的架构如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/wCK3J8MoTASrj9Y.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="generator">Generator</h3><p>生成器的输入有三项：<span class="math inline">\(\tilde{\mathbf{X}}\)</span>，<span class="math inline">\(\mathbf{M}\)</span>和随机噪声<span class="math inline">\(\mathbf{Z}\)</span>，输出设为<span class="math inline">\(\bar{\mathbf{X}}\)</span>。设生成器为映射<span class="math inline">\(G: \tilde{\mathcal{X}}\times\{0,1\}^d\times[0,1]^d\rightarrow \mathcal{X}\)</span>，而<span class="math inline">\(\mathbf{Z}\)</span>为<span class="math inline">\(d\)</span>维的高斯噪声。生成器的输出和填充后的时间序列定义为： <span class="math display">\[\begin{align}\bar{\mathbf{X}}&amp;=G(\tilde{\mathbf{X}},\mathbf{M},(1-\mathbf{M})\odot\mathbf{Z})\\\hat{\mathbf{X}}&amp;=\mathbf{M}\odot\tilde{\mathbf{X}}+(1-\mathbf{M})\odot\bar{\mathbf{X}}\end{align}\]</span> <span class="math inline">\(\bar{\mathbf{X}}\)</span>即为生成器的直接输出，因为其实有些部分没有缺失，生成器还是会为每个部分输出值。</p><p><span class="math inline">\(\hat{\mathbf{X}}\)</span>为填充后的时间序列，对于缺失的部分采用生成器的输出进行填充。</p><h3 id="discriminator">Discriminator</h3><p>和原始的GAN不同的是，我们不需要判断整个样本是真实的或者是生成的，而是需要判断样本的那些部分是真实的或者是生成的，所以判别器为映射<span class="math inline">\(D: \mathcal{X}\rightarrow[0,1]^d\)</span>。判别器的具体目标函数将在后面讨论。</p><h3 id="hint">Hint</h3><p>Hint是一种提示机值，是一个和<span class="math inline">\(\mathbf{X}\)</span>相同维度的随机变量<span class="math inline">\(\mathbf{H}\)</span>，其分布依赖于<span class="math inline">\(\mathbf{M}\)</span>。<span class="math inline">\(\mathbf{H}\)</span>是由用户自己定义的，相当于一种不完整的<span class="math inline">\(\mathbf{M}\)</span>，用来作为判别器的额外输入。</p><h3 id="objective">Objective</h3><p>我们训练判别器最大化正确预测<span class="math inline">\(\mathbf{M}\)</span>的概率，而生成器最小化判别器正确预测<span class="math inline">\(\mathbf{M}\)</span>的概率，目标函数如下： <span class="math display">\[\begin{align}V(D,G)=&amp;\mathbb{E}_{\hat{X},M,H}[\mathbf{M}^T\log D(\hat{\mathbf{X}},\mathbf{H})\\&amp;+(1-\mathbf{M})^T\log(1-D(\hat{\mathbf{X}},\mathbf{H}))]\end{align}\]</span> 按照标准的GAN可以将优化函数写成以下的形式： <span class="math display">\[\min_G\max_D V(D,G)\]</span> 在这里判别器的任务可以看作是一个二分类，而目标函数就是二值交叉熵的定义，因此可以写为： <span class="math display">\[\mathcal{L}(a,b)=\sum\limits_{i=1}^d[a_i\log(b_i)+(1-a_i)\log(1-b_i)]\]</span> <span class="math inline">\(\mathbf{M}\)</span>可以看作Ground Truth，记<span class="math inline">\(\hat{\mathbf{M}}=D(\hat{\mathbf{X},\mathbf{H}})\)</span>，即判别器输出的预测，因此优化函数可以简记为： <span class="math display">\[\min_G\max_D\mathbb{E}[\mathcal{L}(\mathbf{M},\hat{\mathbf{M}})]\]</span></p><h2 id="gain-algorithm">GAIN Algorithm</h2><p>下面讨论GAIN算法的训练流程。</p><p>本文通过理论讨论，给出了生成Hint Vector的一个方法，首先定义随机变量<span class="math inline">\(\mathbf{B}=(B_1,\cdots,B_d)\in\{0,1\}^d\)</span>，<span class="math inline">\(\mathbf{B}\)</span>通过从<span class="math inline">\(\{1,\cdots,d\}\)</span>随机均匀采样一个<span class="math inline">\(k\)</span>，然后由下列公式得到： <span class="math display">\[B_j=\begin{cases}1, &amp;\text{if }j\neq k\\0, &amp;\text{if }j=k\end{cases}\]</span> 定义空间<span class="math inline">\(\mathcal{H}=\{0,0.5,1\}^d\)</span>，Hint Vector为<span class="math inline">\(\mathbf{H}=\mathbf{B}\odot\mathbf{M}+0.5(1-\mathbf{B})\in\mathcal{H}\)</span>。</p><p>判别器的训练过程如下：固定生成器<span class="math inline">\(G\)</span>，对一个大小为<span class="math inline">\(k_D\)</span>的mini-batch，独立同分布采样<span class="math inline">\(k_D\)</span>个<span class="math inline">\(z\)</span>和<span class="math inline">\(b\)</span>，用来计算<span class="math inline">\(\mathbf{Z}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>。判别器的损失函数定义如下： <span class="math display">\[\mathcal{L}_D(m,\hat{m},b)=\sum\limits_{i:b_i=0}[m_i\log(\hat{m}_i)+(1-m_i)\log(1-\hat{m}_i)]\]</span> 判别器的优化函数为： <span class="math display">\[\min_D-\sum\limits_{j=1}^{k_D}\mathcal{L}_D(m(j),\hat{m}(j),b(j))\]</span> 其中<span class="math inline">\(\hat{m}(j)=D(\hat{x}(j),m(j))\)</span>。</p><p>在优化了判别器之后，需要优化生成器，对一个大小为<span class="math inline">\(k_G\)</span>的mini-batch，生成器的损失函数包含两个部分，一个是在缺失部分的损失：</p><p><span class="math display">\[\mathcal{L}_G(m,\hat{m},b)=-\sum\limits_{i:b_i=0}(1-m_i)\log(\hat{m}_i)\]</span> 一个是未缺失部分的损失： <span class="math display">\[\mathcal{L}_M(x,x^\prime)=\sum\limits_{i=1}^d m_iL_M(x_i,x_i^\prime)\]</span> 其中： <span class="math display">\[L_M(x_i,x_i^\prime)=\begin{cases}(x_i^\prime-x_i)^2, &amp;\text{if }x_i\text{ is continuours},\\-x_i\log(x_i^\prime), &amp;\text{if }x_i\text{ is binary}.\end{cases}\]</span> 最终的优化函数为： <span class="math display">\[\min_G\sum\limits_{j=1}^{k_G}\mathcal{L}_G(m(j),\hat{m}(j),b(j))+\alpha\mathcal{L}_M(\tilde{x}(j),\hat{x}(j))\]</span> 算法流程如下：</p><p><img src="https://i.loli.net/2020/06/25/znABi6x9mJuvDOX.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="experiments">Experiments</h1><p>下表为在5个不同数据集上实验，与其他5种方法对比的结果：</p><p><img src="https://i.loli.net/2020/06/25/EenX2YO8aDxQAk3.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>上图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例、样本数量、特征维度下的对比曲线图。</p><p>下表为使用不同模型对时间序列进行填充之后，使用逻辑回归进行回归任务的性能：</p><p><img src="https://i.loli.net/2020/06/25/PzeWKdGu4wADshV.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>下图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例下的AUROC曲线图：</p><p><img src="https://i.loli.net/2020/06/25/IKtoTj8xgG1yJDk.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>下表展示的是作者对时间序列填充算法保持特征-标签关系的能力。作者分别用完整的数据和填充后的数据用逻辑回归模型进行训练，将两者的权重求绝对值和均方根的结果。</p><p><img src="https://i.loli.net/2020/06/25/uy2jPcnbtSrC6vI.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Time Series Imputation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series</title>
    <link href="/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/"/>
    <url>/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>这篇文章提出了一个基于GAN的时间序列异常检测模型。</p><p><a href="https://arxiv.org/abs/1809.04758" target="_blank" rel="noopener">原文</a></p><h2 id="contribution">Contribution</h2><ol type="1"><li>提出了基于GAN的时间序列无监督异常检测模型</li><li>我们使用基于LSTM的GAN来对多变量时间序列进行建模</li><li>结合使用了Residual Loss和Discrimination Loss来进行异常的判断</li></ol><h2 id="background">Background</h2><h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3><h4 id="gans-in-a-nutshell-an-extremely-simple-explanation">GANs In a Nutshell, an extremely simple explanation</h4><ul><li>我们想要从一个复杂的、高维的数据分布<span class="math inline">\(p_r(x)\)</span>上采样得到我们想要的数据点，然而<span class="math inline">\(p_r(x)\)</span>无法直接求得</li><li>代替方法：从一个简单的、已知的分布<span class="math inline">\(p_z(z)\)</span>上采样，然后学习一个Transformation <span class="math inline">\(G(z): z\rightarrow x\)</span>来将<span class="math inline">\(z\)</span>映射到<span class="math inline">\(x\)</span></li></ul><p><img src="https://i.loli.net/2020/06/25/frIYtuao9mexQUT.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h4 id="training-two-player-game">Training: Two-player Game</h4><ul><li><strong>Generator Network: </strong> 从随机分布<span class="math inline">\(p_z(z)\)</span>采样<span class="math inline">\(z\)</span>，通过映射生成样本<span class="math inline">\(x\)</span>，这个生成的样本要尽量“真实”。怎么“真实”？优化生成器参数<span class="math inline">\(\theta_G\)</span>最大化判别器对生成样本的评分即可</li><li><strong>Discriminator Network: </strong>接受一个样本<span class="math inline">\(x\)</span>，判断其是生成的样本还是真实的样本。在训练阶段，我们是知道一个样本<span class="math inline">\(x\)</span>到底是生成的还是真实的，所以优化判别器参数<span class="math inline">\(\theta_D\)</span>最小化判别器对生成样本的评分，最大化对真实样本的评分（即最大化分辨真实样本的能力）</li></ul><p><img src="https://i.loli.net/2020/06/25/ECP2Dkpq6FrSoef.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>形式化的来讲，优化函数如下：</p><p><span class="math display">\[\min\limits_{\theta_G}\max\limits_{\theta_D}V(G,D)=\mathbb{E}_{x\sim p_{data}(x)\log(\underbrace{D_{\theta{D}}(x)}_{判别器对真实样本的评分})}+\mathbb{E}_{z\sim p_z(z)}\log(1-\underbrace{D_{\theta_d}(G_{\theta_G}(z))}_{判别器对生成样本的评分})\]</span></p><p>训练过程如下：</p><p><img src="https://i.loli.net/2020/06/25/57N4yUrfoS1cBWd.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="long-short-time-memory-networks">Long Short Time Memory Networks</h3><h4 id="vanilla-recurrent-neural-networks">Vanilla Recurrent Neural Networks</h4><p>普通的神经网络：</p><p><img src="https://i.loli.net/2020/06/25/U5rxdYR4jKoqQOX.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>概括的来讲，可以涵盖为一个公式<span class="math inline">\(\hat{\mathbf{y}}=f(\mathbf{x})\)</span>。对于一个样本<span class="math inline">\(\mathbf{x}\)</span>，通过多层神经网络映射，输出<span class="math inline">\(\mathbf{y}\)</span>。</p><p>对于RNN，我们处理的是序列数据，也就是说所有样本之间并不是相互独立的。对于一个序列中的一个样本<span class="math inline">\(x_t\in\{x_1,x_2,\cdots,x_n\}\)</span>，将其输入到神经网络的时候，为了建模<span class="math inline">\(x_t\)</span>之前的子序列对<span class="math inline">\(x_t\)</span>的影响关系，需要将这个子序列的信息也输入到神经网络中，怎么做呢？为每一个样本点保存一个State。即定义<span class="math inline">\(h_t=g(\hat{y_t})=g(f(x_t))\)</span>，对于当前样本点，<span class="math inline">\(\hat{y_t}=f(x_t,h_{t-1})\)</span>。也就是说神经网络的输入不仅包含了当前样本点的特征，也包含了上一个样本点的“状态”(上一个样本点的“状态”又隐含了上上个样本点的“状态”...)，就像是为网络加上了短期记忆。</p><p><img src="https://i.loli.net/2020/06/25/cxBk6SQTydsOVYt.png" srcset="/img/loading.gif" style="zoom: 67%;" /></p><p><img src="https://i.loli.net/2020/06/25/ODKWYBI83tJXurM.png" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p><img src="https://i.loli.net/2020/06/25/jq1LAytRKCub3kX.png" srcset="/img/loading.gif" style="zoom:33%;" /></p><h4 id="gradient-flow-of-vanilla-rnn">Gradient Flow of Vanilla RNN</h4><p>下面来进行一些形式化的定义，假设在时刻<span class="math inline">\(t\)</span>网络输入特征为<span class="math inline">\(x_t\)</span>，输出隐含状态为<span class="math inline">\(h_{t}\)</span>，其不仅和当前输入<span class="math inline">\(x_t\)</span>有关，还和上一个隐含状态<span class="math inline">\(h_{t-1}\)</span>有关：</p><ul><li>当前时刻总的净输入<span class="math inline">\(z_t=Uh_{t-1}+Wx_t+b\)</span></li><li>当前时刻输出隐含状态<span class="math inline">\(h_t=f(z_t)\)</span></li><li>当前时刻输出<span class="math inline">\(\hat{y}_t=Vh_t\)</span></li></ul><p>RNN的梯度更新公式(推导过程比较复杂)：</p><p><span class="math display">\[\frac{\partial{\mathcal{L}}}{\partial U}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t \delta_{t,k}\mathbf{h}_{k-1}^T\]</span></p><p><span class="math display">\[\frac{\partial{\mathcal{L}}}{\partial{W}}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t \delta_{t,k}x_k^T\]</span></p><p><span class="math display">\[\frac{\partial\mathcal{L}}{\partial{b}}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t\delta_{t,k}\]</span></p><p>其中<span class="math inline">\(\delta_{t,k}=\frac{\partial{\mathcal{L}}}{\partial{z_k}}=\text{diag}(f^\prime(z_k))U^T\delta_{t,k+1}\)</span>定义为第<span class="math inline">\(t\)</span>时刻的损失对第<span class="math inline">\(k\)</span>时刻隐藏神经层的净输入<span class="math inline">\(z_k\)</span>的导数，且<span class="math inline">\(z_k=Uh_{k-1}+Wx_k+b,1\leq k&lt;t\)</span>。</p><p>RNN的梯度流向如下图红箭头所示：</p><p><img src="https://i.loli.net/2020/06/25/F5xvo9kCiNl8ehZ.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>RNN会遇到梯度消失和梯度爆炸的问题。根据前面的公式，<span class="math inline">\(\delta_{t,k}\)</span>实际上是递归定义的，展开得到：</p><p><span class="math display">\[\delta_{t,k}=\prod\limits_{\tau=k}^{t-1}(\text{diag}(f^\prime(z_\tau))U^T)\delta_{t,t}\]</span></p><p>如果定义<span class="math inline">\(\gamma\cong\parallel\text{diag}(f^\prime(z_\tau))U^T\parallel\)</span>，那么<span class="math inline">\(\delta_{t,k}\cong\gamma^{t-k}\delta_{t,t}\)</span>。在<span class="math inline">\(t-k\)</span>很大时，<span class="math inline">\(\gamma&lt;1\)</span>会导致梯度消失，<span class="math inline">\(\gamma&gt;1\)</span>时会导致梯度爆炸。</p><p><img src="https://i.loli.net/2020/06/25/RGW4oVtQ7KEFUCA.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2020/06/25/i4O9kJQpnZ5GYeq.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h4 id="long-short-time-memory">Long Short Time Memory</h4><p>LSTM是一种解决RNN梯度消失问题的改进版本：</p><p><img src="https://i.loli.net/2020/06/25/B4NXzb6fSdgGowL.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>在LSTM中，维护了两个State，<span class="math inline">\(c_t\)</span>和<span class="math inline">\(h_t\)</span>。其中<span class="math inline">\(c_t\)</span>由遗忘门<span class="math inline">\(f\)</span>与上一个<span class="math inline">\(c_{t-1}\)</span>相乘(代表继承上一个Cell的信息并加以一定程度的遗忘)，加上输出门<span class="math inline">\(i\)</span>与Gate Gate <span class="math inline">\(g\)</span>相乘(Gate Gate代表当前的候选状态，输出门<span class="math inline">\(i\)</span>控制当前候选状态有多少信息需要保存)。最后，输出门<span class="math inline">\(o\)</span>控制当前时刻的Cell State <span class="math inline">\(c_t\)</span>有多少信息需要输出给外部状态<span class="math inline">\(h_t\)</span>。</p><p>三个门的计算方式为：</p><p><span class="math display">\[i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)\]</span></p><p><span class="math display">\[f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)\]</span></p><p><span class="math display">\[o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)\]</span></p><p><img src="https://i.loli.net/2020/06/25/PXQMb9vih1yEKrf.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2020/06/25/1zZQqlI6r9Yjp47.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h2 id="methodology">Methodology</h2><p>总体框架图如Fig 1所示：</p><p><img src="https://i.loli.net/2020/06/25/scEA9Ou1Yi7nThG.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><h3 id="gan-with-lstm-rnn">GAN with LSTM-RNN</h3><p>网络结构上生成器和判别器都是LSTM，优化函数和普通GAN一样：</p><p><span class="math display">\[\min\limits_G\max\limits_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)}[\log (1-D(G(z)))]\]</span></p><h3 id="gan-based-anomaly-score">GAN-based Anomaly Score</h3><p>在测试阶段，需要使用梯度优化寻找一个使得<span class="math inline">\(G_{rnn}(z)\)</span>最接近<span class="math inline">\(X^{test}\)</span>的<span class="math inline">\(z^k\)</span>：</p><p><span class="math display">\[\min\limits_{Z^k}Error(X^{test},G_{rnn}(Z^k))=1-Similarity(X^{test},G_{rnn}(Z^k))\]</span></p><p>本文定义了两种Anomaly Score，一种是Residual Loss：</p><p><span class="math display">\[Res(X^{test}_t)=\sum\limits_{i=1}^n|x^{test,i}_t-G_{rnn}(Z^{k,i}_t)|\]</span></p><p>一种是Discrimination Loss，即判别器的输出<span class="math inline">\(D_{rnn}(x_t^{test})\)</span>。</p><p>总的Anomaly Score：</p><p><span class="math display">\[S^{test}_t=\lambda Res(X^{test}_t)+(1-\lambda)D_{rnn}(x^{test}_t)\]</span></p><h3 id="anomaly-detection-framework">Anomaly Detection Framework</h3><p>模型的算法流程如下：</p><p><img src="https://i.loli.net/2020/06/25/84ZCT1JOEru53Ue.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>由于本文是多变量时间序列预测，而且时间序列的长度有可能比较长，作者使用了滑动窗口和PCA来进行预处理。</p><h2 id="experiments">Experiments</h2><p><img src="https://i.loli.net/2020/06/25/LGsiMw6IjYUtx8T.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ALSR: An Adaptive Label Screening and Relearning Approach for Interval-Oriented Anomaly Detection</title>
    <link href="/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/"/>
    <url>/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文针对面向区间的KPI异常检测提出了Label Screening方法和Relearning Algorithm.</p><p><a href="https://www.sciencedirect.com/science/article/pii/S0957417419304282" target="_blank" rel="noopener">原文</a></p><h2 id="contribution">Contribution</h2><ol type="1"><li>提出了一种Label Screening方法来对区间内不同重要性进行过滤</li><li>提出了一种Relearning Algorithm来对FP和TP进行Relearning，在不减少Recall的条件下增大Precision</li></ol><h2 id="methodology">Methodology</h2><h3 id="overall-structure">Overall Structure</h3><p>算法的整体框架如下：</p><p><img src="https://i.loli.net/2020/06/24/yGMWzdgTf43qImp.png" srcset="/img/loading.gif" /></p><h3 id="label-screening-model">Label Screening Model</h3><p>预训练的结果被分为<span class="math inline">\(TP_{po},FP_{po},TN_{po},FN_{po}\)</span>四类，<span class="math inline">\(TP_{po}\)</span>和<span class="math inline">\(FN_{po}\)</span>可以被细分如下： <span class="math display">\[\begin{align}TP_{po}&amp;=TP_{po,withinT}+TP_{po,afterT}\\&amp;=TP_{po,withinT}+TP_{po,afterT,tpl}+TP_{po,after,fnl}\end{align}\]</span></p><p><span class="math display">\[\begin{align}FN_{po}&amp;=FN_{po,withinT}+FN_{po,afterT}\\&amp;=FN_{po,withinT,tpl}+FN_{po,,withinT,fnl}+FN_{po,afterT,tpl}+FN_{po,afterT,fnl}\end{align}\]</span></p><p>其中下标<span class="math inline">\({}_{withinT}\)</span>代表在异常片段第一个点<span class="math inline">\(T\)</span>距离内的所有点，下标<span class="math inline">\({}_{afterT}\)</span>代表<span class="math inline">\(T\)</span>距离之后。下标<span class="math inline">\({}_{tpl}\)</span>和<span class="math inline">\({}_{fnl}\)</span>分别代表在异常片段中，包含和不包含<span class="math inline">\(TP_{po,withinT}\)</span>的点。</p><p>以TP为例，Point-based的TP包含了在T范围之内的（即在Interval-based的标准中也会被认为是TP的点）和T范围之外的点（即在Interval-based的标准中不认为是TP的点）。而在T范围之外的点又可以细分为该异常片段是否包含<span class="math inline">\(TP_{po,withinT}\)</span>的点（即该点在Interval-based的标准中不会被判定为TP，但该异常片段有其点会被判定为TP）。</p><p>类似的，<span class="math inline">\(TP_{io}\)</span>和<span class="math inline">\(FN_{io}\)</span>可以被分解为： <span class="math display">\[\begin{align}TP_{io}&amp;=TP_{po,withinT}+TP_{po,afterT,tpl}+FN_{po,withinT,tpl}+FN_{po,afterT,tpl}\\&amp;=TP_{po}+FN_{po,withinT,tpl}+FN_{po,afterT,tpl}-TP_{po,afterT,fnl}\end{align}\]</span></p><p><span class="math display">\[\begin{align}FN_{io}&amp;=FN_{po,withinT,fnl}+FN_{po,afterT,fnl}+TP_{po,afterT,fnl}\\&amp;=FN_{po}+TP_{po,afterT,fnl}-FN_{po,withinT,tpl}-FN_{po,afterT,tpl}\end{align}\]</span></p><p>文中对该部分的分析可以分为以下几点：</p><ol type="1"><li>在Interval-oriented的标准中，<span class="math inline">\(FN_{po,tpl}\)</span>的点仍会被认为是<span class="math inline">\(TP_{io}\)</span>，而<span class="math inline">\(TP_{po,afterT}\)</span>（不带<span class="math inline">\({}_{tpl}\)</span>）不会被认为是<span class="math inline">\(TP_{io}\)</span>，所以最终<span class="math inline">\(TP_{io}\)</span>由所有<span class="math inline">\(TP_{po}\)</span>加上那些会被认为是<span class="math inline">\(TP_{io}\)</span>的<span class="math inline">\(FN_{po,tpl}\)</span>再去掉不带<span class="math inline">\({}_{tpl}\)</span>的<span class="math inline">\(TP_{po,afterT}\)</span>组成，即公式(6)</li><li>同时，根据公式(6)，如果<span class="math inline">\(TP_{po}\)</span>变为<span class="math inline">\(FN_{po,tpl}\)</span>，也不会对最终结果造成影响。但是根据公式(5)和公式(7)，<span class="math inline">\(TP_{po,withinT}\)</span>变成<span class="math inline">\(FN_{po,withinT,fnl}\)</span>会减小<span class="math inline">\(TP_{io}\)</span>同时增大<span class="math inline">\(FN_{io}\)</span></li><li>文章指出，虽然<span class="math inline">\(FN_{po,withinT,tpl}\)</span>和<span class="math inline">\(FN_{po,afterT,tpl}\)</span>最后都会被认为是<span class="math inline">\(TP_{io}\)</span>，但作者假设<span class="math inline">\(FN_{po,withinT,tpl}\)</span>更难检测，所以应该保留，而<span class="math inline">\(FN_{po,afterT,tpl}\)</span>应该削减</li><li>Label Screening方法去除了<span class="math inline">\(FN_{po,afterT}\)</span>的点</li><li>Screened之后的训练集被用来训练DNN主模型，但Label Screening的预测结果也会被保留，和DNN主模型的结果进行组合</li></ol><p>算法流程如下：</p><p><img src="https://i.loli.net/2020/06/24/ZDT5fQNojsXm84q.png" srcset="/img/loading.gif" /></p><h3 id="relearning-algorithm">Relearning Algorithm</h3><p>Relearning Model的输入是DNN主模型预测出来的异常，其中包括TP和FP。Relearning Model采用的是随机森林，其输入的样本通过采样得到： <span class="math display">\[\begin{align}\text{relearning}\space&amp;\text{training set}=\\&amp; shuffle\{4C\ast\text{randomof}(TP_{po})\\&amp;+C\cdot\text{randomof}(FP_{po})+C\cdot\text{randomof}(TN_{po})\}\end{align}\]</span> 其中<span class="math inline">\(C\)</span>为常数。TN和FP都看作是负例(正常样本)，TP看作是正例。</p><p><img src="https://i.loli.net/2020/06/24/qgvINaFu9JLfM4j.png" srcset="/img/loading.gif" /></p><h3 id="detection">Detection</h3><p>对于一个滑动窗口<span class="math inline">\(x_t=\{x_{t-w+1},\cdots,x_t\}\)</span>，异常检测算法的目标是输出检测结果<span class="math inline">\(y_t\in\{0,1\}\)</span>来表示时间<span class="math inline">\(t\)</span>是否发生异常。实际上算法输出的是<span class="math inline">\(p_{y_t}\in[0,1]\)</span>概率值来表示在时间<span class="math inline">\(t\)</span>发生异常的概率。文中三个模型会得到三个输出：<span class="math inline">\(y_{t,ls},y_{t,main},y_{t,re}\)</span>。最终结果为： <span class="math display">\[y_t=y_{t,ls}\space\&amp;\space y_{t,main}\space\&amp; \space y_{t,re}\]</span> 在绘制PR曲线时，采用的公式为： <span class="math display">\[\begin{align}p_{y_t}(th)=&amp;(1-sig(p_{y_t,ls},th))\cdot(p_{y_t,ls})\\&amp;+sig(p_{y_t,ls},th)\cdot(1-sig(p_{y_t,main},th))\cdot p_{y_t,main}\\&amp;+sig(p_{y_t,ls},th)\cdot sig(p_{y_t,main},th)\cdot p_{y_t,re}\\\end{align}\]</span></p><p><span class="math display">\[y_t(th)=sig(p_{y_t}(th),th)\]</span></p><p>算法流程如下：</p><p><img src="https://i.loli.net/2020/06/24/LBT59eugKEPymO8.png" srcset="/img/loading.gif" /></p><h2 id="experiments">Experiments</h2><h3 id="datasets">Datasets</h3><p>清华AIOps数据集，选取了25条KPI。</p><h3 id="preprocessing">Preprocessing</h3><ol type="1"><li><strong>Missing Data.</strong> 去除。</li><li><strong>Standardization.</strong> Minmax Standardization，Feature Extraction使用的是Standardization后的数据。</li><li><strong>Feature Extraction.</strong> 使用了12种特征。</li></ol><table><thead><tr class="header"><th>Group</th><th>Feature Name</th></tr></thead><tbody><tr class="odd"><td>Values</td><td>The original values standardized</td></tr><tr class="even"><td>Statistical Features</td><td>Mean, Standard Deviation, Range, Difference...</td></tr><tr class="odd"><td>Fitting Features</td><td>EWMA, AR</td></tr><tr class="even"><td>Wavelet Features</td><td>Db2 wavelet decomposition</td></tr></tbody></table><h2 id="results">Results</h2><h3 id="aucpr">AUCPR</h3><p><img src="https://i.loli.net/2020/06/24/LAqNMW1zvs8eP7X.png" srcset="/img/loading.gif" /></p><p><img src="https://i.loli.net/2020/06/24/YgRwavDIn7izLPX.png" srcset="/img/loading.gif" /></p><h3 id="f1">F1</h3><p><img src="https://i.loli.net/2020/06/24/FM3BUhQXtpo7aiu.png" srcset="/img/loading.gif" /></p><h2 id="remark">Remark</h2><ol type="1"><li>这篇文章的Label Screening方法实际上是在处理样本分类难易度的问题，将异常区间内容易的样本去除了</li><li>对于时间序列的异常检测问题，我们的目标一般是Point-based的异常标签，一个时间点的特征是有限的。如果用窗口的方式，以<span class="math inline">\(\{x_{t-w+1},\cdots,x_t\}\)</span>作为时间<span class="math inline">\(t\)</span>的输入（当然每个<span class="math inline">\(x_t\)</span>可以有多个Channel），然后把预测结果作为时间<span class="math inline">\(t\)</span>的输出</li></ol>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</title>
    <link href="/2019/09/22/Unsupervised-Anomaly-Detection-via-Variational-Auto-Encoder-for-Seasonal-KPIs-in-Web-Applications/"/>
    <url>/2019/09/22/Unsupervised-Anomaly-Detection-via-Variational-Auto-Encoder-for-Seasonal-KPIs-in-Web-Applications/</url>
    
    <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>本文提出了Donut，一个基于VAE的无监督时间序列异常检测系统。</p><p><a href="https://dl.acm.org/citation.cfm?id=3185996" target="_blank" rel="noopener">原文</a></p><p><img src="https://i.loli.net/2020/06/25/aoNWpGDLmJzwXOj.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="contribution">Contribution</h2><ol type="1"><li>Donut中使用到了三个技巧，包括改进后的ELBO、缺失数据注入和MCMC插值；</li><li>提出基于VAE的异常检测训练既需要正常样本也需要异常样本；</li><li>对Donut提出了在z-空间中基于KDE的理论解释。</li></ol><h2 id="background">Background</h2><h3 id="anomaly-detection">Anomaly Detection</h3><p>对于任意时间<span class="math inline">\(t\)</span>，给定历史观察值<span class="math inline">\(x_{t-T+1},\cdots,x_t\)</span>，确定异常是否发生(记为<span class="math inline">\(y_t=1\)</span>)。通常来收异常检测算法给出的是发生异常的可能性，如<span class="math inline">\(p(y_t=1|x_{t-T+1},\cdots,x_t)\)</span>。</p><h2 id="methodology">Methodology</h2><h3 id="problem-statement">Problem Statement</h3><p>本文的目的是<strong>基于深度生成网络开发具有理论解释性的无监督异常检测算法，并且在有标签的情况下能利用标签信息提升性能</strong>。本文基于VAE来构建模型。</p><p><img src="https://i.loli.net/2020/06/25/PoKJmnIpEqNdtCe.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="network-structure">Network Structure</h3><p>算法的总体框架如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/DFP1boZNzdVG9pH.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><p>一共包含了预处理、训练和检测三个部分。</p><p>下图为模型的概率图模型：</p><p><img src="https://i.loli.net/2020/06/25/HlDXkSeFOruVbac.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>图中双实线的框为本文模型有别于传统VAE的地方，其余地方和VAE一样。先验概率<span class="math inline">\(p_\theta(z)\)</span>选为标准正态分布<span class="math inline">\(\mathcal{N}(0,I)\)</span>，后验概率<span class="math inline">\(x\)</span>和<span class="math inline">\(z\)</span>都是对角化高斯分布，即<span class="math inline">\(p_\theta(x|z)=\mathcal{N}(\mu_x,\sigma_x^2 I),q_\phi(z|x)=\mathcal{N}(\mu_z,\sigma_z^2 I)\)</span>。如Figure 4所示，推断网络和生成网络中分别都有隐含层<span class="math inline">\(f_\phi(x)\)</span>和<span class="math inline">\(f_\theta(z)\)</span>对网络的输入进行特征抽取。高斯分布的参数即从这些抽取出来的特征上得到。均值通过线性层得到：<span class="math inline">\(\mu_x=W^T_{\mu_x}f_\theta(z)+b_{\mu_x}, \mu_z=W^T_{\mu_z}f_\theta(x)+b_{\mu_z}\)</span>。标准差通过Soft Plus层加一个高斯噪声得到：<span class="math inline">\(\sigma_x=\text{SoftPlus}[W^T_{\sigma_x}f_\theta(z)+b_{\sigma_x}]+\varepsilon，\sigma_x=\text{SoftPlus}[W^T_{\sigma_z}f_\theta(x)+b_{\sigma_z}]+\varepsilon\)</span>。</p><p>文中提到因为KPI的局部方差非常小，所以采用直接建模<span class="math inline">\(\sigma_x,\sigma_z\)</span>的方式而不是采用对数。除此之外，为了理论上的解释性，文中的神经网络只使用了全连接层。</p><h3 id="training">Training</h3><p>训练可以直接采用经典的SGVB来优化ELBO： <span class="math display">\[\begin{align}\log p_\theta(x)&amp;\geq\log p_\theta(x)-\text{KL}[q_\phi(z|x)\parallel p_\theta(z|x)]\\&amp;=\mathcal{L}\\&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x)+\log p_\theta(z|x)-\log q_\phi(z|x)]\\&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x,z)-\log q_\phi(z|x)]\\&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)+\log p_\theta(z)-\log q_\phi(z|x)]\end{align}\]</span> 但是在实际的训练过程中，训练数据需要保证都是正常样本，但实际上训练样本有可能会包含异常或者是缺失值。一种做法是用缺失值填充的算法来填充这些异常值和缺失值，但作者认为使用缺失值填充算法并不能很好的还原数据的正常模式，从而保证算法的有效性。在文中作者采用了修改ELBO的方法，并将其称之为<strong>Modified ELBO (M-ELBO)</strong>，公式如下： <span class="math display">\[\tilde{\mathcal{L}}=\mathbb{E}_{q_\phi(z|x)}[\sum\limits_{w=1}^W{\alpha_w\log p_\theta(x_w|z)+\beta\log p_\theta(z)-\log q_\phi(z|x)}]\]</span> 其中<span class="math inline">\(\alpha_w\)</span>为指示标记，<span class="math inline">\(\alpha_w=1\)</span>代表不是异常也不是缺失。<span class="math inline">\(\beta\)</span>定义为<span class="math inline">\(\beta=\frac{\sum_{w=1}^W\alpha_w}{W}\)</span>。</p><p>在<strong>M-ELBO</strong>中，异常或缺失值对应的<span class="math inline">\(\log p_\theta(x_w|z)\)</span>的贡献会被排除，同时<span class="math inline">\(\log p_\theta(z)\)</span>在乘以<span class="math inline">\(\beta\)</span>后会相应缩小。作者没有修改<span class="math inline">\(\log q_\phi(z|x)\)</span>这一项的原因有二：一是<span class="math inline">\(q_\phi(z|x)\)</span>仅仅是从<span class="math inline">\(x\)</span>到<span class="math inline">\(z\)</span>的映射，并不需要考虑“正常模式”；二是<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[-\log q_\phi(z|x)]\)</span>就是<span class="math inline">\(q_\phi(z|x)\)</span>的熵，而这个在后面的理论分析中有特别的含义。</p><p>除此之外还有一种解决方法就是把所有包含异常值和缺失值的窗口去除，这种方法的性能在实验中会进行讨论。</p><p>在文中作者还使用了一种<strong>Missing Data Injection</strong>技术，即在每个Epoch随机的按照一个预设比例<span class="math inline">\(\lambda\)</span>将正常的数据设为缺失。作者认为这样有助于性能的提升。</p><h3 id="detection">Detection</h3><p>在检测阶段，对于一个输入样本，我们需要模型输出其异常的概率。因为我们建模了<span class="math inline">\(p_\theta(x|z)\)</span>，一种方法是采样计算<span class="math inline">\(p_\theta(x)=\mathbb{E}_{p_\theta(z)}[p_\theta(x|z)]\)</span>，但这种方法计算代价十分昂贵。其他的一些方案有计算<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[p_\theta(x|z)]\)</span>或<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>，其中后者被称为"<strong>Reconstruction Probability</strong>"，作者便采用了这种方案。</p><p>同时，作者认为输入的检测样本的缺失值会对结果造成较大偏差，于是使用了一种<strong>MCMC-based Missing Data Imputation</strong>的方法来对检测样本的缺失值进行填充。具体做法是将测试样本分为已观测和缺失两部分<span class="math inline">\(x=(x_o,x_m)\)</span>，然后使用训练好的VAE进行重构得到<span class="math inline">\((x^\prime_o,x^\prime_m)\)</span>，然后用<span class="math inline">\(x^\prime_m\)</span>替换<span class="math inline">\(x_m\)</span>，这样不断循环如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/wEenLKz4URfm2FN.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>作者使用了<span class="math inline">\(L\)</span>个样本来计算<strong>Reconstruction Probability</strong>，虽然得到的输出是针对整个窗口每个点的，但作者只使用最后一个点。</p><h2 id="experiments">Experiments</h2><h3 id="datasets">Datasets</h3><p>作者选择了三条KPI作为测试数据，分别记为<span class="math inline">\(\mathcal{A}\)</span>，<span class="math inline">\(\mathcal{B}\)</span>，<span class="math inline">\(\mathcal{C}\)</span>，其基本数据如下表所示：</p><p><img src="https://i.loli.net/2020/06/25/A9qajrZtmhcvyHW.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="metrics">Metrics</h3><p>因为异常检测类别的极不均衡性，传统的性能指标并不太合适（异常样本极少，且异常一般呈连续的片段）。作者认为在实际应用场景中运维人员需要尽量早的获知异常的发生，于是提出了新的评测机制。</p><p><img src="https://i.loli.net/2020/06/25/6LNwizqrWVe5sRv.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>如上图所示，第一行为真实的标签，第二行为预测的异常概率，第三行为预测的标签。第一行中异常片段被加粗表示，对于每一个异常片段的第一个位置<span class="math inline">\({y}_{t^\prime}\)</span>，如果预测的标签中存在<span class="math inline">\(\hat{y}_{t}\)</span>满足<span class="math inline">\(t^\prime&lt;t\)</span>且<span class="math inline">\(|t-t^\prime|\)</span>小于等于预设的阈值<span class="math inline">\(T\)</span>，那么<span class="math inline">\(y_{t^\prime}\)</span>对应的整段异常都被认为正确检测，否则整段异常都认为没有被正确检测。然后在此基础上计算F1-score，AUC等指标作为评测手段。</p><h2 id="results">Results</h2><h3 id="overall-performance">Overall Performance</h3><p>下图展示了不同方法在不同数据集上的表现：</p><p><img src="https://i.loli.net/2020/06/25/2j1Br45MUxTaYEX.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="effects-of-donut-techniques">Effects of Donut Techniques</h3><p>为了探究Donut中所做的各种改进的实际作用，作者做了大量对比实验，结果如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/raHG6Phyxo1j82n.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><ul><li><strong>M-ELBO</strong> 从图中可以看出<strong>M-ELBO</strong>对性能提升最大。作者在文中提到一开始并没期望<strong>M-ELBO</strong>能带来性能的提升，只是希望它能够Work。这表明在VAE的训练中，只使用正常样本是不够的，也需要加入非正常的信息；</li><li><strong>Missing Data Injection</strong> 该技巧的主要作用是增强<strong>M-ELBO</strong>的效果。从结果上来看作用并不是十分的显著，只是在一些情况下获得了少量的提升；</li><li><strong>MCMC Imputation</strong> 作者认为虽然该技巧只在一部分情况下显著提升了性能，但总体来说值得使用。</li></ul><h3 id="impact-of-k">Impact of K</h3><p>该部分作者探究了隐变量<span class="math inline">\(z\)</span>的维度<span class="math inline">\(K\)</span>对性能的影响，结果如下图：</p><p><img src="https://i.loli.net/2020/06/25/OXpIRoe4wr7YzuQ.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>从图上来看，对数据集<span class="math inline">\(\mathcal{A}\)</span>，<span class="math inline">\(\mathcal{B}\)</span>，<span class="math inline">\(\mathcal{C}\)</span>最佳的<span class="math inline">\(K\)</span>分别是<span class="math inline">\(5\)</span>，<span class="math inline">\(4\)</span>和<span class="math inline">\(3\)</span>，但是设定较大的<span class="math inline">\(K\)</span>并不会对性能有严重的损害。作者还发现对于较为平滑的KPI需要较大的<span class="math inline">\(K\)</span>。</p><h2 id="analysis">Analysis</h2><h3 id="kde-interpretation">KDE Interpretation</h3><p>在这一节作者对<strong>Reconstruction Probability</strong>的意义进行了深入的探讨。首先作者对<span class="math inline">\(q_\phi(z|x)\)</span>进行了可视化，在图中作者将时间维度用颜色来表示。如Figure 11(a) 所示，<span class="math inline">\(z\)</span>几乎是按照<span class="math inline">\(x\)</span>对应的时间呈一个连续的流形分布，作者将这种现象称为<strong>Time Gradient</strong>。即使Donut没有显式的用到时间信息，不过因为实验用到的数据基本是平滑的，所以说相邻的<span class="math inline">\(x\)</span>会比较相似，因此经过映射后的<span class="math inline">\(z\)</span>也会比较相似。作者据此提出Donut的一个优势便是对于没有见过的后验分布<span class="math inline">\(q_\phi(z|x)\)</span>，只要其位于训练过的两个后验之间，也会产生合理的分布。</p><p><img src="https://i.loli.net/2020/06/25/BHfP5DUZ48ALnma.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>对于异常的样本<span class="math inline">\(x\)</span>，假设其对应的正常模式为<span class="math inline">\(\tilde{x}\)</span>，作者认为<span class="math inline">\(q_\phi(z|x)\)</span>会在某种程度上对正常的<span class="math inline">\(q_\phi(z|\tilde{x})\)</span>进行近似。因为模型是用正常样本进行训练的，隐变量<span class="math inline">\(z\)</span>的维度通常来说小于样本<span class="math inline">\(x\)</span>，这就导致<span class="math inline">\(z\)</span>只会保留一部分主要的信息。对于异常样本，其异常模式在编码时就被丢掉了。作者还指出如果<span class="math inline">\(x\)</span>包含的异常太多，那么模型将难以对<span class="math inline">\(x\)</span>进行还原。</p><p><img src="https://i.loli.net/2020/06/25/HcX78lUoG4meJq5.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>基于上述讨论，作者对使用<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>作为<strong>Reconstruction Probability</strong>的意义进行了阐释。设输入样本为<span class="math inline">\(x\)</span>，如果其包含异常，假设其对应的正常样本为<span class="math inline">\(\tilde{x}\)</span>，那么<span class="math inline">\(q_\phi(z|x)\)</span>部分地和<span class="math inline">\(q_\phi(z|\tilde{x})\)</span>相似。如果<span class="math inline">\(x\)</span>和<span class="math inline">\(\tilde{x}\)</span>相似程度高，那么<span class="math inline">\(\log p_\theta(x|z)\)</span>就会很大（其中<span class="math inline">\(z\sim q_\phi(z|\tilde{x})\)</span>）。<span class="math inline">\(\log p_\theta(x|z)\)</span>类似于一个密度估计器，代表<span class="math inline">\(x\)</span>在多大程度上与<span class="math inline">\(\tilde{x}\)</span>接近，<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>相当于对每一个<span class="math inline">\(z\)</span>对应的<span class="math inline">\(\log p_\theta(x|z)\)</span>乘以一个权重<span class="math inline">\(q_\phi(z|x)\)</span>然后相加。于是作者提出了<strong>Reconstruction Probability</strong>的<strong>KDE Interpretation</strong>:在Donut模型中，<strong>Reconstruction Probability</strong> <span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>可以看作是以<span class="math inline">\(q_\phi(z|x)\)</span>为权重，<span class="math inline">\(\log p_\theta(x|z)\)</span>为核的核密度估计 (Kernel Density Estimation)。</p><p>三维可视化如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/GeWufVlUDo4QtO6.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>作者还对直接计算<span class="math inline">\(p_\theta(x)=\mathbb{E}_{p_\theta(z)}[p_\theta(x|z)]\)</span>进行了质疑，因为这种方法直接求<span class="math inline">\(x\)</span>的先验，仅仅考虑了<span class="math inline">\(x\)</span>的总体模式，而忽略了<span class="math inline">\(x\)</span>的个体模式。</p><h3 id="find-good-posteriors-for-abnormal-x">Find Good Posteriors for Abnormal <span class="math inline">\(x\)</span></h3><p>通过上面的讨论我们知道了Donut通过找到<span class="math inline">\(x\)</span>的正常后验来估计<span class="math inline">\(x\)</span>在多大程度上与<span class="math inline">\(\tilde{x}\)</span>相似，在这一节作者讨论了文中使用的不同技巧对找到<span class="math inline">\(x\)</span>的后验的作用。对于<strong>Missing Data Injection</strong>作者认为该技巧增强了<strong>M-ELBO</strong>的效果。对于<strong>MCMC Imputation</strong>，作者认为该技巧主要是在检测阶段通过不断迭代提供了更好的后验，如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/8puDRylqZfQkcN6.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>作者认为，虽然对于包含大量异常的样本，Donut不能很好的还原，但在运维场景中，只要对大段异常的开始阶段进行准确预测即可。</p><h3 id="causes-of-time-gradient">Causes of Time Gradient</h3><p>在这一节作者讨论了<strong>Time Gradient</strong>出现的原因。首先假设<span class="math inline">\(x\)</span>都是正常点，这时<span class="math inline">\(x\)</span>的ELBO为： <span class="math display">\[\begin{align}\mathcal{L}(x)&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)+\log p_\theta(z)-\log q_\phi(z|x)]\\&amp;=\mathbb{E}[\log p_\theta(x|z)]+\mathbb{E}[\log p_\theta(z)]+\text{H}[z|x]\end{align}\]</span> 第一项表明在<span class="math inline">\(z\sim q_\phi(z|x)\)</span>下尽可能重构<span class="math inline">\(x\)</span>。第二项表明<span class="math inline">\(q_\phi(z|x)\)</span>尽量与<span class="math inline">\(z\)</span>的先验<span class="math inline">\(\mathcal{N}(0,I)\)</span>接近。第三项为<span class="math inline">\(q_\phi(z|x)\)</span>的熵，表明<span class="math inline">\(q_\phi(z|x)\)</span>应尽量分散。然而第二项又限制了这种分散的区域，如 Figure 11(c) 所示。同时考虑这三项的话，第一项使得<span class="math inline">\(z\)</span>不能自由地分散，对于不相似的<span class="math inline">\(x\)</span>其对应的<span class="math inline">\(z\)</span>也是不相似的，因为要最大化<span class="math inline">\(x\)</span>的重构概率。然而对于相似的<span class="math inline">\(x\)</span>来说，其对应的<span class="math inline">\(q_\phi(z|x)\)</span>会出现很多重复的部分。当达到平衡时，<strong>Time Gradient</strong>就出现了。</p><p><img src="https://i.loli.net/2020/06/25/AaC9oNMShBRHFeY.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>在训练过程中，当<span class="math inline">\(x\)</span>越不相似，<span class="math inline">\(q_\phi(z|x)\)</span>就会相距越远，如上图所示。然而在一开始，参数经过随机初始化，<span class="math inline">\(q_\phi(z|x)\)</span>都是随机散乱的，如 Figure 11(b) 所示。随着训练的进行，<span class="math inline">\(q_\phi(z|x)\)</span>将会不断优化。由于KPI数据往往是光滑的，那么在时间上相距越远的样本就会越不相似，对应的<span class="math inline">\(q_\phi(z|x)\)</span>也会相距更远。这也说明了，训练结束后，时间上相距越远的，<span class="math inline">\(q_\phi(z|x)\)</span>也会相距越远，反之亦然。同时这也表明学习率的设置对本模型的稳定性有至关重要的作用。</p><h3 id="sub-optimal-equilibrium">Sub-Optimal Equilibrium</h3><p>上面我们讨论了随着训练进行<span class="math inline">\(q_\phi(z|x)\)</span>的演变，作者提出在训练过程中可能会遇到模型收敛到次优的情况，如下图所示：</p><p><img src="https://i.loli.net/2020/06/25/udigOl3sJGwSaPz.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>第一行展示的是收敛到最优的情况，第二行展示的是收敛到次优的情况。从第二行的第一个图（Step 100）来看，紫色的点开始穿过绿色的点，随着训练的进行，紫色的点开始将绿色的点推开。到Step 5000的时候，绿色的点已经被分成了两半。下图展示了对应的训练误差和验证误差：</p><p><img src="https://i.loli.net/2020/06/25/23zsxwurICV7aTj.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>这样的现象会导致在两半绿色区域之间的测试样本会被识别为紫色，从而降低性能。作者提出在<span class="math inline">\(K\)</span>较大的时候这种现象不容易发生，但这时训练的收敛又会成为一个问题。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>VAE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Time-Series Anomaly Detection Service at Microsoft</title>
    <link href="/2019/09/22/Time-Series-Anomaly-Detection-Service-at-Microsoft/"/>
    <url>/2019/09/22/Time-Series-Anomaly-Detection-Service-at-Microsoft/</url>
    
    <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>本文借鉴计算机视觉中的显著性检测，提出了一种基于Spectral Residual的时间序列异常检测算法。</p><p><a href="https://www.kdd.org/kdd2019/accepted-papers/view/time-series-anomaly-detection-service-at-microsoft" target="_blank" rel="noopener">原文</a></p><p>这篇文章还提出了几个时间序列异常检测落地的难点：</p><ol type="1"><li><strong>Lack of Labels.</strong> 在实际生产环境中会产生大量的KPI，而很难对每个KPI进行人工标注。</li><li><strong>Generalization.</strong> 不同KPI所表现出来的模式也不尽相同，如Figure 1所示。现有方法很难在所有模式的KPI上都表现良好。</li><li><strong>Efficiency.</strong> 在实际场景中，会产生大量的时间序列数据，同时对异常检测算法的时间效率有要求。</li></ol><p><img src="https://i.loli.net/2020/06/25/o21lnTUtcGuwCq6.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="contribution">Contribution</h2><ul><li>将Visual Saliency Detection的方法引入了时间序列异常检测。</li><li>结合Spectral Residual和CNN提高了异常检测的效果。</li><li>算法具有良好的时间效率和通用性。</li></ul><h2 id="background">Background</h2><h3 id="spectral-residual">Spectral Residual</h3><p>SR(Spectral Residual)算法主要包含三个步骤：</p><ol type="1"><li>通过傅里叶变换得到log amplitude spectrum；</li><li>计算spectral residual；</li><li>通过傅里叶逆变换回到时间域。</li></ol><p>更形式化的表述为如下：</p><p>给定一个序列<span class="math inline">\(\mathbb{x}\)</span>，则有：</p><p><span class="math display">\[A(f)=Amplitude(\mathscr{F}(\mathbb{x}))\]</span></p><p><span class="math display">\[P(f)=Phrase(\mathscr{F}(\mathbb{x}))\]</span></p><p><span class="math display">\[L(f)=\log(A(f))\]</span></p><p><span class="math display">\[AL(F)=h_1(f)\cdot L(f)\]</span></p><p><span class="math display">\[R(f)=L(f)-AL(f)\]</span></p><p><span class="math display">\[S(\mathbb{x})=\parallel\mathscr{F}^{-1}(\exp(R(f)+iP(f)))\parallel\]</span></p><p>其中<span class="math inline">\(\mathscr{F}\)</span>和<span class="math inline">\(\mathscr{F}^{-1}\)</span>分别表示傅里叶变换和傅里叶逆变换；<span class="math inline">\(\mathbb{x}\in \mathbb{R}^{n\times 1}\)</span>表示输入序列；<span class="math inline">\(A(f)\)</span>为幅度谱，<span class="math inline">\(P(f)\)</span>为相位谱，<span class="math inline">\(L(f)\)</span>为对数幅度谱，<span class="math inline">\(AL(F)\)</span>为均值滤波后的对数幅度谱；<span class="math inline">\(R(f)\)</span>为spectral residual；<span class="math inline">\(S(\mathbb{x})\)</span>称为saliency map。Figure 4为文中给出的Saliency Map示意图。</p><p><img src="https://i.loli.net/2020/06/25/OrSlqhBWNdyfEG9.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="methodology">Methodology</h2><h3 id="problem-definition">Problem Definition</h3><blockquote><p>给定一系列实数值<span class="math inline">\(\mathbb{x}=x_1,x_2,\cdots,x_n\)</span>，时间序列异常检测的任务是产生一个输出序列<span class="math inline">\(\mathbb{y}=y_1,y_2,\cdots,y_n\)</span>其中<span class="math inline">\(y_i\in\{0,1\}\)</span>表示<span class="math inline">\(x_i\)</span>是否为异常点。</p></blockquote><h3 id="sr">SR</h3><p>对于给定序列<span class="math inline">\(\mathbb{x}\)</span>，计算Saliency Map <span class="math inline">\(S(\mathbb{x})\)</span>，输出序列<span class="math inline">\(O(\mathbb{x})\)</span>定义为：</p><p><span class="math display">\[O(x_i)=\begin{cases}1,\quad \text{if}\frac{S(x_i)-\overline{S(x_i)}}{\overline{S(x_i)}}&gt;\tau\\\\0,\quad \text{otherwise}\end{cases}\]</span></p><p>其中<span class="math inline">\(S(x_i)\)</span>为<span class="math inline">\(x_i\)</span>对应的Saliency Map的值，<span class="math inline">\(\overline{S(x_i)}\)</span>为<span class="math inline">\(x_i\)</span>附近Saliency Map局部均值。</p><hr /><p>在实际操作中，FFT是在一个滑动窗口中进行的，文中提到SR方法在点位于窗口中央时效果更好，所以在进行测试的时候，按照如下方法对当前点<span class="math inline">\(x_n\)</span>(也就是当前序列最后一个点)之后的点进行预测：</p><p><span class="math display">\[\overline{g}=\frac{1}{m}\sum_{i=1}^m g(x_n,x_{n-i})\]</span></p><p><span class="math display">\[x_{n+1}=x_{n-m+1}+\overline{g}\cdot m\]</span></p><p>其中<span class="math inline">\(g(x_i,x_j)\)</span>代表<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>两点构成的直线的梯度；<span class="math inline">\(\overline{g}\)</span>代表所处理的点的平均梯度；<span class="math inline">\(m\)</span>为所处理的点的数量。在本文中设置<span class="math inline">\(m=5\)</span>。文中发现第一个预测的值很重要，所以直接把<span class="math inline">\(x_{n+1}\)</span>赋值<span class="math inline">\(k\)</span>次添加到序列的末尾。</p><h3 id="sr-cnn">SR-CNN</h3><p><img src="https://i.loli.net/2020/06/25/9pEWXRD5JM4umHn.png" srcset="/img/loading.gif" /></p><p>本文提到，仅仅使用一个阈值来进行异常的判断太过简单，于是提出使用一个判别模型来进行异常的判断。由于训练数据没有标签，所以使用如下的公式人工加入异常：</p><p><span class="math display">\[x=(\overline{x}+mean)(1+var)\cdot r+x\]</span></p><p>其中<span class="math inline">\(\overline{x}\)</span>所处理的点的局部均值；<span class="math inline">\(mean\)</span>和<span class="math inline">\(var\)</span>为当前滑动窗口点的均值和方差；<span class="math inline">\(r\sim \mathcal{N}(0,1)\)</span>为服从标准正态分布的噪声。</p><hr /><p>对于判别模型使用的是CNN，主要包含两个1维卷积层(kernel size等于窗口大小<span class="math inline">\(w\)</span>)和两个全连接层。两个卷积层的channel size分别为<span class="math inline">\(w\)</span>和<span class="math inline">\(2w\)</span>。</p><h2 id="experiments">Experiments</h2><h3 id="datasets">Datasets</h3><p><img src="https://i.loli.net/2020/06/25/dquAjtRpNSY8rLo.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>所用数据集包含清华AIOps竞赛数据、Yahoo和Microsoft的KPI数据。</p><h3 id="evaluation-metrics">Evaluation Metrics</h3><p>算法准确率方面用了precision，recall和<span class="math inline">\(F_1\)</span>-score。</p><p><img src="https://i.loli.net/2020/06/25/EbcjA8X5fYa6Glz.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>由于在实际场景中KPI的异常往往是以一段一段的形式出现，且并不要求某一个时间点出现异常算法就马上检测出来，只要检测出来的时间在一定的容忍范围内即可。本文使用了一些调整的手段，如Figure 6。对于某一段异常，设段首的异常位于时间点<span class="math inline">\(t_{truth}\)</span>，预测为异常的结果中时间在<span class="math inline">\(t_{truth}\)</span>之后且距<span class="math inline">\(t_{truth}\)</span>最近的时间点设为<span class="math inline">\(t_{predict}\)</span>，那么对于一个预先设定的容忍范围<span class="math inline">\(k\)</span>，只要<span class="math inline">\(t_{predict}-t_{truth}\leq k+1\)</span>那么在预测结果中整段异常就会重置为<span class="math inline">\(1\)</span>，否则全部重置为<span class="math inline">\(0\)</span>。</p><h3 id="results">Results</h3><p>实验部分使用了两种训练方式，一种是cold-start，即把所有数据都用来测试，另一种是把数据分为训练测试两部分，在训练集上训练，最后在测试集上进行测试。两种方法适用的baseline不同，最后结果如Table 2和Table 3所示：</p><p><img src="https://i.loli.net/2020/06/25/3L9xe5jJGtnR6AQ.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><hr /><p>在SR的参数设置上，<span class="math inline">\(h_q(f)\)</span>中的<span class="math inline">\(q\)</span>为3，局部平均所用的点数目<span class="math inline">\(z\)</span>为21，阈值<span class="math inline">\(\tau\)</span>为3，估计点的数量<span class="math inline">\(k\)</span>为5，滑动窗口的大小<span class="math inline">\(w\)</span>在KPI、Yahoo、Microsoft三个数据集上分别为1440、64和30。SR-CNN的<span class="math inline">\(q\)</span>，<span class="math inline">\(z\)</span>，<span class="math inline">\(k\)</span>，<span class="math inline">\(w\)</span>设置与SR相同。</p><h3 id="additional-experiments-with-dnn">Additional Experiments with DNN</h3><p>文中还对有监督的情况进行了测试，具体做法是从时间序列提取特征，然后将Saliency Map也作为特征引入，构造一个有监督的Neural Network进行测试。</p><p>提取的特征如Table 5所示：</p><p><img src="https://i.loli.net/2020/06/25/4flipKbc1OtVGg9.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><hr /><p>神经网络的结构为两层全连接层，并添加了Dropout Ratio为0.5的Dropout Layer。两个Layer使用了<span class="math inline">\(L_1=L_2=0.0001\)</span>的正则化。同时为了处理样本不平衡的情况使用了过采样来使正负样本的比例为<span class="math inline">\(1:2\)</span>。结构如Figure 7所示：</p><p><img src="https://i.loli.net/2020/06/25/bxeluFBHv2i7Y5m.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><hr /><p>训练测试集的情况如Table 6所示，最终结果如Table 7所示，P-R曲线如Figure 8所示。可以看到使用了SR特征的DNN效果由于没有使用SR特征的DNN。</p><p><img src="https://i.loli.net/2020/06/25/E6vapzhCiG9HTPu.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p><img src="https://i.loli.net/2020/06/25/tZAOg74flmE39oI.png" srcset="/img/loading.gif" style="zoom:67%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>Spectral</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
