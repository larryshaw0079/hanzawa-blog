<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Classification-based Anomaly Detection for General Data</title>
    <link href="/2020/06/02/Classification-based-Anomaly-Detection-for-General-Data/"/>
    <url>/2020/06/02/Classification-based-Anomaly-Detection-for-General-Data/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文主要是对<a href="http://qfxiao.me/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/">NIPS18这篇异常检测文章</a>的改进，首先是利用了标签信息来提升算法的表现，其次是将算法扩展到了非图像数据。作者对现有的异常检测算法进行了回顾：</p><ul><li><strong>Reconstruction Methods： </strong>这一部分方法假设异常样本和正常样本能够通过重构任务来进行区分。通过在正常样本上学习重构任务，之后对于正常样本，模型能够很好地进行重构，而异常样本则会有较高的重构误差。</li><li><strong>Distributional Methods： </strong>这一部分方法将异常检测看作是密度估计问题。通过对正常样本的分布进行估计，异常样本在该正常分布下的似然将会很低。</li><li><strong>Classification-based Methods： </strong>这一部分方法主要是指的单分类方法和通过几何变换构造分类任务的方法。本文使用的就是这类方法。</li></ul><h1 id="proposed-method">Proposed Method</h1><h2 id="classification-based-anomaly-detection">Classification-based Anomaly Detection</h2><p>假设所有数据位于空间<span class="math inline">\(R^L\)</span>内，而正常数据位于子空间<span class="math inline">\(X\subset R^L\)</span>内。我们假设所有的异常样本位于<span class="math inline">\(X\)</span>之外。为了检测异常，我们希望学习一个分类器<span class="math inline">\(C\)</span>使得对于所有的<span class="math inline">\(x\in X\)</span>有<span class="math inline">\(C(x)=1\)</span>，而对所有的<span class="math inline">\(x\in R^L\backslash X\)</span>有<span class="math inline">\(C(x)=0\)</span>。</p><p>单分类方法的思想是直接学习<span class="math inline">\(P(x\in X)\)</span>，代表的方法有One-Class SVM，DSVDD等。<em>Geometric-transformation classification</em> (GEOM) 则将数据空间<span class="math inline">\(X\)</span>通过<span class="math inline">\(M\)</span>个几何变换转换到一系列子空间<span class="math inline">\(X_1,\cdots,X_M\)</span>。之后训练一个分类器来预测样本<span class="math inline">\(T(x,m)\)</span>对应的几何变换的种类<span class="math inline">\(m\)</span>。转换后的正常图片空间记为<span class="math inline">\(\cup_m X_m\)</span>，所以该方法尝试估计以下条件概率： <span class="math display">\[P(m^\prime|T(x,m))=\frac{P(T(x,m)\in X_{m^\prime})P(m^\prime)}{\sum_{\bar{m}}P(T(x,m)\in X_{\bar{m}})P(\tilde{m})}-\frac{P(T(x,m)\in X_{m^\prime})}{\sum_{\bar{m}}P(T(x,m)\in X_{\bar{m}})}\]</span></p><p>对于异常的样本，</p><h2 id="distance-based-multiple-transformation-classification">Distance-based Multiple Transformation Classification</h2><p><img src="http://qfxiao.me/img/image-20200602230200536.png" srcset="/img/loading.gif" /></p><h2 id="parameterizing-the-set-of-transformations">Parameterizing the Set of Transformations</h2><h1 id="experiments">Experiments</h1><h2 id="image-experiments">Image Experiments</h2><p><img src="http://qfxiao.me/img/image-20200602231400942.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200602231415730.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200602231456162.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200602231554660.png" srcset="/img/loading.gif" /></p><h2 id="tabular-data-experiments">Tabular Data Experiments</h2><p><img src="http://qfxiao.me/img/image-20200602231515504.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200602231612977.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200602231627277.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面向OpenPAI的Docker镜像配置及OpenPAI基本使用方法</title>
    <link href="/2020/06/02/%E9%9D%A2%E5%90%91OpenPAI%E7%9A%84Docker%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E5%8F%8AOpenPAI%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <url>/2020/06/02/%E9%9D%A2%E5%90%91OpenPAI%E7%9A%84Docker%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E5%8F%8AOpenPAI%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>实验室服务器集群采用OpenPAI来进行GPU资源的管理，而OpenPAI采用了Docker作为基础，即代码都放在Docker容器中运行。由于Docker的使用、Docker镜像的配置都有一定的门槛，所以这里写一篇Tutorial来进行介绍。本文不是网上资料的拼凑，而是经过本人走弯路踩坑形成的&quot;Best practice&quot;。主要内容包括Docker的介绍、Docker的基本使用、如何配置自己的Docker镜像以及OpenPAI平台的基本使用，但不包括Docker和OpenPAI的安装。</p><h1 id="docker-from-scratch">Docker from Scratch</h1><p>要理解Docker是什么，从虚拟机开始讲可能会比较好理解。虚拟机大家可能都很熟悉了，比如说我用的系统是Windows，但我需要Linux系统来作为一个Flask编写的网站的服务器，但是又不想单独安装Linux系统，于是可以使用虚拟机来解决这个问题。安装VMWare Workstation，去官网下载Ubuntu系统镜像，然后在VMWare中安装好系统，然后从头配置Flask相关环境。实际上我需要的仅仅是一个Flask运行环境而已，而使用虚拟机却需要如此“大费周章”，这时Docker出现了，网上有大量现成的Flask Docker镜像，配置好了你所需的Flask环境，你只需要下载这些镜像，然后运行它，你就得到了一个Flask运行环境，而与你当前使用的系统无关。如果你需要一个Tomcat的运行环境，那么去找一个Tomcat的Docker镜像就行。Docker将需求或者说服务绑定在了Docker镜像中（<strong>轻量化</strong>，一个需求对应一个Docker镜像，每个镜像都很小），你有什么需求，去找相应的镜像即可（或者自己写一个），镜像的运行是以虚拟机的形式存在，所以他们之间也是互不干扰的。同时，你在写好一个Docker镜像之后，你还可以<strong>分享</strong>给别人，这样其他人就不用重新配置，直接运行你给他的镜像即可。Docker有两个比较关键的概念：</p><ul><li><strong>镜像 Images： </strong>这里的镜像不是指我们安装系统时下载的ISO镜像，Docker镜像就是把你需要的东西（一个系统+需要的服务）集中到一起，相当于做菜的菜谱；</li><li><strong>容器 Containers：</strong> 如果一个Docker镜像启动了，那么就会有一个Docker容器产生，相当于按照菜谱做出来的菜。</li></ul><p><img src="http://qfxiao.me/img/Container@2x_2020-602.png" srcset="/img/loading.gif" style="zoom:33%;" /></p><p><img src="http://qfxiao.me/img/VM@2x_20200602.png" srcset="/img/loading.gif" style="zoom:33%;" /></p><p>这一节我们先不讨论如何自己写Docker镜像，只是先讨论Docker的基本操作。</p><h2 id="basic-operations">Basic Operations</h2><p>Docker新安装好当然是没有什么镜像的，首先我们使用<code>docker pull hello-world</code>来下载一个测试镜像。</p><blockquote><p>拉取镜像 <code>docker pull &lt;image_name&gt;</code></p></blockquote><p>在输入之后，Docker会自动在远程服务器上查找对应的镜像进行下载。由于我的电脑上已经有这个镜像了，所以显示是下面的样子：</p><p><img src="http://qfxiao.me/img/image-20200602142125342.png" srcset="/img/loading.gif" /></p><p>接下来，我们输入<code>docker run hello-world</code>运行这个镜像。</p><blockquote><p>运行镜像<code>docker run &lt;image_name&gt;</code></p></blockquote><p>可以看到，Docker输出了一些信息就自己退出了，这和我们理解的虚拟机不太一样。在Docker里面，我们既可以创建一个完整的系统，用户在运行之后就可以正常使用这个操作系统，也可以创建一个简单的服务，默认运行完一些指令就退出了。这里的<code>hello-world</code>镜像这是输出了一些信息后就自动退出了，因为这就是这个镜像的全部内容。</p><p><img src="http://qfxiao.me/img/image-20200602142314849.png" srcset="/img/loading.gif" /></p><p>我们尝试来运行一个完整的系统，先用<code>docker pull ubuntu</code>拉取Ubuntu Docker镜像：</p><p><img src="http://qfxiao.me/img/image-20200602142718332.png" srcset="/img/loading.gif" /></p><p>接下来我们使用</p><p><img src="http://qfxiao.me/img/image-20200602144354467.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><blockquote><p><code>-it</code>的意思是什么？根据<code>docker run --help</code>：</p><pre><code class="hljs routeros">-i, --interactive                    Keep STDIN open even <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> attached    --ip string                      IPv4<span class="hljs-built_in"> address </span>(e.g., 172.30.100.104)    --ip6 string                    <span class="hljs-built_in"> IPv6 address </span>(e.g., 2001:db8::33)    --ipc string                     IPC mode <span class="hljs-keyword">to</span> use    --isolation string               Container isolation technology    --kernel-memory bytes            Kernel memory limit-t, --tty                            Allocate a pseudo-TTY    --ulimit ulimit                  Ulimit options (default [])</code></pre><p>其实<code>-it</code>是<code>-i</code>和<code>-t</code>的合并写法，意思是运行后进入这个容器并且启用shell，不然运行之后就会放到后台而不会进入容器中。而<code>--rm</code>则代表容器退出之后会被删除（镜像不会被删除），每次运行实际上会创建一个新的容器，如果不加<code>--rm</code>或退出之后不手动删除的话会看到一堆停止运行的容器。</p></blockquote><p>输入<code>cat /etc/issue</code>可以看到默认拉取的是最新的Ubuntu 20.04 LTS：</p><p><img src="http://qfxiao.me/img/image-20200602144424552.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="build-customized-docker-images">Build Customized Docker Images</h1><p>如果没有现成的Docker镜像能满足我们的需求，我们可以考虑自己写一个。要自定义一个Docker镜像需要两步，第一步是编写Dockerfile，第二步是使用<code>docker build</code>命令构建镜像。Dockerfile可以看作是一个脚本，描述了我们构建镜像所需要的全部命令，比如要构建一个用于Python科学计算的Docker镜像，我们需要在Dockerfile中编写安装Python的命令，安装Numpy、Scipy等常用包的命令等等。我们先来上手编写Dockerfile，这里我准备写一个包含<a href="https://hexo.io/" target="_blank" rel="noopener">hexo博客框架</a>的镜像，这个框架需要node作为基础环境，不过我们不需要在Dockerfile里写安装node的命令。因为类似于<code>C++</code>或<code>Python</code>中的对象的继承，Dockerfile也可以“继承”，这意味着我们不必从头写起。我们先来看一下完整的Dockfile和效果，再来一一解释。</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> node--------------------------------------------------------------<span class="hljs-keyword">RUN</span><span class="bash"> npm install -g hexo-cli</span><span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">4000</span><span class="hljs-keyword">CMD</span><span class="bash"> hexo init blog &amp;&amp; <span class="hljs-built_in">cd</span> blog &amp;&amp; hexo generate &amp;&amp; hexo server</span></code></pre><p>运行结果如下所示，可以看到Docker按照我们写的Dockerfile一行一行的进行镜像的构建：</p><p><img src="http://qfxiao.me/img/image-20200602161816051.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>现在来解释Docerfile里的内容。<code>FROM &lt;docker image&gt;</code>表示继承其他的镜像，这里我们使用node官方的镜像。接下来是安装hexo，<code>RUN &lt;command&gt;</code>表示执行命令，这里我们直接用<code>npm install -g hexo-cli</code>进行安装。由于要浏览博客网页需要开放端口，而Docker容器运行的时候和外部主机是完全隔断的，要使外部主机访问Docker容器端口，需要暴露端口。<code>EXPOSE &lt;port&gt;</code>代表暴露端口，这里用的是4000端口。之后是创建博客和启动本地服务，<code>CMD &lt;command&gt;</code>和<code>RUN &lt;command&gt;</code>的区别是RUN会在构建的时候执行，而CMD是在容器启动之后才会执行。<code>hexo init blog &amp;&amp; cd blog &amp;&amp; hexo generate &amp;&amp; hexo server</code>分别代表初始化博客、进入博客所在文件夹、生成博客网站、启动本地服务器。更多指令可以参考<a href="https://docs.docker.com/engine/reference/builder/" target="_blank" rel="noopener">官方文档</a>。</p><p>然后我们使用<code>docker build -t test_hexo .</code>命令构建镜像。</p><blockquote><p>构建镜像 <code>docker build -t &lt;image_name&gt; &lt;direcotry&gt;</code></p></blockquote><p>运行镜像：</p><p><img src="http://qfxiao.me/img/image-20200602161640400.png" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p>可以看到容器启动后开始执行博客初始化。</p><p><img src="http://qfxiao.me/img/image-20200602161713537.png" srcset="/img/loading.gif" style="zoom:33%;" /></p><p>最后在<code>locahost:4000</code>上启动了一个本地服务器，在浏览器中输入这个地址，可以看到刚刚构建好的博客：</p><p><img src="http://qfxiao.me/img/image-20200602161524359.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>值得注意的是，在Dockerfile中我们暴露了4000端口，使用<code>-p</code>标签可以达到同样的效果：<code>docker run -p &lt;docker_port&gt;:&lt;local_port&gt; &lt;image_name&gt;</code>。比如<code>docker run -p 9999:8888 xxxx</code>代表将Docker容器中的9999端口转发到外部主机的8888端口。如果你是在远程服务器上使用的Docker，那么端口只是被转发到了远程服务器上，还得手动将远程服务器再转发到你本机上才能直接在本机浏览器上看到页面。</p><h2 id="build-docker-images-with-aliyun-container-registry">Build Docker Images with Aliyun Container Registry</h2><p>因为某些原因，如果在构建镜像的时候需要通过<code>apt-get update</code>更新源，会发现无论如何都会卡住。这个时候可以使用<a href="https://cr.console.aliyun.com/" target="_blank" rel="noopener">阿里云容器镜像服务</a>，在阿里的服务器上构建好镜像，再拉取到自己的机器上。注册好帐号之后，点击创建镜像仓库：</p><p><img src="http://qfxiao.me/img/image-20200602162228133.png" srcset="/img/loading.gif" /></p><p>这里仓库类型如果没有特殊需求建议使用公开，然后填写一些基本信息：</p><p><img src="http://qfxiao.me/img/image-20200602162334122.png" srcset="/img/loading.gif" style="zoom: 67%;" /></p><p>之后设置代码源，其实就是告诉阿里云从哪儿获取Dockerfile，我这里用的是Github，所以需要先在阿里云中关联Github账号，然后在Github中创建一个用来放Dockerfile的仓库。构建设置里有一个“海外机器构建”，这正是我们使用阿里云容器服务的主要目的，勾选。</p><p><img src="http://qfxiao.me/img/image-20200602162419289.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>镜像仓库创建好之后，点进去，在构建页面点击添加规则：</p><p><img src="http://qfxiao.me/img/image-20200602162710789.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>按下图进行设置即可，镜像版本就是你想要的镜像名字：</p><p><img src="http://qfxiao.me/img/image-20200602162740273.png" srcset="/img/loading.gif" style="zoom: 67%;" /></p><p>点击“立即构建”：</p><p><img src="http://qfxiao.me/img/image-20200602162816545.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>等待一段时间后，如果构建成功，便可以进行拉取了，在镜像仓库的基本信息页面可以看到地址：</p><p><img src="http://qfxiao.me/img/image-20200602162533220.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>将阿里云上的镜像拉取到本机之后一般会想要对镜像改名，可以使用<code>docker tag &lt;old_name&gt; &lt;new_name&gt;</code>。</p><h1 id="build-docker-images-for-deep-learning">Build Docker Images for Deep Learning</h1><h2 id="startup">Startup</h2><p>在Docker中配置适用于OpenPAI的深度学习镜像不是一件容易的事，会有很多的坑，这里专门说一下如何配置。推荐在阿里云容器镜像服务中进行构建，会少很多麻烦。</p><p>第一步是初始镜像，由于需要用到CUDA，这里可以根据自己的需求（比如不同CUDA版本支持的GPU驱动版本不一样，还有Tensorflow不同版本对CUDA和cuDNN要求也不一样）从Nvidia的Dockerhub<a href="https://hub.docker.com/r/nvidia/cuda" target="_blank" rel="noopener">官方页面</a>选择合适的CUDA和cuDNN版本：</p><p><img src="http://qfxiao.me/img/image-20200603145713190.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><p>这里我们选择CUDA10.1 + cuDNN7：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">10.1</span>-cudnn7-devel-ubuntu18.<span class="hljs-number">04</span></code></pre><p>这一条主要是解决乱码问题以及定义用到的软件包的版本，这里Miniconda版本设置为4.5.4的原因是这是最后一个自带Python3.6的版本，我在这儿为了稳定所以用了Python3.6，大家也可以安装最新版的Miniconda：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">ENV</span> LANG=C.UTF-<span class="hljs-number">8</span> LC_ALL=C.UTF-<span class="hljs-number">8</span><span class="hljs-keyword">ENV</span> HADOOP_VERSION=<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span><span class="hljs-keyword">LABEL</span><span class="bash"> HADOOP_VERSION=2.7.2</span><span class="hljs-keyword">ENV</span> MINICONDA_VERSION=<span class="hljs-number">4.5</span>.<span class="hljs-number">4</span></code></pre><p>接下来安装必须的包，大家可以根据需求自行调整，<code>-y</code>标签代表Yes，即自动同意安装：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">RUN</span><span class="bash"> DEBIAN_FRONTEND=noninteractive &amp;&amp; \</span><span class="bash">    apt-get -y update &amp;&amp; \</span><span class="bash">    apt-get -y install build-essential \</span><span class="bash">        wget \</span><span class="bash">        git \</span><span class="bash">        curl \</span><span class="bash">        unzip \</span><span class="bash">        automake \</span><span class="bash">        openjdk-8-jdk \</span><span class="bash">        openssh-server \</span><span class="bash">        openssh-client \</span><span class="bash">        lsof \</span><span class="bash">        libcupti-dev &amp;&amp; \</span><span class="bash">    apt-get clean &amp;&amp; \</span><span class="bash">    rm -rf /var/lib/apt/lists/*</span></code></pre><p>安装Miniconda并设置环境变量，<code>-b</code>标签可以让Miniconda无交互自动安装：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">RUN</span><span class="bash"> wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh &amp;&amp; /bin/bash Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh -b -p /opt/miniconda \</span><span class="bash">&amp;&amp; rm Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh</span><span class="hljs-keyword">ENV</span> PATH /opt/miniconda/bin:$PATH</code></pre><p>安装Hadoop，OpenPAI平台会用到：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">RUN</span><span class="bash"> wget -qO- http://archive.apache.org/dist/hadoop/common/hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span>/hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span>.tar.gz | \</span><span class="bash">    tar xz -C /usr/<span class="hljs-built_in">local</span> &amp;&amp; \</span><span class="bash">    mv /usr/<span class="hljs-built_in">local</span>/hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span> /usr/<span class="hljs-built_in">local</span>/hadoop</span></code></pre><p><code>ENV</code>的作用是配置环境变量。配置JAVA和Hadoop环境变量：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">ENV</span> JAVA_HOME=/usr/lib/jvm/java-<span class="hljs-number">8</span>-openjdk-amd64 \    HADOOP_INSTALL=/usr/local/hadoop \    NVIDIA_VISIBLE_DEVICES=all<span class="hljs-keyword">ENV</span> HADOOP_PREFIX=$&#123;HADOOP_INSTALL&#125; \    HADOOP_BIN_DIR=$&#123;HADOOP_INSTALL&#125;/bin \    HADOOP_SBIN_DIR=$&#123;HADOOP_INSTALL&#125;/sbin \    HADOOP_HDFS_HOME=$&#123;HADOOP_INSTALL&#125; \    HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_INSTALL&#125;/lib/native \    HADOOP_OPTS=<span class="hljs-string">"-Djava.library.path=$&#123;HADOOP_INSTALL&#125;/lib/native"</span></code></pre><p>设置PATH环境变量：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">ENV</span> PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$&#123;HADOOP_BIN_DIR&#125;:$&#123;HADOOP_SBIN_DIR&#125; \LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/targets/x86_64-linux/lib/stubs:$&#123;JAVA_HOME&#125;/jre/lib/amd64/server</code></pre><p>完整的Dockerfile如下：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> nvidia/cuda:<span class="hljs-number">10.1</span>-cudnn7-devel-ubuntu18.<span class="hljs-number">04</span><span class="hljs-keyword">ENV</span> LANG=C.UTF-<span class="hljs-number">8</span> LC_ALL=C.UTF-<span class="hljs-number">8</span><span class="hljs-keyword">ENV</span> HADOOP_VERSION=<span class="hljs-number">2.7</span>.<span class="hljs-number">2</span><span class="hljs-keyword">LABEL</span><span class="bash"> HADOOP_VERSION=2.7.2</span><span class="hljs-keyword">ENV</span> MINICONDA_VERSION=<span class="hljs-number">4.5</span>.<span class="hljs-number">4</span><span class="hljs-keyword">RUN</span><span class="bash"> DEBIAN_FRONTEND=noninteractive &amp;&amp; \</span><span class="bash">    apt-get -y update &amp;&amp; \</span><span class="bash">    apt-get -y install build-essential \</span><span class="bash">        wget \</span><span class="bash">        git \</span><span class="bash">        curl \</span><span class="bash">        unzip \</span><span class="bash">        automake \</span><span class="bash">        openjdk-8-jdk \</span><span class="bash">        openssh-server \</span><span class="bash">        openssh-client \</span><span class="bash">        lsof \</span><span class="bash">        libcupti-dev &amp;&amp; \</span><span class="bash">    apt-get clean &amp;&amp; \</span><span class="bash">    rm -rf /var/lib/apt/lists/*</span><span class="hljs-keyword">RUN</span><span class="bash"> wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh &amp;&amp; /bin/bash Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh -b -p /opt/miniconda \</span><span class="bash">&amp;&amp; rm Miniconda3-<span class="hljs-variable">$&#123;MINICONDA_VERSION&#125;</span>-Linux-x86_64.sh</span><span class="hljs-keyword">ENV</span> PATH /opt/miniconda/bin:$PATH    <span class="hljs-keyword">RUN</span><span class="bash"> wget -qO- http://archive.apache.org/dist/hadoop/common/hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span>/hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span>.tar.gz | \</span><span class="bash">    tar xz -C /usr/<span class="hljs-built_in">local</span> &amp;&amp; \</span><span class="bash">    mv /usr/<span class="hljs-built_in">local</span>/hadoop-<span class="hljs-variable">$&#123;HADOOP_VERSION&#125;</span> /usr/<span class="hljs-built_in">local</span>/hadoop</span>    <span class="hljs-keyword">ENV</span> JAVA_HOME=/usr/lib/jvm/java-<span class="hljs-number">8</span>-openjdk-amd64 \    HADOOP_INSTALL=/usr/local/hadoop \    NVIDIA_VISIBLE_DEVICES=all<span class="hljs-keyword">ENV</span> HADOOP_PREFIX=$&#123;HADOOP_INSTALL&#125; \    HADOOP_BIN_DIR=$&#123;HADOOP_INSTALL&#125;/bin \    HADOOP_SBIN_DIR=$&#123;HADOOP_INSTALL&#125;/sbin \    HADOOP_HDFS_HOME=$&#123;HADOOP_INSTALL&#125; \    HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_INSTALL&#125;/lib/native \    HADOOP_OPTS=<span class="hljs-string">"-Djava.library.path=$&#123;HADOOP_INSTALL&#125;/lib/native"</span><span class="hljs-keyword">ENV</span> PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:$&#123;HADOOP_BIN_DIR&#125;:$&#123;HADOOP_SBIN_DIR&#125;:$PATH \LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/targets/x86_64-linux/lib/stubs:$&#123;JAVA_HOME&#125;/jre/lib/amd64/server</code></pre><p>建议先把这一部分进行构建，作为基础镜像，后面要配置其他环境（如安装Pytorch框架登），就不用重复构建这部分，还减少了出错的可能性。这里说一下，启动带CUDA的Docker镜像需要在<code>docker run</code>加上额外的参数<code>--runtime nvidia</code>。</p><p>接下来安装深度学习框架。</p><h2 id="configure-pytorch">Configure PyTorch</h2><p>假设上面的镜像我们命名为xiaoqinfeng/base，那么构建PyTorch的Dockerfile可以像下面这么写：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> xiaoqinfeng/base<span class="hljs-keyword">RUN</span><span class="bash"> pip install -U pip</span><span class="hljs-keyword">RUN</span><span class="bash"> pip install numpy scipy pandas matplotlib tqdm</span><span class="hljs-keyword">RUN</span><span class="bash"> pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html</span></code></pre><p>因为这里我用的CUDA10.1，其他版本的CUDA安装指令可能不太一样，具体可以参考<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">官网</a>。</p><h2 id="configure-tensorflow">Configure Tensorflow</h2><p>如果是安装Tensorflow，那么构建Tensorflow的Dockerfile可以像下面这么写：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> xiaoqinfeng/base<span class="hljs-keyword">RUN</span><span class="bash"> pip install -U pip</span><span class="hljs-keyword">RUN</span><span class="bash"> pip install numpy scipy pandas matplotlib tqdm tensorflow-gpu</span></code></pre><p>这里会自动安装最新版本的Tensorflow2。Tensorflow不同版本对CUDA和cuDNN版本甚至Python版本的支持都不太一样，可以参考<a href="https://www.tensorflow.org/install/source#linux" target="_blank" rel="noopener">官网</a>的说明。</p><h1 id="deep-learning-with-openpai">Deep Learning with OpenPAI</h1><h2 id="what-is-openpai">What is OpenPAI</h2><p>OpenPAI是一个分布式深度学习计算资源管理平台，对于我们用户来说，只需要定义好Docker镜像，然后编写好任务设置，提交到平台之后，平台便会自动分配计算资源来运行任务。</p><p>OpenPAI界面：</p><p><img src="http://qfxiao.me/img/openpai-3_20200603.png" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p><img src="http://qfxiao.me/img/openpai-4_20200603.png" srcset="/img/loading.gif" style="zoom:33%;" /></p><p>下面我们来讲讲怎么向OpenPAI平台提交任务。</p><h2 id="submit-jobs-to-openpai">Submit Jobs to OpenPAI</h2><h3 id="pack-code-data-files">Pack Code &amp; Data Files</h3><p>假设你已经完成了代码的编写和测试，你的目录结构可能看起来是这样：</p><pre><code class="hljs bash">.├── README.md├── data│   └── dataset.csv├── main.py└── src    ├── data.py    └── net.py</code></pre><p>因为OpenPAI会创建一个虚拟容器来运行你的代码，所以你的数据和代码必须要以某种方式传送到OpenPAI上的虚拟容器中。我们先来打包，在代码目录下执行<code>tar -cvf files.tar ./</code>。之后，运行<code>python -m http.server &lt;port&gt;</code>。打开浏览器输入<code>&lt;server_ip&gt;:&lt;port&gt;</code>应该就能看到你的文件了：</p><p><img src="http://qfxiao.me/img/image-20200603194237439.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>由于这个http进程需要一直运行，所以建议使用<code>screen</code>放到后台执行。</p><h3 id="configure-tasks">Configure Tasks</h3><p>像OpenPAI提交任务可以采用网页提交也可以使用VSCode插件，这里我们采用网页提交。登入OpenPAI界面，点击Submit Job：</p><p><img src="http://qfxiao.me/img/image-20200603194425227.png" srcset="/img/loading.gif" /></p><p>可以看到提交任务的界面：</p><p><img src="http://qfxiao.me/img/image-20200603194501703.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>Job name大家可以自己设置。在Command一栏，是执行任务所需的全部命令，首先我们要做的就是将代码数据压缩包下载到容器中并解压：</p><pre><code class="hljs bash">wget &lt;server_ip&gt;:&lt;port&gt;/files.tartar -xvf files.tar</code></pre><p>然后是运行代码，假设我这里的任务比较简单，只有一行main.py的调用：</p><pre><code class="hljs bash">python main.py</code></pre><p>如果任务的执行比较复杂，也只需把命令填到Command里即可，OpenPAI会自动执行。接下来是设置配置，可以选GPU的数量，内存大小等等：</p><p><img src="http://qfxiao.me/img/image-20200603194711139.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>然后是镜像的选择：</p><p><img src="http://qfxiao.me/img/image-20200603194721231.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>要注意在本机上构建好镜像之后，需要把镜像重命名为<code>&lt;repository_address&gt;/&lt;image_name&gt;</code>的格式（我们的<code>&lt;repository&gt;</code>是<code>lin-ai-27:5000</code>，假设我的镜像名是<code>xiaoqinfeng/pytorch</code>，那就是改成<code>lin-ai-27:5000/xiaoqinfeng/pytorch</code>），然后执行<code>docker push</code>推送到Docker镜像服务器上才能在OpenPAI上使用。</p><p>提交之后，可以在Jobs界面看到任务的运行情况：</p><p><img src="http://qfxiao.me/img/image-20200603194828484.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Technical Notes</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>OpenPAI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ubuntu20.04LTS 深度学习环境配置 CUDA10.2 + cuDNN7.6.5 + Tensorflow + Pytorch</title>
    <link href="/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA10-2-cuDNN7-6-5-Tensorflow-Pytorch/"/>
    <url>/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA10-2-cuDNN7-6-5-Tensorflow-Pytorch/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>Ubuntu的最新LTS版本也更新到了20.04，在给新机器配置深度学习环境的时候发现比以前容易了许多，特此写一篇Tutorial。这里的安装方法只针对Ubuntu20.04LTS，对于其他版本的系统可能不太适用。</p><h1 id="install-gpu-drivers">Install GPU Drivers</h1><p>这里假设安装系统之后已经做好了必要的配置（安装常用软件依赖、修改国内源等）。Ubuntu20.04中GPU驱动可以直接通过GUI界面安装，十分方便，方法是找到软件与更新 (Software &amp; Updates)，在附加驱动 (additional drivers) 选项卡中选择驱动版本，一般是选择“专有，tested” (proprietary, tested) 那个，之后点Apply Changes，重启。</p><p><img src="http://qfxiao.me/img/software_updates_additional_drivers_nvidia.png" srcset="/img/loading.gif" /></p><h1 id="install-cuda-cudnn">Install CUDA &amp; cuDNN</h1><p>这里选择apt-get的方式安装CUDA：</p><pre><code class="hljs bash">sudo apt-get install nvidia-cuda-toolkit</code></pre><p>输入<code>nvcc --version</code>可以测试是否安装成功，输入<code>nvidia-smi</code>可以看到GPU信息和CUDA版本。</p><p>之后安装cuDNN，进入<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">官网</a>，选择Download cuDNN：</p><p><img src="http://qfxiao.me/img/image-20200602112803325.png" srcset="/img/loading.gif" /></p><p>会要求登录，如果没有账号的注册一个即可。在这里根据CUDA版本选择适合的cuDNN，我这里是CUDA10.2。我们选择deb包的方式安装，下载下图中圈出来的三个deb包，依次用<code>sudo dpkg -i xxx.deb</code>命令安装。</p><p><img src="http://qfxiao.me/img/image-20200602112922230.png" srcset="/img/loading.gif" /></p><h1 id="configure-python">Configure Python</h1><p>为了更好地管理Python包和虚拟环境，我们需要安装Anaconda。使用Anaconda之后，我们可以创建虚拟环境，虚拟环境之间互不干扰。做科学实验我们一般需要安装大量的Python包，有的包之间甚至还有冲突，如果我们把他们都安装在同一个环境下就会难以管理，甚至出冲突。而有了虚拟环境之后，我们可以把不同需求放在不同虚拟环境中，比如深度学习开发放在一个虚拟环境中（安装Tensorflow等），网站开发放在一个虚拟环境中（安装Flask等）。Anaconda默认自带大量的包，不过我们一般会创建新的虚拟环境去安装新的包，所以这里我们选用Miniconda。Miniconda和Anaconda唯一的区别是不会自带大量Python包，这里大家自行选择。Anaconda国内镜像下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/，Miniconda国内镜像下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/。</p><p>比如说我们下载的是<code>Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>，执行<code>bash Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>即可安装。显示一大屏用户协议哪儿按<code>q</code>可以直接跳过，其他选项的默认的输入<code>yes</code>即可。在提示是否需要conda init的时候记得输入<code>yes</code>。</p><p>安装成功之后，重开一个终端，可以看到现在处于<code>base</code>环境中：</p><p><img src="http://qfxiao.me/img/image-20200602114523426.png" srcset="/img/loading.gif" /></p><p>我们先配置一下国内镜像，执行</p><pre><code class="hljs bash">vim ~/.condarc</code></pre><p>然后粘贴下列文本使用清华源：</p><pre><code class="hljs less"><span class="hljs-attribute">channels</span>:  - defaults<span class="hljs-attribute">show_channel_urls</span>: true<span class="hljs-attribute">channel_alias</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda</span><span class="hljs-attribute">default_channels</span>:  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro</span>  - <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><span class="hljs-attribute">custom_channels</span>:  <span class="hljs-attribute">conda-forge</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">msys2</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">bioconda</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">menpo</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">pytorch</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>  <span class="hljs-attribute">simpleitk</span>: <span class="hljs-attribute">https</span>:<span class="hljs-comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span></code></pre><p>之后我们输入<code>conda create -n &lt;your_name&gt;</code>创建一个新的虚拟环境，如果需要指定Python版本，则<code>conda create --n &lt;your_name&gt; python=&lt;python_version&gt;</code>。之后输入<code>conda activate &lt;your_name&gt;</code>进入虚拟环境，如果需要退出，则使用<code>conda deactivate</code>。</p><p>安装常用包：</p><pre><code class="hljs bash">conda install --yes numpy scipy pandas matplotlib tqdm pip jupyter</code></pre><p><code>--yes</code>的作用是手动输入<code>y</code>来确认是否安装，这里列出的是一些最常用的Python包，大家可以根据自己的需求自行调整。<code>conda install</code>为Anaconda中安装Python包的方式。</p><h1 id="install-pytorch">Install PyTorch</h1><p>这里来安装PyTorch环境，推荐使用<code>conda create -n pytorch</code>创建一个专有虚拟环境，然后使用<code>conda install</code>安装常用包。对于安装PyTorch，我们可以使用<code>conda</code>也可以使用<code>pip</code>安装。<code>pip</code>是另外一个安装Python包的工具，由于不检查依赖所以比<code>conda</code>安装速度快，而且包的数量比<code>conda</code>多，使用也更广泛。同样<code>pip</code>也可以使用国内镜像加速下载，详见https://mirrors.tuna.tsinghua.edu.cn/help/pypi/。</p><p>对于CUDA10.2，官方给出的用<code>conda</code>安装PyTorch的命令是：</p><pre><code class="hljs bash">conda install pytorch torchvision cudatoolkit=10.2 -c pytorch</code></pre><p>用<code>pip</code>安装PyTorch的命令是：</p><pre><code class="hljs bash">pip install torch torchvision</code></pre><p>对于其他版本的CUDA安装命令可能不一样，可以去<a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">官网</a>查看。</p><p>可以使用以下命令来测试GPU版本的PyTorch是否正常工作：</p><pre><code class="hljs bash">python -c <span class="hljs-string">"import torch; print(torch.cuda.is_available())"</span></code></pre><h1 id="install-tensorflow">Install Tensorflow</h1><p>安装Tensorflow环境同样推荐创建一个专有虚拟环境。对于Tensorflow2的安装，使用<code>pip</code>十分方便，使用</p><pre><code class="hljs bash">pip install tensorflow tensorflow-gpu</code></pre><p>即可。要安装其他版本的Tensorflow可以使用<code>pip install tensorflow==&lt;tf_version&gt; tensorflow-gpu==&lt;tf_version&gt;</code>来指定版本。不过不同版本的Tensorflow要求的CUDA版本都有所不同，可以参考<a href="https://www.tensorflow.org/install/source#linux" target="_blank" rel="noopener">官网</a>的说明。</p><p>可以使用以下命令来测试GPU版本的Tensorflow是否正常工作：</p><pre><code class="hljs bash">python -c <span class="hljs-string">"import tensorflow as tf; tf.config.list_physical_devices('GPU')"</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>Technical Notes</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>Tensorflow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Anomaly Detection Using Geometric Transformations</title>
    <link href="/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/"/>
    <url>/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文考虑图像数据的异常检测问题。与基于重构的方法不同，本文提出的方法通过对正常图片施加不同的几何变换之后，训练一个多分类器将无监督异常检测问题转化为一个有监督问题。本方法背后的直觉是在训练能够分辨不同变换后的图片之后，分类器一定学得了一些显著的几何特征，这些几何特征是正常类别独有的。</p><h1 id="proposed-method">Proposed Method</h1><h2 id="problem-statement">Problem Statement</h2><p>本文考虑针对图像的异常检测。记<span class="math inline">\(\mathcal X\)</span>为所有自然图像的空间，<span class="math inline">\(X\subseteq\mathcal X\)</span>为正常图像集合。给定数据集<span class="math inline">\(S\subseteq X\)</span>，异常检测的目的是学习一个分类器<span class="math inline">\(h_S(x):\mathcal X\rightarrow\{0,1\}\)</span>，其中<span class="math inline">\(h_S(x)=1\Leftrightarrow x\in X\)</span>。</p><p>为了兼顾查准率和查全率，常用的设置是学习一个打分函数<span class="math inline">\(n_S(x):\mathcal X\rightarrow\mathbb R\)</span>，分数越高代表样本属于<span class="math inline">\(X\)</span>的概率越大。之后，通过设定阈值，便可以构建异常分类器： <span class="math display">\[\begin{align}h_S^\lambda(x)=\begin{cases}1 &amp; n_S(x)\leq\lambda\\0 &amp; n_S(x)&lt;\lambda\end{cases}\end{align}\]</span></p><h2 id="discriminative-learning-of-an-anomaly-scoring-function-using-geometric-transformations">Discriminative Learning of an Anomaly Scoring Function Using Geometric Transformations</h2><p>有初始数据集<span class="math inline">\(S\)</span>，几何变换集合<span class="math inline">\(\mathcal T\)</span>，通过对<span class="math inline">\(S\)</span>中每个样本施加这<span class="math inline">\(|\mathcal T|\)</span>个几何变换得到新数据集记为<span class="math inline">\(S_\mathcal{T}\)</span>，且<span class="math inline">\(S_\mathcal{T}\)</span>中每个样本的标签为变换的序号。之后，在<span class="math inline">\(S_\mathcal{T}\)</span>上训练一个<span class="math inline">\(|\mathcal T|\)</span>分类器。在测试阶段，对测试样本同样施加<span class="math inline">\(|\mathcal T|\)</span>个几何变换，分类器会给出经过<span class="math inline">\(\mathrm{softmax}\)</span>的输出向量，最终的异常分数由经过输出的向量构造的分布对数似然得来。</p><h3 id="creating-and-learning-the-self-labeled-dataset">Creating and Learning the Self-Labeled Dataset</h3><p>设<span class="math inline">\(\mathcal T=\{T_0,T_1,\cdots,T_{k-1}\}\)</span>为几何变换集合，<span class="math inline">\(1\leq i\leq k-1,\space T_i:\mathcal X\rightarrow \mathcal X\)</span>，且<span class="math inline">\(T_0(x)=x\)</span>。<span class="math inline">\(S_\mathcal{T}\)</span>定义为：</p><p><span class="math display">\[S_\mathcal T=\{(T_j(x),j):x\in S,T_j\in\mathcal T\}\]</span> 对于每个<span class="math inline">\(x\in S\)</span>，<span class="math inline">\(j\)</span>为<span class="math inline">\(T_j(x)\)</span>的标签。我们直接学习一个<span class="math inline">\(K\)</span>类分类器<span class="math inline">\(f_\theta\)</span>，来预测输入样本对应的几何变换种类，这相当于是一个图像分类问题。</p><h3 id="dirichlet-normality-score">Dirichlet Normality Score</h3><p>我们现在来定义异常分数<span class="math inline">\(n_S(x)\)</span>。设几何变换集合<span class="math inline">\(\mathcal T=\{T_0,T_1,\cdots,T_{k-1}\}\)</span>，且<span class="math inline">\(k\)</span>分类器<span class="math inline">\(f_\theta\)</span>在<span class="math inline">\(S_\mathcal{T}\)</span>上完成训练。对于任意一个样本<span class="math inline">\(x\)</span>，令<span class="math inline">\(\mathbf y(x)=\text{softmax}(f_{\theta}(x))\)</span>，即分类器<span class="math inline">\(f_\theta\)</span>输出的<span class="math inline">\(\text{softmax}\)</span>之后的向量。异常分数<span class="math inline">\(n_S(x)\)</span>定义为：</p><p><span class="math display">\[n_S(x)=\sum\limits_{i=0}^{k-1}\log p(\mathbf y(T_i(x))|T_i)\]</span></p><p>该异常分数定义为每个类别上，在几何变换<span class="math inline">\(T_i\)</span>的条件下，输出的<span class="math inline">\(\mathbf y\)</span>的对数似然之和。每个条件分布假设为<span class="math inline">\(\mathbf y(T_i(x))|T_i\sim\text{Dir}(\boldsymbol \alpha_i)\)</span>，其中<span class="math inline">\(\boldsymbol \alpha_i\in\mathbb R^k_+\)</span>，<span class="math inline">\(x\sim p_X(x)\)</span>，<span class="math inline">\(i\sim\text{Uni}(0,k-1)\)</span>，而<span class="math inline">\(p_X(x)\)</span>代表正常样本的真实数据分布。</p><p><span class="math display">\[n_S(x)=\sum_{i=0}^{k-1}\left[\log\Gamma(\sum_{j=0}^{k-1}[\tilde{\boldsymbol\alpha}_i]_j)-\sum_{j=0}^{k-1}\log\Gamma([\tilde{\boldsymbol\alpha}_i]_j)+\sum_{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]_j-1)\log\mathbf y(T_i(x))_j\right]\]</span></p><p><span class="math display">\[n_S(x)=\sum_{i=0}^{k-1}\sum_{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]_j-1)\log\mathbf y(T_i(x))_j=\sum_{i=0}^{k-1}(\tilde{\boldsymbol \alpha}_i-1)\cdot\log\mathbf y(T_i(x))\]</span></p><p>一个简单的形式是<span class="math inline">\(\hat{n}_S(x)=\frac{1}{k}\sum^{k-1}_{j=0}[\mathbf y(T_j(x))]_j\)</span>，</p><h1 id="experiments">Experiments</h1><h2 id="baselines">Baselines</h2><p>文中用到了如下的Baseline：</p><ul><li><strong>One-class SVM. </strong>单类支持向量机，作者使用了三个变体，分别为<strong>RAW-OC-SVM</strong>——使用原始数据作为输入，<strong>CAE-OC-SVM</strong>——使用一个卷积自编码器来获得低维表示作为输入和<strong>E2E-OC-SVM</strong>——全名为<strong>One-Class Deep Support Vector Data Description</strong>；</li><li><strong>Deep structured energy-based models. </strong></li><li><strong>Deep Autoencoding Gaussian Mixture Model. </strong></li><li><strong>Generative Adversarial Networks. </strong></li></ul><h2 id="datasets">Datasets</h2><ul><li><strong>CIFAR-10</strong></li><li><strong>CIFAR-100</strong></li><li><strong>Fashion-MNIST</strong></li><li><strong>CatsVsDogs</strong></li></ul><h2 id="experimental-protocol">Experimental Protocol</h2><p>设数据集有<span class="math inline">\(C\)</span>个类，我们会进行<span class="math inline">\(C\)</span>次实验，在第<span class="math inline">\(c\)</span>次实验 (<span class="math inline">\(1\leq c \leq C\)</span>)中我们会将第<span class="math inline">\(c\)</span>个类作为正常样本，而其他类作为异常样本。在训练阶段，训练集只包含正常样本，而在测试阶段则会有正常样本和异常样本。在获得异常分数之后，阈值<span class="math inline">\(\lambda\)</span>则根据ROC曲线下面积选择。</p><p>实验中使用的几何变换基于以下三种基变换：</p><ul><li><strong>Horizontal flip: </strong> 记为<span class="math inline">\(T_b^{flip}(x)\)</span>，<span class="math inline">\(b\in\{T,F\}\)</span>代表是否翻转；</li><li><strong>Translation: </strong> 记为<span class="math inline">\(T_{s_h,s_w}^{trans}(x)\)</span>，其中<span class="math inline">\(s_h,s_w\in\{-1,0,1\}\)</span>。在长宽两个维度上位移分别为<span class="math inline">\(0.25\)</span>高度和<span class="math inline">\(0.25\)</span>宽度，这两个维度发生位移的方向由<span class="math inline">\(s_h\)</span>和<span class="math inline">\(s_w\)</span>决定，当<span class="math inline">\(s_h=s_w=0\)</span>时代表不移动；</li><li><strong>Rotation by multiples 90 degrees: </strong> 记为<span class="math inline">\(T_k^{rot}(x)\)</span>，<span class="math inline">\(k\in\{0,1,2,3\}\)</span>。旋转<span class="math inline">\(k\times90\)</span>度。</li></ul><p>将三种基变换叠加有： <span class="math display">\[\mathcal T=\left\{ T_k^{rot}\circ T_{s_h,s_w}^{trans}\circ T_b^{flip} : \begin{matrix} b &amp;\in \{T,F\}\\ s_h,s_w&amp;\in\{-1,0,1\}\\ k&amp;\in\{0,1,2,3\} \end{matrix} \right\}\]</span> 最终几何变换种数为<span class="math inline">\(2\times3\times3\times4=72\)</span>种。</p><p>分类器模型使用的是<strong>Wide Residual Network</strong>，优化器为Adam，Batch size为128，训练轮数为200。</p><h2 id="results">Results</h2><p><img src="http://qfxiao.me/img/image-20200509211119999.png" srcset="/img/loading.gif"  /></p><h2 id="identifying-out-of-distribution-samples-in-labeled-multi-class-datasets">Identifying Out-of-distribution Samples in Labeled Multi-class Datasets</h2><h2 id="on-the-intuition-for-using-geometric-transformations">On the Intuition for Using Geometric Transformations</h2><p><img src="http://qfxiao.me/img/image-20200509211050138.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Cross-dataset Time Series Anomaly Detection for Cloud Systems</title>
    <link href="/2020/06/01/Cross-dataset-Time-Series-Anomaly-Detection-for-Cloud-Systems/"/>
    <url>/2020/06/01/Cross-dataset-Time-Series-Anomaly-Detection-for-Cloud-Systems/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文介绍了一种用于云计算平台的时间序列异常检测框架。为了解决标签不足的问题，文中使用了迁移学习的方法，即在有标签的source domain上训练模型，在没有标签的target domain上检测。同时，文中还使用了主动学习的方法来挑选最有价值的无标签样本进行标记。</p><p><a href="https://www.usenix.org/system/files/atc19-zhang-xu.pdf" target="_blank" rel="noopener">📰Get Paper</a></p><h1 id="background">Background</h1><p>针对云计算平台数据的异常检测通常是应用在云监控数据，如KPI、CPU使用率、系统负载等时序数据上。和传统的异常检测不一样的是，时序异常检测往往更难，文中总结了以下几个挑战：</p><ul><li>异常特征的差异性。在不同的云服务系统中，对异常的容忍度是不同的，所以对每个场景或系统组件设置准确的阈值来进行异常检测是十分困难的；</li><li>时间依赖性。该异常检测问题处理的是时间序列数据，而传统的异常检测并不会考虑时间依赖性；</li><li>无监督学习的性能问题。无监督的异常检测方法的性能有限，会带来大量的误报；</li><li>有监督学习需要大量标签。</li></ul><h1 id="proposed-approach">Proposed Approach</h1><p>为了解决上述挑战，文中提出了一个时间序列异常检测框架ATAD (Active Transfer Anomaly Detection)。该框架结合了迁移学习技术和主动学习技术，示意图如下：</p><p><img src="http://qfxiao.me/img/image-20200313212728223.png" srcset="/img/loading.gif" /></p><p>未标记数据<span class="math inline">\(T_u\)</span>是我们要检测的目标数据 (target domain)，标记数据<span class="math inline">\(T_l\)</span>是我们的源数据 (source domain)，可以是开源数据或者是其他系统的监控数据。</p><h2 id="transfer-learning-component">Transfer Learning Component</h2><p>在应用迁移学习时，我们需要考虑以下几个因素：</p><ul><li>我们处理的是时间序列数据，即在不同的时间点上样本之间不是相互独立的。为了解决这个问题，我们提取了不同的特征，每一个时间点被转换为了高维的特征向量，且每个时间点附近的背景信息被保存在了特征向量之中；</li><li>时间序列的粒度。粗粒度的迁移学习不利于发现异常，本文采用细粒度，即数据点级别的迁移学习；</li><li>迁移学习需要source domain和target domain具有潜在的相似性，所以我们需要对source domain中的样本进行过滤。</li></ul><p><img src="http://qfxiao.me/img/image-20200313212813264.png" srcset="/img/loading.gif" /></p><h3 id="feature-identification">Feature Identification</h3><p>这一节描述特征工程中用到的特征。在提取特征之前，文中使用了离散傅里叶变换来识别时间序列的周期<span class="math inline">\(p\)</span>，并为后面滑动窗口的大小原则作参考。</p><h4 id="statistical-features">Statistical Features</h4><p>统计特征包含了一些基本的统计信息，如均值、方差等，用到的特征如下表所示：</p><p><img src="http://qfxiao.me/img/image-20200313212825252.png" srcset="/img/loading.gif" /></p><p>表中的统计特征都是基于大小等于周期<span class="math inline">\(p\)</span>的滑动窗口的。</p><h4 id="forecasting-error-features">Forecasting Error Features</h4><p>使用预测特征的理由是如果一个数据点偏离预测值很远，那么它很有可能是异常。文中使用了多种时间序列预测模型，如SARIMA、Holt、Holt-Winters、STL等。最终的预测结果使用下式来加权集成： <span class="math display">\[\hat{Y}_t=\sum\limits_{m=1}^{M}\frac{\hat{Y}_{m,t}}{M-1}\left(1-\frac{RMSE_{m,t}}{\sum\limits_{n=1}^M RMSE_{n,t}}\right)\]</span> <span class="math inline">\(M\)</span>代表<span class="math inline">\(M\)</span>个不同模型，<span class="math inline">\(RMSE_{m,t}\)</span>代表模型<span class="math inline">\(m\)</span>在时间<span class="math inline">\(t\)</span>的<span class="math inline">\(RMSE\)</span>，<span class="math inline">\(\hat{Y}_t\)</span>是在时间<span class="math inline">\(t\)</span>的最终预测结果。之后，使用下表中的Metrics来计算不同预测特征：</p><p><img src="http://qfxiao.me/img/image-20200313212836275.png" srcset="/img/loading.gif" /></p><p>同样的，上述特征都是基于窗口的。</p><h4 id="temporal-features">Temporal Features</h4><p>这一部分是一些时间序列相关特征：</p><p><img src="http://qfxiao.me/img/image-20200313212850411.png" srcset="/img/loading.gif" /></p><p>最后，总共提取了37个特征，并且每个特征都进行了正则化。</p><h3 id="the-transfer-between-source-domain-and-target-domain">The Transfer between Source Domain and Target Domain</h3><p>本文结合了基于实例的迁移学习(<strong>Instance-based Transfer Learning</strong>)和基于特征的迁移学习(<strong>Feature-based Transfer Learning</strong>)。</p><p>首先，source domain中的数据差异性是比较大的，所以我们需要选择与target domain相似的样本。</p><p>基于实例的迁移学习(<strong>Instance-based Transfer Learning</strong>)的思想是选择source domain中与target domain相似的样本。对于source domain，在将时间序列<span class="math inline">\(T_l\)</span>转换为特征<span class="math inline">\(F_l\)</span>之后，本文使用<span class="math inline">\(K-means\)</span>算法将<span class="math inline">\(F_l\)</span>分成若干个簇。每个簇<span class="math inline">\(F_l^i, i\in[1,K]\)</span>是<span class="math inline">\(F_l\)</span>的不重叠子集。为了选择合适的样本，我们计算了target domain中的样本和每个簇中心点的欧几里得距离，然后样本会和距离最近的簇<span class="math inline">\(F_l^i\)</span>联系起来。</p><p>之后，为了使source domain和target domain在特征空间的差别更小，作者在每个簇上使用了<strong>CORrelation ALignment</strong> (CORAL) 算法。CORAL是一种领域适应算法 (<strong>Domain Adaption</strong>)，其基本思想是对source domain和target domain进行线性变换使其二阶统计信息（即协方差矩阵）的差别最小化： <span class="math display">\[\min_A\parallel A^\top C^i_lA-C^i_u\parallel_F^2\]</span></p><p>在最后一步，作者在每一个sub source domain <span class="math inline">\(\hat{F}_l^i\)</span>训练了有监督模型（随机森林或SVM），所以最后我们得到了<span class="math inline">\(K\)</span>个基模型。</p><h2 id="active-learning-component">Active Learning Component</h2><p>由于数据的差异性和复杂性太大，仅仅使用迁移学习的技术不足以达到很好的效果。在ATAD中，作者使用了主动学习技术来用较少的成本标注最有价值的样本来提升性能。本文中使用基于<strong>Uncertainty</strong>和<strong>Context Diversity</strong>的主动学习。</p><h3 id="uncertainty">Uncertainty</h3><p>大多数主动学习算法使用不确定性 (Uncertainty) 来作为选择要标记的样本的准则。 <span class="math display">\[Uncertainty=-|Prob(Normal)-Prob(Anomaly)|\]</span> 其中的<span class="math inline">\(Prob\)</span>由基模型给出。</p><h3 id="context-diversity">Context Diversity</h3><p>多样性 (Diversity) 也是一个选择要标记样本的重要参考。如果有两个相似的样本，那么就没有必要将他们都标记。</p><p>时间上相邻的样本往往也是相似的。</p><p>具体的来说，我们对所有样本按照<strong>Uncertainty</strong>排序，然后进行一次扫描，如果当前样本在候选集中某个样本的<strong>Context</strong>之中，我们则忽略当前样本，因为这代表当前样本和候选集中的那个样本是相似的。如果不在<strong>Context</strong>之中，我们则将该样本加入候选集中。</p><p>判断是否在某个样本的<strong>Context</strong>中，如下图所示，直接判断是否落在区间<span class="math inline">\([t-\alpha,t+\alpha]\)</span>中就是了。</p><p><img src="http://qfxiao.me/img/image-20200313212905175.png" srcset="/img/loading.gif" /></p><p>主动学习模块的算法流程图如下图所示：</p><p><img src="http://qfxiao.me/img/image-20200313212948057.png" srcset="/img/loading.gif" /></p><h1 id="experiments">Experiments</h1><p>在实验部分，作者试图回答以下问题：</p><ol type="1"><li>ATAD的效果如何？</li><li>迁移学习模块的有效性如何？</li><li>主动学习模块的有效性如何？</li><li>ATAD在基于公开数据时对公司内部数据检测效果如何？</li></ol><h2 id="dataset-and-setup">Dataset and Setup</h2><p>下表是用到的数据集的一些基本信息：</p><p><img src="http://qfxiao.me/img/image-20200313213006305.png" srcset="/img/loading.gif" /></p><h2 id="evaluation-metric">Evaluation Metric</h2><p>评测标准使用的是F1-score： <span class="math display">\[F1=\frac{2\cdot P\cdot R}{P+R}, \space P=\frac{TP}{TP+FP}, \space R=\frac{TP}{TP+FN}\]</span></p><h2 id="results">Results</h2><h3 id="rq1-how-effective-is-atad">RQ1: How effective is ATAD?</h3><p>Baseline包括孤立森林、K-Sigma、S-H-ESD和随机森林。</p><p>最终结果如下表所示：</p><p><img src="http://qfxiao.me/img/image-20200313213040896.png" srcset="/img/loading.gif" /></p><p>为了评测ATAD利用标签的能力，我们比较了RF在达到和ATAD相似F1 score情况下所需标签的数量，如下表所示：</p><p><img src="http://qfxiao.me/img/image-20200313213058084.png" srcset="/img/loading.gif" /></p><h3 id="rq2-how-effective-is-the-transfer-learning-component">RQ2: How effective is the Transfer Learning Component?</h3><p>我们从以下两个方面来探究模型迁移知识的能力：</p><ul><li>使用文中所用到的特征的重要性</li><li>本模型迁移知识的能力</li></ul><p>对于第一点，作者提出传统的方法一般只提取了统计特征，而本文还提取了多种其他特征。作者对提取不同特征进行了比较试验，结果如下表所示：</p><p><img src="http://qfxiao.me/img/image-20200313213108654.png" srcset="/img/loading.gif" /></p><p>除此之外，作者还展示了不同数据集下前10有效的特征：</p><p><img src="http://qfxiao.me/img/image-20200313213123043.png" srcset="/img/loading.gif" /></p><p>对于第二点，作者比较了是否使用文中的领域适应算法CORAL，在达到相似F1 score下所需的标签数，如下表所示：</p><p><img src="http://qfxiao.me/img/image-20200313213132432.png" srcset="/img/loading.gif" /></p><h3 id="rq3-how-effective-is-the-active-learning-component">RQ3: How effective is the Active Learning component?</h3><p>为了验证本文所用的主动学习的有效性，作者进行了对比试验。第一个模型 (Supervised model) 使用全部标签但不使用迁移学习训练，第二个 (Naïve) 为只使用主动学习而不使用迁移学习，第三个为本文提出的模型。结果如下图所示，为了达到相似的性能，不同模型需要的标签数。</p><p><img src="http://qfxiao.me/img/image-20200313213143405.png" srcset="/img/loading.gif" /></p><p>下表展示了使用不同主动学习策略 (U - conventional uncertainty method, UCD - 本文使用的方法, random - 随机选择) 进行标记得到的结果：</p><p><img src="http://qfxiao.me/img/image-20200313213153698.png" srcset="/img/loading.gif" /></p><p>同时作者还对不同<span class="math inline">\(\alpha\)</span>的选择进行了实验：</p><p><img src="http://qfxiao.me/img/image-20200313213208820.png" srcset="/img/loading.gif" /></p><h3 id="rq4-how-effective-is-atad-in-detecting-anomalies-in-a-companys-local-dataset-based-on-public-datasets">RQ4: How effective is ATAD in detecting anomalies in a company’s local dataset based on public datasets?</h3><p>这里作者对比了不同方法在微软内部数据集上的结果：</p><p><img src="http://qfxiao.me/img/image-20200313213219578.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Transfer Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</title>
    <link href="/2020/05/06/Learning-Representations-of-Ultrahigh-dimensional-Data-for-Random-Distance-based-Outlier-Detection/"/>
    <url>/2020/05/06/Learning-Representations-of-Ultrahigh-dimensional-Data-for-Random-Distance-based-Outlier-Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文提出了一种针对高维数据异常检测的表示学习方法。文中提出了<strong>RAMODO</strong>框架，一种基于排序的结合表示学习和异常检测的无监督框架。除此之外，基于<strong>RAMODO</strong>，文中还提出了基于此框架的模型<strong>REPEN</strong>。</p><p><a href="https://arxiv.org/pdf/1806.04808" target="_blank" rel="noopener">Paper📰</a></p><h1 id="proposed-method">Proposed Method</h1><h2 id="the-proposed-framework-ramodo">The Proposed Framework: <strong>RAMODO</strong></h2><h3 id="problem-statement">Problem Statement</h3><p>我们的目的是为高维数据学习低维表示，同时在学到的低维表示中能够更好地进行异常检测。设有数据集<span class="math inline">\(\mathcal{X}=\{\mathbf x_1,\mathbf x_2,\cdots, \mathbf x_N\}\)</span> (<span class="math inline">\(\mathbf x_i\in \mathbb{R}^D\)</span>) 和一个基于随机距离的异常检测器<span class="math inline">\(\phi:\mathcal{X}\mapsto \mathbb{R}\)</span>，我们的目标是学习一个表示函数<span class="math inline">\(f:\mathcal{X}\mapsto\mathbb{R}^M (M\ll D)\)</span>使得对于所有异常样本<span class="math inline">\(\mathbf x_i\)</span>和正常样本<span class="math inline">\(\mathbf x_j\)</span>都有<span class="math inline">\(\phi(f(\mathbf x_i))&gt;\phi(f(\mathbf x_j))\)</span>。</p><h3 id="ranking-model-based-representation-learning-framework">Ranking Model-based Representation Learning Framework</h3><p><strong>RAMODO</strong>基于<em>pairwise ranking model</em>。第一步是通过一定的预处理算法（原文中称为<em>outlier thresholding</em>）将数据划分为inlier候选集和outlier候选集；第二步通过随机从inlier候选集采样<span class="math inline">\(n\)</span>个样本生成query set <span class="math inline">\((\mathbf x_i,\cdots,\mathbf x_{i+n-1})\)</span>，从inlier候选集采样一个样本生成<em>positive example</em> <span class="math inline">\((\mathbf x^+)\)</span>，从outlier候选集采样一个样本生成<em>negative example</em> <span class="math inline">\((\mathbf x^-)\)</span>，将三者组合生成 <em>metatriplet</em> <span class="math inline">\(T=(&lt;\mathbf x_i,\cdots,\mathbf x_{i+n-1}&gt;,\mathbf x^+,\mathbf x^-)\)</span>；第三步通过神经网络<span class="math inline">\(f\)</span>学习表示；第四步通过<em>outlier score-based ranking loss</em> <span class="math inline">\(L(\phi(f(\mathbf x^+)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;),\phi(f(\mathbf x^-)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;))\)</span>来进行优化，其中<span class="math inline">\(\phi(\cdot|\cdot)\)</span>为基于距离的异常检测器。</p><p><img src="http://qfxiao.me/img/image-20200416151126952.png" srcset="/img/loading.gif" /></p><h2 id="a-ramodo-instance-repen">A <strong>RAMODO</strong> Instance: <strong>REPEN</strong></h2><p><strong>REPEN</strong>为<strong>RAMODO</strong>的实例模型，使用Sp作为异常检测器。</p><h3 id="outlier-thresholding-using-state-of-the-art-detectors-and-cantellis-inequality">Outlier Thresholding Using State-of-the-art Detectors and Cantelli's Inequality</h3><p>第一步使用Sp作为基础获得初始anomaly score：</p><blockquote><p><strong>Definition 1</strong> (<em>Sp-based Outlier Scoring</em>). 给定样本<span class="math inline">\(x_i\)</span>，Sp 以以下方式定义该样本的异常程度： <span class="math display">\[r_i=\frac{1}{m}\sum\limits_{j=1}^m nn\_dist(\mathbf x_i|\mathcal{S}_j)\]</span> 其中<span class="math inline">\(\mathcal S_j\subset \mathcal X\)</span>为数据集随机采样的子集，<span class="math inline">\(m\)</span>为集成大小，<span class="math inline">\(nn_dist(\cdot|\cdot)\)</span>为<span class="math inline">\(\mathcal S_j\)</span>中最近邻居的距离。</p></blockquote><p>接着通过<em>Cantelli's Inequality</em>来定义<em>Pseudo Outlier</em>：</p><blockquote><p><strong>Definition 2 </strong>(<em>Cantelli's Inequality-based Outlier Thresholding</em>). 给定异常分数向量<span class="math inline">\(\mathbf r\in\mathbb R^N\)</span>，更高异常分数代表更高的可能性为异常，设<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\delta^2\)</span>分别为均值和方差，<em>Outlier</em>候选集由以下方式确定： <span class="math display">\[\mathcal{O}=\{\mathbf x_i|r_i \geq \mu + \alpha\delta\}, \space\forall \mathbf x_i\in\mathcal X, \space r_i\in\mathbf r\]</span> 其中<span class="math inline">\(\alpha\geq 0\)</span>为自定义的阈值。</p></blockquote><p><em>Inlier</em>候选集<span class="math inline">\(\mathcal I=\mathcal X\backslash \mathcal O\)</span>。</p><h3 id="triplet-sampling-based-on-outlier-scores">Triplet Sampling Based on Outlier Scores</h3><p>首先，从<span class="math inline">\(\mathcal I\)</span>采样一定数量的样本组成<em>query set</em>，每个样本被采样的概率与其对应的异常分数有关：</p><p><span class="math display">\[p(\mathbf x_i)=\frac{\mathbb Z-r_i}{\sum_{t=1}^{|\mathcal I|}[\mathbb Z-r_t]}\]</span></p><p>其中<span class="math inline">\(\mathbb Z=\sum_{t=1}^{|\mathcal I|}r_t\)</span>。</p><p>之后从<em>inlier set</em>中均匀随机采样一个<em>positive sample</em> <span class="math inline">\(\mathbf x^+\)</span>。最后从<em>outlier set</em>中根据以下概率采样一个<em>negative sample</em> <span class="math inline">\(\mathbf x^-\)</span>： <span class="math display">\[p(\mathbf x_j)=\frac{r_j}{\sum_{t=1}^{|\mathcal O|}r_t}\]</span></p><h3 id="a-shallow-data-representation">A Shallow Data Representation</h3><p>单层神经网络用来获得浅层的表示：</p><blockquote><p><strong>Definition 3 </strong>(<em>Single-layer Fully-connected Representations</em>) 给定输入<span class="math inline">\(x\)</span>， <span class="math display">\[f_\Theta(\mathbf x)=\{\psi(\mathbf w_1^\top\mathbf x),\psi(\mathbf w_2^\top\mathbf x),\cdots,\psi(\mathbf w_M^\top\mathbf x)\}\]</span> 其中<span class="math inline">\(\psi(\cdot)\)</span>为激活函数，<span class="math inline">\(\mathbf w\)</span>为权重矩阵。</p></blockquote><h3 id="ranking-loss-using-random-nearest-neighbor-distance-based-outlier-scores">Ranking Loss Using Random Nearest Neighbor Distance-based Outlier Scores</h3><p>设<span class="math inline">\(\mathcal{Q}=&lt;f_\Theta(\mathbf x_i),\cdots,f_\Theta(\mathbf x_{i+n-1})&gt;\)</span>为<em>query set</em>，给定样本<span class="math inline">\(\mathbf x\)</span>，<strong>REPEN</strong>根据最近邻距离定义了<span class="math inline">\(f_\Theta(\mathbf x)\)</span>的异常程度： <span class="math display">\[\phi(f_\Theta(\mathbf x)|\mathcal{Q})=nn\_dist(f_\Theta(\mathbf x)|\mathcal Q)\]</span> 因此，给定三元组<span class="math inline">\(T=(\mathcal Q,f_\Theta(\mathbf x^+),f_\Theta(\mathbf x^-))\)</span>，我们的目标是学得表示<span class="math inline">\(f(\cdot)\)</span>使得： <span class="math display">\[nn\_dist(f_\Theta(\mathbf x^+)|\mathcal Q)&lt;nn\_dist(f_\Theta(\mathbf x^-)|\mathcal Q)\]</span> 损失函数： <span class="math display">\[J(\Theta;T)=L(\phi(f_\Theta(\mathbf x^+)|\mathcal Q),\phi(f_\Theta(\mathbf x^-)|\mathcal Q))=\\\max\{0, c+nn\_dist(f_\Theta(\mathbf x^+)|\mathcal Q)-nn\_dist(f_\Theta(\mathbf x^-)|\mathcal Q)\}\]</span> 其中<span class="math inline">\(c\)</span>为边界参数。给定一系列三元组，最终优化目标如下： <span class="math display">\[\mathop{\text{arg min}}\limits_{\Theta}\frac{1}{|\mathcal{T}|}\sum\limits_{i=1}^{|\mathcal T|}J(\Theta;T_i)\]</span></p><h3 id="the-algorithm-and-its-time-complexity">The Algorithm and Its Time Complexity</h3><p><img src="http://qfxiao.me/img/image-20200416151141351.png" srcset="/img/loading.gif" /></p><h3 id="leveraging-a-few-labeled-outliers-to-improve-triplet-sampling">Leveraging A Few Labeled Outliers to Improve Triplet Sampling</h3><h1 id="experiments">Experiments</h1><h2 id="datasets">Datasets</h2><ul><li>AD：网络广告检测</li><li>LC：肺癌疾病监测</li><li>p53：异常蛋白质活动检测</li><li>R8：文本分类</li><li>News20：文本分类</li><li>URL：异常网址检测</li><li>Webspam：Pascal Large Scale LearningChallenge</li></ul><h2 id="effectiveness-in-real-world-data-with-thousands-to-millions-of-features">Effectiveness in Real-world Data with Thousands to Millions of Features</h2><p>作者分别使用原始特征和<em>REPEN</em>学到的特征进行异常检测，IMP代表性能提升比例，SU代表加速比例。</p><p><img src="http://qfxiao.me/img/image-20200416151156535.png" srcset="/img/loading.gif" /></p><h2 id="comparing-to-state-of-the-art-representation-learning-competitors">Comparing to State-of-the-art Representation Learning Competitors</h2><ul><li><strong>AE: </strong>自编码器</li><li><strong>HLLE: </strong> <em>Hessian Locally Linear Embedding</em></li><li><strong>SRP: </strong> <em>Sparse Random Projection</em></li><li><strong>CoP: </strong> <em>Coherent Pursuit</em></li></ul><p><img src="http://qfxiao.me/img/image-20200416151203533.png" srcset="/img/loading.gif" /></p><h2 id="the-capability-of-leveraging-labeled-outliers-as-prior-knowledge">The Capability of Leveraging Labeled Outliers as Prior Knowledge</h2><p><img src="http://qfxiao.me/img/image-20200416151224069.png" srcset="/img/loading.gif" /></p><h2 id="sensitivity-test-w.r.t.-the-representation-dimension">Sensitivity Test w.r.t. the Representation Dimension</h2><p><img src="http://qfxiao.me/img/image-20200416151212730.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200416151234583.png" srcset="/img/loading.gif" /></p><p>文中提到了对于R8、URL、News20这三个数据集在维度<span class="math inline">\(M=1\)</span>的时候表现和其他维度一样好，作者给出的解释是在这几个数据集中异常部分是线性可分的，所以1维就足够了，另一个解释是优化问题。</p><h2 id="scalability-test">Scalability Test</h2><p><img src="http://qfxiao.me/img/image-20200416151241749.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Representation Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Weakly-supervised Anomaly Detection</title>
    <link href="/2020/03/30/Deep-Weakly-supervised-Anomaly-Detection/"/>
    <url>/2020/03/30/Deep-Weakly-supervised-Anomaly-Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>在文献中，因为标注成本的昂贵，无监督方法占据了异常检测的主要位置。然而，在现实生活中，我们可能会有少量标签，如何利用这部分标签信息就成为了一个问题，作者将其称之为<em>anomaly-informed modeling</em>。作者提出了两点挑战：</p><ol type="1"><li>少量标签可能无法提供所有类型异常的信息；</li><li>大部分无标签数据为正常样本，但其中包含少部分异常（污染）。</li></ol><p>作者提出了基于pairwise relation learning的方法来解决这些问题。文章的主要贡献如下：</p><ol type="1"><li>提出了一种基于pairing-based data augmentation和ordinal regression来进行弱监督异常检测的框架</li><li>基于该框架提出了PReNet，一种基于双流ordinal regression的网络</li><li>从理论和实践角度分析了方法的有效性</li><li>在40个真实数据集上进行了完善的实验</li></ol><h1 id="proposed-method">Proposed Method</h1><h2 id="learning-anomaly-scores-by-predicting-pairwise-relation">Learning Anomaly Scores by Predicting Pairwise Relation</h2><h3 id="problem-formulation">Problem Formulation</h3><p>给定数据集<span class="math inline">\(\mathcal{X}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N,\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，包含两部分，一部分是五标签数据<span class="math inline">\(\mathcal{U}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N\}\)</span>，另一部分是有标签异常数据<span class="math inline">\(\mathcal{A}=\{\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，其中<span class="math inline">\(K\ll N\)</span>。我们的任务目标是学习一个打分函数<span class="math inline">\(\phi:\mathcal{X}\mapsto \mathbb{R}\)</span>，使得对任任意异常样本的打分高于任意正常样本。</p><p>在这个Formulation里，作者将关系学习和异常打分统一了起来。首先，输入的数据集不再是原始样本，而是样本对。样本对包含三种：<em>anomaly-anomaly</em>，<em>anomaly-unlabeled</em>，<em>unlabeled-unlabeled</em>，记为<span class="math inline">\(C_{\{\mathbf{a},\mathbf{a}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>。每一个样本对包含一个标签<span class="math inline">\(y\)</span>，表示该pair对应的异常分数，整个输入数据集<span class="math inline">\(\mathcal{P}=\{\{\mathbf{x}_i,\mathbf{x}_j,y_{ij}\}|\mathbf{x}_i,\mathbf{x}_j\in\mathcal{X} \space\text{and}\space y_{ij}\in\mathbb{N}\}\)</span>。因为有<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}&gt;y_{\{\mathbf a,\mathbf u\}}&gt;y_{\{\mathbf u,\mathbf u\}}\)</span>，所以对关系的学习也是对异常打分的学习。</p><h3 id="the-instantiated-model-prenet">The Instantiated Model: PReNET</h3><p>下图为模型示意图，<strong>Data Augmentation</strong>模块负责产生pair数据，<strong>End-to-End Anomaly Score Learner <span class="math inline">\(\phi\)</span></strong> 模块负责关系学习（异常打分）。</p><p><img src="http://qfxiao.me/img/image-20200330001932457.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h4 id="data-argumentation-by-pairing">Data Argumentation by Pairing</h4><p>数据的产生分为两步：</p><ol type="1"><li>从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>上随机采样，组成pair；</li><li>对每个pair打上次序(ordinal class feature) 标签<span class="math inline">\(\mathbf{y}\)</span>。</li></ol><p>部分<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>和<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>可能包含异常污染，因为在<span class="math inline">\(\mathcal{U}\)</span>中可能会有未标记的异常样本。</p><h4 id="end-to-end-anomaly-score-learner">End-to-End Anomaly Score Learner</h4><p>令<span class="math inline">\(\mathcal{Z}\in\mathbb{R}^M\)</span>为中间表示空间，那么<strong>Score Learner</strong>可以拆解为特征学习<span class="math inline">\(\psi(\cdot;\Theta_r):\mathcal{X}\mapsto \mathcal{Z}\)</span>和打分函数<span class="math inline">\(\eta((\cdot,\cdot);\Theta_s):(\mathcal{Z},\mathcal{Z})\mapsto\mathbb{R}\)</span>两部分，两部分都由神经网络组成。</p><h4 id="ordinal-regression">Ordinal Regression</h4><p>损失函数定义为： <span class="math display">\[L\left(\phi((\mathbf x_i,\mathbf x_j);\Theta),y_{ij}\right)=|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|\]</span> 采用绝对值而不是均方误差的原因是为了减少异常污染的影响。默认<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}=8\)</span>，<span class="math inline">\(y_{\{\mathbf a,\mathbf u\}}=4\)</span>，<span class="math inline">\(y_{\{\mathbf u,\mathbf u\}}=0\)</span>。最后的优化函数可以写为： <span class="math display">\[\mathop{\text{argmin}}\limits_{\Theta}\frac{1}{|\mathcal{B}|}\sum\limits_{\{\mathbf x_i,\mathbf x_j, y_{ij}\}\in\mathcal{B}}|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|+\lambda R(\Theta)\]</span> <span class="math inline">\(\mathcal{B}\)</span>为一个batch，<span class="math inline">\(R(\Theta)\)</span>为正则项。</p><h3 id="anomaly-detection-using-prenet">Anomaly Detection Using PReNet</h3><h4 id="training-stage">Training Stage</h4><p>训练流程如下图所示：</p><p><img src="http://qfxiao.me/img/image-20200330001945430.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><p>为了保证训练样本类别的平衡，<span class="math inline">\(\frac{|\mathcal{B}|}{2}\)</span>的样本采样自<span class="math inline">\(C_{\{\mathbf u,\mathbf u\}}\)</span>，采样自<span class="math inline">\(C_{\{\mathbf a,\mathbf u\}}\)</span>和<span class="math display">\[C_{\{\mathbf a,\mathbf a\}}\]</span>的样本都占<span class="math inline">\(\frac{|\mathcal{B}|}{4}\)</span>。</p><h4 id="anomaly-scoring-stage">Anomaly Scoring Stage</h4><p>在测试阶段，给定测试样本<span class="math inline">\(\mathbf{x}_k\)</span>，先分别从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>采样，然后定义以下<em>anomaly score</em>： <span class="math display">\[s_{\mathbf{x}_k}=\frac{1}{2E}\left[\sum\limits_{i=1}^E\phi((\mathbf a_i,\mathbf x_k);\Theta^*)+\sum\limits_{j=1}^E\phi((\mathbf x_k,\mathbf u_j);\Theta^*)\right]\]</span> <span class="math inline">\(\mathbf a_i\)</span>和<span class="math inline">\(\mathbf u_j\)</span>为随机采样得到的异常样本和正常样本，采样大小<span class="math inline">\(E\)</span>默认为30。</p><h1 id="experiments">Experiments</h1><p>实验部分主要是回答以下四个问题：</p><ol type="1"><li>在有限的标签异常情况下，PReNet能否有效地检测已知和未知的异常；</li><li>在不同数量标签异常的情况下，PReNet的表现如何；</li><li>PReNet对异常污染的鲁棒性如何；</li><li>PReNet不同组件的重要性如何。</li></ol><h2 id="datasets">Datasets</h2><p>实验一共用到了40个数据集，其中12个用来评测算法检测已知的异常的能力（如Table 2所示），28个用来评测算法检测未知的异常的能力（如Table 3所示）。</p><h2 id="competing-methods-and-parameter-settings">Competing Methods and Parameter Settings</h2><p>用到的baseline有以下几个：</p><ul><li>DevNet：同一作者在KDD2019提出的异常检测框架</li><li>Deep support vector data description (DSVDD)：深度支持向量数据描述</li><li>Prototypical network： few-shot classification中的一种模型</li><li>iForest：孤立森林</li></ul><h2 id="performance-evaluation-metrics">Performance Evaluation Metrics</h2><p>用到的Metrics为AUC-ROC和AUC-PR。</p><h2 id="detection-of-known-anomalies">Detection of Known Anomalies</h2><p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）是固定的，下表为实验结果：</p><p><img src="http://qfxiao.me/img/image-20200330002002763.png" srcset="/img/loading.gif" style="zoom: 80%;" /></p><h2 id="detection-of-unknown-anomalies">Detection of Unknown Anomalies</h2><p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）同样是固定的，下表为实验结果：</p><p><img src="http://qfxiao.me/img/image-20200330002045616.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><p><img src="http://qfxiao.me/img/image-20200330002100038.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h2 id="availability-of-known-anomalies">Availability of Known Anomalies</h2><p>本实验主要是研究不同数量标注异常样本的条件下，算法的性能如何。异常污染的比例固定（2%），标注异常的数量从15到120变化。实验结果如下：</p><p><img src="http://qfxiao.me/img/image-20200330002114989.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h2 id="further-analysis-of-prenet">Further Analysis of PReNet</h2><h3 id="tolerance-to-anomaly-contamination-in-unlabeled-data">Tolerance to Anomaly Contamination in Unlabeled Data</h3><p>本实验主要研究不同异常污染比例下，算法的性能，即探究算法对异常污染的鲁棒性。标注异常样本的数量恒定（60），异常污染比例在<span class="math inline">\(\{0\%,2\%,5\%,10\%\}\)</span>中变化。实验结果如下所示：</p><p><img src="http://qfxiao.me/img/image-20200330002150378.png" srcset="/img/loading.gif" style="zoom:80%;" /></p><h3 id="ablation-study">Ablation Study</h3><p>这一节是消融实验，分别设置了四个变体：</p><ul><li><strong>BOR: </strong>损失函数替换成了二值回归<em>Binary Ordinal Regression</em>；</li><li><strong>OSNet: </strong>将双流结构简化为单流；</li><li><strong>LDM: </strong>将网络中的隐藏层去除；</li><li><strong>A2H: </strong>加入了额外的隐藏层，并且加入了<span class="math inline">\(\ell_2\)</span>-norm防止过拟合。</li></ul><p><img src="http://qfxiao.me/img/image-20200330002158390.png" srcset="/img/loading.gif" style="zoom:80%;" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Discovering Physical Concepts with Neural Networks</title>
    <link href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/"/>
    <url>/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>如题目所示，本文的目的是利用神经网络来发掘物理概念。其思路是从实验数据学到表示，然后用学到的表示来回答物理问题，由此物理概念可以从学到的表示来提取出。作者进行了4个实验：</p><ol type="1"><li>在阻尼振动实验中，模型学到了相关的物理参数；</li><li>在角动量守恒实验中，模型预测了质点的运动；</li><li>给定量子系统的观测数据，模型正确的识别出了量子状态的自由度；</li><li>给定从地球观测的太阳和火星的位置时间序列数据，模型发现了日心说模型。</li></ol><h1 id="preliminaries">Preliminaries</h1><p>作者在附录中对神经网络的基础知识进行了介绍，这里不再赘述，只截取了一些相对前沿的内容。</p><p><img src="http://qfxiao.me/img/image-20200301230151518.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="variational-autoencoders">Variational Autoencoders</h2><p>本文用到的模型基础是VAE：</p><p><img src="http://qfxiao.me/img/image-20200301230200451.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h3 id="representation-learning">Representation Learning</h3><p><em>Representation learning</em>的主要目标是将数据映射到一个隐向量 (encoder)，为了保证隐向量包含了所有相关信息， 那么应该能够从隐向量还原原数据 (decoder)。传统的Autoencoder是这个思想的最简单实现，而VAE则将AE和<em>Variational Inference</em>结合了起来，是一种经典的生成式模型。现在很多研究关注<em>Disentangled Representation Learning</em>，也就是说我们希望模型能够无监督地学习数据，从中学到有意义的表示。</p><h3 id="boldsymbol-beta-vae"><span class="math inline">\(\boldsymbol \beta\)</span>-VAE</h3><p><span class="math inline">\(\beta\)</span>-VAE是一种特殊的VAE，也是一个经典的<em>Disentangled Representation Learning</em>模型，它和VAE主要的区别是对KL散度一项加上了权重<span class="math inline">\(\beta\)</span>进行调节： <span class="math display">\[C_\beta(x)=-\left[\mathbb{E}_{z\sim p_\phi(z|x)}\log p_\theta(x|z)\right] + \beta D_\text{KL}\left[p_\phi(z|x)\parallel h(z)\right]\]</span> 如果假设<span class="math inline">\(p_\phi(z|x)=\mathcal{N}(\mu,\sigma)\)</span>，那么损失函数可以进行简化： <span class="math display">\[C_\beta(x)=\parallel \hat{x} - x \parallel^2_2-\frac{\beta}{2}\left(\sum\limits_i\log(\sigma_i^2)-\mu_i^2-\sigma_i^2\right)+C\]</span></p><h1 id="network-structure">Network Structure</h1><h2 id="network-structure-scinet">Network Structure: <em>SciNet</em></h2><p>模仿物理学家建模物理问题的过程，作者提出了<em>SciNet</em>，如下图所示：</p><p><img src="http://qfxiao.me/img/image-20200301225708559.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>物理学家在建模物理问题的时候，往往是从一些实验数据出发，根据物理常识提取更加精练的表示，然后用学到的表示来回答物理问题。</p><p>对于单纯的输入输出问题，<em>SciNet</em>可以看作是一个映射，<span class="math inline">\(F:\mathcal{O}\times\mathcal{Q}\rightarrow\mathcal{A}\)</span>。<span class="math inline">\(\mathcal{O}\)</span>是可能的实验数据集合，<span class="math inline">\(\mathcal{Q}\)</span>是可能的问题集合，<span class="math inline">\(\mathcal{A}\)</span>是可能的答案集合。可以将其分为两个步骤：编码过程<span class="math inline">\(E:\mathcal{O}\rightarrow\mathcal{R}\)</span>从实验数据学到表示，解码过程<span class="math inline">\(D:\mathcal{R}\times \mathcal{Q}\rightarrow \mathcal{A}\)</span>根据给定的问题从表示来回答问题。由此，<span class="math inline">\(F(o,q)=D(E(o),q)\)</span>。在实现方面，<em>SciNet</em>采用的是全连接网络。</p><h2 id="training-and-testing-scinet">Training and Testing <em>SciNet</em></h2><p>用来训练的数据形式为<span class="math inline">\((o,q,a_{cor}(o,q))\)</span>，观测<span class="math inline">\(o\)</span>和问题<span class="math inline">\(q\)</span>分别从观测集<span class="math inline">\(\mathcal{O}\)</span>和问题集<span class="math inline">\(\mathcal{Q}\)</span>选出，<span class="math inline">\(a_{cor}(o,q)\)</span>为对应的正确答案。在训练过程中，我们希望准确度尽量高，并且学到<em>minimal uncorrelated representations</em>。为此，作者采用<em>disentangling variational autoencoder</em>作为模型。</p><h1 id="results">Results</h1><p>在文中，作者进行了4个实验来验证模型的有效性。</p><h2 id="damped-pendulum">Damped Pendulum</h2><p>阻尼振动实验：</p><ul><li><p>任务：预测一维阻尼振动在不同时间的位置。</p></li><li><p>物理模型：<span class="math inline">\(-kx-b\dot{x}=m\ddot{x}\)</span>，<span class="math inline">\(k\)</span>为弹性模量，<span class="math inline">\(b\)</span>为阻尼系数，通解为<span class="math inline">\(x(t)=A_0e^{-\frac{b}{2m}t}\cos(\omega t+\delta_0), \space \omega=\sqrt{\frac{k}{m}}\sqrt{1-\frac{b^2}{4mk}}\)</span></p></li><li>观测数据：位置时间序列数据<span class="math inline">\(o=[x(t_i)]_{i\in\{1,\cdots,50\}}\in\mathbb{R}^{50}\)</span>，时间间隔相等，质量<span class="math inline">\(m=1\text{kg}\)</span>，振幅<span class="math inline">\(A_0=1\text{m}\)</span>，相位<span class="math inline">\(\delta_0=0\)</span>，弹性模量<span class="math inline">\(k\in[5,10]\text{kg}/\text{s}^2\)</span>，阻尼系数<span class="math inline">\(b\in[0.5,1]\text{kg}/\text{s}\)</span>。</li><li><p>问题：预测<span class="math inline">\(q=t_\text{pred}\in\mathbb{R}\)</span></p></li></ul><p><img src="http://qfxiao.me/img/image-20200301225805576.png" srcset="/img/loading.gif" /></p><p>隐变量大小设置为3，结果如下图所示：</p><p><img src="http://qfxiao.me/img/image-20200301225838353.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>(b)中的三幅图分别是学到的三个隐变量和我们感兴趣的参数<span class="math inline">\(k\)</span>和<span class="math inline">\(b\)</span>的关系图。第一幅图中变量<span class="math inline">\(1\)</span>与<span class="math inline">\(b\)</span>几乎完全线性相关，与<span class="math inline">\(k\)</span>基于线性无关，变量<span class="math inline">\(2\)</span>只和<span class="math inline">\(k\)</span>相关。变量<span class="math inline">\(3\)</span>几乎为一个常数，故不提供额外的信息。由此作者认为<em>SciNet</em>学到了我们关心的两个参数的知识。</p><h2 id="conservation-of-angular-momentum">Conservation of Angular Momentum</h2><p>角动量守恒实验：</p><ul><li>任务：预测一个由长度为<span class="math inline">\(r\)</span>的绳子捆绑着的旋转质点在位置<span class="math inline">\((0,r)\)</span>经一个自由质点撞击后的位置</li><li>物理模型：给定撞击之前的角动量，自由质点撞击之后的速度，旋转质点在撞击之后在时间<span class="math inline">\(t_\text{pred}^\prime\)</span>的位置可以由角动量守恒定律给出：</li></ul><p><span class="math display">\[J=m_\text{rot}r^2\omega-rm_\text{free}(\mathbf{v}_\text{free})_x=m_\text{rot}r^2\omega^\prime-rm_\text{free}(\mathbf{v}^\prime_\text{free})_x=J^\prime\]</span></p><ul><li>观测数据：在撞击之前两个质点的位置数据<span class="math inline">\(o=[(t_i^\text{rot},q_\text{rot}(t_i^\text{rot})),(t_i^\text{free},q_\text{free}(t_i^\text{free}))]_{i\in\{1,\cdots,5\}}\)</span>，质量为固定值，半径<span class="math inline">\(r\)</span>也为固定值。数据添加高斯噪声。</li><li>问题：预测撞击之后自由质点在时间<span class="math inline">\(t_\text{pred}^\prime\)</span>的位置</li></ul><p><img src="http://qfxiao.me/img/image-20200301225858626.png" srcset="/img/loading.gif" /></p><p>实验室意图如下：</p><p><img src="http://qfxiao.me/img/image-20200301225917614.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>实验结果表明<em>SciNet</em>能够正确预测质点撞击之后的位置，同时对噪音鲁棒。根据(b)，隐变量和角动量存在线性相关关系，作者认为<em>SciNet</em>学到了守恒的动量这一概念。</p><h2 id="representation-of-qubits">Representation of Qubits</h2><p>量子比特实验：</p><ul><li>任务：预测在<span class="math inline">\(n=1,2\)</span>的纯<span class="math inline">\(n\)</span>量子位状态<span class="math inline">\(\psi\in\mathbb{C}^{2^n}\)</span>下任何二进制投影测量<span class="math inline">\(\omega\in\mathbb{C}^{2^n}\)</span>的测量概率。</li><li>物理模型：在执行测量<span class="math inline">\(\omega\in\mathbb{C}^{2^n}\)</span>的状态<span class="math inline">\(\psi\in\mathbb{C}^{2^n}\)</span>下测量0的概率<span class="math inline">\(p(\omega,\psi)\)</span>由<span class="math inline">\(p(\omega,\psi)=|\left&lt;\omega,\psi\right&gt;|^2\)</span>给定</li><li>观测数据：状态<span class="math inline">\(\psi: o=[p(\alpha_i,\psi)]_{i\in\{i,\cdots,n_1\}}\)</span>的操作参数化：表示一组固定的随机二元射影测量值<span class="math inline">\(\mathcal{M}_1=\{\alpha_1,\cdots,\alpha_{n_1}\}\)</span>（一个量子位<span class="math inline">\(n_1 = 10\)</span>，两个量子位<span class="math inline">\(n_1 = 30\)</span>）</li><li>问题：对于固定的一组随机二元射影测量<span class="math inline">\(\mathcal{M}_2=\{\beta_1,\cdots,\beta_{n_2}\}\)</span>，测量<span class="math inline">\(\omega:q=[p(\beta_i,\omega)]_{i\in\{1,\cdots,n_2\}}\)</span>的Operational参数化（一个量子位<span class="math inline">\(n_2 = 10\)</span>，两个量子位<span class="math inline">\(n_2 = 30\)</span>）</li></ul><p><img src="http://qfxiao.me/img/image-20200301225929696.png" srcset="/img/loading.gif" /></p><p>实验结果如下：</p><p><img src="http://qfxiao.me/img/image-20200301225958663.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>通过实验发现，<em>SciNet</em>可以在不提供先验物理知识的条件下确定表述状态<span class="math inline">\(\psi\)</span>最小的参数数量。同时，<em>SciNet</em>还能分辨<em>tomographically complete</em>和<em>tomographically incomplete</em>。</p><h2 id="heliocentric-model-of-the-solar-system">Heliocentric Model of the Solar System</h2><p>日心说模型：</p><ul><li>问题：在给定初始条件下预测相对与地球的太阳和火星的角度<span class="math inline">\(\theta_M(t)\)</span>和<span class="math inline">\(\theta_S(t)\)</span></li><li>物理模型：地球和火星围绕太阳以一定角速度做近似圆周运动</li><li>观测数据：给定初始角度，随机选择周周期的哥白尼的观测数据</li></ul><p><img src="http://qfxiao.me/img/image-20200301230035141.png" srcset="/img/loading.gif" /></p><p>模型的实现稍有变化，如下图所示：</p><p><img src="http://qfxiao.me/img/image-20200301230011495.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>这样，对于不同时间都对应一个隐变量<span class="math inline">\(r(t_i)\)</span>，而且隐变量是时间依赖的，对于一个隐变量<span class="math inline">\(r(t_i)\)</span>有一个解码器来输出答案。</p><p><img src="http://qfxiao.me/img/image-20200301230101709.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><p>实验结果表示，<em>SciNet</em>不仅正确预测了太阳和火星相对地球的角度，同时隐变量揭示了火星和地球相对太阳的角度。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transfer Anomaly Detection by Inferring Latent Domain Representations</title>
    <link href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/"/>
    <url>/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>作者提出了一种利用迁移学习提升target domain异常检测性能的算法。文中指出现有的基于迁移学习的异常检测算法需要对每个 target domain 进行单独训练，这样做会带来很大的计算开销。本文通过<em>latent domain vectors</em>来实现无需重新训练的异常检测。<em>latent domain vectors</em>是domain的一种隐含表示，通过该domain中的正常样本得到。在本文中，<em>anomaly score function</em>通过Auto-encoder得到。</p><h1 id="proposed-method">Proposed Method</h1><h2 id="task">Task</h2><p>令<span class="math inline">\(\mathbf{X}_d^+:=\{\mathbf{x}^+_{dn}\}^{N^+_d}_{n=1}\)</span>为第<span class="math inline">\(d\)</span>个domain的异常样本集，<span class="math inline">\(\mathbf{x}_{dn}^+\in\mathbb{R}^M\)</span>为其中第<span class="math inline">\(n\)</span>个样本的<span class="math inline">\(M\)</span>维特征向量，<span class="math inline">\(N^+_d\)</span>为第<span class="math inline">\(d\)</span>个domain异常样本的数量。</p><p>类似的，令<span class="math inline">\(\mathbf{X}_d^-:=\{\mathbf{x}^-_{dn}\}^{N^-_d}_{n=1}\)</span>为第<span class="math inline">\(d\)</span>个domain的正常样本集。我们假设对于每个domain都有<span class="math inline">\(N^+_d\ll N^-_d\)</span>，且特征向量维度都为<span class="math inline">\(M\)</span>。</p><p>假设在 source domain <span class="math inline">\(D_S\)</span>都有正常样本和异常样本，记为<span class="math inline">\(\{\mathbf{X}^+_d\cup\mathbf{X}_d^-\}^{D_S}_{d=1}\)</span>，在 target domain <span class="math inline">\(D_T\)</span>只有正常样本<span class="math inline">\(\{\mathbf{X}_d^-\}^{D_S+D_T}_{d=D_S+1}\)</span>。我们的目标是得到一个对于 target domain 合适的 domain-specific 的异常打分函数。</p><p><img src="http://qfxiao.me/img/image-20200109102606099.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h2 id="domain-specific-anomaly-score-function">Domain-specific Anomaly Score Function</h2><p>我们基于Auto-encoder定义异常打分函数。对于每个domain，我们假设存在一个<span class="math inline">\(K\)</span>维的隐变量<span class="math inline">\(\mathbf{z}_d\in\mathbb{R}^K\)</span>。对于第<span class="math inline">\(d\)</span>个 domain，异常打分函数定义如下： <span class="math display">\[s_\theta(\mathbf{x}_{dn}|\mathbf{z}_d):=\parallel\mathbf{x}_{dn}-G_{\theta_G}(F_{\theta_F}(\mathbf{x}_{dn},\mathbf{z}_d))\parallel^2\]</span> 其中参数<span class="math inline">\(\theta:=(\theta_G,\theta_F)\)</span>在所有 domain 之间共享。</p><h2 id="models-for-latent-domain-vectors">Models for Latent Domain Vectors</h2><p>隐变量<span class="math inline">\(\mathbf{z}_d\)</span>是无法观测到的，只能通过数据来估计。首先<span class="math inline">\(\mathbf{z}_d\)</span>在<span class="math inline">\(\mathbf{X}_d^-\)</span>条件下的条件分布假设为高斯分布：</p><p><span class="math display">\[q_\theta(\mathbf{z}_d|\mathbf{X}_d^-):=\mathcal{N}(\mathbf{z}_d|\mu_\phi(\mathbf{X}_d^-),\text{diag}(\sigma_\phi^2(\mathbf{X}_d^-)))\]</span> 其中均值<span class="math inline">\(\mu_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K\)</span>和方差<span class="math inline">\(\sigma^2_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K_+\)</span>由神经网络建模，且在所有 domain 之间共享。在<span class="math inline">\(\mathbf{X}_d^-\)</span>给定的时候，我们便能够推断出该 domain 对应的隐变量，</p><p><span class="math inline">\(q_\phi\)</span>的输入为正常样本的集合，故神经网络需要满足<em>permutation invariant</em>。<span class="math inline">\(\tau(\mathbf{X}_d^-)=\rho(\sum_{n=1}^{N_d^-}\eta(\mathbf{x}_{dn}^-))\)</span>，其中<span class="math inline">\(\tau(\mathbf{X}_d^-)\)</span>表示<span class="math inline">\(\mu_\phi(\mathbf{X_d^-})\)</span>或<span class="math inline">\(\ln\sigma_\phi^2(\mathbf{X}_d^-)\)</span>，<span class="math inline">\(\rho\)</span>和<span class="math inline">\(\eta\)</span>为神经网络，</p><h2 id="objective-function">Objective Function</h2><p>目标函数由anomaly score函数和隐变量组成。第<span class="math inline">\(d\)</span>个domain在对应的隐变量<span class="math inline">\(\mathbf{z}_d\)</span>条件下的目标函数为：</p><p><span class="math display">\[L_d(\theta|\mathbf{z}_d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)-\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d))\]</span></p><p>其中<span class="math inline">\(\lambda\geq 0\)</span>为超参数，<span class="math inline">\(f\)</span>为sigmoid函数。公式的第一项表示第<span class="math inline">\(d\)</span>个domain正常样本对应的<em>anomaly score</em>。第二项为可微分的AUC。异常样本的<em>anomaly score</em>应当大于正常样本，所以对任何<span class="math inline">\(\mathbf x_{dm}^+\in\mathbf X_d^+, \mathbf x_{dn}^-\in\mathbf X_d^-\)</span>有<span class="math inline">\(s_\theta(\mathbf x_{dm}^+|\mathbf z_d)&gt;s_\theta(\mathbf x_{dn}^-|\mathbf z_d)\)</span>。第二项<span class="math inline">\(\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d))\)</span>的取值范围是<span class="math inline">\([0,1]\)</span>，当所有的<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>时该项为1，当所有的<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\ll s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>时该项为0，所以最小化该项的相反数相当于鼓励<span class="math inline">\(s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}_{dm}^-|\mathbf{z}_d)\)</span>。</p><p>因为隐变量<span class="math inline">\(\mathbf z_d\)</span>包含不确定性，我们应该在目标函数里考虑这一点： <span class="math display">\[\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))\]</span></p><p>第一项是<span class="math inline">\(L_d(\theta|\mathbf z_d)\)</span>关于<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>的期望，第二项是<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>和<span class="math inline">\(p(\mathbf z_d):=\mathcal{N}(\boldsymbol 0,\boldsymbol I)\)</span>的KL散度。第一项可以用<em>monte carlo</em>估计<span class="math inline">\(\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]\approx\frac{1}{L}\sum_{\ell=1}^L L_d(\theta|\mathbf z_d^{(\ell)})\)</span>，除此之外还需要用到<em>reparametrization trick</em>。</p><p>对于第<span class="math inline">\(d\)</span>个target domain，因为没有异常样本（假设），所以<span class="math inline">\(L_d(\theta|\mathbf{z}_d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\)</span>，有： <span class="math display">\[\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z}_d))\]</span></p><p>所以总的损失函数为各domain对应的损失函数之和<span class="math inline">\(\mathcal{L}(\theta,\phi):=\sum_{d=1}^{D_S+D_T}\alpha_d\mathcal{L}_d(\theta,\phi)\)</span>。</p><h2 id="inference">Inference</h2><p>训练好之后，domain-specific的<em>anomaly score</em>可以由下式计算出：</p><p><span class="math display">\[s(\mathbf{x}_{d^\prime}):=\int s_{\theta_*}(\mathbf{x_{d^\prime}}|\mathbf{z}_{d^\prime})q_{\phi_*}(\mathbf{z}_{d^\prime}|\mathbf{X}_{d^\prime}^-)\mathrm{d}\mathbf{z}_{d^\prime}\approx\frac{1}{L}\sum\limits_{\ell=1}^L s_{\theta_*}(\mathbf{x}_{d^\prime}|\mathbf{z}_{d^\prime}^{(\ell)})\]</span></p><h1 id="experiments">Experiments</h1><h2 id="data">Data</h2><p>实验包含五个数据集，第一个是合成数据集。如下图(a)所示，围绕<span class="math inline">\((0,0)\)</span>有<span class="math inline">\(8\)</span>个圈，每个圈包含了一个内圈作为异常样本，第<span class="math inline">\(7\)</span>个圈被选为target domain，其余的为source domain。第二个是MNIST-r，是加入旋转的MNIST，包含6个domain，其中数字“4”被选为异常样本，其余为正常。第三个为Anuran Calls，包含5个domain。第四个是Landmine，主要用在多任务学习中。第五个是IoT，网络流量数据，包含8个domain。</p><p><img src="http://qfxiao.me/img/image-20200109102643644.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><h2 id="comparison-methods">Comparison Methods</h2><p>对比的baseline包括NN（普通多层神经网络），NNAUC（加入可微分AUC作为损失函数），AE（普通Autoencoer），AEAUC（加入可微分AUC的AE），OSVM（单类支持向量机），CCSA，TOSVM和OTL。</p><h2 id="results">Results</h2><p>4个真实数据集的结果如下：</p><p><img src="http://qfxiao.me/img/image-20200109102713247.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="http://qfxiao.me/img/image-20200109102730126.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="http://qfxiao.me/img/image-20200109102742222.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p><img src="http://qfxiao.me/img/image-20200109102753827.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>表5为考虑隐变量不确定性的ablation study。将原来的公式<span class="math inline">\(\mathcal{L}_d(\theta,\phi):=\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))\)</span>中<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)\)</span>用迪利克雷分布<span class="math inline">\(q_\phi(\mathbf z_d|\mathbf X_d^-)=\delta(\mathbf z_d-\mu_\phi(\mathbf X_d^-))\)</span>代替并且去掉KL散度。</p><p><img src="http://qfxiao.me/img/image-20200109102804386.png" srcset="/img/loading.gif" style="zoom: 50%;" /></p><p>表6展示了不同异常比例对效果的影响。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Transfer Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Deep Anomaly Detection with Deviation Networks</title>
    <link href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/"/>
    <url>/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文关注<code>Deep Anomaly Detection</code>，也就是用深度学习的方法来进行异常检测。文中提到现有的<code>Deep Anomaly Detection</code>存在两个弊端：一个是采用深度学习方法来进行特征学习，然后通过下游任务得到<code>Anomaly Score</code>，相比文中End-to-End的<code>Anomaly Score</code>学习，存在优化不充分的风险；另一个是现有的方法主要是无监督学习，无法利用已知的信息（如少量标签）。为此，本文提出了一种端到端的异常检测框架，来解决上述问题。</p><p>本文的主要贡献如下：</p><ul><li>提出了一种端到端的异常检测框架，直接学习<code>Anomaly Score</code>并且可以利用已知信息；</li><li>基于提出的框架，文中提出了一种实例方法 (DevNet)。</li></ul><p><img src="http://qfxiao.me/img/image-20200113104938784.png" srcset="/img/loading.gif" style="zoom:67%;" /></p><h1 id="proposed-model">Proposed Model</h1><h2 id="end-to-end-anomaly-score-learning">End-To-End Anomaly Score Learning</h2><h3 id="problem-statement">Problem Statement</h3><p>为了区别于传统的两阶段异常检测（先学习特征表示，然后在学到的特征上定义一个<code>anomaly measure</code>来得到<code>anomaly score</code>），作者对端到端的异常检测问题重新进行形式化。</p><p>给定<span class="math inline">\(N+K\)</span>个样本<span class="math inline">\(\mathcal{X}=\{\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N,\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}\}\)</span>，其中<span class="math inline">\(\boldsymbol x_i\in\mathbb{R}^D\)</span>，无标签样本集<span class="math inline">\(\mathcal{U}=\{\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N\}\)</span>，有标签样本集<span class="math inline">\(\mathcal{K}=\{\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}\}\)</span>，且<span class="math inline">\(K\ll N\)</span>。异常检测的目标是学习一个<code>anomaly scoring function</code><span class="math inline">\(\phi:\mathcal{X}\mapsto\mathbb{R}\)</span>使得<span class="math inline">\(\phi(\boldsymbol x_i)&gt;\phi(\boldsymbol x_j)\)</span>，其中<span class="math inline">\(\boldsymbol x_i\)</span>为异常样本，<span class="math inline">\(\boldsymbol x_j\)</span>为正常样本。</p><h3 id="the-proposed-framework">The Proposed Framework</h3><p>为了解决这个问题，文中提出了一种通用异常检测框架，模型框架如下图所示：</p><p>模型框架如下图所示：</p><p><img src="http://qfxiao.me/img/image-20200113104953902.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>主要包含三个部分：</p><ol type="1"><li><em>anomaly scoring network</em>. 图中左边的部分，一个函数<span class="math inline">\(\phi\)</span>，输入样本<span class="math inline">\(\mathbf{x}\)</span>，输出<code>anomaly score</code></li><li><em>reference score generator</em>. 图中右边的部分。只有一个<em>anomaly scoring network</em>并不能进行训练，需要训练的目标。为此加入<em>reference score generator</em>，输入为随机选择的<span class="math inline">\(l\)</span>个正常样本，输出<code>reference score</code>（这<span class="math inline">\(l\)</span>个正常样本<code>anomaly score</code>的均值，记为<span class="math inline">\(\mu_\mathcal{R}\)</span>）</li><li><em>deviation loss</em>. <span class="math inline">\(\phi(\mathbf{x})\)</span>，<span class="math inline">\(\mu_\mathcal{R}\)</span>及对应的标准差<span class="math inline">\(\sigma_\mathcal{R}\)</span>作为<code>deviation loss</code>函数的输入。因为<span class="math inline">\(\mu_\mathcal{R}\)</span>和<span class="math inline">\(\sigma_\mathcal{R}\)</span>对应正常样本集的均值和方差，那么异常样本的<code>anomaly score</code>应该和<span class="math inline">\(\mu_\mathcal{R}\)</span>差别比较大，而正常样本则应该接近<span class="math inline">\(\mu_\mathcal{R}\)</span>。</li></ol><h2 id="deviation-networks">Deviation Networks</h2><p>下面是上述三个部件的具体实现。</p><h3 id="end-to-end-anomaly-scoring-network">End-To-End Anomaly Scoring Network</h3><p>记<span class="math inline">\(\mathcal{Q}\in\mathbb{R}^M\)</span>为中间表示空间，<code>anomaly scoring network</code><span class="math inline">\(\phi(\cdot;\Theta):\mathcal{X}\mapsto\mathbb{R}\)</span>可以定义为数据表示学习<span class="math inline">\(\psi(\cdot;\Theta_t):\mathcal{X}\mapsto\mathcal{Q}\)</span>和异常分数学习<span class="math inline">\(\eta(\cdot;\Theta_s):\mathcal{Q}\mapsto\mathbb{R}\)</span>两阶段的组合，其中<span class="math inline">\(\Theta=\{\Theta_t,\Theta_s\}\)</span>。</p><p><span class="math inline">\(\psi(\cdot;\Theta_t)\)</span>可以用一个<span class="math inline">\(H\)</span>层神经网络来实现： <span class="math display">\[\mathrm{q}=\psi(\mathbf{x};\Theta_t)\]</span> 其中<span class="math inline">\(\mathbf{x}\in\mathcal{X}\)</span>，<span class="math inline">\(\mathrm{q}\in\mathcal{Q}\)</span>。</p><p><span class="math inline">\(\eta(\cdot;\Theta_s)\)</span>可以用一个单层的神经网络来实现： <span class="math display">\[\eta(\mathrm q;\Theta_s)=\sum\limits_{i=1}^M w_i^oq_i+w_{M+1}^o\]</span> 其中<span class="math inline">\(\mathrm q\in\mathcal Q\)</span>，<span class="math inline">\(\Theta_s=\{\mathbf{w}^o\}\)</span>。</p><p>所以有： <span class="math display">\[\phi(\mathbf{x};\Theta)=\eta(\psi(\mathbf{x};\Theta_t);\Theta_s)\]</span></p><h3 id="gaussian-prior-based-reference-scores">Gaussian Prior-based Reference Scores</h3><p>有两种方法来获得<span class="math inline">\(\mu_\mathcal{R}\)</span>，一种是data-driven，一种是prior-driven。如果是data-driven的话则采用另一个神经网络，文中表示为了更好的解释性和计算效率，所以采用的是prior-driven。 <span class="math display">\[\begin{align}r_1,r_2,\cdots,r_l\sim \mathcal{N}(\mu,\sigma^2),\\\mu_\mathcal{R}=\frac{1}{l}\sum\limits_{i=1}^l r_i\end{align}\]</span> 在文中，采用的prior是标准高斯分布。</p><h2 id="z-score-based-deviation-loss">Z-Score Based Deviation Loss</h2><p><em>anomaly scoring network</em>的优化目标可以定义为Z-Score的方式： <span class="math display">\[dev(\boldsymbol x)=\frac{\phi(\boldsymbol x;\Theta)-\mu_{\mathcal{R}}}{\sigma_{\mathcal{R}}}\]</span> <span class="math inline">\(dev(\boldsymbol x)\)</span>可以看作是样本偏离标准的程度，而我们肯定希望异常样本偏离标准越大，正常样本越接近标准。文中采用的损失函数是<code>Contrastive Loss</code>： <span class="math display">\[L(\phi(\boldsymbol x;\Theta),\mu_\mathcal{R},\sigma_\mathcal{R})=(1-y)|dev(\boldsymbol x)| + y \max(0, a - dev(\boldsymbol x))\]</span> <code>Contrastive Loss</code>的直观解释可以看下图：</p><p><img src="http://qfxiao.me/img/contrastive_2020_2_24.png" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>对于负例（正常），优化过程将他们尽量向原点靠近，对于正例（异常），优化过程将他们拉向边界。</p><h2 id="the-devnet-algorithm">The DevNet Algorithm</h2><p><code>DevNet</code>的算法流程图如下：</p><p><img src="http://qfxiao.me/img/image-20200113105040134.png" srcset="/img/loading.gif" /></p><h2 id="interpretability-of-anomaly-scores">Interpretability of Anomaly Scores</h2><p>因为<em>reference score generator</em>选择的是确定的高斯分布，于是可以用概率论给出一些解释性。作者给出了一个结论，</p><blockquote><p><strong>PROPOSITION</strong>： 设<span class="math inline">\(\boldsymbol x\in\mathcal{X}\)</span>，<span class="math inline">\(z_p\)</span>为<span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>的分位数，那么<span class="math inline">\(\phi(\boldsymbol x)\)</span>在区间<span class="math inline">\(\mu\pm z_p\sigma\)</span>的概率为<span class="math inline">\(2(1-p)\)</span>。</p></blockquote><p>例如，假设<span class="math inline">\(p=0.95\)</span>，那么<span class="math inline">\(z_{0.95}=1.96\)</span>，表示异常分数高于1.96的样本将以0.95的置信度为异常。</p><h1 id="experiment">Experiment</h1><p>实验用到了9个数据集，4个Baseline (REPEN，DSVDD，FSNET，iForest)，以及ROC和PR曲线两种评测标准。</p><h2 id="effectiveness-in-real-world-data-sets">Effectiveness in Real-world Data Sets</h2><h3 id="experiment-settings">Experiment Settings</h3><p>这一个实验主要是为了验证算法在真实场景下的效果，即大量无标签数据和极少量标签数据。训练集包含两部分，一部分是无标签数据<span class="math inline">\(\mathcal{U}\)</span>,包含<span class="math inline">\(2\%\)</span>的异常样本，另一部分是有标签数据<span class="math inline">\(\mathcal{K}\)</span>，由随机采样<span class="math inline">\(0.005\%-1\%\)</span>的训练数据和<span class="math inline">\(0.08\%-6\%\)</span>的异常样本组成。</p><h3 id="findings">Findings</h3><p>实验结果如下表所示：</p><p><img src="http://qfxiao.me/img/image-20200113110000432.png" srcset="/img/loading.gif" /></p><p>从结果上看来，本文提出的方法在所有数据集上都比Baseline好，说明<code>DevNet</code>端到端直接优化<code>Anomaly Score</code>的方式是有效的。</p><h2 id="data-efficiency">Data Efficiency</h2><h3 id="experiment-settings-1">Experiment Settings</h3><p>这一个实验主要是为了探究基于深度的异常检测方法的<em>data efficiency</em>。和上一个实验一样，无标签数据集包含<span class="math inline">\(2\%\)</span>的异常，而有标签的异常数量从<span class="math inline">\(5\)</span>到<span class="math inline">\(120\)</span>不等。本实验试图回答以下两个问题：</p><ul><li><code>DevNet</code>的<em>data efficiency</em>如何？</li><li>基于深度的方法在多大程度上能够利用标签信息？</li></ul><h3 id="findings-1">Findings</h3><p>在几个基于深度的Baseline中，<code>DevNet</code>的效果是最好的，同时在有标签异常非常有限的情况下，<code>DevNet</code>也能很好的利用标签信息，达到更好的效果。</p><p><img src="http://qfxiao.me/img/image-20200113110017195.png" srcset="/img/loading.gif" /></p><h2 id="robustness-w.r.t.-anomaly-contamination">Robustness w.r.t. Anomaly Contamination</h2><h3 id="experiment-settings-2">Experiment Settings</h3><p>在第一个实验中，无标签数据集<span class="math inline">\(\mathcal{U}\)</span>包含的是固定的异常比例<span class="math inline">\(2\%\)</span>，而在这个实验中，作者测试了从<span class="math inline">\(0\%\)</span>到<span class="math inline">\(20\%\)</span>之间不同异常比例来测试算法的鲁棒性（即使<span class="math inline">\(\mathcal{U}\)</span>中包含异常，由于没有标签，在训练的时候仍然假设都为正常来进行训练）。本实验试图回答以下问题：</p><ul><li>基于深度的异常检测方法的鲁棒性如何？</li><li>当训练集中异常污染的比例较高的时候基于深度的方法能否打败无监督的方法？</li></ul><h3 id="findings-2">Findings</h3><p>下图为实验结果：</p><p><img src="http://qfxiao.me/img/image-20200113110035878.png" srcset="/img/loading.gif" /></p><p>从结果上来看，<code>DevNet</code>比其他基于深度的方法鲁棒性更好，同时在高异常污染的情况下仍然比纯无监督方法效果要好。</p><h2 id="ablation-study">Ablation Study</h2><p>本实验设置了<code>DevNet</code>的三个变体（默认的<code>DevNet-Def</code>为单层隐层加上一个输出层）来进行消融实验，分别是：</p><ul><li><code>DevNet-Rep</code>，去掉了<em>anomaly scoring network</em>网络的输出层，对应<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>；</li><li><code>DevNet-Linear</code>，去掉了网络中的非线性层，对应<em>learning of non-linear features</em>；</li><li><code>DevNet-3HL</code>，隐层数量为3层。</li></ul><p>对比结果如下：</p><p><img src="http://qfxiao.me/img/image-20200113110048598.png" srcset="/img/loading.gif" /></p><p>通过实验可以发现，<code>DevNet-Rep</code>说明了<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>的有效性，而<code>DevNet-Linear</code>说明了<em>learning of non-linear features</em>的重要性。<code>DevNet-3HL</code>说明了加深网络并不总能带来性能的提升。</p><h2 id="scalability-test">Scalability Test</h2><p>这一个实验使用合成的数据来测试算法对大规模数据的处理能力，分别从<em>Data Size</em>和<em>Data Dimensionality</em>两方面来测试。结果如下：</p><p><img src="http://qfxiao.me/img/image-20200113110113423.png" srcset="/img/loading.gif" /></p><p>可以看出，<code>DevNet</code>对<em>Data Size</em>并不敏感，同时，面对高维数据，<code>DevNet</code>也没有表现出劣势。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Geant4 安装教程与调试环境配置</title>
    <link href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/"/>
    <url>/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>Geant4安装的教程很多，版本都很旧了，这里写一个新版本（10.6）基于Ubuntu的安装教程，并且开启CLion IDE调试。</p><h1 id="step-1-download-packages">Step 1: Download Packages</h1><p>首先进入官网(<a href="http://geant4.web.cern.ch/support/download" target="_blank" rel="noopener" class="uri">http://geant4.web.cern.ch/support/download</a>)下载源代码（推荐tar.gz格式）及数据文件，解压。新建一个文件夹专门用来放<code>Geant4</code>相关文件，新建data，source，build文件夹，将Geant4的文件复制进来并按如下结构组织：</p><pre><code class="hljs bash">.├── build├── data│   ├── G4ABLA3.1│   ├── G4EMLOW7.9│   ├── G4ENSDFSTATE2.2│   ├── G4INCL1.0│   ├── G4NDL4.6│   ├── G4PARTICLEXS2.1│   ├── G4PII1.3│   ├── G4SAIDDATA2.0│   ├── G4TENDL1.3.2│   ├── PhotonEvaporation5.5│   ├── RadioactiveDecay5.4│   └── RealSurface2.1.1└── <span class="hljs-built_in">source</span>    └── geant4.10.06</code></pre><p><img src="http://qfxiao.me/img/image-20200131113618716.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200131113628061.png" srcset="/img/loading.gif" /></p><h1 id="step-2-install-dependencies">Step 2: Install Dependencies</h1><p>安装编译所需环境：</p><pre><code class="hljs bash">sudo apt-get install build-essential cmake</code></pre><p>安装相关依赖：</p><pre><code class="hljs bash">sudo apt-get install libgl1-mesa-dev libglu1-mesa-dev libxt-dev libxmu-dev libxi-dev zlib1g-dev libgl2ps-dev libexpat1-dev libxerces-c-dev</code></pre><p>如果要用到QT需要单独安装QT。</p><h1 id="step-3-compile">Step 3: Compile</h1><p>进入build文件夹，用cmake命令：</p><pre><code class="hljs bash">cmake ../<span class="hljs-built_in">source</span>/geant4.10.06/ -DCMAKE_BUILD_TYPE=DEBUG -DGEANT4_USE_GDML=ON -DGEANT4_USE_OPENGL_X11=ON -DGEANT4_USE_RAYTRACER_X11=ON -DGEANT4_BUILD_MULTITHREADED=ON</code></pre><p>其中<code>../source/geant4.10.06/</code>替换成换成（如果版本不一样）你自己的Geant4源代码所在目录，需要QT则加上<code>-DGEANT4_USE_QT=ON</code>。如果不需要调试则把<code>-DCMAKE_BUILD_TYPE=DEBUG</code>改成<code>-DCMAKE_BUILD_TYPE=RELEASE</code>。<code>-DGEANT4_BUILD_MULTITHREADED=ON</code>是多线程，视情况开启。</p><p>完成之后开始编译：</p><pre><code class="hljs bash">make -jX</code></pre><p><code>-jX</code>为多线程编译，如<code>-j8</code>。</p><p>编译完成之后进行安装：</p><pre><code class="hljs bash">sudo make install</code></pre><h1 id="step-4-configure">Step 4: Configure</h1><p>安装的默认路径在<code>/usr/local/share/Geant4-10.6.0</code>，将下载的数据文件复制到该文件夹：</p><pre><code class="hljs bash">sudo cp -r ./data/ /usr/<span class="hljs-built_in">local</span>/share/Geant4-10.6.0/</code></pre><p>之后，在<code>~/.bashrc</code>里添加<code>/usr/local/share/Geant4-10.6.0/geant4make/geant4make.sh</code>，如果你的版本和我的不一样，相应修改即可。</p><h1 id="step-5-clion-configuration">Step 5: CLion Configuration</h1><p>最后我们来配置CLion环境，配好之后可以在IDE中编写<code>Geant4</code>代码，还可以断点调试，非常方便。安装CLion的过程这里省略，打开一个<code>Geant4</code>自带的例子或者自己新建一个项目，打开<code>Edit Configurations</code>。</p><p><img src="http://qfxiao.me/img/image-20200131121415836.png" srcset="/img/loading.gif" /></p><p>随便打开一个终端，输入一下命令获取环境变量：</p><pre><code class="hljs bash">env | grep G4</code></pre><p>在<code>Environment variables</code>填入刚才获取的环境变量（复制之后按一下粘贴就可以了），然后把<code>Working directory</code>设置成当前文件夹。</p><p><img src="http://qfxiao.me/img/image-20200131121655508.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200131121807181.png" srcset="/img/loading.gif" /></p><p>现在就大功告成了！</p>]]></content>
    
    
    <categories>
      
      <category>Technical Notes</category>
      
      <category>Misc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Geant4</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Complementary Set Variational Autoencoder for Supervised Anomaly Detection</title>
    <link href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/"/>
    <url>/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>对于异常检测问题，异常的模式是多种多样的。有监督模型能够较好地处理训练集中出现过的模式，无监督模型能够处理训练集中未出现过的模式，但对于训练集中出现过的异常模型并没有学习。本文提出了一种既能学习训练集中出现过的异常模式，同时能处理未出现过的异常模式的方法。</p><h1 id="proposed-model">Proposed Model</h1><h2 id="conventional-vae">Conventional VAE</h2><p>首先回顾一下原始的VAE。</p><p>原始VAE中的损失函数为： <span class="math display">\[\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})=\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}\parallel p(\boldsymbol{z}))]\]</span> 原文中作者证明了<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\leq\log p(\boldsymbol{x};\boldsymbol{\theta})\)</span>，所以<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\)</span>可以看作是数据分布<span class="math inline">\(p(\boldsymbol{x})\)</span>对数似然的一个下界。<span class="math inline">\(\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\)</span>又被称为证据下界 (ELBO)。<span class="math inline">\(\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]\)</span>中的期望一般用蒙特卡洛来进行估计： <span class="math display">\[\begin{align}\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq&amp; \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})\parallel p(\boldsymbol{z})],\\\boldsymbol{z}^{(l)}&amp;\sim q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}), \space l\in\{1,2,\cdots,L\}\end{align}\]</span> 对于隐变量<span class="math inline">\(\boldsymbol{z}\)</span>，一般假设先验服从标准高斯分布，后验服从均值为<span class="math inline">\(\mu\)</span>，方差为<span class="math inline">\(\sigma^2\)</span>的高斯分布，故KL散度能直接写出解析式： <span class="math display">\[\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-C(-\frac{1}{2}-\log\sigma+\frac{1}{2}\sigma^2+\frac{1}{2}\mu^2)\]</span> 使用VAE来做异常检测通常是在正常数据上进行训练，在检测阶段，如果是异常样本，那么VAE不能很好地重构它，这样会导致较大的重构误差。</p><h2 id="prior-distribution-for-anomalies">Prior Distribution for Anomalies</h2><p><img src="http://qfxiao.me/img/image-20200109102204802.png" srcset="/img/loading.gif" /></p><p>在原始VAE异常检测中，无论输入样本<span class="math inline">\(\boldsymbol{x}\)</span>是否异常，VAE都会使对应编码的后验<span class="math inline">\(p(\boldsymbol{z}|\boldsymbol{x})\)</span>服从高斯分布，且施加标准高斯分布的约束。在本文中，作者对异常和正常样本对应的隐变量的先验分布做了不同假设。首先，正常先验依然是标准高斯分布，记为<span class="math inline">\(p_n(\boldsymbol{z})\)</span>。而对于异常先验，作者认为异常即为“不正常”，和正常是补集的关系。作者在文中定义异常先验分布<span class="math inline">\(p_a(\boldsymbol{z})\)</span>为： <span class="math display">\[p_a(\boldsymbol{z})=\frac{1}{Y^\prime}(\max\limits_{\boldsymbol{z}^\prime}p_n(\boldsymbol{z}^\prime)-p_n(\boldsymbol{z}))\]</span></p><p>其中<span class="math inline">\(Y^\prime\)</span>为使<span class="math inline">\(p_a(\boldsymbol{z})\)</span>成为一个概率分布的调节因子。实际上，<span class="math inline">\(Y^\prime\)</span>往往会成为无限大，因为<span class="math inline">\(p(\boldsymbol z)\)</span>在整个定义域上都有定义。为了解决这个问题，作者加入了<span class="math inline">\(p_w(\boldsymbol z)\)</span>，一个在每个维度都足够宽的辅助分布：</p><p><span class="math display">\[p_a(\boldsymbol z)=\frac{1}{Y}p_w(\boldsymbol z)\left(\max\limits_{\boldsymbol z^\prime}p_n(\boldsymbol z^\prime)-p_n(\boldsymbol z)\right)\]</span></p><p>其中<span class="math inline">\(Y\)</span>为有限的常数。在文中<span class="math inline">\(p_n(\boldsymbol z)\)</span>和<span class="math inline">\(p_w(\boldsymbol z)\)</span>都为高斯分布，那么<span class="math inline">\(p_a(\boldsymbol z)\)</span>的具体形式为：</p><p><span class="math display">\[p_a(\boldsymbol z)=\frac{1}{Y}\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\{\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)-\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol 1)\}\]</span></p><p>其中：</p><p><span class="math display">\[\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)=\frac{1}{\sqrt{2\pi}}\]</span></p><p><span class="math display">\[Y=\int_{-\infty}^{\infty}p_a(\boldsymbol z)\mathrm{d}\boldsymbol z=\frac{1}{\sqrt{2\pi}}\left\{1-\frac{1}{\boldsymbol s^2+1}\right\}\]</span></p><p><span class="math inline">\(\boldsymbol s^2\)</span>为超参数，控制分布的宽度。用文中的先验替换VAE原始的KL散度，可写为：</p><p><span class="math display">\[\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]=\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\frac{\mathcal N(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)}{\frac{1}{Y}\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\left\{\frac{1}{2\pi}-\mathcal N(\boldsymbol z;\boldsymbol0,\boldsymbol 1)\right\}}\mathrm{d}\boldsymbol z\]</span></p><p>展开后：</p><p><span class="math display">\[\begin{align}\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;=\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)\mathrm{d}\boldsymbol z\\&amp;+\log Y\\&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\mathrm{d}\boldsymbol z\\&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\left\{\frac{1}{\sqrt{2\pi}}-\mathcal{N}(\boldsymbol z;\boldsymbol 0, \boldsymbol 1)\right\}\mathrm{d}\boldsymbol z\end{align}\]</span></p><p>使用泰勒展开，<span class="math inline">\(\log (x+\frac{1}{2\pi})\simeq-\log 2\pi+2\pi x\)</span>，KL散度可以用下式估计：</p><p><span class="math display">\[\begin{align}\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;\simeq\sqrt{\frac{2\pi}{\boldsymbol\sigma^2+1}}\exp\left(\frac{-\boldsymbol\mu^2}{2(\boldsymbol\sigma^2+1)}\right)\\&amp;+\frac{\boldsymbol\mu^2+\boldsymbol\sigma^2}{2\boldsymbol s^2}-\log\boldsymbol\sigma+\log\boldsymbol s+\log\left(\sqrt{\boldsymbol s^2+1}-1\right)\\&amp;-\frac{\log(\boldsymbol s^2+1)}{2}+\frac{\log(2\pi)-1}{2}\end{align}\]</span></p><p>下图为一维时<span class="math inline">\(p_n(\boldsymbol z)\)</span>和<span class="math inline">\(p_a(\boldsymbol z)\)</span>的示例：</p><p><img src="http://qfxiao.me/img/image-20200109102255322.png" srcset="/img/loading.gif" /></p><h3 id="implementation-of-proposed-method">Implementation of proposed method</h3><p>文中使用编码器输出的分布<span class="math inline">\(\mathcal{N}(\boldsymbol z;\boldsymbol \mu, \boldsymbol \sigma^2)\)</span>与标准正态分布之间的KL散度来作为异常分数。在每一轮的训练过程中，加入一轮使用Anomaly Prior的训练。</p><p><img src="http://qfxiao.me/img/image-20200109102309048.png" srcset="/img/loading.gif" /></p><h1 id="experiments">Experiments</h1><h2 id="mnist">MNIST</h2><p>作者设计了两个Task：</p><ol type="1"><li>Task 1. <span class="math inline">\(N\)</span> vs. <span class="math inline">\(\bar{N}\)</span>. 将手写数字中的一个作为已知异常，其他作为正常，并加入均匀分布作为未知的异常。</li><li>Task 2. 手写数字被分为3组：已知异常，正常，未知异常。</li></ol><p>细节如下表所示：</p><p><img src="http://qfxiao.me/img/image-20200109102402499.png" srcset="/img/loading.gif" /></p><p>在实现上，使用Adam优化器，<code>batch_size</code>为100，<code>epochs</code>为200。<code>Encoder</code>和<code>Decoder</code>都由三层感知机组成，超参数<span class="math inline">\(s^2\)</span>设置为400。评测标准使用AUC (area under the receiver characteristic curve)。</p><p>下表为实验结果：</p><p><img src="http://qfxiao.me/img/image-20200109102343271.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VAE</tag>
      
      <tag>Anomaly Detection</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Time Series Anomaly Detection Paper List</title>
    <link href="/2020/01/08/Time-Series-Anomaly-Detection-Paper-List/"/>
    <url>/2020/01/08/Time-Series-Anomaly-Detection-Paper-List/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>时间序列异常检测在很多领域例如运维、金融、交通都扮演者重要的角色，其定义如下：</p><blockquote><p>给定时间序列<span class="math inline">\(X=\{\mathbf{x}_1,\mathbf{x}_2,\cdots,\mathbf{x}_n\}\in\mathbb{R}^{m\times n}\)</span>，异常检测的任务是输出异常标签<span class="math inline">\(y=\{y_1,y_2,\cdots,y_n\}\in\mathbb{R}^n\)</span>，其中<span class="math inline">\(y_t=1\)</span>代表<span class="math inline">\(\mathbf{x}_t\)</span>为异常，<span class="math inline">\(y_t=0\)</span>代表<span class="math inline">\(\mathbf{x}_t\)</span>正常</p></blockquote><p>本文罗列了一些时间序列异常检测领域值得读的一些文章，Model一章主要按时间序列、通用和图的异常检测分类，Related主要是一些非异常检测、但是能够解决异常检测研究中的一些问题的文章。</p><p><img src="http://qfxiao.me/img/1571569717718.png" srcset="/img/loading.gif" /></p><h1 id="model">Model</h1><h2 id="time-series">Time Series</h2><h3 id="statistical">Statistical</h3><p>主要包括偏向统计方法的模型：</p><table><colgroup><col style="width: 30%" /><col style="width: 8%" /><col style="width: 30%" /><col style="width: 30%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Temporal Anomaly Detection: Calibrating the Surprise</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1705.10085" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Non-Parametric Outliers Detection in Multiple Time Series A Case Study: Power Grid Data Analysis</td><td></td><td></td><td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16315" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Anomaly Detection in Streams with Extreme Value Theory</td><td>KDD17</td><td>提出了以Extreme Value Theory为基础的时序异常检测和参数选择方法</td><td><a href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-in-streams-with-extreme-value-theory" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Semi-Markov Switching Vector Autoregressive Model-Based Anomaly Detection in Aviation Systems</td><td></td><td></td><td><a href="https://www.kdd.org/kdd2016/subtopic/view/semi-markov-switching-vector-autoregressive-model-based-anomaly-detection-i" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Stochastic Online Anomaly Analysis for Streaming Time Series</td><td></td><td></td><td><a href="https://www.ijcai.org/proceedings/2017/445" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Unsupervised Real-time Anomaly Detection for Streaming Data</td><td></td><td></td><td><a href="https://www.sciencedirect.com/science/article/pii/S0925231217309864" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Automatic Anomaly Detection in the Cloud Via Statistical Learning</td><td></td><td></td><td><a href="https://arxiv.org/abs/1704.07706" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="classic-machine-learning">Classic Machine Learning</h3><table><colgroup><col style="width: 35%" /><col style="width: 9%" /><col style="width: 20%" /><col style="width: 35%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Semi-supervised Anomaly Detection with an Application to Water Analytics</td><td></td><td></td><td><a href="https://people.cs.kuleuven.be/~vincent.vercruyssen/publications/2018/ICDM_conference_manuscript.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>DILOF: Effective and Memory Efficient Local Outlier Detection in Data Streams</td><td></td><td></td><td><a href="https://www.kdd.org/kdd2018/accepted-papers/view/dsilof-effective-and-memory-efficient-local-outlier-detection-in-data-strea" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Robust Random Cut Forest Based Anomaly Detection On Streams</td><td>ICML16</td><td>孤立森林的改进版本来做时序异常检测</td><td><a href="http://proceedings.mlr.press/v48/guha16.pdf" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/kLabUM/rrcf" target="_blank" rel="noopener">📥Code</a></td></tr><tr class="even"><td>Anomaly Detection for an E-commerce Pricing System</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1902.09566" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>An Adaptive Approach for Anomaly Detector Selection and Fine-Tuning in Time Series</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1907.07843" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Opprentice: Towards Practical and Automatic Anomaly Detection Through Machine Learning</td><td>IMC15</td><td>以随机森林为基础的时序异常检测</td><td><a href="http://netman.cs.tsinghua.edu.cn/wp-content/uploads/2015/11/liu_imc15_Opprentice.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Variational Inference for On-line Anomaly Detection in High-Dimensional Time Series</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1602.07109" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>A Self-Learning and Online Algorithm for Time SeriesAnomaly Detection, with Application in CPU Manufacturing</td><td></td><td></td><td><a href="https://dl.acm.org/citation.cfm?id=2983344" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="vae">VAE</h3><table><colgroup><col style="width: 35%" /><col style="width: 9%" /><col style="width: 19%" /><col style="width: 35%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</td><td>WWW18</td><td>普通VAE</td><td><a href="https://arxiv.org/pdf/1802.03903" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/haowen-xu/donut" target="_blank" rel="noopener">📥Code</a> <a href="http://qfxiao.me/2019/09/22/Unsupervised-Anomaly-Detection-via-Variational-Auto-Encoder-for-Seasonal-KPIs-in-Web-Applications/">✍Blog</a></td></tr><tr class="even"><td>Robust and Unsupervised KPI Anomaly Detection Based on Conditional Variational Autoencoder</td><td>IPCCC18</td><td>条件VAE</td><td><a href="https://ieeexplore.ieee.org/document/8710885" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/yantijin/Buzz" target="_blank" rel="noopener">📥Code</a></td></tr><tr class="odd"><td>Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE</td><td>INFOCOM19</td><td>VAE加对抗训练</td><td><a href="https://ieeexplore.ieee.org/abstract/document/8737430/" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Multidimensional Time Series Anomaly Detection: A GRU-based Gaussian Mixture Variational Autoencoder Approach</td><td></td><td></td><td><a href="http://proceedings.mlr.press/v95/guo18a/guo18a.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</td><td>KDD19</td><td>多变量基于VAE和RNN的时序异常检测</td><td><a href="https://www.kdd.org/kdd2019/accepted-papers/view/robust-anomaly-detection-for-multivariate-time-series-through-stochastic-re" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/smallcowbaby/OmniAnomaly" target="_blank" rel="noopener">📥Code</a></td></tr><tr class="even"><td>A Multimodal Anomaly Detector for Robot-Assisted Feeding Using an LSTM-based Variational Autoencoder</td><td></td><td></td><td><a href="http://ieeexplore.ieee.org/document/8279425/" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="rnn">RNN</h3><table><colgroup><col style="width: 39%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1802.04431" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>BINet: Multivariate Business Process Anomaly Detection Using Deep Learning</td><td></td><td></td><td><a href="https://link.springer.com/chapter/10.1007/978-3-319-98648-7_16" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Outlier Detection for Time Series with Recurrent Autoencoder Ensembles</td><td></td><td></td><td><a href="https://www.ijcai.org/proceedings/2019/0378.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1607.00148" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Detecting Anomalies in Space using Multivariate Convolutional LSTM with Mixtures of Probabilistic PCA</td><td>KDD19</td><td></td><td><a href="https://dl.acm.org/doi/10.1145/3292500.3330776" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="gan">GAN</h3><table><colgroup><col style="width: 35%" /><col style="width: 9%" /><col style="width: 20%" /><col style="width: 35%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1809.04758" target="_blank" rel="noopener">📃Paper</a> <a href="http://larryshaw0079.coding.me/blog/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/" target="_blank" rel="noopener">📥Blog</a></td></tr><tr class="even"><td>MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks</td><td>ICANN19</td><td>普通GAN做时序异常检测，没有编码结构</td><td><a href="https://arxiv.org/pdf/1901.04997" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/LiDan456/MAD-GANs" target="_blank" rel="noopener">📥Code</a></td></tr><tr class="odd"><td>BeatGAN: Anomalous Rhythm Detection using Adversarially Generated Time Series</td><td>IJCAI19</td><td>ECG异常检测，没有隐变量约束</td><td><a href="https://www.ijcai.org/proceedings/2019/616" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="miscellaneous">Miscellaneous</h3><table><colgroup><col style="width: 30%" /><col style="width: 16%" /><col style="width: 22%" /><col style="width: 30%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1811.08055" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>ALSR: An Adaptive Label Screening and Relearning Approach for Interval-Oriented Anomaly Detection</td><td>Expert Systems With Applications</td><td>有监督KPI异常检测，使用两阶段训练来提升性能</td><td><a href="https://www.sciencedirect.com/science/article/pii/S0957417419304282" target="_blank" rel="noopener">📃Paper</a> <a href="http://larryshaw0079.coding.me/blog/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/" target="_blank" rel="noopener">✍Blog</a></td></tr><tr class="odd"><td>Time-Series Anomaly Detection Service at Microsoft</td><td>KDD19</td><td>将视觉异常检测中的谱残差应用到了时序异常检测</td><td><a href="https://arxiv.org/pdf/1906.03821" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/microsoft/anomalydetector" target="_blank" rel="noopener">📥Code</a> <a href="http://larryshaw0079.coding.me/blog/2019/09/22/Time-Series-Anomaly-Detection-Service-at-Microsoft/" target="_blank" rel="noopener">✍Blog</a></td></tr><tr class="even"><td>Outlier Detection for Multidimensional Time Series using Deep Neural Networks</td><td></td><td></td><td><a href="https://ieeexplore.ieee.org/document/8411269" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h2 id="general">General</h2><h3 id="survey">Survey</h3><ul><li><em>Deep Learning for Anomaly Detection: A Survey</em> <a href="https://arxiv.org/abs/1901.03407" target="_blank" rel="noopener">[Paper]</a></li></ul><h3 id="classic-machine-learning-1">Classic Machine Learning</h3><table><colgroup><col style="width: 39%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>LOF: Identifying Density-Based Local Outliers</td><td></td><td></td><td><a href="https://dl.acm.org/citation.cfm?id=335388" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Isolation Forest</td><td></td><td></td><td><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Extended Isolation Forest</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1811.02141" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Hidden Markov Anomaly Detection</td><td></td><td></td><td><a href="http://proceedings.mlr.press/v37/goernitz15.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Linear-Time Outlier Detection via Sensitivity</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1605.00519" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Reverse Nearest Neighbors in Unsupervised Distance-Based Outlier Detection</td><td></td><td></td><td><a href="http://ieeexplore.ieee.org/document/6948273" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Theoretical Foundations and Algorithms for Outlier Ensembles</td><td></td><td></td><td><a href="https://www.kdd.org/exploration_files/Article4.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>R1SVM: A Randomised Nonlinear Approach to Large-Scale Anomaly Detection</td><td></td><td></td><td><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9343" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Random Gradient Descent Tree: A Combinatorial Approach for SVM with Outliers</td><td></td><td></td><td><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9477" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Sparse Gaussian Markov Random Field Mixtures for Anomaly Detection</td><td></td><td></td><td><a href="https://ieeexplore.ieee.org/document/7837932/" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Sequential Ensemble Learning for Outlier Detection: A Bias-Variance Perspective</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1609.05528" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Sparse Modeling-Based Sequential Ensemble Learning for Effective Outlier Detection in High-Dimensional Numeric Data</td><td></td><td></td><td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16648" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Contextual Spatial Outlier Detection with Metric Learning</td><td></td><td></td><td><a href="https://www.kdd.org/kdd2017/papers/view/contextual-spatial-outlier-detection-with-metric-learning" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Human-Assisted Online Anomaly Detection with Normal Outlier Retraining</td><td></td><td></td><td><a href="https://www.andrew.cmu.edu/user/lakoglu/odd/accepted_papers/ODD_v50_paper_11.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Efficient Anomaly Detection via Matrix Sketching</td><td></td><td></td><td><a href="https://papers.nips.cc/paper/8030-efficient-anomaly-detection-via-matrix-sketching.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Dual-Regularized Multi-View Outlier Detection</td><td></td><td></td><td><a href="https://www.ijcai.org/Proceedings/15/Papers/572.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Deep Anomaly Detection Using Geometric Transformations</td><td></td><td></td><td><a href="https://papers.nips.cc/paper/8183-deep-anomaly-detection-using-geometric-transformations.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models</td><td></td><td></td><td><a href="https://papers.nips.cc/paper/6456-multi-view-anomaly-detection-via-robust-probabilistic-latent-variable-models.pdf" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Partial Multi-View Outlier Detection Based on Collective Learning</td><td></td><td></td><td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17166" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Multi-View Anomaly Detection: Neighborhood in Locality Matters</td><td></td><td></td><td><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4418" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Latent Discriminant Subspace Representations for Multi-View Outlier Detection</td><td></td><td></td><td><a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17401" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Anomaly Detection with Partially Observed Anomalies</td><td></td><td></td><td><a href="https://dl.acm.org/citation.cfm?id=3184558.3186580" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>One-Class Active Learning for Outlier Detection with Multiple Subspaces</td><td></td><td></td><td><a href="https://dl.acm.org/citation.cfm?id=3357873" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection</td><td>NIPS19</td><td></td><td><a href="http://papers.nips.cc/paper/9274-statistical-analysis-of-nearest-neighbor-methods-for-anomaly-detection" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>SNIPER: Few-shot Learning for Anomaly Detection to Minimize False-negative Rate with Ensured True-positive Rate</td><td>ICASSP19</td><td></td><td><a href="https://ieeexplore.ieee.org/document/8683667" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="aevae">AE/VAE</h3><table><colgroup><col style="width: 39%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Estimation of Dimensions Contributing to Detected Anomalies with Variational Autoencoders</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1811.04576" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Anomaly Detection with Robust Deep Autoencoders</td><td></td><td></td><td><a href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-with-robust-deep-auto-encoders" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Complementary Set Variational Autoencoder for Supervised Anomaly Detection</td><td></td><td></td><td><a href="https://ieeexplore.ieee.org/document/8462181" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>A Two-class Hyper-spherical Autoencoder for Supervised Anomaly Detection</td><td></td><td></td><td><a href="https://ieeexplore.ieee.org/document/8683790" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="gan-1">GAN</h3><table><colgroup><col style="width: 39%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Adversarially Learned Anomaly Detection</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1812.02288" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>AMAD: Adversarial Multiscale Anomaly Detection on High-Dimensional and Time-Evolving Categorical Data</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1907.06582" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Adversarially Learned One-Class Classifier for Novelty Detection</td><td></td><td></td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sabokrou_Adversarially_Learned_One-Class_CVPR_2018_paper.pdf" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/khalooei/ALOCC-CVPR2018" target="_blank" rel="noopener">📥Code</a></td></tr><tr class="even"><td>Generative Probabilistic Novelty Detection with Adversarial Autoencoders</td><td></td><td></td><td><a href="https://papers.nips.cc/paper/7915-generative-probabilistic-novelty-detection-with-adversarial-autoencoders.pdf" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/podgorskiy/GPND" target="_blank" rel="noopener">📥Code</a></td></tr><tr class="odd"><td>OCGAN: One-class Novelty Detection Using GANs with Constrained Latent Representations</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1903.08550" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Fence GAN: Towards Better Anomaly Detection</td><td>Arxiv</td><td></td><td><a href="https://arxiv.org/abs/1904.01209" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Anomaly Detection via Minimum Likelihood Generative Adversarial Networks</td><td>Arxiv</td><td></td><td><a href="https://arxiv.org/abs/1808.00200" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN</td><td>ICDM18</td><td></td><td><a href="https://ieeexplore.ieee.org/document/8594955" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Learning Competitive and Discriminative Reconstructions for Anomaly Detection</td><td>AAAI19</td><td></td><td><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4451" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h3 id="miscellaneous-1">Miscellaneous</h3><table><colgroup><col style="width: 39%" /><col style="width: 10%" /><col style="width: 9%" /><col style="width: 39%" /></colgroup><thead><tr class="header"><th><strong>Title</strong></th><th><strong>Conf/Journal</strong></th><th><strong>Description</strong></th><th><strong>Links</strong></th></tr></thead><tbody><tr class="odd"><td>Deep Structured Energy Based Models for Anomaly Detection</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1605.07717" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Anomaly Detection using One-Class Neural Networks</td><td></td><td></td><td><a href="https://arxiv.org/pdf/1802.06360" target="_blank" rel="noopener">📃Paper</a> <a href="https://github.com/raghavchalapathy/oc-nn" target="_blank" rel="noopener">📥Code</a></td></tr><tr class="odd"><td>High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning</td><td></td><td></td><td><a href="https://www.sciencedirect.com/science/article/pii/S0031320316300267" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection</td><td></td><td></td><td><a href="https://openreview.net/pdf?id=BJJLHbb0-" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Deep Anomaly Detection with Outlier Exposure</td><td></td><td></td><td><a href="https://openreview.net/forum?id=HyxCxhRcY7" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Are Generative Deep Models for Novelty Detection Truly Better?</td><td>KDD18 Workshop</td><td></td><td><a href="https://arxiv.org/abs/1807.05027" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Probabilistic-Mismatch Anomaly Detection: Do one's Medications Match with the Diagnoses?</td><td>ICDM16</td><td></td><td><a href="https://ieeexplore.ieee.org/document/7837890" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Deep Anomaly Detection with Deviation Networks</td><td>KDD19</td><td></td><td><a href="https://dl.acm.org/doi/10.1145/3292500.3330871" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Weakly-supervised Deep Anomaly Detection with Pairwise Relation Learning</td><td>AAAI20</td><td></td><td><a href="https://arxiv.org/abs/1910.13601" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Transfer Anomaly Detection by Inferring Latent Domain Representations</td><td>NIPS19</td><td></td><td><a href="http://papers.nips.cc/paper/8517-transfer-anomaly-detection-by-inferring-latent-domain-representations" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>Multi-view Anomaly Detection via Robust Probabilistic Latent Variable Models</td><td>NIPS16</td><td></td><td><a href="http://papers.nips.cc/paper/6456-multi-view-anomaly-detection-via-robust-probabilistic-latent-variable-models" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="even"><td>Continual Learning for Anomaly Detection with Variational Autoencoder</td><td>ICASSP19</td><td></td><td><a href="https://ieeexplore.ieee.org/document/8682702" target="_blank" rel="noopener">📃Paper</a></td></tr><tr class="odd"><td>AdaFlow: Domain-adaptive Density Estimator with Application to Anomaly Detection and Unpaired Cross-domain Translation</td><td>ICASSP19</td><td></td><td><a href="https://ieeexplore.ieee.org/document/8683072" target="_blank" rel="noopener">📃Paper</a></td></tr></tbody></table><h2 id="graph">Graph</h2><ul><li><em>AddGraph: Anomaly Detection in Dynamic Graph Using Attention-based Temporal GCN</em> <a href="https://www.ijcai.org/proceedings/2019/614" target="_blank" rel="noopener">[Paper]</a></li><li><em>Outlier Detection in Graph Streams</em> <a href="https://ieeexplore.ieee.org/document/5767885" target="_blank" rel="noopener">[Paper]</a></li><li><em>NetWalk: A Flexible Deep Embedding Approach for Anomaly Detection in Dynamic Networks</em> <a href="https://www.kdd.org/kdd2018/accepted-papers/view/netwalk-a-flexible-deep-embedding-approach-for-anomaly-detection-in-dynamic" target="_blank" rel="noopener">[Paper]</a></li><li><em>Anomaly Detection in Dynamic Networks using Multi-view Time-Series Hypersphere Learning</em> <a href="https://dl.acm.org/citation.cfm?id=3132964" target="_blank" rel="noopener">[Paper]</a></li></ul><h1 id="related">Related</h1><p>这一部分主要是一些非异常检测文章，但是可以用来解决异常检测中的问题的相关文章。</p><h2 id="infrastructure">Infrastructure</h2><h3 id="gan-2">GAN</h3><ul><li><em>Generative Adversarial Networks</em> <a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">[Paper]</a></li></ul><h4 id="vae-gan-combination">VAE &amp; GAN Combination</h4><ul><li><em>Variational Approaches for Auto-Encoding Generative Adversarial Networks</em> <a href="https://arxiv.org/abs/1706.04987" target="_blank" rel="noopener">[Paper]</a></li><li><em>Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks</em> <a href="https://arxiv.org/abs/1701.04722" target="_blank" rel="noopener">[Paper]</a></li><li><em>On Unifying Deep Generative Models</em> <a href="https://arxiv.org/abs/1706.00550" target="_blank" rel="noopener">[Paper]</a></li></ul><h4 id="bidirectional-gans">Bidirectional GANs</h4><ul><li><em>Adversarial Feature Learning</em> <a href="Adversarial%20Feature%20Learning">[Paper]</a></li><li><em>Adversarially Learned Inference</em> <a href="https://arxiv.org/abs/1606.00704" target="_blank" rel="noopener">[Paper]</a></li><li><em>It Takes (Only) Two: Adversarial Generator-Encoder Networks</em> <a href="https://arxiv.org/abs/1704.02304" target="_blank" rel="noopener">[Paper]</a></li></ul><h3 id="vae-1">VAE</h3><ul><li><em>Auto-Encoding Variational Bayes</em> <a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="noopener">[Paper]</a></li></ul><h2 id="class-imbalance">Class Imbalance</h2><ul><li><em>Focal Loss for Dense Object Detection</em> <a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="noopener">[Paper]</a></li><li><em>Gradient Harmonized Single-stage Detector</em> <a href="https://arxiv.org/abs/1811.05181" target="_blank" rel="noopener">[Paper]</a></li></ul><h2 id="stochastic-temporal-modeling">Stochastic Temporal Modeling</h2><ul><li><em>Sequential Neural Models with Stochastic Layers</em> <a href="https://arxiv.org/abs/1605.07571" target="_blank" rel="noopener">[Paper]</a></li><li><em>A Recurrent Latent Variable Model for Sequential Data</em> <a href="https://papers.nips.cc/paper/5653-a-recurrent-latent-variable-model-for-sequential-data" target="_blank" rel="noopener">[Paper]</a></li><li><em>Deep State Space Models for Time Series Forecasting</em> <a href="https://papers.nips.cc/paper/8004-deep-state-space-models-for-time-series-forecasting" target="_blank" rel="noopener">[Paper]</a></li><li><em>Bayesian Recurrent Neural Networks</em> <a href="https://arxiv.org/abs/1704.02798" target="_blank" rel="noopener">[Paper]</a></li></ul><h2 id="detection-without-closed-form-likelihood">Detection without Closed-form Likelihood</h2><h3 id="reconstruction">Reconstruction</h3><ul><li><em>Adversarially Learned One-Class Classifier for Novelty Detection</em> <a href="https://arxiv.org/abs/1802.09088" target="_blank" rel="noopener">[Paper]</a></li><li><em>Generative Adversarial Network Based Novelty Detection Using Minimized Reconstruction Error</em> <a href="https://link.springer.com/article/10.1631/FITEE.1700786" target="_blank" rel="noopener">[Paper]</a></li><li><em>Learning Discriminative Reconstructions for Unsupervised Outlier Removal</em> <a href="https://ieeexplore.ieee.org/document/7410534" target="_blank" rel="noopener">[Paper]</a></li></ul><h3 id="discriminator">Discriminator</h3><ul><li><em>A Lipschitz-constrained Anomaly Discriminator Framework</em> <a href="https://arxiv.org/abs/1905.10710" target="_blank" rel="noopener">[Paper]</a></li></ul><h3 id="out-of-distribution">Out-of-distribution</h3><ul><li><em>Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks</em> <a href="https://arxiv.org/abs/1706.02690" target="_blank" rel="noopener">[Paper]</a></li><li><em>Learning Confidence for Out-of-Distribution Detection in Neural Networks</em> <a href="https://arxiv.org/abs/1802.04865" target="_blank" rel="noopener">[Paper]</a></li><li><em>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</em> <a href="https://arxiv.org/abs/1610.02136" target="_blank" rel="noopener">[Paper]</a></li><li><em>A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</em> <a href="https://arxiv.org/abs/1807.03888" target="_blank" rel="noopener">[Paper]</a></li></ul><h2 id="gans-for-incomplete-data">GANs for Incomplete Data</h2><ul><li><em>AmbientGAN: Generative Models From Lossy Measurements</em> <a href="https://openreview.net/forum?id=Hy7fDog0b" target="_blank" rel="noopener">[Paper]</a></li><li><em>MisGAN: Learning from Incomplete Data with Generative Adversarial Networks</em> <a href="https://openreview.net/forum?id=S1lDV3RcKm" target="_blank" rel="noopener">[Paper]</a></li></ul><h1 id="dataset">Dataset</h1><p>详见http://qfxiao.me/2020/02/03/Datasets-for-Time-Series-Anomaly-Detection/。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsupervised Anomaly Detection for Intricate KPIs via Adversarial Training of VAE</title>
    <link href="/2020/01/06/Unsupervised-Anomaly-Detection-for-Intricate-KPIs-via-Adversarial-Training-of-VAE/"/>
    <url>/2020/01/06/Unsupervised-Anomaly-Detection-for-Intricate-KPIs-via-Adversarial-Training-of-VAE/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p><a href="https://ieeexplore.ieee.org/abstract/document/8737430/" target="_blank" rel="noopener">论文📃</a></p><p><a href="https://github.com/yantijin/Buzz" target="_blank" rel="noopener">代码📥</a></p><p>本文介绍了一种利用对抗训练来进行时间序列异常检测的方法<em>Buzz</em>。作者认为在现实中复杂的KPI数据大量存在，这种数据通常带有非高斯分布的噪声，同时数据分布复杂，导致一般的生成式模型无法对数据进行很好的建模，所以作者提出了基于对抗训练的模型。在文中，作者的创新点主要有三个：</p><ol type="1"><li>为了处理复杂数据，将数据空间分为多个子空间，在每个子空间上进行距离的度量；</li><li>采用<em>Wasserstein</em>距离度量模型建模的分布和真实分布之间的距离；</li><li>建立了基于对抗训练的<em>Buzz</em>的损失函数和VAE之间的关系。</li></ol><p><img src="http://qfxiao.me/img/image-20200106141915707.png" srcset="/img/loading.gif" /></p><h1 id="background">Background</h1><h2 id="anomaly-detection">Anomaly Detection</h2><p>对于任意时间<span class="math inline">\(t\)</span>，给定历史观察值<span class="math inline">\(x_{t-T+1},\cdots,x_t\)</span>，确定异常是否发生(记为<span class="math inline">\(y_t=1\)</span>)。通常来收异常检测算法给出的是发生异常的可能性，如<span class="math inline">\(p(y_t=1|x_{t-T+1},\cdots,x_t)\)</span>。</p><h2 id="vae">VAE</h2><table><thead><tr class="header"><th><strong>Model</strong></th><th><strong>Latent</strong></th><th><strong>Data</strong></th></tr></thead><tbody><tr class="odd"><td><strong><em>Auto-encoder (AE)</em></strong></td><td><code>None</code></td><td><code>L1Loss</code></td></tr><tr class="even"><td><strong><em>Variational Auto-encoder (VAE)</em></strong></td><td><code>KL Divergence</code></td><td><code>Log Likelihood</code></td></tr><tr class="odd"><td><strong><em>Adversarial Auto-encoder (AAE)</em></strong></td><td><code>Discriminator</code></td><td><code>L1Loss</code></td></tr><tr class="even"><td><strong><em>Wasserstein Auto-encoder (WAE)</em></strong></td><td><code>MaxMeanDiscrepancy</code> or <code>Discriminator</code></td><td><code>L1Loss</code></td></tr><tr class="odd"><td><strong><em>AlphaGAN</em></strong></td><td><code>Discriminator</code></td><td><code>Discriminator</code>+<code>L1Loss</code></td></tr></tbody></table><h2 id="gan-and-wgan-gp">GAN and WGAN-GP</h2><p>原始GAN等价于优化： <span class="math display">\[\mathbb{E}_{x\sim P_r}\log{\frac{P_r(x)}{\frac{1}{2}\left[P_r(x)+P_g(x)\right]}}+\mathbb{E}_{x\sim P_g}\log{\frac{P_g(x)}{\frac{1}{2}\left[P_r(x)+P_g(x)\right]}}\]</span> 根据KL散度和JS散度的定义： <span class="math display">\[\text{KL}(P_1\parallel P_2)=\mathbb{E}_{x\sim P_1}\log{\frac{P_1}{P_2}}\]</span></p><p><span class="math display">\[\text{JS}(P_1\parallel P_2)=\frac{1}{2}\text{KL}(P_1\parallel \frac{P_1+P_2}{2})+\frac{1}{2}\text{KL}(P_2\parallel \frac{P_1+P_2}{2})\]</span></p><p>可重写为： <span class="math display">\[2\text{JS}(P_r\parallel P_g)-2\log 2\]</span> 当<span class="math inline">\(P_r\)</span>与<span class="math inline">\(P_g\)</span>的支撑集（support）是高维空间中的低维流形（manifold）时，<span class="math inline">\(P_r\)</span>与<span class="math inline">\(P_g\)</span>重叠部分测度（measure）为0的概率为1。</p><ul><li>支撑集（support）其实就是函数的非零部分子集，比如<code>ReLU</code>函数的支撑集就是[公式]，一个概率分布的支撑集就是所有概率密度非零部分的集合。</li><li>流形（manifold）是高维空间中曲线、曲面概念的拓广，我们可以在低维上直观理解这个概念，比如我们说三维空间中的一个曲面是一个二维流形，因为它的本质维度（intrinsic dimension）只有2，一个点在这个二维流形上移动只有两个方向的自由度。同理，三维空间或者二维空间中的一条曲线都是一个一维流形。</li><li>测度（measure）是高维空间中长度、面积、体积概念的拓广，可以理解为“超体积”。</li></ul><p><em>Wasserstein</em>距离定义如下： <span class="math display">\[W(P_r,P_g)=\inf\limits_{\gamma\sim\prod(P_r,P_g)}\mathbb{E}_{(x,y)\sim \gamma}\left[\parallel x-y\parallel\right]\]</span> 下确界<span class="math inline">\(\inf\)</span>没法直接求解，不过根据相关定理其等价于： <span class="math display">\[W(P_r,P_g)=\frac{1}{K}\sup\limits_{\parallel f\parallel_L\leq K}\mathbb{E}_{x\sim P_r}[f(x)]-\mathbb{E}_{x\sim P_g}[f(x)]\]</span> <em>Lipschitz</em>连续是指存在一个常数<span class="math inline">\(K\geq 0\)</span>使得定义域内任意两个元素<span class="math inline">\(x_1\)</span>和<span class="math inline">\(x_2\)</span>都满足： <span class="math display">\[|f(x_1)-f(x_2)|\leq K|x_1-x_2|\]</span> WAN的损失函数： <span class="math display">\[\mathcal{L}=\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_g}\left[D({\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_r}\left[D(\mathbf{x})\right]\]</span> WGAN-GP的损失函数为： <span class="math display">\[\mathcal{L}=\mathop{\mathbb{E}}\limits_{\tilde{\mathbf{x}}\sim\mathbb{P}_g}\left[D(\tilde{\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_r}\left[D(\mathbf{x})\right] + \lambda\mathop{\mathbb{E}}\limits_{\hat{\mathbf{x}}\sim\mathbb{P}_{\hat{\mathbf{x}}}}\left[(\parallel\nabla_{\hat{\mathbf{x}}}D(\hat{\mathbf{x}})\parallel_2-1)^2\right]\]</span></p><h1 id="proposed-method">Proposed Method</h1><p>下图为<em>Buzz</em>的总体流程：</p><p><img src="http://qfxiao.me/img/image-20200106141958376.png" srcset="/img/loading.gif" /></p><p>数据会首先进行一些预处理，之后进行训练。在检测阶段会根据异常分数来判定异常。</p><h2 id="motivation">Motivation</h2><p>在文中，最关键的两个创新点分别是<em>Wasserstein</em>距离和对数据分布进行分区的方法。</p><ul><li><p>在使用距离度量方面， 因为<em>Wasserstein</em>在WGAN中取得了很好的效果，是一种鲁棒的距离度量，所以作者在文中采用了<em>Wasserstein</em>距离来衡量生成的分布和真实的分布之间的距离，并由此引入了对抗训练；</p></li><li><p>在分区方法方面，作者认为原始数据过于复杂，所以将数据空间<span class="math inline">\(\mathcal{X}\)</span>进行划分，然后在每个子空间上使用<em>Wasserstein</em>度量距离，而总体的距离由每个分区的距离的期望求得。</p></li></ul><p>作者还发现，当划分地越来越细时，总体距离接近于特定形式的VAE的重构误差项。</p><p><img src="http://qfxiao.me/img/image-20200106142008894.png" srcset="/img/loading.gif" /></p><h2 id="network-structure">Network Structure</h2><p>下图为模型的网络结构：</p><p><img src="http://qfxiao.me/img/image-20200106142018827.png" srcset="/img/loading.gif" /></p><h2 id="training">Training</h2><h3 id="objective-function">Objective Function</h3><p>先定义一些符号：</p><ul><li><span class="math inline">\(b\)</span>和<span class="math inline">\(s\)</span>分别为Batch的大小和邻居的大小，数据集按<span class="math inline">\(s\)</span>进行切分，然后随机打乱，每个Batch包含<span class="math inline">\(b\)</span>个<span class="math inline">\(s\)</span>，之后<span class="math inline">\(s/=2,b*=2\)</span>；</li><li><span class="math inline">\(\mathcal{W}=\{w_1,w_2,\cdots,w_b\}\)</span>为一个Batch，且满足每个<span class="math inline">\(w_i\)</span>是<span class="math inline">\(s\)</span>的倍数；</li><li><span class="math inline">\(w\in\mathcal{W}\)</span>的邻域集(neighborhood set)为一个时间上的partition，记为<span class="math inline">\(\{w,w+1,\cdots,w+s-1\}\)</span></li><li><span class="math inline">\(\mathbf{x}^{(w)},\mathbf{x}^{(w+1)},\cdots,\mathbf{x}^{(w+s-1)}\)</span>为在空间<span class="math inline">\(\mathcal{X}\)</span>上的一个partition，记为<span class="math inline">\(S_w\)</span>，其中<span class="math inline">\(\mathbf{x}^{(w)}\)</span>表示以<span class="math inline">\(w\)</span>为结尾的时间窗口。</li></ul><p><em>Buzz</em>的损失函数和WGAN-GP类似，但做了一些改进，由下面四部分组成，下面分别解释。</p><p>第一个是每一个partition的<span class="math inline">\(\mathbf{z}\)</span>后验的KL散度： <span class="math display">\[\mathcal{K} = \frac{1}{bs}\sum\limits_{w\in\mathcal{W}}\sum\limits_{i=1}^{s-1}\text{KL}\left[q_\phi(\mathbf{z}|\mathbf{x})\parallel\mathcal{N}(\mathbf{0},\mathbf{1})\right]\]</span></p><p>第二个在训练时是一个常数： <span class="math display">\[Z(\lambda) = \frac{\Gamma(W)}{\Gamma(\frac{W}{2})}2\pi^{\frac{W}{2}}\lambda^{-W}\]</span></p><p>其中<span class="math inline">\(\Gamma\)</span>是<em>Gamma</em>函数。</p><p>第三个是<em>Wasserstein</em>距离： <span class="math display">\[\mathcal{T}(F,w)=\frac{1}{bs}\sum\limits_{i=1}^{s-1}\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x}^{w+i})}\left[F(\mathbf{x}^{(w+i)})-F(G(\mathbf{z}))\right]\]</span></p><p>第四个是<em>Gradient Penalty</em>：</p><p><span class="math display">\[\mathcal{R}(F,w)=\frac{1}{bs}\sum\limits_{i=1}^{s-1}\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x}^{(w+i)})}\left[\mathbb{E}_{\varepsilon\sim[0,1]}(\parallel\nabla_{\hat{\mathbf{x}}}(\hat{\mathbf{x}})\parallel-\mathbf{1})^2\right]\]</span></p><p>其中<span class="math inline">\(\hat{\mathbf{x}}=\varepsilon \mathbf{x}^{w+i}+(1-\varepsilon)G(\mathbf{z})\)</span>为生成数据与真实数据的插值。</p><blockquote><p>原始的WGAN-GP的损失函数为： <span class="math display">\[L=\mathop{\mathbb{E}}\limits_{\tilde{\mathbf{x}}\sim\mathbb{P}_g}\left[D(\tilde{\mathbf{x}})\right]-\mathop{\mathbb{E}}\limits_{\mathbf{x}\sim\mathbb{P}_r}\left[D(\mathbf{x})\right] + \lambda\mathop{\mathbb{E}}\limits_{\hat{\mathbf{x}}\sim\mathbb{P}_{\hat{\mathbf{x}}}}\left[(\parallel\nabla_{\hat{\mathbf{x}}}D(\hat{\mathbf{x}})\parallel_2-1)^2\right]\]</span> 其中<span class="math inline">\(\mathbb{P}_g\)</span>为生成器的分布，<span class="math inline">\(\mathbb{P}_r\)</span>为真实分布，<span class="math inline">\(\mathbb{P}_{\hat{\mathbf{x}}}\)</span>为真实数据和生成数据插值得到的分布。</p></blockquote><p><span class="math display">\[\hat{\mathcal{L}}_{Buzz}=-\lambda\sup\limits_F\left[\sum\limits_{w\in\mathcal{W}}(\left|\mathcal{T}(F,w)\right|-\eta\mathcal{R}(F,w))\right]-\mathcal{K}-\log Z(\lambda)\]</span></p><h3 id="training-procedure">Training Procedure</h3><p><em>Buzz</em>的训练过程与WGAN-GP类似，</p><h2 id="detection">Detection</h2><p>文中假设解码器的输出服从如下分布：</p><p><span class="math display">\[p_\theta(\mathbf{x}|\mathbf{z})=\frac{1}{Z(\lambda)}\exp\{-\lambda\parallel\mathbf{x}-G(\mathbf{z})\parallel\}\]</span></p><p>作者定义异常分数为： <span class="math display">\[\mathcal{S}=\log p_\theta(\mathbf{x})-\log p_\theta(\bar{\mathbf{x}})\]</span> 其中<span class="math inline">\(\bar{\mathbf{x}}\)</span>为经过MCMC填充后的样本。</p><p>异常分数也可以展开为： <span class="math display">\[\log\frac{1}{L}\sum\limits_{l=1}^L\left[\frac{p_\theta(\mathbf{x}|\mathbf{z^{(l)}})p_\theta(\mathbf{z}^{(l)})}{q_\phi(\mathbf{z}^{(l)}|\bar{\mathbf{x}})}\right]-\log\frac{1}{L}\sum\limits_{l=1}^L\left[\frac{p_\theta(\bar{\mathbf{x}}|\mathbf{z}^{(l)})p_\theta(\mathbf{z}^{(l)})}{q_\phi(\mathbf{z}^{(l)}|\bar{\mathbf{x}})}\right]\]</span></p><p>最终算法流程图为：</p><p><img src="http://qfxiao.me/img/image-20200106142053342.png" srcset="/img/loading.gif" /></p><h2 id="theoretical-analysis">Theoretical Analysis</h2><p>在理论分析中，作者主要是想建立<span class="math inline">\(\mathcal{L}_{Buzz}\)</span>和VAE的损失函数<span class="math inline">\(\mathcal{L}_{vae}\)</span>之间的联系，损失函数<span class="math inline">\(\mathcal{\hat{L}}_{Buzz}\)</span>为：</p><p><span class="math display">\[\hat{\mathcal{L}}_{Buzz}=-\lambda\sup\limits_F\left[\sum\limits_{w\in\mathcal{W}}(\left|\mathcal{T}(F,w)\right|-\eta\mathcal{R}(F,w))\right]-\mathcal{K}-\log Z(\lambda)\]</span></p><p>为了便于分析，去掉<em>Gradient Penalty</em>的部分，公式可简化为：</p><p><span class="math display">\[\mathcal{L}_{Buzz}=-\lambda\mathbb{E}_{p(w)}W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]-\mathcal{K}-\log Z(\lambda)\]</span></p><p>实际上<span class="math inline">\(Z(\lambda)=\mathfrak{S}_W\Gamma(W)\lambda^{-W}\)</span>，其中<span class="math inline">\(\mathfrak{S}_W\)</span>为<span class="math inline">\(W\)</span>维单位球的表面积。</p><blockquote><p><span class="math inline">\(n\)</span>维空间单位球表面积公式： <span class="math display">\[\omega_n=\frac{2\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2})}\]</span></p></blockquote><p>而<span class="math inline">\(W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]\)</span>为<em>Wasserstein</em>距离： <span class="math display">\[W^1\left[P(\mathbf{x}|w)\parallel P(\mathbf{y}|w)\right]=\sup\limits_{Lip(f)\leq 1}\left\{\int_\mathcal{X}f(\mathbf{x})p(\mathbf{x}|w)\mathrm{d}\mathbf{x}-\int_\mathcal{X}f(\mathbf{y})p(\mathbf{y}|w)\mathrm{d}\mathbf{y}\right\}\]</span></p><h3 id="lemma-1">Lemma 1</h3><p>通过设定具体形式的后验分布，VAE的损失函数可以写为：</p><blockquote><p>设<span class="math inline">\(\mathbf{x}\)</span>的后验分布<span class="math inline">\(p(\mathbf{x}|\mathbf{z})=\frac{1}{Z(\lambda)}\exp\{-\lambda\parallel\mathbf{x}-G(\mathbf{z})\parallel\}\)</span>，那么VAE的损失函数为： <span class="math display">\[\mathcal{L}_{vae}=\lambda\mathbb{E}_{p(w)}\left[\mathbb{E}_{p(\mathbf{x}|w)}\mathbb{E}_{p_G(\mathbf{y}|\mathbf{x})}-\parallel\mathbf{x}-\mathbf{y}\parallel\right]-\mathcal{K}-\log{Z(\lambda)}\]</span></p></blockquote><p>后验分布实际上是一个Laplace分布：</p><blockquote><p><strong>Laplace Distribution</strong>: <span class="math display">\[f(x|\theta,\lambda)=\frac{1}{2\lambda}\exp{\left(-\frac{|x-\theta|}{\lambda}\right)}\]</span></p></blockquote><p>可以直接把后验分布带入VAE的损失函数就得到了。</p><h3 id="lemma-2">Lemma 2</h3><p><span class="math inline">\(S_w\)</span>定义为数据空间<span class="math inline">\(\mathcal{X}\)</span>的一个partition，而<span class="math inline">\(S=\{(\mathbf{x}_1,\mathbf{x}_2)|\exist w, \mathbf{x}_1\in S_w,\mathbf{x}_2\in S_w\}\)</span>。</p><blockquote><p>当<span class="math inline">\(G,\phi,\lambda\)</span>固定时，<span class="math inline">\(S\downarrow\)</span>有<span class="math inline">\(\mathcal{L}_{Buzz}\downarrow\)</span></p></blockquote><h3 id="lemma-3">Lemma 3</h3><blockquote><p><span class="math inline">\(\max\mathcal{L}_{Buzz}\geq\max{\mathcal{L}_{vae}}\)</span>，同时，当<span class="math inline">\(S\downarrow\text{diag}{\mathcal{X}}\)</span>时<span class="math inline">\(\max\mathcal{L}_{Buzz}\downarrow\max\mathcal{L}_{vae}\)</span></p></blockquote><p><img src="http://qfxiao.me/img/image-20200106183833975.png" srcset="/img/loading.gif" /></p><h3 id="lemma-4">Lemma 4</h3><blockquote><p>令<span class="math inline">\(p^\prime_G(\mathbf{y}|\mathbf{x})\)</span>表示<span class="math inline">\(\mathbb{E}_{q_{\phi^\prime}}\left[p_G(\mathbf{y}|\mathbf{z})\right]\)</span>。如果<span class="math inline">\((G,\phi,\lambda)\)</span>是一个解，那么存在<span class="math inline">\((G,\phi^\prime,\lambda)\)</span>使得： <span class="math display">\[\mathbb{E}_{p(\mathbf{x}|w)}\mathbb{E}_{p_G^\prime(\mathbf{y}|\mathbf{x})}\parallel\mathbf{x}-\mathbf{y}\parallel=W^1\left[P(\mathbf{x}|w)\parallel P_G(\mathbf{y}|w)\right]\]</span> 此时<span class="math inline">\(\mathcal{L}_{Buzz}-\mathcal{L}_{vae}^\prime=\mathcal{K}^\prime-\mathcal{K}\)</span>，其中<span class="math inline">\(\mathcal{L}^\prime,\mathcal{K}^\prime\)</span>分别为<span class="math inline">\((G,\phi^\prime,\lambda)\)</span>时的<span class="math inline">\(\mathcal{L}\)</span>和<span class="math inline">\(\mathcal{K}\)</span>。</p></blockquote><p><span class="math display">\[\mathcal{L}^\dagger_{Buzz}=\mathbb{E}_{p(\mathbf{x})}\left[\mathbb{E}_{q_{\phi^\prime}(\mathbf{z}|\mathbf{x})}\log_{p_\theta}(\mathbf{x}|\mathbf{z})\right]-\min\limits_{\bar{\phi}\sim\phi^\prime}\bar{\mathcal{K}}\]</span></p><h3 id="lemma-5">Lemma 5</h3><p>这里主要是想证明</p><blockquote><p>对于固定的<span class="math inline">\(w\)</span>，令： <span class="math display">\[\mathcal{F}=\{f|Lip(f)\leq 1\}, \space \mathcal{F}^*=\left\{f|_{S_w}\bigg|Lip(f|_{S_w})\leq 1\right\}\]</span> 有<span class="math inline">\(\sup_{f\in\mathcal{F}}\mathcal{T}(f)=\sup_{f|_{S_w}\in\mathcal{F}^*}\mathcal{T}^*\left(f|_{S_w}\right)\)</span>。</p></blockquote><h3 id="theorem-6">Theorem 6</h3><blockquote><p><span class="math inline">\(\mathcal{L}_{Buzz}\)</span>的对偶形式为： <span class="math display">\[\mathcal{L}_{Buzz}=-\lambda\sup\limits_{Lip(F;S)\leq 1}\mathbb{E}_{p(w)}\mathcal{T}^*(F)-\mathcal{K}-\log Z(\lambda)\]</span></p></blockquote><p>近似的<span class="math inline">\(\mathcal{L}_{Buzz}\)</span>的对偶形式为： <span class="math display">\[\bar{\mathcal{L}}_{Buzz}=-\lambda\sup\limits_{Lip(F;S)\leq 1}\mathbb{E}_{p(w)}\mathcal{T}(F)-\mathcal{K}-\log Z(\lambda)\]</span></p><h1 id="experiment">Experiment</h1><p><img src="http://qfxiao.me/img/image-20200106142131500.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200106142155978.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20200106142212110.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VAE</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Variational Approaches for Auto-Encoding Generative Adversarial Networks</title>
    <link href="/2019/11/02/Variational-Approaches-for-Auto-Encoding-Generative-Adversarial-Networks/"/>
    <url>/2019/11/02/Variational-Approaches-for-Auto-Encoding-Generative-Adversarial-Networks/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>本文揭示了对抗生成网络（Generative Adversarial Networks, GAN）和变分自编码器（Variational Auto-encoders, VAE）之间的联系，并据此提出了一种将两者结合的新模型。文中主要是将不可解的似然函数和未知的后验分布用一个非确定的分布（Immplicit Distribution）替代，并加入判别器来使得该分布逼近真实的分布。通过这个方法，作者将VAE中的损失函数进行了替换，变成了GAN中的“生成-判别”模式。</p><p><a href="https://arxiv.org/abs/1706.04987" target="_blank" rel="noopener">原文</a></p><h1 id="contribution">Contribution</h1><p>本文有如下贡献：</p><ul><li>本文提出变分推断（Variational Inference）也能通过对非确定分布的估计应用在GAN中；</li><li>基于似然的模型（Likelihood-based Models）和非似然模型（Likelihood-free Models）能够通过对抗学习结合起来；</li><li>作者根据文中提出的新观点修改了VAE的损失函数，将其称之为Auto-encoding GAN (<span class="math inline">\(\alpha\)</span>-GAN)，并提出了对应的实用的改进；</li><li>本文与众多State-of-Art模型进行了对比</li></ul><h1 id="methodology">Methodology</h1><h2 id="overcoming-intractability-in-generative-models">Overcoming Intractability in Generative Models</h2><h3 id="latent-variable-models">Latent Variable Models</h3><p>隐变量模型通过隐变量的形式描述了数据的产生过程。最简单的形式是假设隐变量<span class="math inline">\(\mathbf{z}\)</span>服从一个先验分布<span class="math inline">\(\mathbf{z}\sim p(\mathbf{z})\)</span>，而数据<span class="math inline">\(\mathbf{x}\)</span>从条件分布<span class="math inline">\(p(\mathbf{x}|\mathbf{z})\)</span>抽样产生。通常来说描述<span class="math inline">\(p(\mathbf{x}|\mathbf{z})\)</span>的模型称为生成器<span class="math inline">\(\mathcal{G}_\theta(\mathbf{z})\)</span>，带有可优化的参数<span class="math inline">\(\theta\)</span>，而<span class="math inline">\(\mathbf{z}\)</span>通常假设为正态分布<span class="math inline">\(\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\)</span>。</p><p>文中区分了两种隐变量模型，一种是<em>Implicit Latent Variable Models</em>，一种是<em>Prescribed Latent Variable Models</em>。文中的描述不太清楚，个人认为两者的区别是前者图模型中的<span class="math inline">\(\mathbf{x}\)</span>不是一个随机变量，在优化的时候需要用一个刻画生成的<span class="math inline">\(\mathbf{x}\)</span>和真实的<span class="math inline">\(\mathbf{x}\)</span>的差别的函数<span class="math inline">\(\delta(\mathbf{x}-\mathcal{G}_\theta(\mathbf{z}))\)</span>，而后者图模型中<span class="math inline">\(\mathbf{x}\)</span>是一个随机变量，这样可以写出似然函数用极大似然估计。</p><p>无论是GAN还是VAE都需要通过边缘分布<span class="math inline">\(p_\theta(\mathbf{x})\)</span>来刻画建模的好坏，比如说根据<span class="math inline">\(p_\theta(\mathbf{x})\)</span>与真实分布<span class="math inline">\(p^*_\theta(\mathbf{x})\)</span>之间的KL散度<span class="math inline">\(\text{KL}\left[p_\theta(\mathbf{x})\parallel p_\theta^*(\mathbf{x})\right]\)</span>。但通常情况下<span class="math inline">\(p_\theta(\mathbf{x})\)</span>都是不可解的，而GAN和VAE通过不同的途径解决了这个问题。</p><h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3><p>GAN没有直接计算<span class="math inline">\(p_\theta(\mathbf{x})\)</span>，而是使用了一个判别器来判别样本是从<span class="math inline">\(p_\theta(\mathbf{x})\)</span>还是<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>采样得到的，如果判别器无法进行区分，那我们认为此时<span class="math inline">\(p_\theta(\mathbf{x})\approx p_\theta^*(\mathbf{x})\)</span>。</p><p>令随机变量<span class="math inline">\(y\in\{0,1\}\)</span>，<span class="math inline">\(y=1\)</span>表示样本<span class="math inline">\(\mathbf{x}\)</span>来自真实分布，<span class="math inline">\(y=0\)</span>表示样本<span class="math inline">\(\mathbf{x}\)</span>来自生成分布，而判别器的输出为<span class="math inline">\(\mathbf{x}\)</span>来自真实分布的概率<span class="math inline">\(\mathcal{D}_\phi(\mathbf{x})=p(y=1|\mathbf{x})\)</span>。GAN通过对来自真实分布和生成分布的样本求二元交叉熵来作为判别器损失函数： <span class="math display">\[\textbf{Discriminator Loss: }\mathbb{E}_{p^*(\mathbf{x})}[-\log\mathcal{D}_\phi(\mathbf{x})]+\mathbb{E}_{p_\theta(\mathbf{x})}[-\log(1-\mathcal{D}_\phi(\mathbf{x}))]\]</span></p><p>生成器将最大化判别器对生成样本判定为真的概率作为损失函数，同时还有一个等价的但在实践中表现更好的替代版本：</p><p><span class="math display">\[\textbf{Generator Loss: }\mathbb{E}_{p_\theta(\mathbf{x})}[\log(1-\mathcal{D}_\phi(\mathbf{x}))];\textbf{ Alternative Loss: }\mathbf{E}_{p_\theta(\mathbf{x})}[-\log\mathcal{D}_\phi(\mathbf{x})]\]</span></p><h3 id="the-density-ratio-trick">The Density Ratio Trick</h3><p>令<span class="math inline">\(p^*(\mathbf{x})=p(\mathbf{x}|y=1)\)</span>，<span class="math inline">\(p_\theta(\mathbf{x})=p(\mathbf{x}|y=0)\)</span>。定义<em>Density Ratio</em> <span class="math inline">\(r_\phi(\mathbf{x})\)</span>为真实分布和生成分布之间的比例：</p><p><span class="math display">\[r_\phi(\mathbf{x})=\frac{p^*(\mathbf{x})}{p_\theta(\mathbf{x})}=\frac{p(\mathbf{x}|y=1)}{p(\mathbf{x}|y=0)}=\frac{p(y=1|\mathbf{x})}{p(y=0|\mathbf{x})}=\frac{\mathcal{D}_\phi(\mathbf{x})}{1-\mathcal{D}_\phi(\mathbf{x})}\]</span></p><p>上式表明了<em>Density Ratio</em>的计算可以仅通过从两个分布上采样得到的样本加上一个二分类器<span class="math inline">\(\mathcal{D}_\phi(\mathbf{x})\)</span>实现（假设<span class="math inline">\(p(y=0)=p(y=1)\)</span>）。更深入的说，对于不可解的分布<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>，我们可以通过计算<em>Density Ratio</em>来了解我们近似的分布<span class="math inline">\(p_\theta(\mathbf{x})\)</span>和真实的<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>之间的相对性。而且我们只需要能够在两个分布上进行采样，并且训练一个判别器即可。因为判别器是一个普通的分类器，所以大量的主流分类器都可以使用。</p><h3 id="variational-inference">Variational Inference</h3><p>现在来看VAE，另一种解决不可解分布的方法是近似。<em>Variational Inference</em>通过引入一个变分分布<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>推出了不可解的<span class="math inline">\(\mathbf{x}\)</span>的对数似然的下界（常被称为证据下界ELBO）：</p><p><span class="math display">\[\log p_\theta(\mathbf{x})=\log\int p_\theta(\mathbb{x}|\mathbb{z})p(\mathbf{z})\text{d}\mathbf{z}\geq \mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log p_\theta(\mathbf{x}|\mathbf{z})\right]-\text{KL}\left[q_\eta(\mathbf{z}|\mathbf{x})\parallel p(\mathbf{z})\right]=\mathcal{F}(\boldsymbol{\theta}, \boldsymbol{\eta})\]</span></p><p>VAE是<em>Variational Inference</em>的一种实现，变分分布通过一个神经网络进行建模，并且建立起了完整的可优化的模型。</p><h3 id="synthetic-likelihood">Synthetic Likelihood</h3><p>当似然函数未知（GAN中没有显式的似然函数，而VAE中有）的时候，<em>Variational Inference</em>便无法直接使用。对于没有显式的似然函数的情况，以VAE的ELBO的第一项为例，假设<span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\)</span>分布的具体形式未知，我们只有从<span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\)</span>采样得到的样本，如何计算<span class="math inline">\(\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]\)</span>呢？一个方法是乘以<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>再除以<span class="math inline">\(p_\theta^*(\mathbf{x})\)</span>：</p><p><span class="math display">\[\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p_\theta(\mathbf{x}|\mathbf{z})]=\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{p_\theta(\mathbf{x}|\mathbf{z})}{p^*(\mathbf{x})}\right]+\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}[\log p^*(\mathbf{x})]\]</span></p><p>公式(5)中的第一项包括了合成似然<span class="math inline">\(R(\theta)=\frac{p_\theta(\mathbf{x}|\mathbf{z})}{p^*(\mathbf{x})}\)</span>，优化<span class="math inline">\(R(\theta)\)</span>相当于优化<span class="math inline">\(\log p_\theta(\mathbf{x}|\mathbf{z})\)</span>。第二项与生成网络的参数<span class="math inline">\(\theta\)</span>无关，所以在优化的时候可以忽略。</p><h2 id="a-fusion-of-variational-and-adversarial-learning">A Fusion of Variational and Adversarial Learning</h2><p>GAN和VAE分别从不同的角度解决了生成模型的推断问题，我们下面从VAE出发，考虑将两者结合起来。</p><h3 id="implicit-variational-distributions">Implicit Variational Distributions</h3><p>变分推断<strong>Variational Inference</strong>的主要任务就是确定<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>，通常的做法如<strong>Mean-field Variational Inference</strong>会假设一个简单的分布，如高斯分布。在本文中不对<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>的形式作假设，仅假设其为一个隐含的分布。运用上文提到的<em>Density Ratio Trick</em>，我们可以将VAE损失函数中的第二项改写为：</p><p><span class="math display">\[-\text{KL}[q_\eta(\mathbf{z}|\mathbf{x})\parallel p(\mathbf{z})]=\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{p(\mathbf{z})}{q_\eta(\mathbf{z}|\mathbf{x})}\right]\approx\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}{1-\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}\right]\]</span></p><p>文中引入了一个隐变量分类器（Latent Classifier）<span class="math inline">\(\mathcal{C}_{\boldsymbol{\omega}}(\mathbf{z})\)</span>，用来判别<span class="math inline">\(\mathbf{z}\)</span>是从编码网络还是从标准高斯分布中采样得到的（猜测这样做的好处是不用再对<span class="math inline">\(\mathbf{z}\)</span>的后验做高斯分布的假设了，也不需要在变分网络输出形成的高斯分布上采样得到<span class="math inline">\(\mathbf{z}\)</span>了，这样重参数技巧也省了）。具体实现上，期望可以用蒙特卡洛方法（采样多次取均值）进行计算。</p><h3 id="likelihood-choice">Likelihood Choice</h3><p>对于VAE损失函数第一项，对应生成网络，我们可以选择对<span class="math inline">\(p(\mathbf{x}|\mathbf{z})\)</span>分布的具体形式做假设， 这样对应<em>Likelihood-based</em>的情况。文中选择的是<em>Zero-mean Laplace Distribution</em> <span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\propto\exp(-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1)\)</span>（不就是<span class="math inline">\(L_1\)</span> Loss吗？？？）。</p><p>对于<em>Likelihood-free</em>的情况，可以继续使用上面提到的<em>Density Ratio Trick</em>，这时需要加一个一个判别器。</p><p><span class="math display">\[\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1\right]\space\space\text{  or  }\space\space\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\log\frac{\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}{1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}\right]\]</span></p><p>对于两种选择，前者对应VAE，好处是不会出现模式崩溃的情况，后者对应GAN，容易出现模式崩溃的情况，但是可以使用对抗学习的方式（这是优点？？？），本文选择两种都用（我全都要.jpg）。</p><h3 id="hybrid-loss-functions">Hybrid Loss Functions</h3><p>将前面的讨论结合起来，最后的损失函数就是：</p><p><span class="math display">\[\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\eta})=\mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[-\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1+\log\frac{\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}{1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))}+\log\frac{\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}{1-\mathcal{C}_\boldsymbol{\omega}(\mathbf{z})}\right]\]</span></p><p>最后模型包含四个网络：生成网络<span class="math inline">\(p_\theta(\mathbf{x}|\mathbf{z})\)</span>、推断网络<span class="math inline">\(q_\eta(\mathbf{z}|\mathbf{x})\)</span>以及两个判别器<span class="math inline">\(\mathcal{C}_{\boldsymbol{\omega}}\)</span>和<span class="math inline">\(\mathcal{D}_\phi\)</span>，作者将其命名为<span class="math inline">\(\alpha\)</span>-GAN。</p><p>算法流程如下：</p><p><img src="http://qfxiao.me/img/image-20191106220923748.png" srcset="/img/loading.gif" /></p><h3 id="improved-techniques">Improved Techniques</h3><p>作者为了改进模型的稳定性和效率，将生成器的Loss中的<span class="math inline">\(-\log(1-\mathcal{D}_\phi)\)</span>修改为了<span class="math inline">\(\log\mathcal{D}_\phi-\log(1-\mathcal{D}_\phi)\)</span>，并声称这样能提供非饱和（Non-saturating）的梯度：</p><p><span class="math display">\[\textbf{Generator Loss: } \mathbb{E}_{q_\eta(\mathbf{z}|\mathbf{x})}\left[\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1-\log\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z}))+\log(1-\mathcal{D}_\phi(\mathcal{G}_\theta(\mathbf{z})))\right]\]</span></p><p>作者认为在生成器损失函数中加入<span class="math inline">\(\lambda\parallel\mathbf{x}-\mathcal{G}_\theta(\mathbf{z})\parallel_1\)</span>能够在一定程度防止模式崩溃。</p><p>除此之外，作者发现将真实样本（原文是The Samples）作为生成的样本输入到判别器中能够提升性能。作者给出的解释是根据Jensen不等式：<span class="math inline">\(\log p_\theta(\mathbf{x})=\log\int p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})\text{d}\mathbf{z}\geq \mathbb{E}_{p(\mathbf{z})}[\log p_\theta(\mathbf{x}|\mathbf{z})]\)</span>，</p><p>[TODO]</p><h2 id="related-work">Related Work</h2><p>[TODO]</p><p><img src="http://qfxiao.me/img/image-20191102233101258.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20191102233151987.png" srcset="/img/loading.gif" /></p><h1 id="experiments">Experiments</h1><p>[TODO]</p><h2 id="metrics">Metrics</h2><p>本文使用了几种不同的评测生成模型的方法：</p><ul><li><strong>Inception Score: </strong></li><li><strong>Multi-scale Structural Similarity (MS-SSIM): </strong></li><li><strong>Independent Wasserstein Critic: </strong></li></ul><h2 id="results-on-colormnist">Results on ColorMNIST</h2><p><img src="http://qfxiao.me/img/image-20191102233220700.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20191102233244847.png" srcset="/img/loading.gif" /></p><h2 id="results-on-celeba">Results on CelebA</h2><p><img src="http://qfxiao.me/img/image-20191102233303345.png" srcset="/img/loading.gif" /></p><h2 id="results-on-cifar-10">Results on CIFAR-10</h2><p><img src="http://qfxiao.me/img/image-20191102233338213.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/image-20191102233350494.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>GAN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Variational Inference</tag>
      
      <tag>VAE</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anomaly Detection in Streams with Extreme Value Theory</title>
    <link href="/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/"/>
    <url>/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文基于<strong>Extreme Value Theory</strong>提出了一种不需要手动设置阈值也不需要对数据分布作任何假设的时间序列异常检测方法。除此之外，本方法可以用在通用的自动阈值选择的场合中。</p><p><a href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-in-streams-with-extreme-value-theory" target="_blank" rel="noopener">原文</a></p><h1 id="background">Background</h1><p>在很多情况下我们需要进行阈值的选择。阈值的选择可以通过实验的方法或者对数据分布进行假设的方法来得到，不过这样做通常不准确。借助<strong>Extreme Value Theory</strong>我们可以在不需要对原始数据的分布作很强的假设的情况下，推断我们想要的极端事件的分布（在异常检测中就是异常值）。</p><p>下面给出一些数学符号，<span class="math inline">\(X\)</span>为随机变量，<span class="math inline">\(F\)</span>为累积分布函数，即<span class="math inline">\(F(x)=\mathbb{P}(X\leq x)\)</span>。记<span class="math inline">\(F\)</span>的“末尾”分布<span class="math inline">\(\bar{F}(x)=1-F(x)=\mathbb{P}(X&gt;x)\)</span>。对于一个随机变量<span class="math inline">\(X\)</span>和给定的概率<span class="math inline">\(q\)</span>，记<span class="math inline">\(z_q\)</span>为在<span class="math inline">\(1-q\)</span>水平的分位数，即<span class="math inline">\(z_q\)</span>为满足<span class="math inline">\(\mathbb{P}(X\leq z_q)\geq 1-q\)</span>最小的值。</p><h2 id="extreme-value-distributions">Extreme Value Distributions</h2><p><strong>Extreme Value Theory</strong>主要是为了找出极端事件发生的规律，有学者证明，在很弱的条件下，所有极端事件都服从一个特定的分布，而不管原始分布如何。具体形式如下：</p><p><span class="math display">\[G_\gamma:x\mapsto \exp(-(1+\gamma x)^{-\frac{1}{\gamma}}), \space\space\space\space\space\gamma\in\mathbb{R}, \space\space\space\space\space 1+\gamma x&gt;0\]</span></p><p>其中<span class="math inline">\(\gamma\)</span>称为<strong>Extreme Value Index</strong>，由原始分布决定。</p><p>更严谨的说法是Fisher-Tippett-Gnedenko定理（极值理论第一定理）：</p><blockquote><p><strong>THEOREM: </strong>(Fisher-Tippett-Gnedenko). 令<span class="math inline">\(X_1,X_2,\cdots,X_n,\cdots\)</span>为独立同分布的随机变量序列，<span class="math inline">\(M_n=\max \{X_1,\cdots,X_n\}\)</span>。如果实数对序列<span class="math inline">\((a_n,b_n)\)</span>存在且满足<span class="math inline">\(a_n&gt;0\)</span>和<span class="math inline">\(\lim\limits_{n\rightarrow \infty}P\left(\frac{M_n-b_n}{a_n}\leq x\right)=F(x)\)</span>，其中<span class="math inline">\(F\)</span>为非退化分布函数，那么<span class="math inline">\(F\)</span>属于Gumbel、Fréchet或Weibull分布族（或总称Generalized Extreme Value Distribution）中的一种。</p></blockquote><p>这是一个反直觉的结论，但是想到当事件发生变得极端时，即<span class="math inline">\(\mathbb{P}(X&gt;x)\rightarrow 0\)</span>，<span class="math inline">\(\bar{F}(x)=\mathbb{P}(X&gt;x)\)</span>分布的形状其实并没有很多种选择。Table 1展示了几种不同分布对应的<span class="math inline">\(\gamma\)</span>：</p><p><img src="http://qfxiao.me/img/image-20191029162343219.png" srcset="/img/loading.gif" /></p><p>Figure 1展示了几种不同<span class="math inline">\(\gamma\)</span>情况下的“末尾”分布：</p><p><img src="http://qfxiao.me/img/image-20191029113903386.png" srcset="/img/loading.gif" /></p><h2 id="power-of-evt">Power of EVT</h2><p>根据<strong>Extreme Value Theory</strong>，我们可以在原始分布未知的情况下计算极端事件的概率。但是<span class="math inline">\(\bar{G}_\gamma\)</span>分布中参数<span class="math inline">\(\gamma\)</span>是未知的，我们需要一种高效的方法来进行估计。<strong>The Peaks-Over-Threshold</strong> (POT) 方法是本文介绍的一种方法。</p><p><img src="http://qfxiao.me/img/image-20191029162456947.png" srcset="/img/loading.gif" /></p><h2 id="peaks-over-threshold-approach">Peaks-Over-Threshold Approach</h2><p>POT方法依赖于Pickands-Balkema-De Haan定理（极值理论第二定理），维基百科版：</p><blockquote><p>考虑一个未知分布<span class="math inline">\(F\)</span>和随机变量<span class="math inline">\(X\)</span>，我们的目标是估计<span class="math inline">\(X\)</span>在超过确定阈值<span class="math inline">\(u\)</span>下的条件分布<span class="math inline">\(F_u\)</span>，定义为： <span class="math display">\[F_u(y)=P(X-u\leq y|X&gt;u)=\frac{F(u+y)-F(u)}{1-F(u)}\]</span> 其中<span class="math inline">\(0\leq y\leq x_F-u\)</span>，<span class="math inline">\(x_F\)</span>为<span class="math inline">\(F\)</span>的右端点。<span class="math inline">\(F_u\)</span>描述了超过特征阈值<span class="math inline">\(u\)</span>的分布，称为<strong>Conditional Excess Distribution Function</strong>。</p><p><strong>STATEMENT: </strong>(Pickands-Balkema-De Haan). 设<span class="math inline">\((X_1,X_2,\cdots)\)</span>为独立同分布随机变量序列，<span class="math inline">\(F_u\)</span>为相应的Conditional Excess Distribution Function。对于一大类的<span class="math inline">\(F\)</span>和很大的<span class="math inline">\(u\)</span>，<span class="math inline">\(F_u\)</span>能够很好的被Generalized Pareto Distribution所拟合： <span class="math display">\[F_u(y)\rightarrow G_{k,\sigma}(y),\space\space \text{as } u\rightarrow \infty\]</span> 其中： <span class="math display">\[G_{k,\sigma}(y)=\begin{cases}1-(1+ky/\sigma)^{-1/k}, &amp;\text{if }k\neq 0\\1-e^{-y/\sigma}, &amp;\text{if }k=0\end{cases}\]</span> 当<span class="math inline">\(k\geq 0\)</span>时<span class="math inline">\(\sigma&gt;0, y\geq 0\)</span>，<span class="math inline">\(k&lt;0\)</span>时<span class="math inline">\(0\leq y\leq -\sigma/k\)</span>。</p></blockquote><p>论文中给出的定理如下：</p><blockquote><p><strong>THEOREM: </strong>(Pickands-Balkema-De Haan). 累积概率密度函数<span class="math inline">\(F\in\mathcal{D}_\gamma\)</span>当且仅当函数<span class="math inline">\(\sigma\)</span>存在时，对所有<span class="math inline">\(x\in\mathbb{R}\)</span>在<span class="math inline">\(1+\gamma x&gt;0\)</span>的条件下有： <span class="math display">\[\frac{\bar{F}(t+\sigma(t)x)}{\bar{F}(t)}\mathop{\rightarrow}\limits_{t\rightarrow\tau}(1+\gamma x)^{-\frac{1}{\gamma}}\]</span></p></blockquote><p>上式可以写成如下形式： <span class="math display">\[\bar{F}_t(x)=\mathbb{P}(X-t&gt;x|X&gt;t)\mathop{\sim}\limits_{t\rightarrow\tau}\left(1+\frac{\gamma x}{\sigma(t)}\right)^{-\frac{1}{\gamma}}\]</span> 该式表明<span class="math inline">\(X\)</span>超过阈值<span class="math inline">\(t\)</span>的概率（写为<span class="math inline">\(X-t\)</span>）服从<strong>Generalized Pareto Distribution</strong> (GPD)，参数为<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\sigma\)</span>。POT主要是拟合GPD而不是EVT分布。</p><p>如果我们要估计参数<span class="math inline">\(\hat{\gamma}\)</span>和<span class="math inline">\(\hat{\sigma}\)</span>，分位数可以通过下式计算得到： <span class="math display">\[z_q\simeq t+\frac{\hat{\sigma}}{\hat{\gamma}}\left(\left(\frac{qn}{N_t}\right)^{-\hat{\gamma}}-1\right)\]</span></p><p>其中<span class="math inline">\(t\)</span>是一个“很高”的阈值，<span class="math inline">\(q\)</span>是给定的概率值，<span class="math inline">\(n\)</span>是所有观测样本的数量，<span class="math inline">\(N_t\)</span>是peaks的数量，即<span class="math inline">\(X_i&gt;t\)</span>的数量。为了进行高效的参数估计，文中使用了极大似然估计。</p><h2 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h2><p>设<span class="math inline">\(X_1,\cdots,X_n\)</span>为独立同分布的随机变量，概率密度函数记为<span class="math inline">\(f_\theta\)</span>，<span class="math inline">\(\theta\)</span>为分布中的参数，那么似然函数可以写为：</p><p><span class="math display">\[\mathcal{L}(X_1,\cdots,X_n;\theta)=\prod\limits_{i=1}^n f_\theta(X_i)\]</span></p><p>在极大似然估计中，我们需要找到合适的参数使得似然函数最大化。在我们的问题中，似然函数如下： <span class="math display">\[\log\mathcal{L}(\gamma,\sigma)=-N_t\log\sigma-\left(1+\frac{1}{\gamma}\right)\sum\limits_{i=1}^{N_t}\log\left(1+\frac{\gamma}{\sigma}Y_i\right)\]</span> 其中<span class="math inline">\(Y_i&gt;0\)</span>表示<span class="math inline">\(X_i\)</span>超过阈值<span class="math inline">\(t\)</span>的部分。</p><p>文中使用了<strong>Grimshaw's Trick</strong>来将含两个参数的优化问题转换为只含一个参数的优化问题。记<span class="math inline">\(\ell(\gamma,\sigma)=\log\mathcal{L}(\gamma,\sigma)\)</span>，对于所有极值来说有<span class="math inline">\(\nabla \ell(\gamma, \sigma)=0\)</span>。Grimshaw's Trick表明对于满足<span class="math inline">\(\nabla \ell(\gamma, \sigma)=0\)</span>的一对<span class="math inline">\((\gamma^*,\sigma^*)\)</span>，<span class="math inline">\(x^*=\frac{\gamma^*}{\sigma^*}\)</span>为等式<span class="math inline">\(u(X)v(X)=1\)</span>的解，其中： <span class="math display">\[\begin{align}u(x)&amp;=\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\frac{1}{1+xY_i}\\v(x)&amp;=1+\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\log(1+xY_i)\end{align}\]</span> 在找到满足该等式的解<span class="math inline">\(x^*\)</span>后，我们可以得到<span class="math inline">\(\gamma^*=v(x^*)-1\)</span>和<span class="math inline">\(\sigma^*=\gamma^*/x^*\)</span>，于是问题就变成了如何寻找方程的所有根。</p><p>因为<span class="math inline">\(\log\)</span>的存在，所以有<span class="math inline">\(1+xY_i&gt;0\)</span>。而<span class="math inline">\(Y_i\)</span>是正数，所以<span class="math inline">\(x^*\)</span>的范围一定在<span class="math inline">\(\left(-\frac{1}{Y^M},+\infty\right)\)</span>，其中<span class="math inline">\(Y^M=\max Y_i\)</span>。</p><p>Grimshaw（作者参考的一篇<a href="https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485040" target="_blank" rel="noopener">论文</a>）还给出了一个上界： <span class="math display">\[x^*_{\text{max}}=2\frac{\bar{Y}-Y^m}{(Y^m)^2}\]</span> 其中<span class="math inline">\(Y^m=\min Y_i\)</span>，<span class="math inline">\(\bar{Y}\)</span>为<span class="math inline">\(Y_i\)</span>的均值。详细的优化方法会在下文讨论。</p><p>背景部分到此结束，接下来的部分就是作者提出的新方法。</p><h1 id="methodology">Methodology</h1><p>Extreme Value Theory给出了在对原始分布未知的情况下估计使得<span class="math inline">\(\mathbb{P}(X&gt;z_q)&lt;q\)</span>的<span class="math inline">\(z_q\)</span>的方法。</p><p>本文据此提出了时间序列流的异常检测方法。首先根据已知的观测值<span class="math inline">\(X_1,\cdots,X_n\)</span>得到阈值<span class="math inline">\(z_q\)</span>，然后根据数据的特性运用两种不同方法来更新<span class="math inline">\(z_q\)</span>。对于平稳时间序列，使用SPOT；对于非平稳时间序列，使用DSPOT。</p><h2 id="initialization-step">Initialization Step</h2><p>在进行异常检测之前，需要根据已有的观测数据进行<span class="math inline">\(z_q\)</span>的估计。给定<span class="math inline">\(n\)</span>个观测值<span class="math inline">\(X_1,\cdots,X_n\)</span>和一个固定的概率值<span class="math inline">\(q\)</span>，我们的目标是估计阈值<span class="math inline">\(z_q\)</span>使得<span class="math inline">\(\mathbb{P}(X&gt;z_q)&lt;q\)</span>。其主要流程是首先设定一个较大的阈值<span class="math inline">\(t\)</span>，然后通过拟合GPD分布来计算<span class="math inline">\(z_q\)</span>。过程如下图所示：</p><p><img src="http://qfxiao.me/img/image-20191029114030518.png" srcset="/img/loading.gif" /></p><p>算法流程如下所示：</p><p><img src="http://qfxiao.me/img/image-20191029114001461.png" srcset="/img/loading.gif" /></p><p><span class="math inline">\(Y_t\)</span>代表大于<span class="math inline">\(t\)</span>的观测值的集合，GPD分布的拟合使用了前文提到的Grimshaw's Trick。</p><h2 id="finding-anomalies-in-a-stream">Finding Anomalies in a Stream</h2><p>通过Initialization Step使用POT算法得到的<span class="math inline">\(z_q\)</span>，我们定义其为&quot;Normality Bound&quot;，用于后面的检测。在后面的步骤中，我们会根据新得到的观测值来更新<span class="math inline">\(z_q\)</span>。</p><h3 id="stationary-case">Stationary Case</h3><p>我们首先来讨论时间序列没有时间依赖性的情况（<span class="math inline">\(X_1,\cdots,X_n\)</span>之间独立同分布）。通过POT算法对所有观测值得到<span class="math inline">\(z_q\)</span>之后，Streaming POT (SPOT) 算法会检查<span class="math inline">\(X_n\)</span>之后的值（数据流场景，<span class="math inline">\(X_1,\cdots,X_n\)</span>是历史数据，还会有新的数据进来），如果大于<span class="math inline">\(z_q\)</span>，则将<span class="math inline">\(X_i\)</span>加入异常点集合中；如果大于<span class="math inline">\(t\)</span>但小于<span class="math inline">\(z_q\)</span>，则将<span class="math inline">\(X_i\)</span>加入观测值集合中，更新<span class="math inline">\(z_q\)</span>；其他情况我们<span class="math inline">\(X_i\)</span>是正常情况。算法流程图如下：</p><p><img src="http://qfxiao.me/img/image-20191029114020571.png" srcset="/img/loading.gif" /></p><h3 id="drifting-case">Drifting Case</h3><p>SPOT算法只适用于平稳分布的情况，但在现实生活中这样的假设过强了。于是作者提出了能处理时间依赖性的Streaming POT with Drift (DSPOT) 算法。</p><p><img src="http://qfxiao.me/img/image-20191029114054583.png" srcset="/img/loading.gif" /></p><p>在DSPOT中，我们不使用<span class="math inline">\(X_i\)</span>的绝对值，而是用相对值<span class="math inline">\(X^\prime_i=X_i-M_i\)</span>，其中<span class="math inline">\(M_i\)</span>是<span class="math inline">\(i\)</span>时刻的局部特征，如Figure 4所示。最简单的实现是使用局部均值，即<span class="math inline">\(M_i=(1/d)\cdot\sum\limits_{k=1}^d X_{i-k}^*\)</span>，<span class="math inline">\(X_{i-1}^*,\cdots,X_{i-d}^*\)</span>是长度为<span class="math inline">\(d\)</span>的窗口。我们假设<span class="math inline">\(X^\prime_i\)</span>服从平稳分布的假设。</p><p>算法流程图如下所示：</p><p><img src="http://qfxiao.me/img/image-20191029114113201.png" srcset="/img/loading.gif" /></p><h2 id="numerical-optimization">Numerical Optimization</h2><p>现在剩下的问题就是优化了，前文已经提到对GPD的拟合已经被优化成一个参数的优化问题，下面将会详细讨论优化算法。</p><h3 id="reduction-of-the-optimal-parameters-search">Reduction of the Optimal Parameters Search</h3><p>前文已经得到了一个初步的<span class="math inline">\(x^*\)</span>的Bound，即<span class="math inline">\(x^*&gt;-\frac{1}{Y^M}\)</span>和<span class="math inline">\(x^*\leq 2\frac{\bar{Y}-Y^m}{(Y^m)^2}\)</span>，下面将给出一个更严格的Bound。</p><blockquote><p><strong>PROPOSITION: </strong>如果<span class="math inline">\(x^*\)</span>是<span class="math inline">\(u(x)v(x)=1\)</span>的解，那么： <span class="math display">\[x^*\leq 0 \text{ or } x^*\geq 2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m}\]</span></p></blockquote><p>证明见论文原文。</p><p>这样<span class="math inline">\(x^*\)</span>的范围就进一步缩小了，于是有<span class="math inline">\(u(x)v(X)=1\)</span>的解<span class="math inline">\(x^*\)</span>在以下范围之内： <span class="math display">\[\left(-\frac{1}{Y^M},0\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]\]</span></p><h3 id="how-can-we-maximize-the-likelihood-function">How Can We Maximize the Likelihood Function?</h3><p>接下来是优化的具体实现问题。文中首先设定了一个很小的值<span class="math inline">\(\epsilon&gt;0\space(\sim 10^{-8})\)</span>，然后在下面的范围内寻找函数<span class="math inline">\(w:x\mapsto u(x)v(x)-1\)</span>的根： <span class="math display">\[\left[-\frac{1}{Y^M}+\epsilon,-\epsilon\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]\]</span> 作者没有使用现有的寻找函数根的算法，而是转换为如下优化问题： <span class="math display">\[\min\limits_{x_1,\cdots,x_k\in I}\sum\limits_{i=1}^k w(x_k)^2\]</span> 其中<span class="math inline">\(I\)</span>就是<span class="math inline">\(x^*\)</span>的Bound。该问题是一个很典型的优化问题，可以被很多成熟的算法所解决。</p><h3 id="initial-threshold">Initial Threshold</h3><p>在算法的Initialization Step，需要事先设定一个阈值<span class="math inline">\(t\)</span>，如果设定的太大，那么<span class="math inline">\(Y_t\)</span>的数量就会很少。作者给出的建议是保证<span class="math inline">\(t&lt;z_q\)</span>，即<span class="math inline">\(t\)</span>对应的概率值应该小于<span class="math inline">\(1-q\)</span>。</p><h1 id="experiments">Experiments</h1><p>在实验部分，作者在合成数据和真实数据上试验了SPOT算法和DSPOT算法的有效性。</p><h2 id="dspot-reliability">(D)SPOT Reliability</h2><p>作者首先在合成数据上验证SPOT的有效性。具体做法是使用高斯分布生成数据（高斯分布的分位数能够直接计算），然后将SPOT得出的<span class="math inline">\(z_q\)</span>和理论值进行对比。误差定义如下： <span class="math display">\[\text{error rate}=\left|\frac{z^{\text{SPOT}}-z^{\text{th}}}{z^{\text{th}}}\right|\]</span> 下图是采用不同数量观测值的结果：</p><p><img src="http://qfxiao.me/img/image-20191029114230661.png" srcset="/img/loading.gif" /></p><h2 id="finding-anomalies-with-spot">Finding Anomalies with SPOT</h2><p>在这一节作者在真实数据集上进行了实验以验证SPOT算法的有效性，结果如下图：</p><p><img src="http://qfxiao.me/img/image-20191029114255259.png" srcset="/img/loading.gif" /></p><p>在文中作者说算法的True Positive达到了<span class="math inline">\(86\%\)</span>，False Positive小于<span class="math inline">\(4\%\)</span>。</p><p><img src="http://qfxiao.me/img/image-20191029114316348.png" srcset="/img/loading.gif" /></p><h2 id="finding-anomalies-with-dspot">Finding Anomalies with DSPOT</h2><p>在这一节作者使用DSPOT在真实数据集上进行了实验。窗口大小<span class="math inline">\(d=450\)</span>，预设的风险概率值<span class="math inline">\(q=10^{-3}\)</span>。结果如下图所示：</p><p><img src="http://qfxiao.me/img/image-20191029114340936.png" srcset="/img/loading.gif" /></p><p>在图中可以看出在<span class="math inline">\(8000\)</span> Minutes之后上界显著提高，作者分析了原因，认为是因为超过阈值<span class="math inline">\(t\)</span>的点<span class="math inline">\(Y_t\)</span>的存储是全局的，在前<span class="math inline">\(8000\)</span> Minutes算法存储了很多较高的<span class="math inline">\(Y_t\)</span>值，而在<span class="math inline">\(8000\)</span> Minutes之后，真实数据的趋势开始下降，但算法仍是根据全局的<span class="math inline">\(Y_t\)</span>来进行<span class="math inline">\(z_q\)</span>的计算（这一段没有特别明白）。作者给出的修正方法是只保存固定数量的Peaks。</p><p>下图是作者在股票数据上得到的实验结果：</p><p><img src="http://qfxiao.me/img/image-20191029114356004.png" srcset="/img/loading.gif" /></p><h2 id="performances">Performances</h2><p>作者还验证了算法的时间效率。</p><p><img src="http://qfxiao.me/img/image-20191029114419226.png" srcset="/img/loading.gif" /></p><p>表中T代表的是每个Iteration的时间，M代表的是Peaks的比例，&quot;bi-&quot;前缀代表的是同时计算上界和下界。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Time Series</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Statistics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>An Introduction to Variational Autoencoders</title>
    <link href="/2019/10/22/An-Introduction-to-Variational-Autoencoders/"/>
    <url>/2019/10/22/An-Introduction-to-Variational-Autoencoders/</url>
    
    <content type="html"><![CDATA[<h1 id="deep-generative-models">Deep Generative Models</h1><p>生成模型是指一系列用于随机生成可观测数据的模型。假设在一个高维空间<span class="math inline">\(\mathcal{X}\)</span>中，存在一个随机向量<span class="math inline">\(\mathbf{X}\)</span>服从一个未知的分布<span class="math inline">\(p_r(x),x\in \mathcal{X}\)</span>。生成模型就是根据一些可观测的样本<span class="math inline">\(x^{(1)},x^{(2)},\cdots,x^{(N)}\)</span>来学习一个参数化的模型<span class="math inline">\(p_\theta(x)\)</span>来近似未知分布<span class="math inline">\(p_r(x)\)</span>。</p><p>生成模型主要用于密度估计和样本生成。</p><hr /><p>密度估计即给定一组数据<span class="math inline">\(\mathcal{D}=\{x^{(i)}\},1\leq i\leq N\)</span>，假设他们都是从相同的概率密度函数<span class="math inline">\(p_r(x)\)</span>独立产生的。密度估计就是根据数据集<span class="math inline">\(\mathcal{D}\)</span>来估计其概率密度函数<span class="math inline">\(p_r(x)\)</span>。</p><p>如果将生成模型用于监督学习，那么就是输出标签的条件概率分布<span class="math inline">\(p(y|x)\)</span>，根据贝叶斯公式：</p><p><span class="math display">\[p(y|x)=\frac{p(x,y)}{\sum_y p(x,y)}\]</span></p><p>问题就变为了联合概率<span class="math inline">\(p(x,y)\)</span>的密度估计问题。</p><hr /><p>样本生成即根据给定的概率分布<span class="math inline">\(p_\theta(x)\)</span>生成一些服从这个分布的样本，即采样。在含隐变量的生成模型中，生成<span class="math inline">\(x\)</span>的过程一般包含两步：</p><ol type="1"><li>根据隐变量的分布<span class="math inline">\(p_\theta(z)\)</span>采样得到<span class="math inline">\(z\)</span>；</li><li>根据条件分布<span class="math inline">\(p_\theta(x|z;\theta)\)</span>进行采样得到<span class="math inline">\(x\)</span>。</li></ol><p>所以在生成模型中的重点是估计条件分布<span class="math inline">\(p(x|z;\theta)\)</span>。</p><h1 id="parameter-estimation-for-hidden-variable-with-em-algorithm">Parameter Estimation for Hidden Variable with EM Algorithm</h1><p>如果图模型中存在隐变量，就需要使用EM算法进行参数估计。</p><p>在一个包含隐变量的图模型中，令<span class="math inline">\(\mathbf{X}\)</span>为可观测变量集合，<span class="math inline">\(\mathbf{Z}\)</span>为隐变量集合，则一个样本<span class="math inline">\(x\)</span>的边际似然函数为：</p><p><span class="math display">\[p(x;\theta)=\sum_z p(x,z;\theta)\]</span></p><p>给定包含<span class="math inline">\(N\)</span>个训练样本的训练集<span class="math inline">\(\mathcal{D}=\{x^{(n)}\},1\leq i\leq N\)</span>，则训练集的对数边际似然为：</p><p><span class="math display">\[\begin{align}\mathcal{L}(\mathcal{D};\theta)&amp;=\frac{1}{N}\sum_{n=1}^N \log p(x^{(n)};\theta)\\&amp;=\frac{1}{N}\sum_{n=1}^N \log \sum_z p(x^{(n)},z;\theta)\end{align}\]</span></p><hr /><p>这时，只要最大化整个训练集的对数边际似然<span class="math inline">\(\mathcal{L}(\mathcal{D};\theta)\)</span>，即可估计出最优的参数<span class="math inline">\(\theta^*\)</span>。不过在计算梯度的时候，需要在对数函数内部进行求和或积分计算。为了更好的计算<span class="math inline">\(\log p(x;\theta)\)</span>，我们引入一个额外的变分函数<span class="math inline">\(q(z)\)</span>，<span class="math inline">\(q(z)\)</span>为定义在隐变量<span class="math inline">\(z\)</span>上的分布。样本<span class="math inline">\(x\)</span>的对数边际似然函数为：</p><p><span class="math display">\[\begin{align}\log p(x;\theta)&amp;=\log \sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\\&amp;\geq\sum_z q(z)\log \frac{p(x,z;\theta)}{q(z)}\\&amp;\triangleq ELBO(q,x;\theta)\end{align}\]</span></p><p>其中<span class="math inline">\(ELBO(q,x;\theta)\)</span>为对数边际似然函数<span class="math inline">\(\log p(x;\theta)\)</span>的下界，称为证据下界。公式中使用了Jensen不等式(即对于凹函数<span class="math inline">\(g\)</span>，有<span class="math inline">\(g(\mathbb{E}[x])\geq\mathbb{E}[g(X)]\)</span>)。在这里，<span class="math inline">\(\frac{p(x,z;\theta)}{q(z)}\)</span>可视为<span class="math inline">\(q(z)\)</span>的函数，记为<span class="math inline">\(f(q(z))\)</span>，那么<span class="math inline">\(f(q(z))\)</span>的期望即<span class="math inline">\(\mathbb{E}[f(q(z))]=\sum_z q(z)f(q(z))=\sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\)</span>。而根据Jensen不等式，有<span class="math inline">\(g(\mathbb{E}[f(q(z))])\geq\mathbb{E}[g(f(q(z)))]\Leftrightarrow g(\sum_z q(z)\frac{p(x,z;\theta)}{q(z)})\geq \sum_z q(z)g(\frac{p(x,z;\theta)}{q(z)})\)</span>，在这里<span class="math inline">\(g\)</span>就是对数函数。</p><hr /><p>根据Jensen不等式取等的条件：<span class="math inline">\(\frac{p(x,z;\theta)}{q(z)}=c\)</span>，<span class="math inline">\(c\)</span>为常数，有：</p><p><span class="math display">\[\begin{align}\sum_z p(x,z;\theta)&amp;=c\sum_z q(z)\\\Leftrightarrow\sum_z p(x,z;\theta)&amp;=c\cdot1\end{align}\]</span></p><p>因此：</p><p><span class="math display">\[\begin{align}q(z)&amp;=\frac{p(x,z;\theta)}{\sum_z p(x,z;\theta)}\\&amp;=\frac{p(x,z;\theta)}{p(x;\theta)}\\&amp;=p(z|x;\theta)\end{align}\]</span></p><p>所以，当且仅当<span class="math inline">\(q(z)=p(z|x;\theta)\)</span>时，<span class="math inline">\(\log p(x;\theta)=ELBO(q,x;\theta)\)</span>。</p><hr /><p>于是最大化对数边际似然函数<span class="math inline">\(\log p(x;\theta)\)</span>的过程可以分解为两个步骤：</p><ol type="1"><li>先找到近似分布<span class="math inline">\(q(z)\)</span>使得<span class="math inline">\(\log p(x;\theta)=ELBO(q,x;\theta)\)</span>；</li><li>再寻找参数<span class="math inline">\(\theta\)</span>最大化<span class="math inline">\(ELBO(q,x;\theta)\)</span>。</li></ol><p>这就是期望最大化(Expectation-Maximum,EM)算法。</p><hr /><p>EM算法通过迭代的方法，不断重复直到收敛到某个局部最优解。在第<span class="math inline">\(t\)</span>步更新时，E步和M步分别为：</p><ol type="1"><li><p>E步：固定参数<span class="math inline">\(\theta_t\)</span>，找到一个分布使<span class="math inline">\(ELBO(q,x;\theta_t)\)</span>最大，即等于<span class="math inline">\(\log p(x;\theta_t)\)</span>：<span class="math inline">\(q_{t+1}(z)=\text{arg}_q \max ELBO(q,x;\theta_t)\)</span>；</p></li><li><p>M步：固定<span class="math inline">\(q_{t+1}(z)\)</span>，找到一组参数使得证据下界最大，即：<span class="math inline">\(\theta_{t+1}=\text{arg}_\theta\max ELBO(q_{t+1},x;\theta)\)</span>。</p></li></ol><hr /><p>对数边际似然也可以通过信息论的视角来进行分解：</p><p><span class="math display">\[\begin{align}\log p(x;\theta)&amp;=\sum_z q(z)\log p(x;\theta)\\&amp;=\sum_z q(z)(\log p(x,z;\theta)-\log p(z|x;\theta))\\&amp;=\sum_z q(z)\log\frac{p(x,z;\theta)}{q(z)}-\sum_z q(z)\log\frac{p(z|x;\theta)}{q(z)}\\&amp;=ELBO(q,x;\theta)+D_{KL}(q(z)\parallel p(z|x;\theta))\end{align}\]</span></p><p>其中<span class="math inline">\(D_{KL}(q(z)\parallel p(z|x;\theta))\)</span></p><h1 id="generative-model-with-hidden-variable">Generative Model with Hidden Variable</h1><p>假设一个生成模型包含不可观测的隐变量，其中可观测变量<span class="math inline">\(x\)</span>为一个高维空间中的随机向量，而不可观测的隐变量<span class="math inline">\(z\)</span>为一个相对低维空间中的随机向量。</p><p>这个生成模型的联合概率密度函数可以表达为：</p><p><span class="math display">\[p(x,z;\theta)=p(x|z;\theta)p(z;\theta)\]</span></p><p>其中<span class="math inline">\(p(z;\theta)\)</span>为隐变量<span class="math inline">\(z\)</span>的先验概率分布；<span class="math inline">\(p(x|z;\theta)\)</span>为已知<span class="math inline">\(z\)</span>条件下<span class="math inline">\(x\)</span>的概率分布。通常情况下，我们可以假设<span class="math inline">\(p(z;\theta)\)</span>和<span class="math inline">\(p(x|z;\theta)\)</span>服从某种带参的分布族，其形式已知，而参数可以通过最大似然来进行估计。</p><p>给定一个样本<span class="math inline">\(x\)</span>，其对数边际似然<span class="math inline">\(\log p(x;\theta)\)</span>可以分解为：</p><p><span class="math display">\[\log p(x;\theta)=ELBO(q,x;\theta,\phi)+D_{KL}(q(z;\phi)\parallel p(z|x;\theta))\]</span></p><p>其中<span class="math inline">\(q(z;\phi)\)</span>为额外引入的变分密度函数，<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>为证据下界：</p><p><span class="math display">\[ELBO(q,x;\theta,\phi)=\mathbb{E}_{z\sim q(z;\phi)}[\log{\frac{p(x,z;\theta)}{q(z;\phi)}}]\]</span></p><p>最大化<span class="math inline">\(\log p(x;\theta)\)</span>可以用EM算法来求解：</p><ul><li><strong>E-step:</strong> 寻找一个密度函数<span class="math inline">\(q(z;\phi)\)</span>使其等于或接近于后验密度函数<span class="math inline">\(p(z|x;\theta)\)</span>;</li><li><strong>M-step:</strong> 保持<span class="math inline">\(q(z;\phi)\)</span>固定，寻找<span class="math inline">\(\theta\)</span>来最大化<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>。</li></ul><p>在EM算法的每次迭代中，理论上最优的<span class="math inline">\(q(z;\phi)\)</span>为隐变量的后验概率密度函数<span class="math inline">\(p(z|x;\theta)\)</span>：</p><p><span class="math display">\[p(z|x;\theta)=\frac{p(x|z;\theta)p(z;\theta)}{\int_z p(x|z;\theta)p(z;\theta)\text{d}z}\]</span></p><p>后验密度函数<span class="math inline">\(p(z|x;\theta)\)</span>的计算是一个统计推断的问题，在一般情况下<span class="math inline">\(p(x|z;\theta)\)</span>也比较难以计算。</p><h1 id="variational-autoencoder">Variational Autoencoder</h1><p>变分自编码器(Variational Autoencoder, VAE)的主要思想是利用神经网络来分别建模两个复杂的条件概率密度函数：</p><ol type="1"><li>用神经网络来产生变分分布<span class="math inline">\(q(z;\phi)\)</span>，称为推断网络。推断网络的输入为<span class="math inline">\(x\)</span>，输出为变分分布<span class="math inline">\(q(z|x;\phi)\)</span>；</li><li>用神经网络来产生概率分布<span class="math inline">\(p(x|z;\theta)\)</span>，称为生成网络。生成网络的输入为<span class="math inline">\(z\)</span>，输出为概率分布<span class="math inline">\(p(x|z;\theta)\)</span>。</li></ol><p><img src="http://qfxiao.me/img/autoencoder23849248011.png" srcset="/img/loading.gif" /></p><p>VAE的图模型如下图所示：</p><p><img src="http://qfxiao.me/img/1565532060281.png" srcset="/img/loading.gif" /></p><h2 id="variational-network">Variational Network</h2><p>假设<span class="math inline">\(q(z|x;\phi)\)</span>是服从对角化协方差的高斯分布：</p><p><span class="math display">\[q(z|x;\phi)=\mathcal{N}(z;\mu_I,\sigma^2_I I)\]</span></p><p>其中<span class="math inline">\(\mu_I\)</span>和<span class="math inline">\(\sigma_I^2\)</span>是高斯分布的均值和方差，可以通过推断网络<span class="math inline">\(f_I(x;\phi)\)</span>来预测：</p><p><span class="math display">\[\left[\begin{matrix}\mu_I\\\sigma_I\end{matrix}\right]=f_I(x;\phi)\]</span> 推断网络<span class="math inline">\(f_I(x;\phi)\)</span>可以是一般的全连接网络或卷积网络，比如一个两层的神经网络：</p><p><span class="math display">\[\begin{align}h&amp;=\sigma(W^{(1)}x+b^{(1)})\\\mu_I&amp;=W^{(2)}h+b^{(2)}\\\sigma_I&amp;=\text{softplus}(W^{(3)}h+b^{(3)})\end{align}\]</span></p><p>其中所有网络参数<span class="math inline">\(\{W^{(1)},W^{(2)},W^{(3)},b^{(1)},b^{(2)},b^{(3)}\}\)</span>即对应了变分参数<span class="math inline">\(\phi\)</span>。</p><hr /><p>推断网络的目标是使得<span class="math inline">\(q(z|x;\phi)\)</span>来尽可能接近真实的后验<span class="math inline">\(p(z|x;\theta)\)</span>，需要找到变分参数<span class="math inline">\(\phi^*\)</span>来最小化两个分布的KL散度：</p><p><span class="math display">\[\phi^*=\text{arg}_\phi\min{D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))}\]</span></p><p>由于<span class="math inline">\(p(z|x;\theta)\)</span>未知，故KL散度无法直接计算，不过由于<span class="math inline">\(D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))=\log p(x;\theta)-ELBO(q,x;\theta,\phi)\)</span>，所以可以直接最大化证据下界，有：</p><p><span class="math display">\[\phi^*=\text{arg}_\phi\max{ELBO(q,x;\theta,\phi)}\]</span></p><h2 id="generative-network">Generative Network</h2><p>生成模型的联合分布可以分解为两部分：隐变量<span class="math inline">\(z\)</span>的先验分布<span class="math inline">\(p(z;\theta)\)</span>和条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>。为简单起见，一般假设隐变量<span class="math inline">\(z\)</span>的先验分布为标准正态分布<span class="math inline">\(\mathcal{N}(z|0,I)\)</span>，隐变量每一维之间都是独立的。条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>可以通过生成网络来建模，我们同样用参数化的分布族来表示条件概率分布<span class="math inline">\(p(x|z;\theta)\)</span>，这些分布族的函数可以用生成网络计算得到。根据变量<span class="math inline">\(x\)</span>的类型不同，可以假设<span class="math inline">\(p(x|z;\theta)\)</span>服从不同的分布族。如果<span class="math inline">\(x\in\{0,1\}^d\)</span>是<span class="math inline">\(d\)</span>维的二值向量，可以假设<span class="math inline">\(\log p(x|z;\theta)\)</span>服从多变量的伯努利分布，即：</p><p><span class="math display">\[\begin{align}p(x|z;\theta)&amp;=\prod\limits_{i=1}^d p(x_i|z;\theta)\\&amp;=\prod\limits_{i=1}^d \gamma_i^{x_i}(1-\gamma_i)^{(1-x_i)}\end{align}\]</span></p><p>如果<span class="math inline">\(x\in\mathbb{R}^d\)</span>是<span class="math inline">\(d\)</span>维的连续向量，可以假设<span class="math inline">\(p(x|z;\theta)\)</span>服从对角化协方差的高斯分布，即：</p><p><span class="math display">\[p(x|z;\theta)=\mathcal{N}(x;\mu_G,\sigma_G^2 I)\]</span></p><hr /><p>生成网络的目标是找到一组<span class="math inline">\(\theta^*\)</span>最大化证据下界<span class="math inline">\(ELBO(q,x;\theta,\phi)\)</span>：</p><p><span class="math display">\[\theta^*=\text{arg}_\theta\max ELBO(q,x;\theta,\phi)\]</span></p><h2 id="model-combination">Model Combination</h2><p>推断网络和生成网络的目标都是最大化证据下界因此总的目标函数为：</p><p><span class="math display">\[\begin{align}\max_{\theta,\phi}ELBO(q,x;\theta,\phi)&amp;=\max_{\theta,\phi}\mathbb{E}_{z\sim q(z;\phi)}[\log\frac{p(x|z;\theta)p(z;\theta)}{q(z;\theta)}]\\&amp;=\max_{\theta,\phi}\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]-D_{KL}(q(z|x;\phi)\parallel p(z;\theta))\end{align}\]</span></p><p>其中先验分布<span class="math inline">\(p(z;\theta)=\mathcal{N}(z|0,I)\)</span>。</p><p>公式中<span class="math inline">\(\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]\)</span>一般通过采样的方式进行计算，最后取平均值。</p><h2 id="model-training">Model Training</h2><p>给定数据集<span class="math inline">\(\mathcal{D}\)</span>，包含<span class="math inline">\(N\)</span>个从未知数据分布中抽取的独立同分布样本<span class="math inline">\(x^{(1)},x^{(2)},\cdots,x^{(N)}\)</span>。变分自编码器的目标函数为：</p><p><span class="math display">\[\mathcal{J}(\phi,\theta|\mathcal{D})=\sum\limits_{n=1}^N(\frac{1}{M}\sum\limits_{m=1}^M\log p(x^{(n)}|z^{(n,m)};\theta)-D_{KL}(q(z|x^{(n)};\phi)\parallel\mathcal{N}(z;0,I)))\]</span></p><p>如果采用随机梯度下降法，每次从数据集中采一个样本<span class="math inline">\(x\)</span>，然后根据<span class="math inline">\(q(z|x;\phi)\)</span>采一个隐变量<span class="math inline">\(z\)</span>，则目标函数变为：</p><p><span class="math display">\[\mathcal{J}(\phi,\theta|x)=\log p(x|z;\theta)-D_{KL}(q(z|x;\phi)\parallel\mathcal{N}(z;0,I))\]</span></p><p>假设<span class="math inline">\(q(z|x;\phi)\)</span>是正态分布，KL散度可直接算出：</p><p><span class="math display">\[D_{KL}(\mathcal{N}(\mu_1,\Sigma_1)\parallel\mathcal(\mu_2,\Sigma_2))\\=\frac{1}{2}(\text{tr}(\sigma_I^2 I)+\mu_I^T\mu_I-d-\log(|\sigma_I^2 I|))\]</span></p><hr /><p>再参数化是将一个参数为<span class="math inline">\(u\)</span>的函数<span class="math inline">\(f(u)\)</span>，通过一个函数<span class="math inline">\(u=g(v)\)</span>，转换为参数为<span class="math inline">\(v\)</span>的函数<span class="math inline">\(\hat{f}(v)=f(g(v))\)</span>。在变分自编码器中，一个问题是如何求随机变量<span class="math inline">\(z\)</span>关于<span class="math inline">\(\phi\)</span>的导数。但由于是采样的方式，无法直接刻画<span class="math inline">\(z\)</span>和<span class="math inline">\(\phi\)</span>之间的函数关系，因此也无法计算导数。</p><p>如果<span class="math inline">\(z\sim q(z|x;\phi)\)</span>的随机性独立于参数<span class="math inline">\(\phi\)</span>，我们可以通过再参数化的方法来计算导数。假设<span class="math inline">\(q(z|x;\phi)\)</span>为正态分布<span class="math inline">\(\mathcal{N}(\mu_I,\sigma^2_I I)\)</span>，其中<span class="math inline">\(\mu_I\)</span>和<span class="math inline">\(\sigma_I\)</span>是推断网络<span class="math inline">\(f_I(x;\phi)\)</span>的输出。我们可以通过下面的方式采样<span class="math inline">\(z\)</span>：</p><p><span class="math display">\[z=\mu_I+\sigma_I\odot \varepsilon\]</span></p><p>其中<span class="math inline">\(\varepsilon\sim\mathcal{N}(0,I)\)</span>。这样<span class="math inline">\(z\)</span>和<span class="math inline">\(\mu_I,\sigma_I\)</span>的关系从采样关系变为函数关系。</p><hr /><p>如果进一步假设<span class="math inline">\(p(x|z;\theta)\)</span>服从高斯分布<span class="math inline">\(\mathcal{N}(x|\mu_G,I)\)</span>，其中<span class="math inline">\(\mu_G=f_G(z;\theta)\)</span>是生成网络的输出，则目标函数可以简化为：</p><p><span class="math display">\[\mathcal{J}(\phi,\theta|x)=-\parallel x-\mu_G\parallel^2+D_{KL}(\mathcal{N}(\mu_I,\sigma_I)\parallel\mathcal{N}(0,I))\]</span></p><p>其中第一项可以近似看作是输入<span class="math inline">\(x\)</span>的重构正确性，第二项可以看作是正则化项。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Variational Inference</tag>
      
      <tag>VAE</tag>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Recurrent Neural Networks for Multivariate Time Series with Missing Values</title>
    <link href="/2019/10/18/Recurrent-Neural-Networks-for-Multivariate-Time-Series-with-Missing-Values/"/>
    <url>/2019/10/18/Recurrent-Neural-Networks-for-Multivariate-Time-Series-with-Missing-Values/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>文中提出了一种可以处理带缺失值多为时间序列的GRU模型：<strong>GRU-D</strong>。本模型不仅可以捕捉时间序列中的长期依赖模式，并且还能利用时间序列中的缺失模式来达到更好的时间序列预测效果。</p><p><a href="https://www.nature.com/articles/s41598-018-24271-9" target="_blank" rel="noopener">原文</a></p><h1 id="methodology">Methodology</h1><h2 id="notations">Notations</h2><p>记包含<span class="math inline">\(D\)</span>个变量的多变量时间序列为<span class="math inline">\(X=(x_1,x_2,\cdots,x_T)^T\in\mathbb{R}^{T\times D}\)</span>，其中对于每个<span class="math inline">\(t\in\{1,2,\cdots,T\},x_t\in\mathbb{R}^D\)</span>表示时间序列在时间<span class="math inline">\(t\)</span>的观测值，<span class="math inline">\(x_t^d\)</span>表示<span class="math inline">\(x_t\)</span>的第<span class="math inline">\(d\)</span>个成分。记<span class="math inline">\(s_t\in\mathbb{R}\)</span>为<span class="math inline">\(t\)</span>时刻的时间戳，并假设第一个观测值的时间戳为<span class="math inline">\(0\)</span>。对于包含缺失值的时间序列，我们用<strong>Masking Vector</strong> <span class="math inline">\(m_t\in\{0,1\}\)</span>进行标记，同时对每个<span class="math inline">\(x_t^d\)</span>维护距离上一个观测值的<strong>Time Interval</strong> <span class="math inline">\(\delta_t^d\in\mathbb{R}\)</span>，公式如下： <span class="math display">\[m_t^d=\begin{cases}1, &amp;\text{if }x_t^d\text{ is observed}\\0, &amp;\text{otherwise}\end{cases}\]</span></p><p><span class="math display">\[\delta_t^d=\begin{cases}s_t-s_{t-1}+\delta_{t-1}^d, &amp;t&gt;1,m_{t-1}^d=0\\s_t-s_{t-1}, &amp;t&gt;1, m_{t-1}^d=1\\0, &amp;t=1\end{cases}\]</span></p><p>下图是一些示例：</p><p><img src="http://qfxiao.me/img/1571476968374.png" srcset="/img/loading.gif" /></p><p>在本文中，我们主要关注时间序列的分类问题，即给定数据集<span class="math inline">\(\mathcal{D}=\{(X_n,s_n,M_n)\}_{n=1}^N\)</span>，我们要对每个样本的类别进行预测<span class="math inline">\(l_n\in\{1,\cdots,L\}\)</span>。</p><h2 id="gru-rnn-for-time-series-classification">GRU-RNN for Time Series Classification</h2><p>GRU是一种改进版本的RNN，其最大不同是加入了门控机制。GRU单元的结构如下图所示：</p><p><img src="http://qfxiao.me/img/GRU49241911247.png" srcset="/img/loading.gif" /></p><p>GRU包含了重置门和更新门，其中重置门<span class="math inline">\(R_t\)</span>负责控制上一时间的隐状态<span class="math inline">\(h_{t-1}\)</span>有多少部分需要保留，而更新门则决定由<span class="math inline">\(R_t\)</span>计算出来的候选隐状态<span class="math inline">\(\tilde{h}_t\)</span>有多少部分需要保留。最后当前时间的隐状态由<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(\tilde{h}_t\)</span>共同算出。GRU的状态更新公式如下： <span class="math display">\[\begin{align}R_t&amp;=\sigma(W_rx_t+U_rh_{t-1}+b_r)\\Z_t&amp;=\sigma(W_zx_t+U_zh_{t-1}+b_z)\\\tilde{h}_t&amp;=\text{tanh}(Wx_t+U(R_t\odot h_{t-1})+b)\\h_t&amp;=(1-Z_t)\odot h_{t-1}+Z_t\odot \tilde{h}_t\end{align}\]</span> 文中提出了一些处理缺失值的简单方法：</p><ol type="1"><li>直接用均值替代：<span class="math inline">\(x_t^d\leftarrow m_t^dx_t^d+(1-m_t^d)\tilde{x}^d\)</span>，其中<span class="math inline">\(\tilde{x}^d=\frac{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d x_{t,n}^d}{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d\tilde{x}^d}\)</span>。这种方法称为<strong>GRU-Mean</strong>；</li><li>用上一个观测值替代：<span class="math inline">\(x_t^d\leftarrow m_t^d x_t^d+(1-m_t^d)x_{t^\prime}^d\)</span>。这种方法称为<strong>GRU-Forward</strong>；</li><li>不填充，将是否缺失，距离上一个观测值的时间作为额外信息输入：<span class="math inline">\(x_t^{(n)}\leftarrow[x_t^{(n)};m_t^{(n)};\delta_t^{(n)}]\)</span>。这种方法称为<strong>GRU-Simple</strong>。</li></ol><h3 id="gru-d-model-with-trainable-decays">GRU-D: Model with Trainable Decays</h3><p>文中提出了时间序列缺失值的两个性质：一个是在上一个观测值距离很远的情况下缺失值倾向于接近一个默认的值，第二个是缺失值的影响会随着时间减弱。为了体现上述两点，文中提出了GPU-D模型，模型框架如下：</p><p><img src="http://qfxiao.me/img/1571412138245.png" srcset="/img/loading.gif" /></p><p>在模型中，<strong>Decay Rates</strong>被设定为一个带参数的函数和GRU一起训练： <span class="math display">\[\gamma_t=\exp\{-\max(0,W_\gamma\delta_t+b_\gamma)\}\]</span></p><p><span class="math display">\[\hat{x}_t^d=m_t^dx_t^d+(1-m_t^d)(\gamma_{x_t}^dx_{t^\prime}^d+(1-\gamma_{x_t}^d)\tilde{x}^d)\]</span> 其中<span class="math inline">\(x_{t^\prime}^d\)</span>是第<span class="math inline">\(d\)</span>个变量的上一个观测值，<span class="math inline">\(\tilde{x}^d\)</span>是第<span class="math inline">\(d\)</span>个变量的经验均值。这样<span class="math inline">\(\hat{x}_t^d\)</span>就代表经过<strong>Input Decay</strong>的输入。</p><p>文中提到只用<strong>Input Decay</strong>是不够的，除此之外作者还使用了<strong>Hidden State Decay</strong>，即对<span class="math inline">\(h_{t-1}\)</span>进行Decay，公式如下： <span class="math display">\[\hat{h}_{t-1}=\gamma_{h_t}\odot h_{t-1}\]</span> 用Decay之后的<span class="math inline">\(\hat{x}_t\)</span>和<span class="math inline">\(\hat{h}_{t-1}\)</span>替换原始的GRU公式就得到了GRU-D模型： <span class="math display">\[\begin{align}R_t&amp;=\sigma(W_r\hat{x}_t-U_r\hat{h}_{t-1}+V_rm_t+b_r)\\Z_t&amp;=\sigma(W_z\hat{x}_t+U_z\hat{h}_{t-1}+V_zm_t+b_z)\\\tilde{h}_t&amp;=\text{tanh}(W\hat{x}_t+U(R_t\odot \hat{h}_{t-1})+Vm_t+b)\\h_t&amp;=(1-z_t)\odot \hat{h}_{t-1}+z_t\odot\tilde{h}_t\end{align}\]</span></p><h1 id="experiments">Experiments</h1><h2 id="baseline-imputation-methods">Baseline Imputation Methods</h2><p>下图为文中比较中用到的Baseline：</p><p><img src="http://qfxiao.me/img/1571673314402.png" srcset="/img/loading.gif" /></p><h2 id="baseline-prediction-methods">Baseline Prediction Methods</h2><p>下图为文中用到的用来预测的Baseline：</p><p><img src="http://qfxiao.me/img/1571673403956.png" srcset="/img/loading.gif" /></p><h2 id="results">Results</h2><p>文中用到的数据集如下：</p><ul><li><em>Gesture phase segmentation dataset (Gesture)</em>.</li><li><em>PhysioNet Challenge 2012 dataset (PhysioNet)</em>.</li><li><em>MIMIC-Ⅲ dataset (MIMIC-Ⅲ)</em>.</li></ul><p>下图展示了不同方法在人工合成数据集上的表现：</p><p><img src="http://qfxiao.me/img/1571412162954.png" srcset="/img/loading.gif" /></p><p>下表展示了不同模型在预测任务表现的对比：</p><p><img src="http://qfxiao.me/img/1571412183650.png" srcset="/img/loading.gif" /></p><p>下表展示了不同方法在MIMIC-Ⅲ和PhysioNet数据集上的多任务表现：</p><p><img src="http://qfxiao.me/img/1571412196724.png" srcset="/img/loading.gif" /></p><p>下图分别展示了模型学到的<strong>Input Decay</strong>和<strong>Hidden State Decay</strong>：</p><p><img src="http://qfxiao.me/img/1571412221428.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>RNN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</title>
    <link href="/2019/10/18/Robust-Anomaly-Detection-for-Multivariate-Time-Series-through-Stochastic-Recurrent-Neural-Network/"/>
    <url>/2019/10/18/Robust-Anomaly-Detection-for-Multivariate-Time-Series-through-Stochastic-Recurrent-Neural-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>本文提出了<em>OmniAnomaly</em>：一种针对多变量时间序列的随机循环神经网络异常检测算法。该模型运用了一系列技术来捕捉多变量时间序列的正常模式，并在检测阶段基于重构误差来检测异常，同时本文还提供了一定的理论解释。</p><p><a href="https://www.kdd.org/kdd2019/accepted-papers/view/robust-anomaly-detection-for-multivariate-time-series-through-stochastic-re" target="_blank" rel="noopener">原文</a></p><h1 id="contribution">Contribution</h1><ol type="1"><li>提出了<em>OmniAnomaly</em>，一种基于随机循环神经网络的多变量时间序列异常检测算法；</li><li>提出了针对多变量时间序列异常检测的解释方法；</li><li>通过实验证明了<em>OmniAnomaly</em>中所用的关键技术的有效性，包括GRU，planar NF, stochastic variable connection和adjusted Peaks-Over-Threshold method；</li><li>通过大量的实验我们证明了<em>OmniAnomaly</em>的有效性；</li><li>发布了代码和数据集。</li></ol><h1 id="background">Background</h1><h2 id="linear-gaussian-state-space-model">Linear Gaussian State Space Model</h2><p>状态空间模型（State Space Model, SSM）的概念来自于控制理论，在这里我们主要讨论其在时间序列中的应用。其大概思想是我们认为时间序列在时刻<span class="math inline">\(t\)</span>的观测值<span class="math inline">\(z_t\)</span>是一个隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>的条件分布<span class="math inline">\(p(z_t|\boldsymbol{l}_t)\)</span>，而这个隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>刻画了时间序列的内在规律，同时隐含状态会随着时间更新，即服从条件分布<span class="math inline">\(p(\boldsymbol{l}_t|\boldsymbol{l}_{t-1})\)</span>。</p><p>在线性状态空间模型（Linear State Space Model）中我们以如下的方式刻画隐含状态的更新： <span class="math display">\[\boldsymbol{l}_t=\boldsymbol{F}_t\boldsymbol{l}_{t-1}+\boldsymbol{g}_t\varepsilon_t, \space\space\space\varepsilon_t\sim\mathcal{N}(0,1)\]</span> <span class="math inline">\(\boldsymbol{F}_t\)</span>为确定的状态转移矩阵，而<span class="math inline">\(\boldsymbol{g}_t\varepsilon_t\)</span>则表示了状态转移的随机性。</p><p>观测值<span class="math inline">\(z_t\)</span>从隐含状态<span class="math inline">\(\boldsymbol{l}_t\)</span>计算而来： <span class="math display">\[\begin{align}z_t&amp;=y_t+\sigma_t\epsilon_t,\\y_t&amp;=\boldsymbol{a}_t^\top\boldsymbol{l}_{t-1}+b_t,\\\epsilon_t&amp;\sim\mathcal{N}(0,1)\end{align}\]</span> 其中<span class="math inline">\(\boldsymbol{a}_t\in\mathbb{R}^L,\sigma_t\in \mathbb{R},b_t\in\mathbb{R}\)</span>都是额外的参数。初始状态<span class="math inline">\(\boldsymbol{l}_0\)</span>则从一个独立的高斯分布得来，即<span class="math inline">\(\boldsymbol{l}_0\sim N(\boldsymbol\mu_0,\text{diag}(\boldsymbol{\sigma}_0^2))\)</span>。</p><p>令参数集合<span class="math inline">\(\Theta_t=(\boldsymbol{\mu}_0,\boldsymbol{\Sigma}_0,\boldsymbol{F}_t,\boldsymbol{g}_t,\boldsymbol{a}_t,b_t,\sigma_t),\forall t&gt;0\)</span>，一般来说参数集合不会随着时间变化，即每个时刻<span class="math inline">\(t\)</span>共享同样的参数<span class="math inline">\(\Theta_t=\Theta,\forall t&gt;0\)</span>。对参数的估计可以采用极大似然估计： <span class="math display">\[\begin{align}\Theta^*_{1:T}&amp;=\arg\max_{\Theta_{1:T}}p(z_{1:T}|\Theta_{1:T}),\\\end{align}\]</span> 其中： <span class="math display">\[\begin{align}p(z_{1:T}|\Theta_{1:T})&amp;=p(z_1|\Theta_1)\prod\limits_{t=2}^T p(z_t|z_{1:t-1},\Theta_{1:t})\\&amp;=\int p(\boldsymbol{l}_0)\left[\prod\limits_{t=1}^T p(z_t|\boldsymbol{l}_t)p(\boldsymbol{l}_t|\boldsymbol{l}_{t-1})\right]\mathrm{d}\boldsymbol{l}_{0:T}\end{align}\]</span></p><h2 id="planar-normalizing-flow">Planar Normalizing Flow</h2><h3 id="normalizing-flows">Normalizing Flows</h3><p>VAE采用一个变分分布<span class="math inline">\(q_\phi(z|x)\)</span>来近似真实的后验分布<span class="math inline">\(p(z|x)\)</span>，并推导出<span class="math inline">\(\log p_\theta(x)\)</span>的下界（称为ELBO）来作为优化目标函数： <span class="math display">\[\begin{align}\log p_\theta(x)&amp;=\log \int p_\theta(x|z)p(z)\mathrm{d}z\\&amp;=\log\int\frac{q_\phi(z|x)}{q_\phi(z|x)}p_\theta(x|z)p(z)\mathrm{d}z\\&amp;\geq-D_{KL}[q_\phi(z|x)\parallel p(z)]+\mathbb{E}_q[\log p_\theta(x|z)]\end{align}\]</span> <span class="math inline">\(\log p_\theta(x)\)</span>与ELBO取等的条件是<span class="math inline">\(D_{KL}[q_\phi(z|x)\parallel p(z)]\)</span>，表明变分分布完全匹配了真实的后验分布。但在实际应用中，真实的后验分布可能会非常复杂，而我们的变分分布通常是一个确定的较为简单的分布，如高斯分布。这样变分分布可能很难对真实后验分布得到一个很好的拟合。</p><p>一个解决方案是使用标准化流（Normalizing Flows）。标准化流是从一个相对简单的分布出发，执行一系列可逆的映射，将原始简单的分布转化为一个复杂的分布。</p><p>首先考虑一个光滑的、可逆的映射<span class="math inline">\(f:\mathbb{R}^d\mapsto \mathbb{R}^d\)</span>，记<span class="math inline">\(g=f^{-1}\)</span>，那么<span class="math inline">\(g\circ f(\mathbf{z})=\mathbf{z}\)</span>。令<span class="math inline">\(\mathbf{z}^\prime=f(\mathbf{z})\)</span>，那么<span class="math inline">\(\mathbf{z}^\prime\)</span>的分布为： <span class="math display">\[q(\mathbf{z}^\prime)=q(\mathbf{z})\left|\text{det}\frac{\partial f^{-1}}{\partial \mathbf{z}^\prime}\right|=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}\]</span> 式中<span class="math inline">\(q(\mathbf{z}^\prime)=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}\)</span>说明了<span class="math inline">\(\mathbf{z}^\prime\)</span>的分布等于<span class="math inline">\(\mathbf{z}\)</span>的分布乘上<span class="math inline">\(f\)</span>的Jacobian矩阵的行列式的倒数。那么对于映射多次的情况： <span class="math display">\[\mathbf{z}_K=f_K\circ\cdots\circ f_2\circ f_1(\mathbf{z}_0)\]</span> <span class="math inline">\(\mathbf{z}_K\)</span>的分布可以通过链式计算得到： <span class="math display">\[\ln q_K(\mathbf{z}_K)=\ln q_0(\mathbf{z}_0)-\sum\limits_{k=1}^K\ln\left|\text{det}\frac{\partial f_k}{\partial \mathbf{z}_{k-1}}\right|\]</span></p><h3 id="planar-flows">Planar Flows</h3><p>考虑一个变换族： <span class="math display">\[f(\mathbf{z})=\mathbf{z}+\mathbf{u}h(\mathbf{w}^\top\mathbf{z}+b)\]</span> 其中<span class="math inline">\(\lambda=\{\mathbf{w}\in \mathbb{R}^d,\mathbf{u}\in\mathbb{R}^d,b\in\mathbb{R}\}\)</span>为参数集合，<span class="math inline">\(h(\cdot)\)</span>为元素级的非线性函数（如各种激活函数）。令<span class="math inline">\(\psi(\mathbf{z})=h^\prime(\mathbf{w}^\top\mathbf{z}+b)\mathbf{w}\)</span>，则<span class="math inline">\(f\)</span>的Jacobian矩阵行列式绝对值等于： <span class="math display">\[\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|=\left|\text{det}(\mathbf{I}+\mathbf{u}\psi(\mathbf{z})^\top)\right|=\left|1+\mathbf{u}^\top\psi(\mathbf{z})\right|\]</span> 但是<span class="math inline">\(f\)</span>并不保证总是可逆的，如<span class="math inline">\(h(x)=\tanh(x)\)</span>时，<span class="math inline">\(f\)</span>可逆的条件是<span class="math inline">\(\mathbf{w}^\top \mathbf{u}\geq-1\)</span>。</p><p>下面讨论如何保证可逆的条件。考虑将<span class="math inline">\(\mathbf{z}\)</span>分解为<span class="math inline">\(\mathbf{z}=\mathbf{z}_\bot+\mathbf{z}_\parallel\)</span>，其中<span class="math inline">\(\mathbf{z}_\bot\)</span>与<span class="math inline">\(\mathbf{w}\)</span>正交，<span class="math inline">\(\mathbf{z}_\parallel\)</span>与<span class="math inline">\(\mathbf{w}\)</span>平行，那么： <span class="math display">\[f(z)=\mathbf{z}_\bot+\mathbf{z}_\parallel+\mathbf{u}h(\mathbf{w}^\top \mathbf{z}_\parallel +b)\]</span> 实际上得到<span class="math inline">\(\mathbf{z}_\parallel\)</span>之后可以很容易的得到<span class="math inline">\(\mathbf{z}_\bot\)</span>，令<span class="math inline">\(\mathbf{y}=f(\mathbf{z})\)</span>，有： <span class="math display">\[\mathbf{z}_\bot=\mathbf{y}-\mathbf{z}_\parallel-\mathbf{u}h(\mathbf{w}^\top\mathbf{z}_\parallel+b)\]</span> 而<span class="math inline">\(\mathbf{z}_\parallel\)</span>与<span class="math inline">\(\mathbf{w}\)</span>平行，易知<span class="math inline">\(\mathbf{z}_\parallel=\alpha\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}\)</span>，其中<span class="math inline">\(\alpha\in\mathbb{R}\)</span>。</p><p>对式(16)两边同时乘以<span class="math inline">\(\mathbf{w}^\top\)</span>可得： <span class="math display">\[\mathbf{w}^\top f(\mathbf{z})=\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\]</span> 当<span class="math inline">\(\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\)</span>对于<span class="math inline">\(\alpha\)</span>是非递减函数的时候，<span class="math inline">\(f\)</span>是可逆的。因为<span class="math inline">\(\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)\)</span>是非递减函数时有<span class="math inline">\(1+\mathbf{w}^\top\mathbf{u}h^\prime(\alpha+b)\geq 0\equiv \mathbf{w}^\top \mathbf{u}\geq -\frac{1}{h^\prime(\alpha + b)}\)</span>，而<span class="math inline">\(0\leq h^\prime(\alpha + b) \leq 1\)</span>（<span class="math inline">\(\tanh\)</span>函数的性质），所以总是有<span class="math inline">\(\mathbf{w}^\top \mathbf{u}\geq-1\)</span>。</p><p>对于任意一个<span class="math inline">\(\mathbf{u}\)</span>，我们可以通过特定的方式构造一个<span class="math inline">\(\hat{\mathbf{u}}\)</span>使得<span class="math inline">\(\mathbf{w}^\top\hat{\mathbf{u}}&gt;-1\)</span>，即令<span class="math inline">\(\hat{\mathbf{u}}(\mathbf{w},\mathbf{u})=\mathbf{u}+[m(\mathbf{w}^\top\mathbf{u})-(\mathbf{w}^\top\mathbf{u})]\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}\)</span>，其中<span class="math inline">\(m(x)=-1+\log(1+e^x)\)</span>。</p><p><img src="http://qfxiao.me/img/image-20191031162515819.png" srcset="/img/loading.gif" /></p><h1 id="methodology">Methodology</h1><h2 id="problem-statement">Problem Statement</h2><p>本文针对的是多变量时间序列<span class="math inline">\(x=\{x_1,x_2,\cdots,x_N\}\in R^{M\times N}\)</span>，<span class="math inline">\(N\)</span>为时间长度，其中某一时刻的观测值<span class="math inline">\(x_t\in R^M\)</span>为一个<span class="math inline">\(M\)</span>维的向量。作者使用<span class="math inline">\(x_{t-T:t}\in R^{M\times(T+1)}\)</span>来表示<span class="math inline">\(t-T\)</span>到<span class="math inline">\(t\)</span>之间的时间序列。</p><p><img src="http://qfxiao.me/img/image-20191024112404542.png" srcset="/img/loading.gif" /></p><h2 id="overall-structure">Overall Structure</h2><p>算法的总体框架如下图所示：</p><p><img src="http://qfxiao.me/img/1571411043958.png" srcset="/img/loading.gif" /></p><p>预处理模块主要是对数据进行标准化以及窗口切分。训练模块则根据输入的数据对正常模式进行捕捉，输出异常分数。在线检测模块则会定期执行。</p><h2 id="network-architecture">Network Architecture</h2><p>模型的总体结构如下图所示：</p><p><img src="http://qfxiao.me/img/1571411093161.png" srcset="/img/loading.gif" /></p><p>在qnet中，首先GRU被用来建模样本的时间依赖关系，之后VAE将样本<span class="math inline">\(\mathbf{x}\)</span>映射到隐空间<span class="math inline">\(\mathbf{z}\)</span>。文中使用了Linear Gaussian State Space Model来建模隐变量之间的时间依赖关系。除此之外，作者还使用了Planar Normalizing Flow来将隐变量映射到复杂的非高斯分布。在pnet中，隐变量<span class="math inline">\(\mathbf{z}_{t-T:t}\)</span>被用来重建<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>，直观上来说，对样本的好的隐变量表示可以带来更好的重构效果。</p><p>从细节上来说，在时间<span class="math inline">\(t\)</span>，qnet的输入为<span class="math inline">\(\mathbf{x}_t\)</span>和<span class="math inline">\(\mathbf{e}_{t-1}\)</span>，两者经过GRU Cell之后会产生<span class="math inline">\(t\)</span>时间的<span class="math inline">\(\mathbf{e_t}\)</span>。<span class="math inline">\(\mathbf{e}_t\)</span>是GRU捕捉时间依赖性的关键，可以认为它包含了<span class="math inline">\(\mathbf{x}_{1:t}\)</span>的信息。之后<span class="math inline">\(\mathbf{e}_t\)</span>会和<span class="math inline">\(\mathbf{z}_{t-1}\)</span>进行拼接，进入标准的VAE变分网络结构，通过网络输出的参数<span class="math inline">\(\mu_{z_t},\sigma_{z_t}\)</span>采样得到隐变量<span class="math inline">\(\mathbf{z}_t^0\)</span>，此时隐变量可以说捕捉了时间依赖性。</p><p>网络中涉及到的公式如下所示：</p><p><span class="math display">\[\begin{align}e_t&amp;=(1-c_t^e)\circ\text{tanh}(w^ex_t+u^e(r_t^e\circ e_{t-1})+b^e)+c_t^e\circ e_{t-1}\\\mu_{z_t}&amp;=w^{\mu_z}h^\phi([z_{t-1},e_t])+b^{\mu_z}\\\sigma_{z_t}&amp;=\text{softplus}(w^{\sigma_z}h^\phi([z_{t-1},e_t])+b^{\sigma_z})+\epsilon^{\sigma_z}\end{align}\]</span></p><p>其中<span class="math inline">\(r_t^e=\text{sigmoid}(\mathbf{w}^{r^e}\mathbf{x}_t+\mathbf{u}^{r^e}\mathbf{e}_{t-1}+b^{r^e})\)</span>是GRU中的重置门，<span class="math inline">\(c_t^e=\text{sigmoid}(\mathbf{w}^{c^e}\mathbf{x}_t+\mathbf{u}^{c^e}\mathbf{e}_{t-1}+b^{c^e})\)</span>是GRU中的更新门。</p><p>此时<span class="math inline">\(\mathbf{z}_t^0\)</span>服从高斯分布，为了拟合复杂的后验分布，我们使用Planar Normalizing Flow来对<span class="math inline">\(\mathbf{z}_t^0\)</span>进行变换，最后得到经<span class="math inline">\(K\)</span>次变换后的随机变量<span class="math inline">\(\mathbf{z}_t^K\)</span>。</p><p>在时间<span class="math inline">\(t\)</span>，pnet试图通过<span class="math inline">\(\mathbf{z}_t^K\)</span>来重构<span class="math inline">\(\mathbf{x}_t\)</span>。首先<span class="math inline">\(\mathbf{z}\)</span>空间中的变量会根据Linear Gaussian State Space Model来进行“连接“，公式为<span class="math inline">\(\mathbf{z}_t=\mathbf{O}_\theta(\mathbf{T}_\theta\mathbf{z}_{t-1}+\mathbf{v}_t)+\boldsymbol{\epsilon}_t\)</span>，其中<span class="math inline">\(\mathbf{O}_\theta\)</span>和<span class="math inline">\(\mathbf{T}_\theta\)</span>为状态转移矩阵，<span class="math inline">\(\mathbf{v}_t\)</span>和<span class="math inline">\(\boldsymbol{\epsilon}_t\)</span>为随机噪声。之后<span class="math inline">\(\mathbf{z}_t\)</span>和<span class="math inline">\(\mathbf{d}_{t-1}\)</span>会作为GRU的输入，产生<span class="math inline">\(\mathbf{d}_t\)</span>。之后<span class="math inline">\(\mathbf{d}_t\)</span>会经过标准VAE中的生成网络，通过网络输出的高斯分布参数<span class="math inline">\(\mu_{x_t},\sigma_{x_t}\)</span>采样得到重构后的样本<span class="math inline">\(\mathbf{x}^\prime_t\)</span>。pnet中涉及到的公式如下所示： <span class="math display">\[\begin{align}d_t&amp;=(1-c_t^d)\circ\text{tanh}(w^dz_t+u^d(r_t^d\circ d_{t-1})+b^d)+c_t^d\circ d_{t-1}\\\mu_{x_t}&amp;=w^{\mu_x}h^\theta(d_t)+b^{\mu_x}\\\sigma_{x_t}&amp;=\text{softplus}(w^{\sigma_x}h^\theta(d_t)+b^{\sigma_x})+\epsilon^{\sigma_x}\end{align}\]</span></p><p>其中<span class="math inline">\(r_t^d=\text{sigmoid}(\mathbf{w}^{r^d}\mathbf{x}_t+\mathbf{u}^{r^d}\mathbf{d}_{t-1}+b^{r^d})\)</span>是GRU中的重置门，<span class="math inline">\(c_t^d=\text{sigmoid}(\mathbf{w}^{c^d}\mathbf{x}_t+\mathbf{u}^{c^d}\mathbf{d}_{t-1}+b^{c^d})\)</span>是GRU中的更新门。</p><h2 id="offline-model-training">Offline Model Training</h2><p>和传统VAE类似，模型的训练可以通过优化ELBO来完成。记长度为<span class="math inline">\(T+1\)</span>的输入序列为<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>，隐空间变量采样次数为<span class="math inline">\(L\)</span>，第<span class="math inline">\(l\)</span>个隐空间变量为<span class="math inline">\(\mathbf{l}^{(l)}_{t-T:t}\)</span>，损失函数可以写成如下形式：</p><p><span class="math display">\[\tilde{\mathcal{L}}(\mathbf{x}_{t-T:t})\approx\frac{1}{L}\sum_{t=1}^L[\log(p_\theta(\mathbf{x}_{t-T:t}|\mathbf{z}_{t-T:t}^{(l)}))+\log(p_\theta(\mathbf{z}_{t-T:t}^{(l)}))-\log(q_\phi(\mathbf{z}_{t-T:t}^|\mathbf{x}_{t-T:t}))]\]</span></p><p>第一项<span class="math inline">\(\log(p_\theta(\mathbf{x}_{t-T:t}|\mathbf{z}_{t-T:t}^{(l)}))\)</span>可以看作是重构误差；第二项<span class="math inline">\(\log(p_\theta(\mathbf{z}_{t-T:t}))=\sum_{i=t-T}^t \log(p_\theta(\mathbf{z}_i|\mathbf{z}_{i-1}))\)</span>通过Linear Gaussian State Space Model计算；第三项<span class="math inline">\(-\log(q_\phi(\mathbf{z}_{t-T:t}|\mathbf{x}_{t-T:t}))=-\sum_{i=t-T}^t\log(q_\phi(\mathbf{z}_i|\mathbf{z}_{i-1},\mathbf{x}_{t-T:i}))\)</span>为隐变量<span class="math inline">\(\mathbf{z}\)</span>后验分布的估计，同时<span class="math inline">\(\mathbf{z}_i\)</span>是经Planar Normalizing Flow转换过的。</p><h2 id="online-detection">Online Detection</h2><p>在训练好模型之后，就可以进行异常检测了。在时间<span class="math inline">\(t\)</span>，我们通过根据长度为<span class="math inline">\(T+1\)</span>的序列<span class="math inline">\(\mathbf{x}_{t-T:t}\)</span>来重构<span class="math inline">\(\mathbf{x}_t\)</span>，并根据重构概率<span class="math inline">\(\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))\)</span>来判定异常。定义<span class="math inline">\(\mathbf{x}_t\)</span>对应的异常分数<span class="math inline">\(S_t=\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))\)</span>，高异常分数代表样本<span class="math inline">\(\mathbf{x}_t\)</span>能够以大概率重构（因为模型是用正常样本训练，可以认为模型建模的是正常样本的分布，重构概率高就代表符合正常分布）。给定阈值之后便可根据异常分数来进行异常的判定。</p><h2 id="automatic-threshold-selection">Automatic Threshold Selection</h2><p>在异常检测阶段，需要根据设定的阈值和每个样本的异常分数来判断该样本是否为异常，所以阈值的选择十分重要。文中用到了一种根据<strong>Extreme Value Theory</strong>自动选择阈值的算法。对于一个分布，其中的极端事件往往位于分布的末尾，而Extreme Value Theory第一定理给出不管原始分布如何，这些极端事件的分布服从一个带参的分布族。因此，可以在对数据分布未知的情况下估计极端事件的分布。</p><p>除了Extreme Value Theory第一定理之外，Extreme Value Theory第二定理给出随机变量大于特定阈值<span class="math inline">\(t\)</span>的分布可以用Generalized Pareto Distribution来描述。作者使用了基于Extreme Value Theory第二定理的Peaks-Over-Threshold算法来进行阈值的选择。因为Extreme Value Theory第二定理给出随机变量大于特定阈值<span class="math inline">\(t\)</span>的分布，而在本文的场景中我们需要刻画的异常点的分布应该是小于一个给定阈值的分布，所以需要修改一下公式。</p><p>对于给定的数据，模型会给出对应的异常分数序列<span class="math inline">\(\{S_1,S_2,\cdots,S_{N^\prime}\}\)</span>，给定预先设定的阈值<span class="math inline">\(th\)</span>，<span class="math inline">\(S_i\)</span>极端部分（即小于<span class="math inline">\(th\)</span>的部分）的分布符合Generalized Pareto Distribution，公式如下： <span class="math display">\[\bar{F}(s)=P(th-S&gt;s|S&lt;th)\sim(1+\frac{\gamma s}{\beta})^{-\frac{1}{\gamma}}\]</span></p><p>其中<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>为分布的形状参数，本文使用极大似然估计来对参数进行估计。设参数的估计值分别为<span class="math inline">\(\hat{\gamma}\)</span>和<span class="math inline">\(\hat{\beta}\)</span>，最终的阈值<span class="math inline">\(th_F\)</span>由拟合得到的分布的分位数确定：</p><p><span class="math display">\[th_F\simeq th-\frac{\hat{\beta}}{\hat{\gamma}}((\frac{qN^\prime}{N^\prime_{th}})^{-\hat{\gamma}}-1)\]</span></p><p>其中<span class="math inline">\(q\)</span>为期望<span class="math inline">\(S&lt;th\)</span>的概率，<span class="math inline">\(N^\prime\)</span>为观测值的数量，<span class="math inline">\(N^\prime_{th}\)</span>为<span class="math inline">\(S_i&lt;th\)</span>的个数。</p><h2 id="anomaly-interpretation">Anomaly Interpretation</h2><p><span class="math display">\[\log(p_\theta(\mathbf{x}_t|\mathbf{z}_{t-T:t}))=\sum_{i=1}^M\log(p_\theta(x_t^i|\mathbf{z}_{t-T:t}))\]</span></p><h1 id="experiments">Experiments</h1><h2 id="datasets-and-metrics">Datasets and Metrics</h2><h2 id="overall-performance">Overall Performance</h2><p><img src="http://qfxiao.me/img/1571411131954.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1571411148667.png" srcset="/img/loading.gif" /></p><h2 id="effects-of-major-techniques">Effects of Major Techniques</h2><p><img src="http://qfxiao.me/img/1571411161522.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1571411178434.png" srcset="/img/loading.gif" /></p><h2 id="visualization-on-z-space-representations">Visualization on Z-Space Representations</h2><p><img src="http://qfxiao.me/img/1571411190279.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1571411212926.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1571411223173.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VAE</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>RNN</tag>
      
      <tag>Flow-based Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAIN: Missing Data Imputation using Generative Adversarial Nets</title>
    <link href="/2019/10/16/GAIN-Missing-Data-Imputation-using-Generative-Adversarial-Nets/"/>
    <url>/2019/10/16/GAIN-Missing-Data-Imputation-using-Generative-Adversarial-Nets/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>本文基于GAN提出了一种时间序列缺失值填充（Time Series Imputation）的方法。其主要的思路为生成器<span class="math inline">\(G\)</span>从隐空间<span class="math inline">\(Z\)</span>生成完整的样本，而判别器<span class="math inline">\(D\)</span>则输出样本中不同部分为真实的概率。除此之外，作者提出了使用Hint Vector来揭示原始数据中缺失部分的信息，来优化训练过程。</p><p><a href="https://arxiv.org/abs/1806.02920" target="_blank" rel="noopener">原文</a></p><h1 id="methodology">Methodology</h1><h2 id="problem-formulation">Problem Formulation</h2><p>考虑一个<span class="math inline">\(d\)</span>维的空间<span class="math inline">\(\mathcal{X}=\mathcal{X}_1\times \cdots\times \mathcal{X}_d\)</span>，设<span class="math inline">\(\mathbf{X}=(X_1,\cdots,X_d)\)</span>维空间<span class="math inline">\(\mathcal{X}\)</span>上的随机向量（即理想的完整的时间序列），记其分布为<span class="math inline">\(P(\mathbf{X})\)</span>。设<span class="math inline">\(\mathbf{M}=(M_1,\cdots,M_d)\)</span>为Mask向量表示<span class="math inline">\(\mathbf{X}\)</span>中被观察到的部分。（即标识时间序列哪些部分有缺失），取值为<span class="math inline">\(\{0,1\}^d\)</span>。</p><p>对于每一个<span class="math inline">\(i\in\{1,\cdots,d\}\)</span>，我们定义一个新空间<span class="math inline">\(\tilde{\mathcal{X}}=\mathcal{X}\cup\{*\}\)</span>，其中<span class="math inline">\(*\)</span>表示不属于任意<span class="math inline">\(\mathcal{X}_i\)</span>的一个点。令<span class="math inline">\(\tilde{\mathcal{X}}=\tilde{\mathcal{X}_1}\times\cdots\times\tilde{\mathcal{X}_d}\)</span>，同时定义一个新的随机变量（即我们观测到的含有缺失值的时间序列）<span class="math inline">\(\tilde{\mathbf{X}}=(\tilde{X}_1,\cdots,\tilde{X}_d)\in \tilde{\mathcal{X}}\)</span>： <span class="math display">\[\tilde{X}_i=\begin{cases}X_i,&amp;\text{if } M_i=1\\*,&amp;\text{otherwise}\end{cases}\]</span> 假设数据集的形式为<span class="math inline">\(\mathcal{D}=\{(\tilde{x}^i,m^i)\}^n_{i=1}\)</span>，我们的任务是从<span class="math inline">\(P(\mathbf{X}|\tilde{\mathbf{X}}=\tilde{x}^i)\)</span>上采样来对缺失值进行填充。</p><h2 id="model-architecture">Model Architecture</h2><p>模型的架构如下图所示：</p><p><img src="http://qfxiao.me/img/1571228929894.png" srcset="/img/loading.gif" /></p><h3 id="generator">Generator</h3><p>生成器的输入有三项：<span class="math inline">\(\tilde{\mathbf{X}}\)</span>，<span class="math inline">\(\mathbf{M}\)</span>和随机噪声<span class="math inline">\(\mathbf{Z}\)</span>，输出设为<span class="math inline">\(\bar{\mathbf{X}}\)</span>。设生成器为映射<span class="math inline">\(G: \tilde{\mathcal{X}}\times\{0,1\}^d\times[0,1]^d\rightarrow \mathcal{X}\)</span>，而<span class="math inline">\(\mathbf{Z}\)</span>为<span class="math inline">\(d\)</span>维的高斯噪声。生成器的输出和填充后的时间序列定义为： <span class="math display">\[\begin{align}\bar{\mathbf{X}}&amp;=G(\tilde{\mathbf{X}},\mathbf{M},(1-\mathbf{M})\odot\mathbf{Z})\\\hat{\mathbf{X}}&amp;=\mathbf{M}\odot\tilde{\mathbf{X}}+(1-\mathbf{M})\odot\bar{\mathbf{X}}\end{align}\]</span> <span class="math inline">\(\bar{\mathbf{X}}\)</span>即为生成器的直接输出，因为其实有些部分没有缺失，生成器还是会为每个部分输出值。</p><p><span class="math inline">\(\hat{\mathbf{X}}\)</span>为填充后的时间序列，对于缺失的部分采用生成器的输出进行填充。</p><h3 id="discriminator">Discriminator</h3><p>和原始的GAN不同的是，我们不需要判断整个样本是真实的或者是生成的，而是需要判断样本的那些部分是真实的或者是生成的，所以判别器为映射<span class="math inline">\(D: \mathcal{X}\rightarrow[0,1]^d\)</span>。判别器的具体目标函数将在后面讨论。</p><h3 id="hint">Hint</h3><p>Hint是一种提示机值，是一个和<span class="math inline">\(\mathbf{X}\)</span>相同维度的随机变量<span class="math inline">\(\mathbf{H}\)</span>，其分布依赖于<span class="math inline">\(\mathbf{M}\)</span>。<span class="math inline">\(\mathbf{H}\)</span>是由用户自己定义的，相当于一种不完整的<span class="math inline">\(\mathbf{M}\)</span>，用来作为判别器的额外输入。</p><h3 id="objective">Objective</h3><p>我们训练判别器最大化正确预测<span class="math inline">\(\mathbf{M}\)</span>的概率，而生成器最小化判别器正确预测<span class="math inline">\(\mathbf{M}\)</span>的概率，目标函数如下： <span class="math display">\[\begin{align}V(D,G)=&amp;\mathbb{E}_{\hat{X},M,H}[\mathbf{M}^T\log D(\hat{\mathbf{X}},\mathbf{H})\\&amp;+(1-\mathbf{M})^T\log(1-D(\hat{\mathbf{X}},\mathbf{H}))]\end{align}\]</span> 按照标准的GAN可以将优化函数写成以下的形式： <span class="math display">\[\min_G\max_D V(D,G)\]</span> 在这里判别器的任务可以看作是一个二分类，而目标函数就是二值交叉熵的定义，因此可以写为： <span class="math display">\[\mathcal{L}(a,b)=\sum\limits_{i=1}^d[a_i\log(b_i)+(1-a_i)\log(1-b_i)]\]</span> <span class="math inline">\(\mathbf{M}\)</span>可以看作Ground Truth，记<span class="math inline">\(\hat{\mathbf{M}}=D(\hat{\mathbf{X},\mathbf{H}})\)</span>，即判别器输出的预测，因此优化函数可以简记为： <span class="math display">\[\min_G\max_D\mathbb{E}[\mathcal{L}(\mathbf{M},\hat{\mathbf{M}})]\]</span></p><h2 id="gain-algorithm">GAIN Algorithm</h2><p>下面讨论GAIN算法的训练流程。</p><p>本文通过理论讨论，给出了生成Hint Vector的一个方法，首先定义随机变量<span class="math inline">\(\mathbf{B}=(B_1,\cdots,B_d)\in\{0,1\}^d\)</span>，<span class="math inline">\(\mathbf{B}\)</span>通过从<span class="math inline">\(\{1,\cdots,d\}\)</span>随机均匀采样一个<span class="math inline">\(k\)</span>，然后由下列公式得到： <span class="math display">\[B_j=\begin{cases}1, &amp;\text{if }j\neq k\\0, &amp;\text{if }j=k\end{cases}\]</span> 定义空间<span class="math inline">\(\mathcal{H}=\{0,0.5,1\}^d\)</span>，Hint Vector为<span class="math inline">\(\mathbf{H}=\mathbf{B}\odot\mathbf{M}+0.5(1-\mathbf{B})\in\mathcal{H}\)</span>。</p><p>判别器的训练过程如下：固定生成器<span class="math inline">\(G\)</span>，对一个大小为<span class="math inline">\(k_D\)</span>的mini-batch，独立同分布采样<span class="math inline">\(k_D\)</span>个<span class="math inline">\(z\)</span>和<span class="math inline">\(b\)</span>，用来计算<span class="math inline">\(\mathbf{Z}\)</span>和<span class="math inline">\(\mathbf{B}\)</span>。判别器的损失函数定义如下： <span class="math display">\[\mathcal{L}_D(m,\hat{m},b)=\sum\limits_{i:b_i=0}[m_i\log(\hat{m}_i)+(1-m_i)\log(1-\hat{m}_i)]\]</span> 判别器的优化函数为： <span class="math display">\[\min_D-\sum\limits_{j=1}^{k_D}\mathcal{L}_D(m(j),\hat{m}(j),b(j))\]</span> 其中<span class="math inline">\(\hat{m}(j)=D(\hat{x}(j),m(j))\)</span>。</p><p>在优化了判别器之后，需要优化生成器，对一个大小为<span class="math inline">\(k_G\)</span>的mini-batch，生成器的损失函数包含两个部分，一个是在缺失部分的损失：</p><p><span class="math display">\[\mathcal{L}_G(m,\hat{m},b)=-\sum\limits_{i:b_i=0}(1-m_i)\log(\hat{m}_i)\]</span> 一个是未缺失部分的损失： <span class="math display">\[\mathcal{L}_M(x,x^\prime)=\sum\limits_{i=1}^d m_iL_M(x_i,x_i^\prime)\]</span> 其中： <span class="math display">\[L_M(x_i,x_i^\prime)=\begin{cases}(x_i^\prime-x_i)^2, &amp;\text{if }x_i\text{ is continuours},\\-x_i\log(x_i^\prime), &amp;\text{if }x_i\text{ is binary}.\end{cases}\]</span> 最终的优化函数为： <span class="math display">\[\min_G\sum\limits_{j=1}^{k_G}\mathcal{L}_G(m(j),\hat{m}(j),b(j))+\alpha\mathcal{L}_M(\tilde{x}(j),\hat{x}(j))\]</span> 算法流程如下：</p><p><img src="http://qfxiao.me/img/1571228999385.png" srcset="/img/loading.gif" /></p><h1 id="experiments">Experiments</h1><p>下表为在5个不同数据集上实验，与其他5种方法对比的结果：</p><p><img src="http://qfxiao.me/img/1571229028097.png" srcset="/img/loading.gif" /></p><p>上图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例、样本数量、特征维度下的对比曲线图。</p><p>下表为使用不同模型对时间序列进行填充之后，使用逻辑回归进行回归任务的性能：</p><p><img src="http://qfxiao.me/img/1571229051484.png" srcset="/img/loading.gif" /></p><p>下图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例下的AUROC曲线图：</p><p><img src="http://qfxiao.me/img/1571229066600.png" srcset="/img/loading.gif" /></p><p>下表展示的是作者对时间序列填充算法保持特征-标签关系的能力。作者分别用完整的数据和填充后的数据用逻辑回归模型进行训练，将两者的权重求绝对值和均方根的结果。</p><p><img src="http://qfxiao.me/img/1571229079632.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Time Series Imputation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series</title>
    <link href="/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/"/>
    <url>/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>这篇文章提出了一个基于GAN的时间序列异常检测模型。</p><p><a href="https://arxiv.org/abs/1809.04758" target="_blank" rel="noopener">原文</a></p><h2 id="contribution">Contribution</h2><ol type="1"><li>提出了基于GAN的时间序列无监督异常检测模型</li><li>我们使用基于LSTM的GAN来对多变量时间序列进行建模</li><li>结合使用了Residual Loss和Discrimination Loss来进行异常的判断</li></ol><h2 id="background">Background</h2><h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3><h4 id="gans-in-a-nutshell-an-extremely-simple-explanation">GANs In a Nutshell, an extremely simple explanation</h4><ul><li>我们想要从一个复杂的、高维的数据分布<span class="math inline">\(p_r(x)\)</span>上采样得到我们想要的数据点，然而<span class="math inline">\(p_r(x)\)</span>无法直接求得</li><li>代替方法：从一个简单的、已知的分布<span class="math inline">\(p_z(z)\)</span>上采样，然后学习一个Transformation <span class="math inline">\(G(z): z\rightarrow x\)</span>来将<span class="math inline">\(z\)</span>映射到<span class="math inline">\(x\)</span></li></ul><p><img src="http://qfxiao.me/img/1565836885697.png" srcset="/img/loading.gif" /></p><h4 id="training-two-player-game">Training: Two-player Game</h4><ul><li><strong>Generator Network: </strong> 从随机分布<span class="math inline">\(p_z(z)\)</span>采样<span class="math inline">\(z\)</span>，通过映射生成样本<span class="math inline">\(x\)</span>，这个生成的样本要尽量“真实”。怎么“真实”？优化生成器参数<span class="math inline">\(\theta_G\)</span>最大化判别器对生成样本的评分即可</li><li><strong>Discriminator Network: </strong>接受一个样本<span class="math inline">\(x\)</span>，判断其是生成的样本还是真实的样本。在训练阶段，我们是知道一个样本<span class="math inline">\(x\)</span>到底是生成的还是真实的，所以优化判别器参数<span class="math inline">\(\theta_D\)</span>最小化判别器对生成样本的评分，最大化对真实样本的评分（即最大化分辨真实样本的能力）</li></ul><p><img src="http://qfxiao.me/img/1565836982707.png" srcset="/img/loading.gif" /></p><p>形式化的来讲，优化函数如下：</p><p><span class="math display">\[\min\limits_{\theta_G}\max\limits_{\theta_D}V(G,D)=\mathbb{E}_{x\sim p_{data}(x)\log(\underbrace{D_{\theta{D}}(x)}_{判别器对真实样本的评分})}+\mathbb{E}_{z\sim p_z(z)}\log(1-\underbrace{D_{\theta_d}(G_{\theta_G}(z))}_{判别器对生成样本的评分})\]</span></p><p>训练过程如下：</p><p><img src="http://qfxiao.me/img/1565837745003.png" srcset="/img/loading.gif" /></p><h3 id="long-short-time-memory-networks">Long Short Time Memory Networks</h3><h4 id="vanilla-recurrent-neural-networks">Vanilla Recurrent Neural Networks</h4><p>普通的神经网络：</p><p><img src="http://images0.cnblogs.com/blog2015/680781/201508/021735264703915.png" srcset="/img/loading.gif" /></p><p>概括的来讲，可以涵盖为一个公式<span class="math inline">\(\hat{\mathbf{y}}=f(\mathbf{x})\)</span>。对于一个样本<span class="math inline">\(\mathbf{x}\)</span>，通过多层神经网络映射，输出<span class="math inline">\(\mathbf{y}\)</span>。</p><p>对于RNN，我们处理的是序列数据，也就是说所有样本之间并不是相互独立的。对于一个序列中的一个样本<span class="math inline">\(x_t\in\{x_1,x_2,\cdots,x_n\}\)</span>，将其输入到神经网络的时候，为了建模<span class="math inline">\(x_t\)</span>之前的子序列对<span class="math inline">\(x_t\)</span>的影响关系，需要将这个子序列的信息也输入到神经网络中，怎么做呢？为每一个样本点保存一个State。即定义<span class="math inline">\(h_t=g(\hat{y_t})=g(f(x_t))\)</span>，对于当前样本点，<span class="math inline">\(\hat{y_t}=f(x_t,h_{t-1})\)</span>。也就是说神经网络的输入不仅包含了当前样本点的特征，也包含了上一个样本点的“状态”(上一个样本点的“状态”又隐含了上上个样本点的“状态”...)，就像是为网络加上了短期记忆。</p><p><img src="https://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1565838639451.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1565838658532.png" srcset="/img/loading.gif" /></p><h4 id="gradient-flow-of-vanilla-rnn">Gradient Flow of Vanilla RNN</h4><p>下面来进行一些形式化的定义，假设在时刻<span class="math inline">\(t\)</span>网络输入特征为<span class="math inline">\(x_t\)</span>，输出隐含状态为<span class="math inline">\(h_{t}\)</span>，其不仅和当前输入<span class="math inline">\(x_t\)</span>有关，还和上一个隐含状态<span class="math inline">\(h_{t-1}\)</span>有关：</p><ul><li>当前时刻总的净输入<span class="math inline">\(z_t=Uh_{t-1}+Wx_t+b\)</span></li><li>当前时刻输出隐含状态<span class="math inline">\(h_t=f(z_t)\)</span></li><li>当前时刻输出<span class="math inline">\(\hat{y}_t=Vh_t\)</span></li></ul><p>RNN的梯度更新公式(推导过程比较复杂)：</p><p><span class="math display">\[\frac{\partial{\mathcal{L}}}{\partial U}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t \delta_{t,k}\mathbf{h}_{k-1}^T\]</span></p><p><span class="math display">\[\frac{\partial{\mathcal{L}}}{\partial{W}}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t \delta_{t,k}x_k^T\]</span></p><p><span class="math display">\[\frac{\partial\mathcal{L}}{\partial{b}}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t\delta_{t,k}\]</span></p><p>其中<span class="math inline">\(\delta_{t,k}=\frac{\partial{\mathcal{L}}}{\partial{z_k}}=\text{diag}(f^\prime(z_k))U^T\delta_{t,k+1}\)</span>定义为第<span class="math inline">\(t\)</span>时刻的损失对第<span class="math inline">\(k\)</span>时刻隐藏神经层的净输入<span class="math inline">\(z_k\)</span>的导数，且<span class="math inline">\(z_k=Uh_{k-1}+Wx_k+b,1\leq k&lt;t\)</span>。</p><p>RNN的梯度流向如下图红箭头所示：</p><p><img src="http://qfxiao.me/img/1565838776226.png" srcset="/img/loading.gif" /></p><p>RNN会遇到梯度消失和梯度爆炸的问题。根据前面的公式，<span class="math inline">\(\delta_{t,k}\)</span>实际上是递归定义的，展开得到：</p><p><span class="math display">\[\delta_{t,k}=\prod\limits_{\tau=k}^{t-1}(\text{diag}(f^\prime(z_\tau))U^T)\delta_{t,t}\]</span></p><p>如果定义<span class="math inline">\(\gamma\cong\parallel\text{diag}(f^\prime(z_\tau))U^T\parallel\)</span>，那么<span class="math inline">\(\delta_{t,k}\cong\gamma^{t-k}\delta_{t,t}\)</span>。在<span class="math inline">\(t-k\)</span>很大时，<span class="math inline">\(\gamma&lt;1\)</span>会导致梯度消失，<span class="math inline">\(\gamma&gt;1\)</span>时会导致梯度爆炸。</p><p><img src="http://qfxiao.me/img/1565839064829.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1565839083693.png" srcset="/img/loading.gif" /></p><h4 id="long-short-time-memory">Long Short Time Memory</h4><p>LSTM是一种解决RNN梯度消失问题的改进版本：</p><p><img src="http://qfxiao.me/img/1565839135232.png" srcset="/img/loading.gif" /></p><p>在LSTM中，维护了两个State，<span class="math inline">\(c_t\)</span>和<span class="math inline">\(h_t\)</span>。其中<span class="math inline">\(c_t\)</span>由遗忘门<span class="math inline">\(f\)</span>与上一个<span class="math inline">\(c_{t-1}\)</span>相乘(代表继承上一个Cell的信息并加以一定程度的遗忘)，加上输出门<span class="math inline">\(i\)</span>与Gate Gate <span class="math inline">\(g\)</span>相乘(Gate Gate代表当前的候选状态，输出门<span class="math inline">\(i\)</span>控制当前候选状态有多少信息需要保存)。最后，输出门<span class="math inline">\(o\)</span>控制当前时刻的Cell State <span class="math inline">\(c_t\)</span>有多少信息需要输出给外部状态<span class="math inline">\(h_t\)</span>。</p><p>三个门的计算方式为：</p><p><span class="math display">\[i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)\]</span></p><p><span class="math display">\[f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)\]</span></p><p><span class="math display">\[o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)\]</span></p><p><img src="http://qfxiao.me/img/1565839263008.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1565839279246.png" srcset="/img/loading.gif" /></p><h2 id="methodology">Methodology</h2><p>总体框架图如Fig 1所示：</p><p><img src="http://qfxiao.me/img/1565783729478.png" srcset="/img/loading.gif" /></p><h3 id="gan-with-lstm-rnn">GAN with LSTM-RNN</h3><p>网络结构上生成器和判别器都是LSTM，优化函数和普通GAN一样：</p><p><span class="math display">\[\min\limits_G\max\limits_D V(D,G)=\mathbb{E}_{x\sim p_{data}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)}[\log (1-D(G(z)))]\]</span></p><h3 id="gan-based-anomaly-score">GAN-based Anomaly Score</h3><p>在测试阶段，需要使用梯度优化寻找一个使得<span class="math inline">\(G_{rnn}(z)\)</span>最接近<span class="math inline">\(X^{test}\)</span>的<span class="math inline">\(z^k\)</span>：</p><p><span class="math display">\[\min\limits_{Z^k}Error(X^{test},G_{rnn}(Z^k))=1-Similarity(X^{test},G_{rnn}(Z^k))\]</span></p><p>本文定义了两种Anomaly Score，一种是Residual Loss：</p><p><span class="math display">\[Res(X^{test}_t)=\sum\limits_{i=1}^n|x^{test,i}_t-G_{rnn}(Z^{k,i}_t)|\]</span></p><p>一种是Discrimination Loss，即判别器的输出<span class="math inline">\(D_{rnn}(x_t^{test})\)</span>。</p><p>总的Anomaly Score：</p><p><span class="math display">\[S^{test}_t=\lambda Res(X^{test}_t)+(1-\lambda)D_{rnn}(x^{test}_t)\]</span></p><h3 id="anomaly-detection-framework">Anomaly Detection Framework</h3><p>模型的算法流程如下：</p><p><img src="http://qfxiao.me/img/1565784946781.png" srcset="/img/loading.gif" /></p><p>由于本文是多变量时间序列预测，而且时间序列的长度有可能比较长，作者使用了滑动窗口和PCA来进行预处理。</p><h2 id="experiments">Experiments</h2><p><img src="http://qfxiao.me/img/1565847175265.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ALSR: An Adaptive Label Screening and Relearning Approach for Interval-Oriented Anomaly Detection</title>
    <link href="/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/"/>
    <url>/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/</url>
    
    <content type="html"><![CDATA[<h1 id="introduction">Introduction</h1><p>本文针对面向区间的KPI异常检测提出了Label Screening方法和Relearning Algorithm.</p><p><a href="https://www.sciencedirect.com/science/article/pii/S0957417419304282" target="_blank" rel="noopener">原文</a></p><h2 id="contribution">Contribution</h2><ol type="1"><li>提出了一种Label Screening方法来对区间内不同重要性进行过滤</li><li>提出了一种Relearning Algorithm来对FP和TP进行Relearning，在不减少Recall的条件下增大Precision</li></ol><h2 id="methodology">Methodology</h2><h3 id="overall-structure">Overall Structure</h3><p>算法的整体框架如下：</p><p><img src="http://qfxiao.me/img/image-20191025152359346.png" srcset="/img/loading.gif" /></p><h3 id="label-screening-model">Label Screening Model</h3><p>预训练的结果被分为<span class="math inline">\(TP_{po},FP_{po},TN_{po},FN_{po}\)</span>四类，<span class="math inline">\(TP_{po}\)</span>和<span class="math inline">\(FN_{po}\)</span>可以被细分如下： <span class="math display">\[\begin{align}TP_{po}&amp;=TP_{po,withinT}+TP_{po,afterT}\\&amp;=TP_{po,withinT}+TP_{po,afterT,tpl}+TP_{po,after,fnl}\end{align}\]</span></p><p><span class="math display">\[\begin{align}FN_{po}&amp;=FN_{po,withinT}+FN_{po,afterT}\\&amp;=FN_{po,withinT,tpl}+FN_{po,,withinT,fnl}+FN_{po,afterT,tpl}+FN_{po,afterT,fnl}\end{align}\]</span></p><p>其中下标<span class="math inline">\({}_{withinT}\)</span>代表在异常片段第一个点<span class="math inline">\(T\)</span>距离内的所有点，下标<span class="math inline">\({}_{afterT}\)</span>代表<span class="math inline">\(T\)</span>距离之后。下标<span class="math inline">\({}_{tpl}\)</span>和<span class="math inline">\({}_{fnl}\)</span>分别代表在异常片段中，包含和不包含<span class="math inline">\(TP_{po,withinT}\)</span>的点。</p><p>以TP为例，Point-based的TP包含了在T范围之内的（即在Interval-based的标准中也会被认为是TP的点）和T范围之外的点（即在Interval-based的标准中不认为是TP的点）。而在T范围之外的点又可以细分为该异常片段是否包含<span class="math inline">\(TP_{po,withinT}\)</span>的点（即该点在Interval-based的标准中不会被判定为TP，但该异常片段有其点会被判定为TP）。</p><p>类似的，<span class="math inline">\(TP_{io}\)</span>和<span class="math inline">\(FN_{io}\)</span>可以被分解为： <span class="math display">\[\begin{align}TP_{io}&amp;=TP_{po,withinT}+TP_{po,afterT,tpl}+FN_{po,withinT,tpl}+FN_{po,afterT,tpl}\\&amp;=TP_{po}+FN_{po,withinT,tpl}+FN_{po,afterT,tpl}-TP_{po,afterT,fnl}\end{align}\]</span></p><p><span class="math display">\[\begin{align}FN_{io}&amp;=FN_{po,withinT,fnl}+FN_{po,afterT,fnl}+TP_{po,afterT,fnl}\\&amp;=FN_{po}+TP_{po,afterT,fnl}-FN_{po,withinT,tpl}-FN_{po,afterT,tpl}\end{align}\]</span></p><p>文中对该部分的分析可以分为以下几点：</p><ol type="1"><li>在Interval-oriented的标准中，<span class="math inline">\(FN_{po,tpl}\)</span>的点仍会被认为是<span class="math inline">\(TP_{io}\)</span>，而<span class="math inline">\(TP_{po,afterT}\)</span>（不带<span class="math inline">\({}_{tpl}\)</span>）不会被认为是<span class="math inline">\(TP_{io}\)</span>，所以最终<span class="math inline">\(TP_{io}\)</span>由所有<span class="math inline">\(TP_{po}\)</span>加上那些会被认为是<span class="math inline">\(TP_{io}\)</span>的<span class="math inline">\(FN_{po,tpl}\)</span>再去掉不带<span class="math inline">\({}_{tpl}\)</span>的<span class="math inline">\(TP_{po,afterT}\)</span>组成，即公式(6)</li><li>同时，根据公式(6)，如果<span class="math inline">\(TP_{po}\)</span>变为<span class="math inline">\(FN_{po,tpl}\)</span>，也不会对最终结果造成影响。但是根据公式(5)和公式(7)，<span class="math inline">\(TP_{po,withinT}\)</span>变成<span class="math inline">\(FN_{po,withinT,fnl}\)</span>会减小<span class="math inline">\(TP_{io}\)</span>同时增大<span class="math inline">\(FN_{io}\)</span></li><li>文章指出，虽然<span class="math inline">\(FN_{po,withinT,tpl}\)</span>和<span class="math inline">\(FN_{po,afterT,tpl}\)</span>最后都会被认为是<span class="math inline">\(TP_{io}\)</span>，但作者假设<span class="math inline">\(FN_{po,withinT,tpl}\)</span>更难检测，所以应该保留，而<span class="math inline">\(FN_{po,afterT,tpl}\)</span>应该削减</li><li>Label Screening方法去除了<span class="math inline">\(FN_{po,afterT}\)</span>的点</li><li>Screened之后的训练集被用来训练DNN主模型，但Label Screening的预测结果也会被保留，和DNN主模型的结果进行组合</li></ol><p>算法流程如下：</p><p><img src="http://qfxiao.me/img/1567780638004.png" srcset="/img/loading.gif" /></p><h3 id="relearning-algorithm">Relearning Algorithm</h3><p>Relearning Model的输入是DNN主模型预测出来的异常，其中包括TP和FP。Relearning Model采用的是随机森林，其输入的样本通过采样得到： <span class="math display">\[\begin{align}\text{relearning}\space&amp;\text{training set}=\\&amp; shuffle\{4C\ast\text{randomof}(TP_{po})\\&amp;+C\cdot\text{randomof}(FP_{po})+C\cdot\text{randomof}(TN_{po})\}\end{align}\]</span> 其中<span class="math inline">\(C\)</span>为常数。TN和FP都看作是负例(正常样本)，TP看作是正例。</p><p><img src="http://qfxiao.me/img/1567868588559.png" srcset="/img/loading.gif" /></p><h3 id="detection">Detection</h3><p>对于一个滑动窗口<span class="math inline">\(x_t=\{x_{t-w+1},\cdots,x_t\}\)</span>，异常检测算法的目标是输出检测结果<span class="math inline">\(y_t\in\{0,1\}\)</span>来表示时间<span class="math inline">\(t\)</span>是否发生异常。实际上算法输出的是<span class="math inline">\(p_{y_t}\in[0,1]\)</span>概率值来表示在时间<span class="math inline">\(t\)</span>发生异常的概率。文中三个模型会得到三个输出：<span class="math inline">\(y_{t,ls},y_{t,main},y_{t,re}\)</span>。最终结果为： <span class="math display">\[y_t=y_{t,ls}\space\&amp;\space y_{t,main}\space\&amp; \space y_{t,re}\]</span> 在绘制PR曲线时，采用的公式为： <span class="math display">\[\begin{align}p_{y_t}(th)=&amp;(1-sig(p_{y_t,ls},th))\cdot(p_{y_t,ls})\\&amp;+sig(p_{y_t,ls},th)\cdot(1-sig(p_{y_t,main},th))\cdot p_{y_t,main}\\&amp;+sig(p_{y_t,ls},th)\cdot sig(p_{y_t,main},th)\cdot p_{y_t,re}\\\end{align}\]</span></p><p><span class="math display">\[y_t(th)=sig(p_{y_t}(th),th)\]</span></p><p>算法流程如下：</p><p><img src="http://qfxiao.me/img/1567933937509.png" srcset="/img/loading.gif" /></p><h2 id="experiments">Experiments</h2><h3 id="datasets">Datasets</h3><p>清华AIOps数据集，选取了25条KPI。</p><h3 id="preprocessing">Preprocessing</h3><ol type="1"><li><strong>Missing Data.</strong> 去除。</li><li><strong>Standardization.</strong> Minmax Standardization，Feature Extraction使用的是Standardization后的数据。</li><li><strong>Feature Extraction.</strong> 使用了12种特征。</li></ol><table><thead><tr class="header"><th>Group</th><th>Feature Name</th></tr></thead><tbody><tr class="odd"><td>Values</td><td>The original values standardized</td></tr><tr class="even"><td>Statistical Features</td><td>Mean, Standard Deviation, Range, Difference...</td></tr><tr class="odd"><td>Fitting Features</td><td>EWMA, AR</td></tr><tr class="even"><td>Wavelet Features</td><td>Db2 wavelet decomposition</td></tr></tbody></table><h2 id="results">Results</h2><h3 id="aucpr">AUCPR</h3><p><img src="http://qfxiao.me/img/1567780198147.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1567780227803.png" srcset="/img/loading.gif" /></p><h3 id="f1">F1</h3><p><img src="http://qfxiao.me/img/1567780241121.png" srcset="/img/loading.gif" /></p><h2 id="remark">Remark</h2><ol type="1"><li>这篇文章的Label Screening方法实际上是在处理样本分类难易度的问题，将异常区间内容易的样本去除了</li><li>对于时间序列的异常检测问题，我们的目标一般是Point-based的异常标签，一个时间点的特征是有限的。如果用窗口的方式，以<span class="math inline">\(\{x_{t-w+1},\cdots,x_t\}\)</span>作为时间<span class="math inline">\(t\)</span>的输入（当然每个<span class="math inline">\(x_t\)</span>可以有多个Channel），然后把预测结果作为时间<span class="math inline">\(t\)</span>的输出</li></ol>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</title>
    <link href="/2019/09/22/Unsupervised-Anomaly-Detection-via-Variational-Auto-Encoder-for-Seasonal-KPIs-in-Web-Applications/"/>
    <url>/2019/09/22/Unsupervised-Anomaly-Detection-via-Variational-Auto-Encoder-for-Seasonal-KPIs-in-Web-Applications/</url>
    
    <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>本文提出了Donut，一个基于VAE的无监督时间序列异常检测系统。</p><p><a href="https://dl.acm.org/citation.cfm?id=3185996" target="_blank" rel="noopener">原文</a></p><p><img src="http://qfxiao.me/img/1571719484833.png" srcset="/img/loading.gif" /></p><h2 id="contribution">Contribution</h2><ol type="1"><li>Donut中使用到了三个技巧，包括改进后的ELBO、缺失数据注入和MCMC插值；</li><li>提出基于VAE的异常检测训练既需要正常样本也需要异常样本；</li><li>对Donut提出了在z-空间中基于KDE的理论解释。</li></ol><h2 id="background">Background</h2><h3 id="anomaly-detection">Anomaly Detection</h3><p>对于任意时间<span class="math inline">\(t\)</span>，给定历史观察值<span class="math inline">\(x_{t-T+1},\cdots,x_t\)</span>，确定异常是否发生(记为<span class="math inline">\(y_t=1\)</span>)。通常来收异常检测算法给出的是发生异常的可能性，如<span class="math inline">\(p(y_t=1|x_{t-T+1},\cdots,x_t)\)</span>。</p><h2 id="methodology">Methodology</h2><h3 id="problem-statement">Problem Statement</h3><p>本文的目的是<strong>基于深度生成网络开发具有理论解释性的无监督异常检测算法，并且在有标签的情况下能利用标签信息提升性能</strong>。本文基于VAE来构建模型。</p><p><img src="http://qfxiao.me/img/1565532060281.png" srcset="/img/loading.gif" /></p><h3 id="network-structure">Network Structure</h3><p>算法的总体框架如下图所示：</p><p><img src="http://qfxiao.me/img/1565532453958.png" srcset="/img/loading.gif" /></p><p>一共包含了预处理、训练和检测三个部分。</p><p>下图为模型的概率图模型：</p><p><img src="http://qfxiao.me/img/1565532474201.png" srcset="/img/loading.gif" /></p><p>图中双实线的框为本文模型有别于传统VAE的地方，其余地方和VAE一样。先验概率<span class="math inline">\(p_\theta(z)\)</span>选为标准正态分布<span class="math inline">\(\mathcal{N}(0,I)\)</span>，后验概率<span class="math inline">\(x\)</span>和<span class="math inline">\(z\)</span>都是对角化高斯分布，即<span class="math inline">\(p_\theta(x|z)=\mathcal{N}(\mu_x,\sigma_x^2 I),q_\phi(z|x)=\mathcal{N}(\mu_z,\sigma_z^2 I)\)</span>。如Figure 4所示，推断网络和生成网络中分别都有隐含层<span class="math inline">\(f_\phi(x)\)</span>和<span class="math inline">\(f_\theta(z)\)</span>对网络的输入进行特征抽取。高斯分布的参数即从这些抽取出来的特征上得到。均值通过线性层得到：<span class="math inline">\(\mu_x=W^T_{\mu_x}f_\theta(z)+b_{\mu_x}, \mu_z=W^T_{\mu_z}f_\theta(x)+b_{\mu_z}\)</span>。标准差通过Soft Plus层加一个高斯噪声得到：<span class="math inline">\(\sigma_x=\text{SoftPlus}[W^T_{\sigma_x}f_\theta(z)+b_{\sigma_x}]+\varepsilon，\sigma_x=\text{SoftPlus}[W^T_{\sigma_z}f_\theta(x)+b_{\sigma_z}]+\varepsilon\)</span>。</p><p>文中提到因为KPI的局部方差非常小，所以采用直接建模<span class="math inline">\(\sigma_x,\sigma_z\)</span>的方式而不是采用对数。除此之外，为了理论上的解释性，文中的神经网络只使用了全连接层。</p><h3 id="training">Training</h3><p>训练可以直接采用经典的SGVB来优化ELBO： <span class="math display">\[\begin{align}\log p_\theta(x)&amp;\geq\log p_\theta(x)-\text{KL}[q_\phi(z|x)\parallel p_\theta(z|x)]\\&amp;=\mathcal{L}\\&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x)+\log p_\theta(z|x)-\log q_\phi(z|x)]\\&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x,z)-\log q_\phi(z|x)]\\&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)+\log p_\theta(z)-\log q_\phi(z|x)]\end{align}\]</span> 但是在实际的训练过程中，训练数据需要保证都是正常样本，但实际上训练样本有可能会包含异常或者是缺失值。一种做法是用缺失值填充的算法来填充这些异常值和缺失值，但作者认为使用缺失值填充算法并不能很好的还原数据的正常模式，从而保证算法的有效性。在文中作者采用了修改ELBO的方法，并将其称之为<strong>Modified ELBO (M-ELBO)</strong>，公式如下： <span class="math display">\[\tilde{\mathcal{L}}=\mathbb{E}_{q_\phi(z|x)}[\sum\limits_{w=1}^W{\alpha_w\log p_\theta(x_w|z)+\beta\log p_\theta(z)-\log q_\phi(z|x)}]\]</span> 其中<span class="math inline">\(\alpha_w\)</span>为指示标记，<span class="math inline">\(\alpha_w=1\)</span>代表不是异常也不是缺失。<span class="math inline">\(\beta\)</span>定义为<span class="math inline">\(\beta=\frac{\sum_{w=1}^W\alpha_w}{W}\)</span>。</p><p>在<strong>M-ELBO</strong>中，异常或缺失值对应的<span class="math inline">\(\log p_\theta(x_w|z)\)</span>的贡献会被排除，同时<span class="math inline">\(\log p_\theta(z)\)</span>在乘以<span class="math inline">\(\beta\)</span>后会相应缩小。作者没有修改<span class="math inline">\(\log q_\phi(z|x)\)</span>这一项的原因有二：一是<span class="math inline">\(q_\phi(z|x)\)</span>仅仅是从<span class="math inline">\(x\)</span>到<span class="math inline">\(z\)</span>的映射，并不需要考虑“正常模式”；二是<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[-\log q_\phi(z|x)]\)</span>就是<span class="math inline">\(q_\phi(z|x)\)</span>的熵，而这个在后面的理论分析中有特别的含义。</p><p>除此之外还有一种解决方法就是把所有包含异常值和缺失值的窗口去除，这种方法的性能在实验中会进行讨论。</p><p>在文中作者还使用了一种<strong>Missing Data Injection</strong>技术，即在每个Epoch随机的按照一个预设比例<span class="math inline">\(\lambda\)</span>将正常的数据设为缺失。作者认为这样有助于性能的提升。</p><h3 id="detection">Detection</h3><p>在检测阶段，对于一个输入样本，我们需要模型输出其异常的概率。因为我们建模了<span class="math inline">\(p_\theta(x|z)\)</span>，一种方法是采样计算<span class="math inline">\(p_\theta(x)=\mathbb{E}_{p_\theta(z)}[p_\theta(x|z)]\)</span>，但这种方法计算代价十分昂贵。其他的一些方案有计算<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[p_\theta(x|z)]\)</span>或<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>，其中后者被称为&quot;<strong>Reconstruction Probability</strong>&quot;，作者便采用了这种方案。</p><p>同时，作者认为输入的检测样本的缺失值会对结果造成较大偏差，于是使用了一种<strong>MCMC-based Missing Data Imputation</strong>的方法来对检测样本的缺失值进行填充。具体做法是将测试样本分为已观测和缺失两部分<span class="math inline">\(x=(x_o,x_m)\)</span>，然后使用训练好的VAE进行重构得到<span class="math inline">\((x^\prime_o,x^\prime_m)\)</span>，然后用<span class="math inline">\(x^\prime_m\)</span>替换<span class="math inline">\(x_m\)</span>，这样不断循环如下图所示：</p><p><img src="http://qfxiao.me/img/1571719553095.png" srcset="/img/loading.gif" /></p><p>作者使用了<span class="math inline">\(L\)</span>个样本来计算<strong>Reconstruction Probability</strong>，虽然得到的输出是针对整个窗口每个点的，但作者只使用最后一个点。</p><h2 id="experiments">Experiments</h2><h3 id="datasets">Datasets</h3><p>作者选择了三条KPI作为测试数据，分别记为<span class="math inline">\(\mathcal{A}\)</span>，<span class="math inline">\(\mathcal{B}\)</span>，<span class="math inline">\(\mathcal{C}\)</span>，其基本数据如下表所示：</p><p><img src="http://qfxiao.me/img/1571719583781.png" srcset="/img/loading.gif" /></p><h3 id="metrics">Metrics</h3><p>因为异常检测类别的极不均衡性，传统的性能指标并不太合适（异常样本极少，且异常一般呈连续的片段）。作者认为在实际应用场景中运维人员需要尽量早的获知异常的发生，于是提出了新的评测机制。</p><p><img src="http://qfxiao.me/img/1571719605541.png" srcset="/img/loading.gif" /></p><p>如上图所示，第一行为真实的标签，第二行为预测的异常概率，第三行为预测的标签。第一行中异常片段被加粗表示，对于每一个异常片段的第一个位置<span class="math inline">\({y}_{t^\prime}\)</span>，如果预测的标签中存在<span class="math inline">\(\hat{y}_{t}\)</span>满足<span class="math inline">\(t^\prime&lt;t\)</span>且<span class="math inline">\(|t-t^\prime|\)</span>小于等于预设的阈值<span class="math inline">\(T\)</span>，那么<span class="math inline">\(y_{t^\prime}\)</span>对应的整段异常都被认为正确检测，否则整段异常都认为没有被正确检测。然后在此基础上计算F1-score，AUC等指标作为评测手段。</p><h2 id="results">Results</h2><h3 id="overall-performance">Overall Performance</h3><p>下图展示了不同方法在不同数据集上的表现：</p><p><img src="http://qfxiao.me/img/1571719653801.png" srcset="/img/loading.gif" /></p><h3 id="effects-of-donut-techniques">Effects of Donut Techniques</h3><p>为了探究Donut中所做的各种改进的实际作用，作者做了大量对比实验，结果如下图所示：</p><p><img src="http://qfxiao.me/img/1571719667067.png" srcset="/img/loading.gif" /></p><ul><li><strong>M-ELBO</strong> 从图中可以看出<strong>M-ELBO</strong>对性能提升最大。作者在文中提到一开始并没期望<strong>M-ELBO</strong>能带来性能的提升，只是希望它能够Work。这表明在VAE的训练中，只使用正常样本是不够的，也需要加入非正常的信息；</li><li><strong>Missing Data Injection</strong> 该技巧的主要作用是增强<strong>M-ELBO</strong>的效果。从结果上来看作用并不是十分的显著，只是在一些情况下获得了少量的提升；</li><li><strong>MCMC Imputation</strong> 作者认为虽然该技巧只在一部分情况下显著提升了性能，但总体来说值得使用。</li></ul><h3 id="impact-of-k">Impact of K</h3><p>该部分作者探究了隐变量<span class="math inline">\(z\)</span>的维度<span class="math inline">\(K\)</span>对性能的影响，结果如下图：</p><p><img src="http://qfxiao.me/img/1571719682591.png" srcset="/img/loading.gif" /></p><p>从图上来看，对数据集<span class="math inline">\(\mathcal{A}\)</span>，<span class="math inline">\(\mathcal{B}\)</span>，<span class="math inline">\(\mathcal{C}\)</span>最佳的<span class="math inline">\(K\)</span>分别是<span class="math inline">\(5\)</span>，<span class="math inline">\(4\)</span>和<span class="math inline">\(3\)</span>，但是设定较大的<span class="math inline">\(K\)</span>并不会对性能有严重的损害。作者还发现对于较为平滑的KPI需要较大的<span class="math inline">\(K\)</span>。</p><h2 id="analysis">Analysis</h2><h3 id="kde-interpretation">KDE Interpretation</h3><p>在这一节作者对<strong>Reconstruction Probability</strong>的意义进行了深入的探讨。首先作者对<span class="math inline">\(q_\phi(z|x)\)</span>进行了可视化，在图中作者将时间维度用颜色来表示。如Figure 11(a) 所示，<span class="math inline">\(z\)</span>几乎是按照<span class="math inline">\(x\)</span>对应的时间呈一个连续的流形分布，作者将这种现象称为<strong>Time Gradient</strong>。即使Donut没有显式的用到时间信息，不过因为实验用到的数据基本是平滑的，所以说相邻的<span class="math inline">\(x\)</span>会比较相似，因此经过映射后的<span class="math inline">\(z\)</span>也会比较相似。作者据此提出Donut的一个优势便是对于没有见过的后验分布<span class="math inline">\(q_\phi(z|x)\)</span>，只要其位于训练过的两个后验之间，也会产生合理的分布。</p><p><img src="http://qfxiao.me/img/1571719693393.png" srcset="/img/loading.gif" /></p><p>对于异常的样本<span class="math inline">\(x\)</span>，假设其对应的正常模式为<span class="math inline">\(\tilde{x}\)</span>，作者认为<span class="math inline">\(q_\phi(z|x)\)</span>会在某种程度上对正常的<span class="math inline">\(q_\phi(z|\tilde{x})\)</span>进行近似。因为模型是用正常样本进行训练的，隐变量<span class="math inline">\(z\)</span>的维度通常来说小于样本<span class="math inline">\(x\)</span>，这就导致<span class="math inline">\(z\)</span>只会保留一部分主要的信息。对于异常样本，其异常模式在编码时就被丢掉了。作者还指出如果<span class="math inline">\(x\)</span>包含的异常太多，那么模型将难以对<span class="math inline">\(x\)</span>进行还原。</p><p><img src="http://qfxiao.me/img/1571719738542.png" srcset="/img/loading.gif" /></p><p>基于上述讨论，作者对使用<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>作为<strong>Reconstruction Probability</strong>的意义进行了阐释。设输入样本为<span class="math inline">\(x\)</span>，如果其包含异常，假设其对应的正常样本为<span class="math inline">\(\tilde{x}\)</span>，那么<span class="math inline">\(q_\phi(z|x)\)</span>部分地和<span class="math inline">\(q_\phi(z|\tilde{x})\)</span>相似。如果<span class="math inline">\(x\)</span>和<span class="math inline">\(\tilde{x}\)</span>相似程度高，那么<span class="math inline">\(\log p_\theta(x|z)\)</span>就会很大（其中<span class="math inline">\(z\sim q_\phi(z|\tilde{x})\)</span>）。<span class="math inline">\(\log p_\theta(x|z)\)</span>类似于一个密度估计器，代表<span class="math inline">\(x\)</span>在多大程度上与<span class="math inline">\(\tilde{x}\)</span>接近，<span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>相当于对每一个<span class="math inline">\(z\)</span>对应的<span class="math inline">\(\log p_\theta(x|z)\)</span>乘以一个权重<span class="math inline">\(q_\phi(z|x)\)</span>然后相加。于是作者提出了<strong>Reconstruction Probability</strong>的<strong>KDE Interpretation</strong>:在Donut模型中，<strong>Reconstruction Probability</strong> <span class="math inline">\(\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)]\)</span>可以看作是以<span class="math inline">\(q_\phi(z|x)\)</span>为权重，<span class="math inline">\(\log p_\theta(x|z)\)</span>为核的核密度估计 (Kernel Density Estimation)。</p><p>三维可视化如下图所示：</p><p><img src="http://qfxiao.me/img/1571736459981.png" srcset="/img/loading.gif" /></p><p>作者还对直接计算<span class="math inline">\(p_\theta(x)=\mathbb{E}_{p_\theta(z)}[p_\theta(x|z)]\)</span>进行了质疑，因为这种方法直接求<span class="math inline">\(x\)</span>的先验，仅仅考虑了<span class="math inline">\(x\)</span>的总体模式，而忽略了<span class="math inline">\(x\)</span>的个体模式。</p><h3 id="find-good-posteriors-for-abnormal-x">Find Good Posteriors for Abnormal <span class="math inline">\(x\)</span></h3><p>通过上面的讨论我们知道了Donut通过找到<span class="math inline">\(x\)</span>的正常后验来估计<span class="math inline">\(x\)</span>在多大程度上与<span class="math inline">\(\tilde{x}\)</span>相似，在这一节作者讨论了文中使用的不同技巧对找到<span class="math inline">\(x\)</span>的后验的作用。对于<strong>Missing Data Injection</strong>作者认为该技巧增强了<strong>M-ELBO</strong>的效果。对于<strong>MCMC Imputation</strong>，作者认为该技巧主要是在检测阶段通过不断迭代提供了更好的后验，如下图所示：</p><p><img src="http://qfxiao.me/img/1571736476085.png" srcset="/img/loading.gif" /></p><p>作者认为，虽然对于包含大量异常的样本，Donut不能很好的还原，但在运维场景中，只要对大段异常的开始阶段进行准确预测即可。</p><h3 id="causes-of-time-gradient">Causes of Time Gradient</h3><p>在这一节作者讨论了<strong>Time Gradient</strong>出现的原因。首先假设<span class="math inline">\(x\)</span>都是正常点，这时<span class="math inline">\(x\)</span>的ELBO为： <span class="math display">\[\begin{align}\mathcal{L}(x)&amp;=\mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)+\log p_\theta(z)-\log q_\phi(z|x)]\\&amp;=\mathbb{E}[\log p_\theta(x|z)]+\mathbb{E}[\log p_\theta(z)]+\text{H}[z|x]\end{align}\]</span> 第一项表明在<span class="math inline">\(z\sim q_\phi(z|x)\)</span>下尽可能重构<span class="math inline">\(x\)</span>。第二项表明<span class="math inline">\(q_\phi(z|x)\)</span>尽量与<span class="math inline">\(z\)</span>的先验<span class="math inline">\(\mathcal{N}(0,I)\)</span>接近。第三项为<span class="math inline">\(q_\phi(z|x)\)</span>的熵，表明<span class="math inline">\(q_\phi(z|x)\)</span>应尽量分散。然而第二项又限制了这种分散的区域，如 Figure 11(c) 所示。同时考虑这三项的话，第一项使得<span class="math inline">\(z\)</span>不能自由地分散，对于不相似的<span class="math inline">\(x\)</span>其对应的<span class="math inline">\(z\)</span>也是不相似的，因为要最大化<span class="math inline">\(x\)</span>的重构概率。然而对于相似的<span class="math inline">\(x\)</span>来说，其对应的<span class="math inline">\(q_\phi(z|x)\)</span>会出现很多重复的部分。当达到平衡时，<strong>Time Gradient</strong>就出现了。</p><p><img src="http://qfxiao.me/img/1571719784233.png" srcset="/img/loading.gif" /></p><p>在训练过程中，当<span class="math inline">\(x\)</span>越不相似，<span class="math inline">\(q_\phi(z|x)\)</span>就会相距越远，如上图所示。然而在一开始，参数经过随机初始化，<span class="math inline">\(q_\phi(z|x)\)</span>都是随机散乱的，如 Figure 11(b) 所示。随着训练的进行，<span class="math inline">\(q_\phi(z|x)\)</span>将会不断优化。由于KPI数据往往是光滑的，那么在时间上相距越远的样本就会越不相似，对应的<span class="math inline">\(q_\phi(z|x)\)</span>也会相距更远。这也说明了，训练结束后，时间上相距越远的，<span class="math inline">\(q_\phi(z|x)\)</span>也会相距越远，反之亦然。同时这也表明学习率的设置对本模型的稳定性有至关重要的作用。</p><h3 id="sub-optimal-equilibrium">Sub-Optimal Equilibrium</h3><p>上面我们讨论了随着训练进行<span class="math inline">\(q_\phi(z|x)\)</span>的演变，作者提出在训练过程中可能会遇到模型收敛到次优的情况，如下图所示：</p><p><img src="http://qfxiao.me/img/1571719796266.png" srcset="/img/loading.gif" /></p><p>第一行展示的是收敛到最优的情况，第二行展示的是收敛到次优的情况。从第二行的第一个图（Step 100）来看，紫色的点开始穿过绿色的点，随着训练的进行，紫色的点开始将绿色的点推开。到Step 5000的时候，绿色的点已经被分成了两半。下图展示了对应的训练误差和验证误差：</p><p><img src="http://qfxiao.me/img/1571719807778.png" srcset="/img/loading.gif" /></p><p>这样的现象会导致在两半绿色区域之间的测试样本会被识别为紫色，从而降低性能。作者提出在<span class="math inline">\(K\)</span>较大的时候这种现象不容易发生，但这时训练的收敛又会成为一个问题。</p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VAE</tag>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Time-Series Anomaly Detection Service at Microsoft</title>
    <link href="/2019/09/22/Time-Series-Anomaly-Detection-Service-at-Microsoft/"/>
    <url>/2019/09/22/Time-Series-Anomaly-Detection-Service-at-Microsoft/</url>
    
    <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>本文借鉴计算机视觉中的显著性检测，提出了一种基于Spectral Residual的时间序列异常检测算法。</p><p><a href="https://www.kdd.org/kdd2019/accepted-papers/view/time-series-anomaly-detection-service-at-microsoft" target="_blank" rel="noopener">原文</a></p><p>这篇文章还提出了几个时间序列异常检测落地的难点：</p><ol type="1"><li><strong>Lack of Labels.</strong> 在实际生产环境中会产生大量的KPI，而很难对每个KPI进行人工标注。</li><li><strong>Generalization.</strong> 不同KPI所表现出来的模式也不尽相同，如Figure 1所示。现有方法很难在所有模式的KPI上都表现良好。</li><li><strong>Efficiency.</strong> 在实际场景中，会产生大量的时间序列数据，同时对异常检测算法的时间效率有要求。</li></ol><p><img src="http://qfxiao.me/img/1565414991291.png" srcset="/img/loading.gif" /></p><h2 id="contribution">Contribution</h2><ul><li>将Visual Saliency Detection的方法引入了时间序列异常检测。</li><li>结合Spectral Residual和CNN提高了异常检测的效果。</li><li>算法具有良好的时间效率和通用性。</li></ul><h2 id="background">Background</h2><h3 id="spectral-residual">Spectral Residual</h3><p>SR(Spectral Residual)算法主要包含三个步骤：</p><ol type="1"><li>通过傅里叶变换得到log amplitude spectrum；</li><li>计算spectral residual；</li><li>通过傅里叶逆变换回到时间域。</li></ol><p>更形式化的表述为如下：</p><p>给定一个序列<span class="math inline">\(\mathbb{x}\)</span>，则有：</p><p><span class="math display">\[A(f)=Amplitude(\mathscr{F}(\mathbb{x}))\]</span></p><p><span class="math display">\[P(f)=Phrase(\mathscr{F}(\mathbb{x}))\]</span></p><p><span class="math display">\[L(f)=\log(A(f))\]</span></p><p><span class="math display">\[AL(F)=h_1(f)\cdot L(f)\]</span></p><p><span class="math display">\[R(f)=L(f)-AL(f)\]</span></p><p><span class="math display">\[S(\mathbb{x})=\parallel\mathscr{F}^{-1}(\exp(R(f)+iP(f)))\parallel\]</span></p><p>其中<span class="math inline">\(\mathscr{F}\)</span>和<span class="math inline">\(\mathscr{F}^{-1}\)</span>分别表示傅里叶变换和傅里叶逆变换；<span class="math inline">\(\mathbb{x}\in \mathbb{R}^{n\times 1}\)</span>表示输入序列；<span class="math inline">\(A(f)\)</span>为幅度谱，<span class="math inline">\(P(f)\)</span>为相位谱，<span class="math inline">\(L(f)\)</span>为对数幅度谱，<span class="math inline">\(AL(F)\)</span>为均值滤波后的对数幅度谱；<span class="math inline">\(R(f)\)</span>为spectral residual；<span class="math inline">\(S(\mathbb{x})\)</span>称为saliency map。Figure 4为文中给出的Saliency Map示意图。</p><p><img src="http://qfxiao.me/img/1565425536516.png" srcset="/img/loading.gif" /></p><h2 id="methodology">Methodology</h2><h3 id="problem-definition">Problem Definition</h3><blockquote><p>给定一系列实数值<span class="math inline">\(\mathbb{x}=x_1,x_2,\cdots,x_n\)</span>，时间序列异常检测的任务是产生一个输出序列<span class="math inline">\(\mathbb{y}=y_1,y_2,\cdots,y_n\)</span>其中<span class="math inline">\(y_i\in\{0,1\}\)</span>表示<span class="math inline">\(x_i\)</span>是否为异常点。</p></blockquote><h3 id="sr">SR</h3><p>对于给定序列<span class="math inline">\(\mathbb{x}\)</span>，计算Saliency Map <span class="math inline">\(S(\mathbb{x})\)</span>，输出序列<span class="math inline">\(O(\mathbb{x})\)</span>定义为：</p><p><span class="math display">\[O(x_i)=\begin{cases}1,\quad \text{if}\frac{S(x_i)-\overline{S(x_i)}}{\overline{S(x_i)}}&gt;\tau\\\\0,\quad \text{otherwise}\end{cases}\]</span></p><p>其中<span class="math inline">\(S(x_i)\)</span>为<span class="math inline">\(x_i\)</span>对应的Saliency Map的值，<span class="math inline">\(\overline{S(x_i)}\)</span>为<span class="math inline">\(x_i\)</span>附近Saliency Map局部均值。</p><hr /><p>在实际操作中，FFT是在一个滑动窗口中进行的，文中提到SR方法在点位于窗口中央时效果更好，所以在进行测试的时候，按照如下方法对当前点<span class="math inline">\(x_n\)</span>(也就是当前序列最后一个点)之后的点进行预测：</p><p><span class="math display">\[\overline{g}=\frac{1}{m}\sum_{i=1}^m g(x_n,x_{n-i})\]</span></p><p><span class="math display">\[x_{n+1}=x_{n-m+1}+\overline{g}\cdot m\]</span></p><p>其中<span class="math inline">\(g(x_i,x_j)\)</span>代表<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>两点构成的直线的梯度；<span class="math inline">\(\overline{g}\)</span>代表所处理的点的平均梯度；<span class="math inline">\(m\)</span>为所处理的点的数量。在本文中设置<span class="math inline">\(m=5\)</span>。文中发现第一个预测的值很重要，所以直接把<span class="math inline">\(x_{n+1}\)</span>赋值<span class="math inline">\(k\)</span>次添加到序列的末尾。</p><h3 id="sr-cnn">SR-CNN</h3><p><img src="http://qfxiao.me/img/1565427857981.png" srcset="/img/loading.gif" /></p><p>本文提到，仅仅使用一个阈值来进行异常的判断太过简单，于是提出使用一个判别模型来进行异常的判断。由于训练数据没有标签，所以使用如下的公式人工加入异常：</p><p><span class="math display">\[x=(\overline{x}+mean)(1+var)\cdot r+x\]</span></p><p>其中<span class="math inline">\(\overline{x}\)</span>所处理的点的局部均值；<span class="math inline">\(mean\)</span>和<span class="math inline">\(var\)</span>为当前滑动窗口点的均值和方差；<span class="math inline">\(r\sim \mathcal{N}(0,1)\)</span>为服从标准正态分布的噪声。</p><hr /><p>对于判别模型使用的是CNN，主要包含两个1维卷积层(kernel size等于窗口大小<span class="math inline">\(w\)</span>)和两个全连接层。两个卷积层的channel size分别为<span class="math inline">\(w\)</span>和<span class="math inline">\(2w\)</span>。</p><h2 id="experiments">Experiments</h2><h3 id="datasets">Datasets</h3><p><img src="http://qfxiao.me/img/1565445958030.png" srcset="/img/loading.gif" /></p><p>所用数据集包含清华AIOps竞赛数据、Yahoo和Microsoft的KPI数据。</p><h3 id="evaluation-metrics">Evaluation Metrics</h3><p>算法准确率方面用了precision，recall和<span class="math inline">\(F_1\)</span>-score。</p><p><img src="http://qfxiao.me/img/1565449295457.png" srcset="/img/loading.gif" /></p><p>由于在实际场景中KPI的异常往往是以一段一段的形式出现，且并不要求某一个时间点出现异常算法就马上检测出来，只要检测出来的时间在一定的容忍范围内即可。本文使用了一些调整的手段，如Figure 6。对于某一段异常，设段首的异常位于时间点<span class="math inline">\(t_{truth}\)</span>，预测为异常的结果中时间在<span class="math inline">\(t_{truth}\)</span>之后且距<span class="math inline">\(t_{truth}\)</span>最近的时间点设为<span class="math inline">\(t_{predict}\)</span>，那么对于一个预先设定的容忍范围<span class="math inline">\(k\)</span>，只要<span class="math inline">\(t_{predict}-t_{truth}\leq k+1\)</span>那么在预测结果中整段异常就会重置为<span class="math inline">\(1\)</span>，否则全部重置为<span class="math inline">\(0\)</span>。</p><h3 id="results">Results</h3><p>实验部分使用了两种训练方式，一种是cold-start，即把所有数据都用来测试，另一种是把数据分为训练测试两部分，在训练集上训练，最后在测试集上进行测试。两种方法适用的baseline不同，最后结果如Table 2和Table 3所示：</p><p><img src="http://qfxiao.me/img/1565500578722.png" srcset="/img/loading.gif" /></p><hr /><p>在SR的参数设置上，<span class="math inline">\(h_q(f)\)</span>中的<span class="math inline">\(q\)</span>为3，局部平均所用的点数目<span class="math inline">\(z\)</span>为21，阈值<span class="math inline">\(\tau\)</span>为3，估计点的数量<span class="math inline">\(k\)</span>为5，滑动窗口的大小<span class="math inline">\(w\)</span>在KPI、Yahoo、Microsoft三个数据集上分别为1440、64和30。SR-CNN的<span class="math inline">\(q\)</span>，<span class="math inline">\(z\)</span>，<span class="math inline">\(k\)</span>，<span class="math inline">\(w\)</span>设置与SR相同。</p><h3 id="additional-experiments-with-dnn">Additional Experiments with DNN</h3><p>文中还对有监督的情况进行了测试，具体做法是从时间序列提取特征，然后将Saliency Map也作为特征引入，构造一个有监督的Neural Network进行测试。</p><p>提取的特征如Table 5所示：</p><p><img src="http://qfxiao.me/img/1565502099432.png" srcset="/img/loading.gif" /></p><hr /><p>神经网络的结构为两层全连接层，并添加了Dropout Ratio为0.5的Dropout Layer。两个Layer使用了<span class="math inline">\(L_1=L_2=0.0001\)</span>的正则化。同时为了处理样本不平衡的情况使用了过采样来使正负样本的比例为<span class="math inline">\(1:2\)</span>。结构如Figure 7所示：</p><p><img src="http://qfxiao.me/img/1565502126919.png" srcset="/img/loading.gif" /></p><hr /><p>训练测试集的情况如Table 6所示，最终结果如Table 7所示，P-R曲线如Figure 8所示。可以看到使用了SR特征的DNN效果由于没有使用SR特征的DNN。</p><p><img src="http://qfxiao.me/img/1565502349310.png" srcset="/img/loading.gif" /></p><p><img src="http://qfxiao.me/img/1565502360013.png" srcset="/img/loading.gif" /></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Anomaly Detection</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Time Series</tag>
      
      <tag>Anomaly Detection</tag>
      
      <tag>Machine Learning</tag>
      
      <tag>Spectral</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
