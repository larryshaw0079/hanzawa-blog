<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="website"><meta property="og:title" content="Hanzawa の 部屋"><meta property="og:url" content="https://larryshaw0079.github.io/hanzawa-blog"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://larryshaw0079.github.io/img/og_image.png"><meta property="article:author" content="Hanzawa"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://larryshaw0079.github.io/hanzawa-blog"},"headline":"Hanzawa の 部屋","image":["https://larryshaw0079.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":null}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-06T04:03:27.000Z" title="2020-6-6 12:03:27 ├F10: PM┤">2020-06-06</time>发表</span><span class="level-item"><time dateTime="2020-06-25T08:14:29.250Z" title="2020-6-25 4:14:29 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/06/Generative-Probabilistic-Novelty-Detection-with-Adversarial-Autoencoders/">Generative Probabilistic Novelty Detection with Adversarial Autoencoders</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这篇文章介绍了一种基于概率分布的异常检测方法。其基本思想是假设正常样本服从定义在流形$M$上的分布，而对于任意一点$\bar x$，通过投影到流形$M$上$x^\parallel$，可以分解为平行于切空间的部分$x^\parallel$和正交与切空间的部分$x^\bot$。原始的坐标$\bar x$被转换到$x^\parallel$局部坐标系中，然后似然通过转换后的坐标系进行计算。</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="Generative-Probabilistic-Novelty-Detection"><a href="#Generative-Probabilistic-Novelty-Detection" class="headerlink" title="Generative Probabilistic Novelty Detection"></a>Generative Probabilistic Novelty Detection</h2><p>我们假设训练数据$x_1,\cdots,x_N$，其中$x_i\in\mathbb{R}^m$，从一个分布采样的来，并带有随机噪声$\xi$：<br>$$<br>x_i=f(z_i)+\xi_i, \space\space\space i=1,\cdots,N<br>$$<br>其中$z_i\in\mathbb{R}^n$，$f:\Omega\mapsto\mathbb{R}^m$定义了一个$n$维带参流形$\mathcal{M}\equiv f(\Omega)$。注意这里噪声的加入使得样本的值域扩展到了整个实数空间。同时假设存在$g:\mathbb{R}^m\mapsto\mathbb{R}^n$，对任意$x\in\mathcal{M}$都有$f(g(x))=x$。$f$和$g$后面会通过神经网络实现。</p>
<p>对于一个测试样本$\bar{x}\in\mathbb{R}^m$，我们可以得到其在$M$上的投影，这是通过逆变换$\bar z = g(\bar x)$得到对应$z$的然后再通过$\bar x^{\parallel}=f(\bar z)$得到。$f$在$\bar z$的一阶泰勒展开为：<br>$$<br>f(z)=f(\bar z)+J_f(\bar z)(z-\bar z)+O(\parallel z-\bar z\parallel ^2)<br>$$<br><img src="https://i.loli.net/2020/06/25/oi9xKMO3ID7jANJ.png" style="zoom:67%;" /></p>
<p>其中$J_f(\bar z)$为$f$在点$\bar z$的雅各比矩阵。$\mathcal T=\text{span}(J_f(\bar z))$代表点$\bar z$处由$J_f(\bar z)$的$n$个独立向量组成的切空间。通过对$J_f(\bar z)$进行奇异值分解$J_f(\bar z)=U^\parallel SV^\top$。<br>$$<br>\bar w=U^\top\bar x=\left[\begin{matrix}U^{\parallel^\top}\bar x\ U^{\bot^\top}\bar x\end{matrix}\right]=\left[\begin{matrix}\bar w^\parallel\ \bar w^\bot\end{matrix}\right]<br>$$<br>坐标$\bar w$可以分解为平行于$\mathcal T$和正交于$\mathcal T$两部分。</p>
<p>定义在施加变换前后的坐标系上的概率分布$p_X(x)$和$p_W(w)$是等价的，不过对于$p_W(w)$，我们假设平行部分和正交部分是独立的，即：<br>$$<br>p_X(x)=p_W(w)=p_W(w^\parallel,w^\bot)=p_{W^\parallel}(w^\parallel)p_{W^\bot}(w^\bot)<br>$$<br>这一假设的依据是随机噪声部分假设主要是往流形之外偏离的，即与$\mathcal T$正交，所以$W^\bot$主要是反映噪声的部分。而噪声与样本分布相独立的假设是合理的。于是，异常分数可以定义为：<br>$$<br>p_X(\bar x)=p_{W^\parallel}(\bar w^\parallel)p_{W^\bot}(\bar w^\bot)=\begin{cases}\geq \gamma \Rightarrow \text{Inlier}\&lt;\gamma\Rightarrow\text{Outlier}\end{cases}<br>$$</p>
<h2 id="Computing-the-Distribution-of-Data-Samples"><a href="#Computing-the-Distribution-of-Data-Samples" class="headerlink" title="Computing the Distribution of Data Samples"></a>Computing the Distribution of Data Samples</h2><p>上面的异常分数需要计算$p_{W^\parallel}(\bar w^\parallel)$和$p_{W^\bot}(\bar w^\bot)$。给定测试样本$\bar x$，投影到流形$\bar x^\parallel=f(g(\bar x))$。$\bar w^\parallel$可以重写为$\bar w^\parallel=U^{\parallel^\top}\bar x=U^{\parallel^\top}(\bar x-\bar x^{\parallel})+U^{\parallel^\top}\bar x^\parallel=U^{\parallel^\top}\bar x^\parallel$，即我们假设$U^{\parallel^\top}(\bar x-\bar x^\parallel)\approx 0$。于是有$w^\parallel(z)=U^{\parallel^\top}f(\bar z)+SV^\top(z-\bar z)+O(\parallel z-\bar z\parallel^2)$。</p>
<p>如果$Z$为定义在流形上的概率分布，那么：<br>$$<br>p_{W^\parallel}(w^\parallel)=|\text{det}S^{-1}|p_Z(z)<br>$$<br>$p_{W^\bot}(w^\bot)$由半径为$\parallel w^\bot\parallel$的超球体$\mathcal S^{m-n-1}$来进行估计：<br>$$<br>p_{W^\bot}(w^\bot)\approx\frac{\Gamma(\frac{m-n}{2})}{2\pi^{\frac{m-n}{2}}\parallel w^\bot\parallel^{m-n}}p_{\parallel W^\bot\parallel}(\parallel w^\bot\parallel)<br>$$</p>
<p>其中$\Gamma(\cdot)$代表Gamma函数。</p>
<h2 id="Manifold-Learning-with-Adversarial-Autoencoders"><a href="#Manifold-Learning-with-Adversarial-Autoencoders" class="headerlink" title="Manifold Learning with Adversarial Autoencoders"></a>Manifold Learning with Adversarial Autoencoders</h2><p>为了学习映射$f$和$g$，我们使用了AAE框架，如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/sQhO3D4gKJqBPXv.png" style="zoom: 67%;" />

<p>除了常规的AAE外，我们还为$x$添加了一个额外的判别器。</p>
<h3 id="Adversarial-Losses"><a href="#Adversarial-Losses" class="headerlink" title="Adversarial Losses"></a>Adversarial Losses</h3><p>对于隐变量$z$，对抗损失函数为：<br>$$<br>\mathcal L_{adv-d_z}(x,g,D_z)=E[\log(D_z(\mathcal N(0,1)))]+E[\log(1-D_z(g(x)))]<br>$$<br>对于样本$x$，对抗损失函数为：<br>$$<br>\mathcal L_{adv-d_x}(x,D_x,f)=E[\log(D_x(x))]+E[\log(1-D_x(f(\mathcal N(0,1))))]<br>$$</p>
<h3 id="Autoencoder-Loss"><a href="#Autoencoder-Loss" class="headerlink" title="Autoencoder Loss"></a>Autoencoder Loss</h3><p>$$<br>\mathcal L_\text{error}(x,g,f)=-E_z[\log(p(f(g(x))|x))]<br>$$</p>
<h3 id="Full-Objective"><a href="#Full-Objective" class="headerlink" title="Full Objective"></a>Full Objective</h3><p>$$<br>\mathcal L(x,g,D_z,D_x,f)=\mathcal L_{adv-d_z}+\mathcal L_{adv-d_x}+\lambda \mathcal L_\text{error}<br>$$</p>
<p>下图为模型重构的例子：</p>
<img src="https://i.loli.net/2020/06/25/i7ytlgjoIbYV6uF.png" style="zoom:67%;" />



<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li>**MNIST. ** 手册数字识别数据集。</li>
<li>**The Coil-100. **包含7200张100个不同物体的不同角度的图片。</li>
<li>**Fashion-MNIST. ** 手册数字识别数据集彩色版。</li>
<li>**Others. ** 前三个数据集都是采用一个类作为inlier，而其他类作为outlier。在这一设置中inlier采样自数据集CIFAR-10(CIFAR-100)，而outlier采样自TinyImageNet、LSUN和iSUN。</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="MNIST-Dataset"><a href="#MNIST-Dataset" class="headerlink" title="MNIST Dataset"></a>MNIST Dataset</h3><p><img src="https://i.loli.net/2020/06/25/5a71oidmK2ZGLyh.png"></p>
<img src="https://i.loli.net/2020/06/25/4lcGeHDrhbdKN5W.png" style="zoom:67%;" />

<h3 id="Coil-100-Dataset"><a href="#Coil-100-Dataset" class="headerlink" title="Coil-100 Dataset"></a>Coil-100 Dataset</h3><img src="https://i.loli.net/2020/06/25/ofVGBgR7a3WmyvU.png" style="zoom:67%;" />



<h3 id="Fashion-MNIST"><a href="#Fashion-MNIST" class="headerlink" title="Fashion-MNIST"></a>Fashion-MNIST</h3><p><img src="https://i.loli.net/2020/06/25/avURoBw6ny8SIEq.png"></p>
<h3 id="CIFAR-10-CIFAR-100"><a href="#CIFAR-10-CIFAR-100" class="headerlink" title="CIFAR-10 (CIFAR-100)"></a>CIFAR-10 (CIFAR-100)</h3><img src="https://i.loli.net/2020/06/25/piteKy1m9kvQ6EU.png" style="zoom: 67%;" />



<h3 id="Ablation"><a href="#Ablation" class="headerlink" title="Ablation"></a>Ablation</h3><p><img src="https://i.loli.net/2020/06/25/xgni9wBtYkheGZq.png"></p>
<img src="https://i.loli.net/2020/06/25/idhqkCbAKvzMt68.png" style="zoom:67%;" />



</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-02T15:01:34.000Z" title="2020-6-2 11:01:34 ├F10: PM┤">2020-06-02</time>发表</span><span class="level-item"><time dateTime="2020-06-26T12:55:43.589Z" title="2020-6-26 8:55:43 ├F10: PM┤">2020-06-26</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/02/Classification-based-Anomaly-Detection-for-General-Data/">Classification-based Anomaly Detection for General Data</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文主要是对<a target="_blank" rel="noopener" href="http://qfxiao.me/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/">NIPS18这篇异常检测文章</a>的改进，首先是利用了标签信息来提升算法的表现，其次是将算法扩展到了非图像数据。作者对现有的异常检测算法进行了回顾：</p>
<ul>
<li>**Reconstruction Methods： **这一部分方法假设异常样本和正常样本能够通过重构任务来进行区分。通过在正常样本上学习重构任务，之后对于正常样本，模型能够很好地进行重构，而异常样本则会有较高的重构误差。</li>
<li>**Distributional Methods： **这一部分方法将异常检测看作是密度估计问题。通过对正常样本的分布进行估计，异常样本在该正常分布下的似然将会很低。</li>
<li>**Classification-based Methods： **这一部分方法主要是指的单分类方法和通过几何变换构造分类任务的方法。本文使用的就是这类方法。</li>
</ul>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><h2 id="Classification-based-Anomaly-Detection"><a href="#Classification-based-Anomaly-Detection" class="headerlink" title="Classification-based Anomaly Detection"></a>Classification-based Anomaly Detection</h2><p>假设所有数据位于空间$R^L$内，而正常数据位于子空间$X\subset R^L$内。我们假设所有的异常样本位于$X$之外。为了检测异常，我们希望学习一个分类器$C$使得对于所有的$x\in X$有$C(x)=1$，而对所有的$x\in R^L\backslash X$有$C(x)=0$。</p>
<p>单分类方法的思想是直接学习$P(x\in X)$，代表的方法有One-Class SVM，DSVDD等。传统的OC-SVM直接在原始空间或者核空间学习分类器。比较新的方法，如Deep-SVDD则是先将样本转换到一个特征空间，然后在这个特征空间上学习使得半径$R$最小的超球体（球心$c_0$），来覆盖住所有正常样本。异常的判定则通过计算$\parallel f(x)-c_0\parallel^2-R^2$来实现。不过学习一个好的样本到特征空间的变换并不是一件容易的事情，比如说$f(x)=0, \forall x \in X$就是一个使得超球体最小的解。所以需要很多trick来避免诸如此类的情况。</p>
<p><em>Geometric-transformation classification</em> (GEOM) 则将数据空间$X$通过$M$个几何变换转换到一系列子空间$X_1,\cdots,X_M$。之后训练一个分类器来预测样本$T(x,m)$对应的几何变换的种类$m$。转换后的正常图片空间记为$\cup_m X_m$，所以该方法尝试估计以下条件概率：<br>$$<br>P(m^\prime|T(x,m))=\frac{P(T(x,m)\in X_{m^\prime})P(m^\prime)}{\sum_{\bar{m}}P(T(x,m)\in X_{\bar{m}})P(\tilde{m})}-\frac{P(T(x,m)\in X_{m^\prime})}{\sum_{\bar{m}}P(T(x,m)\in X_{\bar{m}})}<br>$$</p>
<p>对于异常的样本$x\in R^L\backslash X$，在经过几何变换之后，都不会位于正确的子空间中，即$T(x,m)\in R^L\backslash X_m$。之后，使用$P(m|T(x,m))$来判定异常。</p>
<p>作者认为，这种方法的问题是分类器$P(m^\prime|T(x,m))$只在正常数据上训练，而对于异常样本的异常分数会出现方差很大的问题。</p>
<p>一种解决方式是加入异常样本进行训练，但是作者认为在有的任务中标签很难获取，于是作者使用了另外一种方法来解决这个问题。</p>
<h2 id="Distance-based-Multiple-Transformation-Classification"><a href="#Distance-based-Multiple-Transformation-Classification" class="headerlink" title="Distance-based Multiple Transformation Classification"></a>Distance-based Multiple Transformation Classification</h2><p>和GEOM一样，先对每个样本进行$M$个几何变换，然后学习一个特征提取器$f(x)$，将$X_m$映射到特征空间。之后和OC-SVM类似，假设特征${f(x)|x\in X_m}$为球心为$c_m=\frac{1}{N}\sum_{x\in X} f(T(x,m))$的超球体。样本属于某一类$m^\prime$的概率由下式给出：</p>
<p>$$<br>P(m^\prime|T(x,m))=\frac{e^{-\parallel f(T(x,m))-c_{m^\prime}\parallel^2}}{\sum_{\bar m}e^{-\parallel f(T(x,m))-c_{\bar m}\parallel^2}}<br>$$</p>
<p>目标函数采用的是Triplet Loss：</p>
<p>$$<br>L=\sum_i\max(\parallel f(T(x_i,m))-c_m\parallel^2+s-\min_{m^\prime\neq m}\parallel f(T(x_i,m))-c_{m^\prime}\parallel^2,0)<br>$$</p>
<p>$\parallel f(T(x_i,m))-c_m\parallel^2$相当于最小化了类内距离，$\min_{m^\prime\neq m}\parallel f(T(x_i,m))-c_{m^\prime}\parallel^2$最大化了每个类对应的集簇间距离。在检测阶段，为了避免一些数值问题，作者做了一些平滑操作：</p>
<p>$$<br>\tilde P(m^\prime|T(x,m))=\frac{e^{-\parallel f(T(x,m))-c_{m^\prime}\parallel^2+\epsilon}}{\sum_{\tilde m}e^{-\parallel f(T(x,m))-c_{\tilde m}\parallel^2+M\cdot\epsilon}}<br>$$</p>
<p>最后的评判分数由下式给出：</p>
<p>$$<br>Score(x)=-\log P(x\in X)=-\sum_m\log \tilde{P}(T(x,m)\in X_m)=-\sum_m\log\tilde{P}(m|T(x,m))<br>$$</p>
<p>算法流程图如下：</p>
<img src="https://i.loli.net/2020/06/24/r48h1RJxcXF6YDM.png" style="zoom:67%;" />

<h2 id="Parameterizing-the-Set-of-Transformations"><a href="#Parameterizing-the-Set-of-Transformations" class="headerlink" title="Parameterizing the Set of Transformations"></a>Parameterizing the Set of Transformations</h2><p>在GEOM中，由于使用的几何变换都是针对图像的，所以对于其他类型的数据并不适用。本文中作者对非图像数据设计了以下变换：</p>
<p>$$<br>T(x,m)=W_mx+b_m<br>$$</p>
<p>不同的参数$W_m$和$b_m$即为不同的几何变换，可以考虑采用随机采样的方式。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Image-Experiments"><a href="#Image-Experiments" class="headerlink" title="Image Experiments"></a>Image Experiments</h2><p>对于图像数据的异常检测实验，作者采用了CIFAR10、FasionMNIST这两个数据集，实验结果如下：</p>
<img src="https://i.loli.net/2020/06/24/j4Y29tB6k1Aipgo.png" style="zoom:67%;" />

<img src="https://i.loli.net/2020/06/24/D3opwrLnSGmcsyM.png" style="zoom:67%;" />

<h2 id="Tabular-Data-Experiments"><a href="#Tabular-Data-Experiments" class="headerlink" title="Tabular Data Experiments"></a>Tabular Data Experiments</h2><p>对于非图像数据，作者采用了几个小的数据集：Arrhythmia、Thyroid、KDD和KDDRev。采用的Baseline包括OC-SVM、E2E-AE、LOF、DAGMM和FB-AE (Feature Bagging Autoencoder)。对于几何变换的参数，采样自标准正态分布。结果如下：</p>
<img src="https://i.loli.net/2020/06/24/e6PfIDOVwlzrSWi.png" style="zoom:67%;" />

<h1 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h1><p>结合<a target="_blank" rel="noopener" href="https://openreview.net/forum?id=H1lK_lBtvS">OpenReview</a>上的一些讨论，这里提出一些问题和总结：</p>
<ul>
<li>KDD数据集太简单了，正常、异常样本能够很容易被分开；</li>
<li>对于图像数据作者只使用了CIFAR10和FashionMNIST这两个比较小的数据集，而在GEOM中还使用了CIFAR100和CatsVsDogs。并且GEOM原文中提到数据集（指图像大小）越大，GEOM的优势就越明显，所以在本文的实验中只使用这两个数据集说服力略显不够；</li>
<li>关于评测标准的问题，作者在图像数据中用的是AUROC，而非图像数据用的是F1 score。像AUPR、AUROC这种评测标准往往更加全面，而F1 score依赖于阈值的选取。如果是遍历阈值找到最好的那个F1 score，则无法全面考察模型的鲁棒性，模型有可能只是在特定的阈值下表现很好，而阈值稍微偏差一下性能可能就会大幅下降。我看到的大多数异常检测文章都是使用AUROC或者F1加上AUROC作为评测指标；</li>
<li>文中在第二节“CLASSIFICATION-BASED ANOMALY DETECTION”的末尾两段关于GEOM方法的缺点说的很模糊。异常分数的方差大到底指的是什么；</li>
<li>关于作者提出的变换$T(x,m)=W_mx+b_m$并没有用到图像数据的实验上，而且在实验中$b_m$这个参数实际上是被忽略掉了的，$b_m$的作用究竟如何不得而知。而且GEOM中的几何变换的Motivation在原文中是做了实验充分讨论了的，GEOM的作者认为这些几何变换保留了图像的高阶语义信息。而本文中的变换中的参数只是随机采样而来，并不存在说保留原始数据中的结构信息。如果忽略掉这一层变换，那就类似于加了神经网络提取特征的OC-SVM。</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-02T04:26:13.000Z" title="2020-6-2 12:26:13 ├F10: PM┤">2020-06-02</time>发表</span><span class="level-item"><time dateTime="2020-08-01T04:00:16.325Z" title="2020-8-1 12:00:16 ├F10: PM┤">2020-08-01</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/02/%E9%9D%A2%E5%90%91OpenPAI%E7%9A%84Docker%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E5%8F%8AOpenPAI%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/">面向OpenPAI的Docker镜像配置及OpenPAI基本使用方法</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>实验室服务器集群采用OpenPAI来进行GPU资源的管理，而OpenPAI采用了Docker作为基础，即代码都放在Docker容器中运行。由于Docker的使用、Docker镜像的配置都有一定的门槛，所以这里写一篇Tutorial来进行介绍。本文不是网上资料的拼凑，而是经过本人走弯路踩坑形成的”Best practice”。主要内容包括Docker的介绍、Docker的基本使用、如何配置自己的Docker镜像以及OpenPAI平台的基本使用，但不包括Docker和OpenPAI的安装。</p>
<blockquote>
<p>2020.8.1 Update: 加入通过HDFS读取容器保存的文件的方法</p>
</blockquote>
<h1 id="Docker-from-Scratch"><a href="#Docker-from-Scratch" class="headerlink" title="Docker from Scratch"></a>Docker from Scratch</h1><p>要理解Docker是什么，从虚拟机开始讲可能会比较好理解。虚拟机大家可能都很熟悉了，比如说我用的系统是Windows，但我需要Linux系统来作为一个Flask编写的网站的服务器，但是又不想单独安装Linux系统，于是可以使用虚拟机来解决这个问题。安装VMWare Workstation，去官网下载Ubuntu系统镜像，然后在VMWare中安装好系统，然后从头配置Flask相关环境。实际上我需要的仅仅是一个Flask运行环境而已，而使用虚拟机却需要如此“大费周章”，这时Docker出现了，网上有大量现成的Flask Docker镜像，配置好了你所需的Flask环境，你只需要下载这些镜像，然后运行它，你就得到了一个Flask运行环境，而与你当前使用的系统无关。如果你需要一个Tomcat的运行环境，那么去找一个Tomcat的Docker镜像就行。Docker将需求或者说服务绑定在了Docker镜像中（<strong>轻量化</strong>，一个需求对应一个Docker镜像，每个镜像都很小），你有什么需求，去找相应的镜像即可（或者自己写一个），镜像的运行是以虚拟机的形式存在，所以他们之间也是互不干扰的。同时，你在写好一个Docker镜像之后，你还可以<strong>分享</strong>给别人，这样其他人就不用重新配置，直接运行你给他的镜像即可。Docker有两个比较关键的概念：</p>
<ul>
<li><strong>镜像 Images：</strong> 这里的镜像不是指我们安装系统时下载的ISO镜像，Docker镜像就是把你需要的东西（一个系统+需要的服务）集中到一起，相当于做菜的菜谱；</li>
<li><strong>容器 Containers：</strong> 如果一个Docker镜像启动了，那么就会有一个Docker容器产生，相当于按照菜谱做出来的菜。</li>
</ul>
<img src="https://i.loli.net/2020/06/24/H2x9mnPwkVQyO6p.png" alt="img" style="zoom: 33%;" />

<img src="https://i.loli.net/2020/06/24/7k6S3yecX2lbvQY.png" alt="img" style="zoom: 33%;" />

<p>这一节我们先不讨论如何自己写Docker镜像，只是先讨论Docker的基本操作。</p>
<h2 id="Basic-Operations"><a href="#Basic-Operations" class="headerlink" title="Basic Operations"></a>Basic Operations</h2><p>Docker新安装好当然是没有什么镜像的，首先我们使用<code>docker pull hello-world</code>来下载一个测试镜像。</p>
<blockquote>
<p>拉取镜像 <code>docker pull &lt;image_name&gt;</code></p>
</blockquote>
<p>在输入之后，Docker会自动在远程服务器上查找对应的镜像进行下载。由于我的电脑上已经有这个镜像了，所以显示是下面的样子：</p>
<p><img src="https://i.loli.net/2020/06/24/rvqcAwzOYJtF6T8.png"></p>
<p>接下来，我们输入<code>docker run hello-world</code>运行这个镜像。</p>
<blockquote>
<p>运行镜像<code>docker run &lt;image_name&gt;</code></p>
</blockquote>
<p>可以看到，Docker输出了一些信息就自己退出了，这和我们理解的虚拟机不太一样。在Docker里面，我们既可以创建一个完整的系统，用户在运行之后就可以正常使用这个操作系统，也可以创建一个简单的服务，默认运行完一些指令就退出了。这里的<code>hello-world</code>镜像这是输出了一些信息后就自动退出了，因为这就是这个镜像的全部内容。</p>
<p><img src="https://i.loli.net/2020/06/24/Zb1VKjFyMh8gHf2.png"></p>
<p>我们尝试来运行一个完整的系统，先用<code>docker pull ubuntu</code>拉取Ubuntu Docker镜像：</p>
<p><img src="https://i.loli.net/2020/06/24/zEerb2gPwYWX8O7.png"></p>
<p>接下来我们使用：</p>
<p><img src="https://i.loli.net/2020/06/24/6cMz1WGH4fgpBsS.png"></p>
<blockquote>
<p><code>-it</code>的意思是什么？根据<code>docker run --help</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-i, --interactive                    Keep STDIN open even if not attached</span><br><span class="line">    --ip string                      IPv4 address (e.g., 172.30.100.104)</span><br><span class="line">    --ip6 string                     IPv6 address (e.g., 2001:db8::33)</span><br><span class="line">    --ipc string                     IPC mode to use</span><br><span class="line">    --isolation string               Container isolation technology</span><br><span class="line">    --kernel-memory bytes            Kernel memory limit</span><br><span class="line">-t, --tty                            Allocate a pseudo-TTY</span><br><span class="line">    --ulimit ulimit                  Ulimit options (default [])</span><br><span class="line">Copy</span><br></pre></td></tr></table></figure>

<p>其实<code>-it</code>是<code>-i</code>和<code>-t</code>的合并写法，意思是运行后进入这个容器并且启用shell，不然运行之后就会放到后台而不会进入容器中。而<code>--rm</code>则代表容器退出之后会被删除（镜像不会被删除），每次运行实际上会创建一个新的容器，如果不加<code>--rm</code>或退出之后不手动删除的话会看到一堆停止运行的容器。</p>
</blockquote>
<p>输入<code>cat /etc/issue</code>可以看到默认拉取的是最新的Ubuntu 20.04 LTS：</p>
<p><img src="https://i.loli.net/2020/06/24/wldBMFtx3eIk98C.png"></p>
<h1 id="Build-Customized-Docker-Images"><a href="#Build-Customized-Docker-Images" class="headerlink" title="Build Customized Docker Images"></a>Build Customized Docker Images</h1><p>如果没有现成的Docker镜像能满足我们的需求，我们可以考虑自己写一个。要自定义一个Docker镜像需要两步，第一步是编写Dockerfile，第二步是使用<code>docker build</code>命令构建镜像。Dockerfile可以看作是一个脚本，描述了我们构建镜像所需要的全部命令，比如要构建一个用于Python科学计算的Docker镜像，我们需要在Dockerfile中编写安装Python的命令，安装Numpy、Scipy等常用包的命令等等。我们先来上手编写Dockerfile，这里我准备写一个包含<a target="_blank" rel="noopener" href="https://hexo.io/">hexo博客框架</a>的镜像，这个框架需要node作为基础环境，不过我们不需要在Dockerfile里写安装node的命令。因为类似于<code>C++</code>或<code>Python</code>中的对象的继承，Dockerfile也可以“继承”，这意味着我们不必从头写起。我们先来看一下完整的Dockfile和效果，再来一一解释。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM node</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">RUN npm install -g hexo-cli</span><br><span class="line">EXPOSE 4000</span><br><span class="line">CMD hexo init blog &amp;&amp; cd blog &amp;&amp; hexo generate &amp;&amp; hexo serverCopy</span><br></pre></td></tr></table></figure>

<p>运行结果如下所示，可以看到Docker按照我们写的Dockerfile一行一行的进行镜像的构建：</p>
<p><img src="https://i.loli.net/2020/06/24/tZ3JW4SPNhxUwEn.png"></p>
<p>现在来解释Docerfile里的内容。<code>FROM &lt;docker image&gt;</code>表示继承其他的镜像，这里我们使用node官方的镜像。接下来是安装hexo，<code>RUN &lt;command&gt;</code>表示执行命令，这里我们直接用<code>npm install -g hexo-cli</code>进行安装。由于要浏览博客网页需要开放端口，而Docker容器运行的时候和外部主机是完全隔断的，要使外部主机访问Docker容器端口，需要暴露端口。<code>EXPOSE &lt;port&gt;</code>代表暴露端口，这里用的是4000端口。之后是创建博客和启动本地服务，<code>CMD &lt;command&gt;</code>和<code>RUN &lt;command&gt;</code>的区别是RUN会在构建的时候执行，而CMD是在容器启动之后才会执行。<code>hexo init blog &amp;&amp; cd blog &amp;&amp; hexo generate &amp;&amp; hexo server</code>分别代表初始化博客、进入博客所在文件夹、生成博客网站、启动本地服务器。更多指令可以参考<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">官方文档</a>。</p>
<p>然后我们使用<code>docker build -t test_hexo .</code>命令构建镜像。</p>
<blockquote>
<p>构建镜像 <code>docker build -t &lt;image_name&gt; &lt;direcotry&gt;</code></p>
</blockquote>
<p>运行镜像：</p>
<p><img src="https://i.loli.net/2020/06/24/m4FSe15kMIDCHXy.png"></p>
<p>可以看到容器启动后开始执行博客初始化。</p>
<p><img src="https://i.loli.net/2020/06/24/BEdibvgPTMz4sn8.png"></p>
<p>最后在<code>locahost:4000</code>上启动了一个本地服务器，在浏览器中输入这个地址，可以看到刚刚构建好的博客：</p>
<p><img src="https://i.loli.net/2020/06/24/vmSUb5QH6l4afzF.png"></p>
<p>值得注意的是，在Dockerfile中我们暴露了4000端口，使用<code>-p</code>标签可以达到同样的效果：<code>docker run -p &lt;docker_port&gt;:&lt;local_port&gt; &lt;image_name&gt;</code>。比如<code>docker run -p 9999:8888 xxxx</code>代表将Docker容器中的9999端口转发到外部主机的8888端口。如果你是在远程服务器上使用的Docker，那么端口只是被转发到了远程服务器上，还得手动将远程服务器再转发到你本机上才能直接在本机浏览器上看到页面。</p>
<h2 id="Build-Docker-Images-with-Aliyun-Container-Registry"><a href="#Build-Docker-Images-with-Aliyun-Container-Registry" class="headerlink" title="Build Docker Images with Aliyun Container Registry"></a>Build Docker Images with Aliyun Container Registry</h2><p>因为某些原因，如果在构建镜像的时候需要通过<code>apt-get update</code>更新源，会发现无论如何都会卡住。这个时候可以使用<a target="_blank" rel="noopener" href="https://cr.console.aliyun.com/">阿里云容器镜像服务</a>，在阿里的服务器上构建好镜像，再拉取到自己的机器上。注册好帐号之后，点击创建镜像仓库：</p>
<p><img src="https://i.loli.net/2020/06/24/u4DGHv3En1iLI5z.png"></p>
<p>这里仓库类型如果没有特殊需求建议使用公开，然后填写一些基本信息：</p>
<p><img src="https://i.loli.net/2020/06/24/Qipw5hAg136afnL.png"></p>
<p>之后设置代码源，其实就是告诉阿里云从哪儿获取Dockerfile，我这里用的是Github，所以需要先在阿里云中关联Github账号，然后在Github中创建一个用来放Dockerfile的仓库。构建设置里有一个“海外机器构建”，这正是我们使用阿里云容器服务的主要目的，勾选。</p>
<p><img src="https://i.loli.net/2020/06/24/ksvZJG9lKfdh6aR.png"></p>
<p>镜像仓库创建好之后，点进去，在构建页面点击添加规则：</p>
<p><img src="https://i.loli.net/2020/06/24/irBm3QcqjVhGMsv.png"></p>
<p>按下图进行设置即可，镜像版本就是你想要的镜像名字：</p>
<p><img src="https://i.loli.net/2020/06/24/7pAFvM8CJQbq6mU.png"></p>
<p>点击“立即构建”：</p>
<p><img src="https://i.loli.net/2020/06/24/GlMwqDInL7zVOpf.png"></p>
<p>等待一段时间后，如果构建成功，便可以进行拉取了，在镜像仓库的基本信息页面可以看到地址：</p>
<p><img src="https://i.loli.net/2020/06/24/57jmEv8PYkr2nQG.png"></p>
<p>将阿里云上的镜像拉取到本机之后一般会想要对镜像改名，可以使用<code>docker tag &lt;old_name&gt; &lt;new_name&gt;</code>。</p>
<h1 id="Build-Docker-Images-for-Deep-Learning"><a href="#Build-Docker-Images-for-Deep-Learning" class="headerlink" title="Build Docker Images for Deep Learning"></a>Build Docker Images for Deep Learning</h1><h2 id="Startup"><a href="#Startup" class="headerlink" title="Startup"></a>Startup</h2><p>在Docker中配置适用于OpenPAI的深度学习镜像不是一件容易的事，会有很多的坑，这里专门说一下如何配置。推荐在阿里云容器镜像服务中进行构建，会少很多麻烦。</p>
<p>第一步是初始镜像，由于需要用到CUDA，这里可以根据自己的需求（比如不同CUDA版本支持的GPU驱动版本不一样，还有Tensorflow不同版本对CUDA和cuDNN要求也不一样）从Nvidia的Dockerhub<a target="_blank" rel="noopener" href="https://hub.docker.com/r/nvidia/cuda">官方页面</a>选择合适的CUDA和cuDNN版本：</p>
<p><img src="https://i.loli.net/2020/06/24/pftdhmHiWkTq8Fj.png"></p>
<p>这里我们选择CUDA10.1 + cuDNN7：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia&#x2F;cuda:10.1-cudnn7-devel-ubuntu18.04Copy</span><br></pre></td></tr></table></figure>

<p>这一条主要是解决乱码问题以及定义用到的软件包的版本，这里Miniconda版本设置为4.5.4的原因是这是最后一个自带Python3.6的版本，我在这儿为了稳定所以用了Python3.6，大家也可以安装最新版的Miniconda：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ENV LANG&#x3D;C.UTF-8 LC_ALL&#x3D;C.UTF-8</span><br><span class="line">ENV HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">LABEL HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">ENV MINICONDA_VERSION&#x3D;4.5.4Copy</span><br></pre></td></tr></table></figure>

<p>接下来安装必须的包，大家可以根据需求自行调整，<code>-y</code>标签代表Yes，即自动同意安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">RUN DEBIAN_FRONTEND&#x3D;noninteractive &amp;&amp; \</span><br><span class="line">    apt-get -y update &amp;&amp; \</span><br><span class="line">    apt-get -y install build-essential \</span><br><span class="line">        wget \</span><br><span class="line">        git \</span><br><span class="line">        curl \</span><br><span class="line">        unzip \</span><br><span class="line">        automake \</span><br><span class="line">        openjdk-8-jdk \</span><br><span class="line">        openssh-server \</span><br><span class="line">        openssh-client \</span><br><span class="line">        lsof \</span><br><span class="line">        libcupti-dev &amp;&amp; \</span><br><span class="line">    apt-get clean &amp;&amp; \</span><br><span class="line">    rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*Copy</span><br></pre></td></tr></table></figure>

<p>安装Miniconda并设置环境变量，<code>-b</code>标签可以让Miniconda无交互自动安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN wget --quiet https:&#x2F;&#x2F;repo.anaconda.com&#x2F;miniconda&#x2F;Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh &amp;&amp; &#x2F;bin&#x2F;bash Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh -b -p &#x2F;opt&#x2F;miniconda \</span><br><span class="line">&amp;&amp; rm Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh</span><br><span class="line">ENV PATH &#x2F;opt&#x2F;miniconda&#x2F;bin:$PATHCopy</span><br></pre></td></tr></table></figure>

<p>安装Hadoop，OpenPAI平台会用到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RUN wget -qO- http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;hadoop&#x2F;common&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;.tar.gz | \</span><br><span class="line">    tar xz -C &#x2F;usr&#x2F;local &amp;&amp; \</span><br><span class="line">    mv &#x2F;usr&#x2F;local&#x2F;hadoop-$&#123;HADOOP_VERSION&#125; &#x2F;usr&#x2F;local&#x2F;hadoopCopy</span><br></pre></td></tr></table></figure>

<p><code>ENV</code>的作用是配置环境变量。配置JAVA和Hadoop环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ENV JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-8-openjdk-amd64 \</span><br><span class="line">    HADOOP_INSTALL&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop \</span><br><span class="line">    NVIDIA_VISIBLE_DEVICES&#x3D;all</span><br><span class="line"></span><br><span class="line">ENV HADOOP_PREFIX&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_BIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;bin \</span><br><span class="line">    HADOOP_SBIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;sbin \</span><br><span class="line">    HADOOP_HDFS_HOME&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_COMMON_LIB_NATIVE_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native \</span><br><span class="line">    HADOOP_OPTS&#x3D;&quot;-Djava.library.path&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native&quot;Copy</span><br></pre></td></tr></table></figure>

<p>设置PATH环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ENV PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin:$&#123;HADOOP_BIN_DIR&#125;:$&#123;HADOOP_SBIN_DIR&#125; \</span><br><span class="line">LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;targets&#x2F;x86_64-linux&#x2F;lib&#x2F;stubs:$&#123;JAVA_HOME&#125;&#x2F;jre&#x2F;lib&#x2F;amd64&#x2F;serverCopy</span><br></pre></td></tr></table></figure>

<p>完整的Dockerfile如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">FROM nvidia&#x2F;cuda:10.1-cudnn7-devel-ubuntu18.04</span><br><span class="line"></span><br><span class="line">ENV LANG&#x3D;C.UTF-8 LC_ALL&#x3D;C.UTF-8</span><br><span class="line">ENV HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">LABEL HADOOP_VERSION&#x3D;2.7.2</span><br><span class="line">ENV MINICONDA_VERSION&#x3D;4.5.4</span><br><span class="line"></span><br><span class="line">RUN DEBIAN_FRONTEND&#x3D;noninteractive &amp;&amp; \</span><br><span class="line">    apt-get -y update &amp;&amp; \</span><br><span class="line">    apt-get -y install build-essential \</span><br><span class="line">        wget \</span><br><span class="line">        git \</span><br><span class="line">        curl \</span><br><span class="line">        unzip \</span><br><span class="line">        automake \</span><br><span class="line">        openjdk-8-jdk \</span><br><span class="line">        openssh-server \</span><br><span class="line">        openssh-client \</span><br><span class="line">        lsof \</span><br><span class="line">        libcupti-dev &amp;&amp; \</span><br><span class="line">    apt-get clean &amp;&amp; \</span><br><span class="line">    rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;*</span><br><span class="line"></span><br><span class="line">RUN wget --quiet https:&#x2F;&#x2F;repo.anaconda.com&#x2F;miniconda&#x2F;Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh &amp;&amp; &#x2F;bin&#x2F;bash Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh -b -p &#x2F;opt&#x2F;miniconda \</span><br><span class="line">&amp;&amp; rm Miniconda3-$&#123;MINICONDA_VERSION&#125;-Linux-x86_64.sh</span><br><span class="line">ENV PATH &#x2F;opt&#x2F;miniconda&#x2F;bin:$PATH</span><br><span class="line">    </span><br><span class="line">RUN wget -qO- http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;hadoop&#x2F;common&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;&#x2F;hadoop-$&#123;HADOOP_VERSION&#125;.tar.gz | \</span><br><span class="line">    tar xz -C &#x2F;usr&#x2F;local &amp;&amp; \</span><br><span class="line">    mv &#x2F;usr&#x2F;local&#x2F;hadoop-$&#123;HADOOP_VERSION&#125; &#x2F;usr&#x2F;local&#x2F;hadoop</span><br><span class="line">    </span><br><span class="line">ENV JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jvm&#x2F;java-8-openjdk-amd64 \</span><br><span class="line">    HADOOP_INSTALL&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop \</span><br><span class="line">    NVIDIA_VISIBLE_DEVICES&#x3D;all</span><br><span class="line"></span><br><span class="line">ENV HADOOP_PREFIX&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_BIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;bin \</span><br><span class="line">    HADOOP_SBIN_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;sbin \</span><br><span class="line">    HADOOP_HDFS_HOME&#x3D;$&#123;HADOOP_INSTALL&#125; \</span><br><span class="line">    HADOOP_COMMON_LIB_NATIVE_DIR&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native \</span><br><span class="line">    HADOOP_OPTS&#x3D;&quot;-Djava.library.path&#x3D;$&#123;HADOOP_INSTALL&#125;&#x2F;lib&#x2F;native&quot;</span><br><span class="line"></span><br><span class="line">ENV PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin:$&#123;HADOOP_BIN_DIR&#125;:$&#123;HADOOP_SBIN_DIR&#125;:$PATH \</span><br><span class="line">LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;extras&#x2F;CUPTI&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib:&#x2F;usr&#x2F;local&#x2F;nvidia&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;lib64:&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;targets&#x2F;x86_64-linux&#x2F;lib&#x2F;stubs:$&#123;JAVA_HOME&#125;&#x2F;jre&#x2F;lib&#x2F;amd64&#x2F;serverCopy</span><br></pre></td></tr></table></figure>

<p>建议先把这一部分进行构建，作为基础镜像，后面要配置其他环境（如安装Pytorch框架登），就不用重复构建这部分，还减少了出错的可能性。这里说一下，启动带CUDA的Docker镜像需要在<code>docker run</code>加上额外的参数<code>--runtime nvidia</code>。</p>
<p>接下来安装深度学习框架。</p>
<h2 id="Configure-PyTorch"><a href="#Configure-PyTorch" class="headerlink" title="Configure PyTorch"></a>Configure PyTorch</h2><p>假设上面的镜像我们命名为xiaoqinfeng/base，那么构建PyTorch的Dockerfile可以像下面这么写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM xiaoqinfeng&#x2F;base</span><br><span class="line"></span><br><span class="line">RUN pip install -U pip</span><br><span class="line">RUN pip install numpy scipy pandas matplotlib tqdm</span><br><span class="line">RUN pip install torch&#x3D;&#x3D;1.5.0+cu101 torchvision&#x3D;&#x3D;0.6.0+cu101 -f https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;torch_stable.htmlCopy</span><br></pre></td></tr></table></figure>

<p>因为这里我用的CUDA10.1，其他版本的CUDA安装指令可能不太一样，具体可以参考<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">官网</a>。</p>
<h2 id="Configure-Tensorflow"><a href="#Configure-Tensorflow" class="headerlink" title="Configure Tensorflow"></a>Configure Tensorflow</h2><p>如果是安装Tensorflow，那么构建Tensorflow的Dockerfile可以像下面这么写：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM xiaoqinfeng&#x2F;base</span><br><span class="line"></span><br><span class="line">RUN pip install -U pip</span><br><span class="line">RUN pip install numpy scipy pandas matplotlib tqdm tensorflow-gpuCopy</span><br></pre></td></tr></table></figure>

<p>这里会自动安装最新版本的Tensorflow2。Tensorflow不同版本对CUDA和cuDNN版本甚至Python版本的支持都不太一样，可以参考<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source#linux">官网</a>的说明。</p>
<h1 id="Deep-Learning-with-OpenPAI"><a href="#Deep-Learning-with-OpenPAI" class="headerlink" title="Deep Learning with OpenPAI"></a>Deep Learning with OpenPAI</h1><h2 id="What-is-OpenPAI"><a href="#What-is-OpenPAI" class="headerlink" title="What is OpenPAI"></a>What is OpenPAI</h2><p>OpenPAI是一个分布式深度学习计算资源管理平台，对于我们用户来说，只需要定义好Docker镜像，然后编写好任务设置，提交到平台之后，平台便会自动分配计算资源来运行任务。</p>
<p>OpenPAI界面：</p>
<img src="https://i.loli.net/2020/06/24/CDrZjgYR7lNcieh.png" style="zoom: 50%;" />

<img src="https://i.loli.net/2020/06/24/FGJpVCbls7YP4aM.png" style="zoom: 50%;" />

<p>下面我们来讲讲怎么向OpenPAI平台提交任务。</p>
<h2 id="Submit-Jobs-to-OpenPAI"><a href="#Submit-Jobs-to-OpenPAI" class="headerlink" title="Submit Jobs to OpenPAI"></a>Submit Jobs to OpenPAI</h2><h3 id="Pack-Code-amp-Data-Files"><a href="#Pack-Code-amp-Data-Files" class="headerlink" title="Pack Code &amp; Data Files"></a>Pack Code &amp; Data Files</h3><p>假设你已经完成了代码的编写和测试，你的目录结构可能看起来是这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── README.md</span><br><span class="line">├── data</span><br><span class="line">│   └── dataset.csv</span><br><span class="line">├── main.py</span><br><span class="line">└── src</span><br><span class="line">    ├── data.py</span><br><span class="line">    └── net.pyCopy</span><br></pre></td></tr></table></figure>

<p>因为OpenPAI会创建一个虚拟容器来运行你的代码，所以你的数据和代码必须要以某种方式传送到OpenPAI上的虚拟容器中。我们先来打包，在代码目录下执行<code>tar -cvf files.tar ./</code>。之后，运行<code>python -m http.server &lt;port&gt;</code>。打开浏览器输入<code>&lt;server_ip&gt;:&lt;port&gt;</code>应该就能看到你的文件了：</p>
<p><img src="https://i.loli.net/2020/06/24/Cbgiw3XKnGYsNHp.png"></p>
<p>由于这个http进程需要一直运行，所以建议使用<code>screen</code>放到后台执行。</p>
<h3 id="Configure-Tasks"><a href="#Configure-Tasks" class="headerlink" title="Configure Tasks"></a>Configure Tasks</h3><p>像OpenPAI提交任务可以采用网页提交也可以使用VSCode插件，这里我们采用网页提交。登入OpenPAI界面，点击Submit Job：</p>
<p><img src="https://i.loli.net/2020/06/24/bhXAcJfOmo8RtT2.png"></p>
<p>可以看到提交任务的界面：</p>
<p><img src="https://i.loli.net/2020/06/24/KME8GuN49gyY1ph.png"></p>
<p>Job name大家可以自己设置。在Command一栏，是执行任务所需的全部命令，首先我们要做的就是将代码数据压缩包下载到容器中并解压：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget &lt;server_ip&gt;:&lt;port&gt;&#x2F;files.tar</span><br><span class="line"></span><br><span class="line">tar -xvf files.tarCopy</span><br></pre></td></tr></table></figure>

<p>然后是运行代码，假设我这里的任务比较简单，只有一行main.py的调用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.pyCopy</span><br></pre></td></tr></table></figure>

<p>如果任务的执行比较复杂，也只需把命令填到Command里即可，OpenPAI会自动执行。接下来是设置配置，可以选GPU的数量，内存大小等等：</p>
<img src="https://i.loli.net/2020/06/24/T7ryOxjpJP4FWYR.png" style="zoom:67%;" />

<p>然后是镜像的选择：</p>
<img src="https://i.loli.net/2020/06/24/3n7SpNlsLjBRvgE.png" style="zoom:67%;" />

<p>要注意在本机上构建好镜像之后，需要把镜像重命名为<code>&lt;repository_address&gt;/&lt;image_name&gt;</code>的格式（我们的<code>&lt;repository&gt;</code>是<code>lin-ai-27:5000</code>，假设我的镜像名是<code>xiaoqinfeng/pytorch</code>，那就是改成<code>lin-ai-27:5000/xiaoqinfeng/pytorch</code>），然后执行<code>docker push</code>推送到Docker镜像服务器上才能在OpenPAI上使用。</p>
<p>提交之后，可以在Jobs界面看到任务的运行情况：</p>
<img src="https://i.loli.net/2020/06/24/ZD4cUgOCIqnyHdB.png" style="zoom: 50%;" />

<h1 id="Misc"><a href="#Misc" class="headerlink" title="Misc"></a>Misc</h1><h2 id="Store-Files-in-Containers"><a href="#Store-Files-in-Containers" class="headerlink" title="Store Files in Containers"></a>Store Files in Containers</h2><p>我们往往需要在程序运行的时候保存文件，如checkpoints等。在OpenPAI上执行程序的话文件是保存在程序中的，如果我们想要在运行完之后把文件复制到本地电脑上呢？这个时候就需要在任务的配置文件里加上复制文件到HDFS的语句。首先确认你的HDFS的URL：如<code>hdfs://172.31.246.52:9000/你的OpenPAI用户名/</code>。</p>
<p>如果要创建文件夹，则可以使用<code>hdfs dfs -mkdir -p &lt;HDFS URL&gt;+&lt;New Folder&gt;</code>。这里<code>&lt;New Folder&gt;</code>是你要创建的的文件夹的路径，用起来和Linux的<code>mkdir</code>命令其实是差不多的。</p>
<p>要复制文件（夹）则使用<code>hdfs dfs -cp &lt;Source Dir&gt; &lt;Dest Dir&gt;</code>。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-02T02:26:13.000Z" title="2020-6-2 10:26:13 ├F10: AM┤">2020-06-02</time>发表</span><span class="level-item"><time dateTime="2020-12-21T14:05:52.434Z" title="2020-12-21 10:05:52 ├F10: PM┤">2020-12-21</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA11-cuDNN8-Tensorflow-Pytorch/">Ubuntu20.04LTS 深度学习环境配置 CUDA11 + cuDNN8 + Tensorflow + Pytorch</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Ubuntu的最新LTS版本也更新到了20.04，在给新机器配置深度学习环境的时候发现比以前容易了许多，特此写一篇Tutorial。这里的安装方法只针对Ubuntu20.04LTS，对于其他版本的系统可能不太适用。</p>
<h1 id="Install-GPU-Drivers"><a href="#Install-GPU-Drivers" class="headerlink" title="Install GPU Drivers"></a>Install GPU Drivers</h1><p>这里假设安装系统之后已经做好了必要的配置（安装常用软件依赖、修改国内源等）。Ubuntu20.04中GPU驱动可以直接通过GUI界面安装，十分方便，方法是找到软件与更新 (Software &amp; Updates)，在附加驱动 (additional drivers) 选项卡中选择驱动版本，一般是选择“专有，tested” (proprietary, tested) 那个，之后点Apply Changes，重启。</p>
<p><img src="https://i.loli.net/2020/06/24/xevtVosYOH7D68I.png"></p>
<p>如果是服务器的话，也可以采用命令行的安装方法。在命令行输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu-drivers devices</span><br></pre></td></tr></table></figure>

<p>会列出驱动信息：</p>
<p><img src="https://i.loli.net/2020/12/19/CGWnoRFPz23r46K.png" alt="image-20201219103330645"></p>
<p>可以看到推荐驱动版本是455，直接使用命令<code>ubuntu-drivers autoinstall</code>就可以自动安装推荐版本的驱动了。</p>
<h1 id="Install-CUDA-amp-cuDNN"><a href="#Install-CUDA-amp-cuDNN" class="headerlink" title="Install CUDA &amp; cuDNN"></a>Install CUDA &amp; cuDNN</h1><p>最常用的方式是按照[官网](<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-11.0-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal">CUDA Toolkit 11.0 Download | NVIDIA Developer</a>)的指导来安装CUDA，之后需要设置环境变量，在<code>~/.bashrc</code>中添加下列指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-11.0/bin<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-11.0/lib64\</span><br><span class="line">                         <span class="variable">$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span></span><br></pre></td></tr></table></figure>

<p>还可以选择简单的方式，直接使用apt-get来安装CUDA：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install nvidia-cuda-toolkit</span><br></pre></td></tr></table></figure>

<p>不过这样会直接安装最新的CUDA版本，而PyTorch等深度学习框架不一定对最新的CUDA版本都有很好的支持，所以不推荐这种方式。</p>
<p>安装完CUDA之后输入<code>nvcc --version</code>可以测试是否安装成功，输入<code>nvidia-smi</code>可以看到GPU信息和CUDA版本。</p>
<p>之后安装cuDNN，进入<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cudnn">官网</a>，选择Download cuDNN：</p>
<p><img src="https://i.loli.net/2020/06/24/VYBRoINFDC9ObA1.png"></p>
<p>会要求登录，如果没有账号的注册一个即可。在这里根据CUDA版本选择适合的cuDNN，我这里是CUDA10.2。我们选择deb包的方式安装，下载下图中圈出来的三个deb包，依次用<code>sudo dpkg -i xxx.deb</code>命令安装。</p>
<p><img src="https://i.loli.net/2020/06/24/qcDG5t3JY6vfoQM.png"></p>
<h1 id="Configure-Python"><a href="#Configure-Python" class="headerlink" title="Configure Python"></a>Configure Python</h1><p>为了更好地管理Python包和虚拟环境，我们需要安装Anaconda。使用Anaconda之后，我们可以创建虚拟环境，虚拟环境之间互不干扰。做科学实验我们一般需要安装大量的Python包，有的包之间甚至还有冲突，如果我们把他们都安装在同一个环境下就会难以管理，甚至出冲突。而有了虚拟环境之后，我们可以把不同需求放在不同虚拟环境中，比如深度学习开发放在一个虚拟环境中（安装Tensorflow等），网站开发放在一个虚拟环境中（安装Flask等）。Anaconda默认自带大量的包，不过我们一般会创建新的虚拟环境去安装新的包，所以这里我们选用Miniconda。Miniconda和Anaconda唯一的区别是不会自带大量Python包，这里大家自行选择。Anaconda国内镜像下载地址为：<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/%EF%BC%8CMiniconda%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80%E4%B8%BA%EF%BC%9Ahttps://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/%E3%80%82">https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/，Miniconda国内镜像下载地址为：https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/。</a></p>
<p>比如说我们下载的是<code>Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>，执行<code>bash Miniconda3-py38_4.8.2-Linux-x86_64.sh</code>即可安装。显示一大屏用户协议哪儿按<code>q</code>可以直接跳过，其他选项的默认的输入<code>yes</code>即可。在提示是否需要conda init的时候记得输入<code>yes</code>。</p>
<p>安装成功之后，重开一个终端，可以看到现在处于<code>base</code>环境中：</p>
<p><img src="https://i.loli.net/2020/06/24/giml1UKzWuyqTjr.png"></p>
<p>我们先配置一下国内镜像，执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.condarc</span><br></pre></td></tr></table></figure>

<p>然后粘贴下列文本使用清华源：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;r</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;pro</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  msys2: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  bioconda: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  menpo: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  pytorch: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  simpleitk: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br></pre></td></tr></table></figure>

<p>之后我们输入<code>conda create -n &lt;your_name&gt;</code>创建一个新的虚拟环境，如果需要指定Python版本，则<code>conda create --n &lt;your_name&gt; python=&lt;python_version&gt;</code>。之后输入<code>conda activate &lt;your_name&gt;</code>进入虚拟环境，如果需要退出，则使用<code>conda deactivate</code>。</p>
<p>安装常用包：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install --yes numpy scipy pandas matplotlib tqdm pip jupyter</span><br></pre></td></tr></table></figure>

<p><code>--yes</code>的作用是手动输入<code>y</code>来确认是否安装，这里列出的是一些最常用的Python包，大家可以根据自己的需求自行调整。<code>conda install</code>为Anaconda中安装Python包的方式。</p>
<h1 id="Install-PyTorch"><a href="#Install-PyTorch" class="headerlink" title="Install PyTorch"></a>Install PyTorch</h1><p>这里来安装PyTorch环境，推荐使用<code>conda create -n pytorch</code>创建一个专有虚拟环境，然后使用<code>conda install</code>安装常用包。对于安装PyTorch，我们可以使用<code>conda</code>也可以使用<code>pip</code>安装。<code>pip</code>是另外一个安装Python包的工具，由于不检查依赖所以比<code>conda</code>安装速度快，而且包的数量比<code>conda</code>多，使用也更广泛。同样<code>pip</code>也可以使用国内镜像加速下载，详见<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/%E3%80%82">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/。</a></p>
<p>对于CUDA11.0，官方给出的用<code>conda</code>安装PyTorch的命令（CUDA11.1也是这个）是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch</span><br></pre></td></tr></table></figure>

<p>用<code>pip</code>安装PyTorch的命令是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html</span><br></pre></td></tr></table></figure>

<p>对于其他版本的CUDA安装命令可能不一样，可以去<a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally/">官网</a>查看。</p>
<p>可以使用以下命令来测试GPU版本的PyTorch是否正常工作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import torch; print(torch.cuda.is_available())&quot;</span></span><br></pre></td></tr></table></figure>



<h1 id="Install-Tensorflow"><a href="#Install-Tensorflow" class="headerlink" title="Install Tensorflow"></a>Install Tensorflow</h1><p>安装Tensorflow环境同样推荐创建一个专有虚拟环境。对于Tensorflow2的安装，使用<code>pip</code>十分方便，使用</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorflow tensorflow-gpu</span><br></pre></td></tr></table></figure>

<p>即可。要安装其他版本的Tensorflow可以使用<code>pip install tensorflow==&lt;tf_version&gt; tensorflow-gpu==&lt;tf_version&gt;</code>来指定版本。不过不同版本的Tensorflow要求的CUDA版本都有所不同，可以参考<a target="_blank" rel="noopener" href="https://www.tensorflow.org/install/source#linux">官网</a>的说明。</p>
<p>可以使用以下命令来测试GPU版本的Tensorflow是否正常工作：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c <span class="string">&quot;import tensorflow as tf; tf.config.list_physical_devices(&#x27;GPU&#x27;)&quot;</span></span><br></pre></td></tr></table></figure>

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-01T15:17:03.000Z" title="2020-6-1 11:17:03 ├F10: PM┤">2020-06-01</time>发表</span><span class="level-item"><time dateTime="2020-06-26T16:25:25.079Z" title="2020-6-27 12:25:25 ├F10: AM┤">2020-06-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/">Deep Anomaly Detection Using Geometric Transformations</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文考虑图像数据的异常检测问题。与基于重构的方法不同，本文提出的方法通过对正常图片施加不同的几何变换之后，训练一个多分类器将无监督异常检测问题转化为一个有监督问题。本方法背后的直觉是在训练能够分辨不同变换后的图片之后，分类器一定学得了一些显著的几何特征，这些几何特征是正常类别独有的。</p>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>本文考虑针对图像的异常检测。记$\mathcal X$为所有自然图像的空间，$X\subseteq\mathcal X$为正常图像集合。给定数据集$S\subseteq X$，异常检测的目的是学习一个分类器$h_S(x):\mathcal X\rightarrow{0,1}$，其中$h_S(x)=1\Leftrightarrow x\in X$。</p>
<p>为了兼顾查准率和查全率，常用的设置是学习一个打分函数$n_S(x):\mathcal X\rightarrow\mathbb R$，分数越高代表样本属于$X$的概率越大。之后，通过设定阈值，便可以构建异常分类器：<br>$$<br>\begin{align}<br>h_S^\lambda(x)=<br>\begin{cases}<br>1 &amp; n_S(x)\leq\lambda\<br>0 &amp; n_S(x)&lt;\lambda<br>\end{cases}<br>\end{align}<br>$$</p>
<h2 id="Discriminative-Learning-of-an-Anomaly-Scoring-Function-Using-Geometric-Transformations"><a href="#Discriminative-Learning-of-an-Anomaly-Scoring-Function-Using-Geometric-Transformations" class="headerlink" title="Discriminative Learning of an Anomaly Scoring Function Using Geometric Transformations"></a>Discriminative Learning of an Anomaly Scoring Function Using Geometric Transformations</h2><p>有初始数据集$S$，几何变换集合$\mathcal T$，通过对$S$中每个样本施加这$|\mathcal T|$个几何变换得到新数据集记为$S_\mathcal{T}$，且$S_\mathcal{T}$中每个样本的标签为变换的序号。之后，在$S_\mathcal{T}$上训练一个$|\mathcal T|$分类器。在测试阶段，对测试样本同样施加$|\mathcal T|$个几何变换，分类器会给出经过$\mathrm{softmax}$的输出向量，最终的异常分数由经过输出的向量构造的分布对数似然得来。</p>
<h3 id="Creating-and-Learning-the-Self-Labeled-Dataset"><a href="#Creating-and-Learning-the-Self-Labeled-Dataset" class="headerlink" title="Creating and Learning the Self-Labeled Dataset"></a>Creating and Learning the Self-Labeled Dataset</h3><p>设$\mathcal T={T_0,T_1,\cdots,T_{k-1}}$为几何变换集合，$1\leq i\leq k-1,\space T_i:\mathcal X\rightarrow \mathcal X$，且$T_0(x)=x$。$S_\mathcal{T}$定义为：</p>
<p>$$<br>S_\mathcal T={(T_j(x),j):x\in S,T_j\in\mathcal T}<br>$$<br>对于每个$x\in S$，$j$为$T_j(x)$的标签。我们直接学习一个$K$类分类器$f_\theta$，来预测输入样本对应的几何变换种类，这相当于是一个图像分类问题。</p>
<img src="https://i.loli.net/2020/06/24/XBFKcPio64u3U1C.png" style="zoom:67%;" />

<h3 id="Dirichlet-Normality-Score"><a href="#Dirichlet-Normality-Score" class="headerlink" title="Dirichlet Normality Score"></a>Dirichlet Normality Score</h3><p>接下来要做的是如何定义异常分数，记为$n_S(x)$，这是文中的一个重要的部分。设几何变换集合$\mathcal T={T_0,T_1,\cdots,T_{k-1}}$，且$k$分类器$f_\theta$在$S_\mathcal{T}$上完成训练。对于任意一个样本$x$，令$\mathbf y(x)=\text{softmax}(f_{\theta}(x))$，即分类器$f_\theta$输出的$\text{softmax}$之后的向量。异常分数$n_S(x)$定义为：</p>
<p>$$<br>n_S(x)=\sum\limits_{i=0}^{k-1}\log p(\mathbf y(T_i(x))|T_i)<br>$$</p>
<p>该异常分数定义为每个类别上，在几何变换$T_i$的条件下，输出的$\mathbf y$的对数似然之和。在文中，作者假设$\mathbf y(T_i(x)|T_i$服从迪利克雷分布：$\mathbf y(T_i(x))|T_i\sim\text{Dir}(\boldsymbol \alpha_i)$，其中$\boldsymbol \alpha_i\in\mathbb R^k_+$，$x\sim p_X(x)$，$i\sim\text{Uni}(0,k-1)$，而$p_X(x)$代表正常样本的真实数据分布。于是：</p>
<p>$$<br>n_S(x)=\sum_{i=0}^{k-1}\left[\log\Gamma(\sum_{j=0}^{k-1}[\tilde{\boldsymbol\alpha}<em>i]<em>j)-\sum</em>{j=0}^{k-1}\log\Gamma([\tilde{\boldsymbol\alpha}_i]_j)+\sum</em>{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]_j-1)\log\mathbf y(T_i(x))_j\right]<br>$$</p>
<p>因为$\tilde{\alpha}<em>i$相对于$x$来说是常数，所以可以直接忽略，于是式子简化为：<br>$$<br>n_S(x)=\sum_{i=0}^{k-1}\sum</em>{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]<em>j-1)\log\mathbf y(T_i(x))_j=\sum</em>{i=0}^{k-1}(\tilde{\boldsymbol \alpha}_i-1)\cdot\log\mathbf y(T_i(x))<br>$$</p>
<p>注意这里的每个$\boldsymbol \alpha_i$都是一个向量，即对于每个变换$i$，都对应一个迪利克雷分布，其参数为$\boldsymbol\alpha_i$；在对训练集进行第$i$个几何变换之后，我们得到了${T_i(x)}$，然后分类器$f_\theta(\cdot)$的输出$\mathbf y(T_i(x))$相当于迪利克雷分布的观测值，我们需要根据观测值来估计参数$\boldsymbol \alpha_i$，然后根据这个参数来计算$n_S(x)$。对于$\boldsymbol\alpha_i$，可以知道其第$i$个分量应该是相对比较大的，下面是运行官方代码得到的$\boldsymbol\alpha_i$的结果（$i=69$，$i$从$0$开始，总共为$72$维），可以看到第$69$个分量是最大的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[INFO] value of mle_alpha_t:</span><br><span class="line"> [ 0.10228925  0.08997199  0.13083569  0.10862965  0.09811163  0.08527119</span><br><span class="line">  0.17637901  0.27628416  0.12873376  0.19197053  0.11587154  0.09873095</span><br><span class="line">  0.12700618  0.07688542  0.10488203  0.12499191  0.11637607  0.07739511</span><br><span class="line">  0.13049147  0.51031647  0.20546597  0.15558449  0.09288609  0.12134945</span><br><span class="line">  0.09324992  0.14650162  0.16281216  0.11827823  0.08214853  0.15618336</span><br><span class="line">  0.28129761  0.45293697  0.11485838  1.78598954  0.16556983  0.1141158</span><br><span class="line">  0.10909459  0.13916602  0.11563799  0.07309986  0.11049714  0.12974086</span><br><span class="line">  0.15930642  0.13714361  0.13938356  0.70619553  0.11174039  0.07201538</span><br><span class="line">  0.16626109  0.12153727  0.09548811  0.07940956  0.15832209  0.11035474</span><br><span class="line">  0.12487912  0.16937875  0.23212662  0.37041831  0.08557451  0.0839439</span><br><span class="line">  0.09924258  0.39766872  0.14917286  0.08704662  0.09554555  0.31047109</span><br><span class="line">  0.24504759  0.16812463  0.11508187 63.98878807  0.12971073  0.07972932]</span><br></pre></td></tr></table></figure>

<p>下图也展示了对于每个变换$i$，$\mathbf y(T_i(x)|T_i$分布的情况：</p>
<img src="https://i.loli.net/2020/06/23/fLkst4i7Hu6PhQl.png" style="zoom:67%;" />

<p>作者还给出了一种简化的形式，$\hat{n}<em>S(x)=\frac{1}{k}\sum^{k-1}</em>{j=0}[\mathbf y(T_j(x))]_j$。相当于说，对于每个变换$T_i$分类器都会给出一个$\text{softmax}$向量，取其第$i$个分量$[\mathbf y(T_j(x))]_j$，然后把每个变换对应的$[\mathbf y(T_j(x))]_j$加起来。</p>
<p>整个算法的流程如下：</p>
<img src="https://i.loli.net/2020/06/23/z8MpdeoD6ZGavlN.png" style="zoom:67%;" />

<p>这里结合作者的源代码简单说一下检测阶段的流程。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t_ind <span class="keyword">in</span> <span class="built_in">range</span>(transformer.n_transforms):</span><br><span class="line">        observed_dirichlet = mdl.predict(transformer.transform_batch(observed_data, [t_ind] * <span class="built_in">len</span>(observed_data)), batch_size=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>

<p>在训练好模型之后，对于训练集的所有样本，对其进行$K$个几何变换之后，得到$K$个样本${T_i(x)}$，对于所有第$i$个几何变换对应的样本${T_i(x)}$，通过分类器$f_\theta$会给出输出$\mathbf y(T_i(x))$。这里对应算法中的第$7-8$行，这个<code>observed_dirichlet</code>就是$S_i$。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log_p_hat_train = np.log(observed_dirichlet).mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">alpha_sum_approx = calc_approx_alpha_sum(observed_dirichlet)</span><br><span class="line">alpha_0 = observed_dirichlet.mean(axis=<span class="number">0</span>) * alpha_sum_approx</span><br></pre></td></tr></table></figure>

<p>之后这部分主要对应算法中的$9-11$行。作者把所有的第$i$个变换，分类器的输出的集合（也就是变量<code>observed_dirichlet</code>）记为$S_i$，$\bar s$为$S_i$的平均，$\bar l$为$S_i$对数的平均（变量<code>log_p_hat_train</code>），初始值$\tilde{\alpha}_i$由$\bar s\frac{(k-1)(-\Psi(1))}{\bar s\cdot\log\bar s-\bar s\cdot\bar l}$给出（变量<code>alpha_0</code>）。函数<code>calc_approx_alpha_sum</code>实现的是算法中第$11$行的$\frac{(k-1)(-\Psi(1))}{\bar s\cdot\log\bar s-\bar s\cdot\bar l}$，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_approx_alpha_sum</span>(<span class="params">observations</span>):</span></span><br><span class="line">    N = <span class="built_in">len</span>(observations)</span><br><span class="line">    f = np.mean(observations, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (N * (<span class="built_in">len</span>(f) - <span class="number">1</span>) * (-psi(<span class="number">1</span>))) / (</span><br><span class="line">        N * np.<span class="built_in">sum</span>(f * np.log(f)) - np.<span class="built_in">sum</span>(f * np.<span class="built_in">sum</span>(np.log(observations), axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>

<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mle_alpha_t = fixed_point_dirichlet_mle(alpha_0, log_p_hat_train)</span><br></pre></td></tr></table></figure>

<p>这里对应算法中的$12-14$行，即重复$\tilde\alpha_i\leftarrow\Psi^{-1}\left(\Psi(\sum_j[\alpha_i]_j)+\bar l\right)$来估计$\alpha$，函数<code>fixed_point_dirichlet_mle</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fixed_point_dirichlet_mle</span>(<span class="params">alpha_init, log_p_hat, max_iter=<span class="number">1000</span></span>):</span></span><br><span class="line">    alpha_new = alpha_old = alpha_init</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        alpha_new = inv_psi(psi(np.<span class="built_in">sum</span>(alpha_old)) + log_p_hat)</span><br><span class="line">        <span class="keyword">if</span> np.sqrt(np.<span class="built_in">sum</span>((alpha_old - alpha_new) ** <span class="number">2</span>)) &lt; <span class="number">1e-9</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        alpha_old = alpha_new</span><br><span class="line">    <span class="keyword">return</span> alpha_new</span><br></pre></td></tr></table></figure>

<p>$\Psi^{-1}(\cdot)$是通过数值方法来估计的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inv_psi</span>(<span class="params">y, iters=<span class="number">5</span></span>):</span></span><br><span class="line">    <span class="comment"># initial estimate</span></span><br><span class="line">    cond = y &gt;= <span class="number">-2.22</span></span><br><span class="line">    x = cond * (np.exp(y) + <span class="number">0.5</span>) + (<span class="number">1</span> - cond) * <span class="number">-1</span> / (y - psi(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        x = x - (psi(x) - y) / polygamma(<span class="number">1</span>, x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<hr>
<p>最后，在得到对$\alpha$的估计之后，可以来计算测试样本的分数了。这里对应的是算法中的第$16$行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_test_p = mdl.predict(transformer.transform_batch(x_test, [t_ind] * <span class="built_in">len</span>(x_test)), batch_size=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">scores += dirichlet_normality_score(mle_alpha_t, x_test_p)</span><br></pre></td></tr></table></figure>

<p>函数<code>dirichlet_normality_score</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dirichlet_normality_score</span>(<span class="params">alpha, p</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>((alpha - <span class="number">1</span>) * np.log(p), axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h2><p>文中用到了如下的Baseline：</p>
<ul>
<li><strong>One-class SVM. **单类支持向量机，作者使用了三个变体，分别为</strong>RAW-OC-SVM<strong>——使用原始数据作为输入，</strong>CAE-OC-SVM<strong>——使用一个卷积自编码器来获得低维表示作为输入和</strong>E2E-OC-SVM<strong>——全名为</strong>One-Class Deep Support Vector Data Description**；</li>
<li>**Deep structured energy-based models. **</li>
<li>**Deep Autoencoding Gaussian Mixture Model. **</li>
<li>**Generative Adversarial Networks. **</li>
</ul>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>文中用到了一下几个数据集：</p>
<ul>
<li><strong>CIFAR-10</strong></li>
<li><strong>CIFAR-100</strong></li>
<li><strong>Fashion-MNIST</strong></li>
<li><strong>CatsVsDogs</strong></li>
</ul>
<p>在实验中所有图片都被归一化到$[-1,1]$的范围。</p>
<h2 id="Experimental-Protocol"><a href="#Experimental-Protocol" class="headerlink" title="Experimental Protocol"></a>Experimental Protocol</h2><p>设数据集有$C$个类，我们会进行$C$次实验，在第$c$次实验 ($1\leq c \leq C$)中我们会将第$c$个类作为正常样本，而其他类作为异常样本。在训练阶段，训练集只包含正常样本，而在测试阶段则会有正常样本和异常样本。在获得异常分数之后，阈值$\lambda$则根据ROC曲线下面积选择。</p>
<p>实验中使用的几何变换基于以下三种基变换：</p>
<ul>
<li>**Horizontal flip: ** 记为$T_b^{flip}(x)$，$b\in{T,F}$代表是否翻转；</li>
<li>**Translation: ** 记为$T_{s_h,s_w}^{trans}(x)$，其中$s_h,s_w\in{-1,0,1}$。在长宽两个维度上位移分别为$0.25$高度和$0.25$宽度，这两个维度发生位移的方向由$s_h$和$s_w$决定，当$s_h=s_w=0$时代表不移动；</li>
<li>**Rotation by multiples 90 degrees: ** 记为$T_k^{rot}(x)$，$k\in{0,1,2,3}$。旋转$k\times90$度。</li>
</ul>
<p>将三种基变换叠加有：<br>$$<br>\mathcal T=\left{<br> T_k^{rot}\circ T_{s_h,s_w}^{trans}\circ T_b^{flip} : \begin{matrix}<br> b &amp;\in {T,F}\<br> s_h,s_w&amp;\in{-1,0,1}\<br> k&amp;\in{0,1,2,3}<br> \end{matrix}<br>\right}<br>$$<br>最终几何变换种数为$2\times3\times3\times4=72$种。</p>
<p>分类器模型使用的是<strong>Wide Residual Network</strong>，优化器为Adam，Batch size为128，训练轮数为200。</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>下面是不同方法在不同数据集上的实验结果：</p>
<p><img src="https://i.loli.net/2020/06/24/gHZohrvz9MGYmxi.png"></p>
<p>评测标准使用的是AUROC。作者关于结果的分析主要有以下几点：</p>
<ol>
<li>在绝大多数情况下，我们的算法都比Baseline要好，而且是越大的数据集效果越好。CatsVsDogs数据集每张图片的大小比其他几个数据集都要大，而Baseline在这个数据集上的结果都在$50%$或不到$50%$，这基本等同于瞎猜；</li>
<li>在CIFAR-100数据集里，由于将这100类聚合为了20类，所以存在类内样本差异大的问题。比如在类$5$、类$7$和类$13$上，模型表现就不够好；</li>
<li>在Fashion-MNIST数据集上几乎所有方法（除了DAGMM）都表现很好。</li>
</ol>
<h2 id="On-the-Intuition-for-Using-Geometric-Transformations"><a href="#On-the-Intuition-for-Using-Geometric-Transformations" class="headerlink" title="On the Intuition for Using Geometric Transformations"></a>On the Intuition for Using Geometric Transformations</h2><p>这里作者对所选用的几何变换做了一些解释。实验中选用的三种基本几何变换都是可逆的线性几何变换（且为双射），作者也试过一些复杂的非线性变换，如高斯模糊、锐化、伽马校正等等，但是效果并不好。</p>
<p>作者认为分类器能够分辨不同变换的能力与最终性能成正比，为了验证这一点，进行了$3$个实验。从MNIST数据集选择一个数字作为正常样本，几何变换只采用两个，然后选择另一个数字作为异常样本，结果如下：</p>
<ul>
<li>**Normal digit: 8，Anomaly: 3，Transformations: Identity and horizontal flip. **由于数字$8$是对称的，所以要让分类器分辨原始的$8$和翻转之后的$8$是很难的，AUROC只有$0.646$；</li>
<li>**Normal digit: 3，Anomaly: 8，Transformations: Identity and horizontal flip. **这里把$3$作为正常样本，由于$3$不是对称的，所以两种变换是可以分辨的，AUROC达到了$0.957$；</li>
<li>**Normal digit: 8，Anomaly: 3，Transformations: Identity and translation by 7 pixels. **同样是把$8$作为正常样本，但变换用的是平移，AUROC达到了$0.919$。</li>
</ul>
<p>除此之外，作者还设计了一个实验，目的是测试什么样的图像会获得较高的分数$n_S(x)$。在给定训练好的分类器的情况下，优化输入的图像，目标函数是最大化分数$n_S(x)$。下图为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/24/8Pi4EKCRuBXrYgp.png"></p>
<p>在左图中，将数字$3$作为正常样本训练的分类器、原始输入为数字$0$的图片时，随着优化的进行，图片慢慢地变得像数字$3$。在右图中，同样是将数字$3$作为正常样本训练的分类器，不过原始输入也是数字$3$，这时图像却没有怎么变化。</p>
<h1 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h1><ul>
<li>文中提到的在CIFAR100数据的实验上，由于类间差异比较大导致效果较差，那么很自然地，不同的变换样本对应的集簇实际上应当足够分开，集簇内的样本要足够进，这样对于分类器来说才能比较好的分类。不过采用的几何变换并没有针对这一点进行特别设计；</li>
<li>文中强调了所使用的变换为几何变换，其实除此之外，所使用的变换还都是可以用矩阵表示的可逆的变换。</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-01T08:14:08.000Z" title="2020-6-1 4:14:08 ├F10: PM┤">2020-06-01</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:28:22.135Z" title="2020-6-25 1:28:22 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/06/01/Cross-dataset-Time-Series-Anomaly-Detection-for-Cloud-Systems/">Cross-dataset Time Series Anomaly Detection for Cloud Systems</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文介绍了一种用于云计算平台的时间序列异常检测框架。为了解决标签不足的问题，文中使用了迁移学习的方法，即在有标签的source domain上训练模型，在没有标签的target domain上检测。同时，文中还使用了主动学习的方法来挑选最有价值的无标签样本进行标记。</p>
<p><a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/atc19-zhang-xu.pdf">📰Get Paper</a></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>针对云计算平台数据的异常检测通常是应用在云监控数据，如KPI、CPU使用率、系统负载等时序数据上。和传统的异常检测不一样的是，时序异常检测往往更难，文中总结了以下几个挑战：</p>
<ul>
<li>异常特征的差异性。在不同的云服务系统中，对异常的容忍度是不同的，所以对每个场景或系统组件设置准确的阈值来进行异常检测是十分困难的；</li>
<li>时间依赖性。该异常检测问题处理的是时间序列数据，而传统的异常检测并不会考虑时间依赖性；</li>
<li>无监督学习的性能问题。无监督的异常检测方法的性能有限，会带来大量的误报；</li>
<li>有监督学习需要大量标签。</li>
</ul>
<h1 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h1><p>为了解决上述挑战，文中提出了一个时间序列异常检测框架ATAD (Active Transfer Anomaly Detection)。该框架结合了迁移学习技术和主动学习技术，示意图如下：</p>
<img src="https://i.loli.net/2020/06/25/jOB4rC2gnH9VcQW.png" style="zoom:67%;" />

<p>未标记数据$T_u$是我们要检测的目标数据 (target domain)，标记数据$T_l$是我们的源数据 (source domain)，可以是开源数据或者是其他系统的监控数据。</p>
<h2 id="Transfer-Learning-Component"><a href="#Transfer-Learning-Component" class="headerlink" title="Transfer Learning Component"></a>Transfer Learning Component</h2><p>在应用迁移学习时，我们需要考虑以下几个因素：</p>
<ul>
<li>我们处理的是时间序列数据，即在不同的时间点上样本之间不是相互独立的。为了解决这个问题，我们提取了不同的特征，每一个时间点被转换为了高维的特征向量，且每个时间点附近的背景信息被保存在了特征向量之中；</li>
<li>时间序列的粒度。粗粒度的迁移学习不利于发现异常，本文采用细粒度，即数据点级别的迁移学习；</li>
<li>迁移学习需要source domain和target domain具有潜在的相似性，所以我们需要对source domain中的样本进行过滤。</li>
</ul>
<img src="https://i.loli.net/2020/06/25/aM7Qvt6DwGXnThm.png" style="zoom:67%;" />

<h3 id="Feature-Identification"><a href="#Feature-Identification" class="headerlink" title="Feature Identification"></a>Feature Identification</h3><p>这一节描述特征工程中用到的特征。在提取特征之前，文中使用了离散傅里叶变换来识别时间序列的周期$p$，并为后面滑动窗口的大小原则作参考。</p>
<h4 id="Statistical-Features"><a href="#Statistical-Features" class="headerlink" title="Statistical Features"></a>Statistical Features</h4><p>统计特征包含了一些基本的统计信息，如均值、方差等，用到的特征如下表所示：</p>
<img src="https://i.loli.net/2020/06/25/jI9EbCy1XueDViw.png" style="zoom:67%;" />

<p>表中的统计特征都是基于大小等于周期$p$的滑动窗口的。</p>
<h4 id="Forecasting-Error-Features"><a href="#Forecasting-Error-Features" class="headerlink" title="Forecasting Error Features"></a>Forecasting Error Features</h4><p>使用预测特征的理由是如果一个数据点偏离预测值很远，那么它很有可能是异常。文中使用了多种时间序列预测模型，如SARIMA、Holt、Holt-Winters、STL等。最终的预测结果使用下式来加权集成：<br>$$<br>\hat{Y}<em>t=\sum\limits</em>{m=1}^{M}\frac{\hat{Y}<em>{m,t}}{M-1}\left(1-\frac{RMSE</em>{m,t}}{\sum\limits_{n=1}^M RMSE_{n,t}}\right)<br>$$<br>$M$代表$M$个不同模型，$RMSE_{m,t}$代表模型$m$在时间$t$的$RMSE$，$\hat{Y}_t$是在时间$t$的最终预测结果。之后，使用下表中的Metrics来计算不同预测特征：</p>
<img src="https://i.loli.net/2020/06/25/wRmfHj5xFcsLIXp.png" style="zoom:67%;" />

<p>同样的，上述特征都是基于窗口的。</p>
<h4 id="Temporal-Features"><a href="#Temporal-Features" class="headerlink" title="Temporal Features"></a>Temporal Features</h4><p>这一部分是一些时间序列相关特征：</p>
<img src="https://i.loli.net/2020/06/25/mnBrzjfgV716yiR.png" style="zoom:67%;" />

<p>最后，总共提取了37个特征，并且每个特征都进行了正则化。</p>
<h3 id="The-Transfer-between-Source-Domain-and-Target-Domain"><a href="#The-Transfer-between-Source-Domain-and-Target-Domain" class="headerlink" title="The Transfer between Source Domain and Target Domain"></a>The Transfer between Source Domain and Target Domain</h3><p>本文结合了基于实例的迁移学习(<strong>Instance-based Transfer Learning</strong>)和基于特征的迁移学习(<strong>Feature-based Transfer Learning</strong>)。</p>
<p>首先，source domain中的数据差异性是比较大的，所以我们需要选择与target domain相似的样本。</p>
<p>基于实例的迁移学习(<strong>Instance-based Transfer Learning</strong>)的思想是选择source domain中与target domain相似的样本。对于source domain，在将时间序列$T_l$转换为特征$F_l$之后，本文使用$K-means$算法将$F_l$分成若干个簇。每个簇$F_l^i, i\in[1,K]$是$F_l$的不重叠子集。为了选择合适的样本，我们计算了target domain中的样本和每个簇中心点的欧几里得距离，然后样本会和距离最近的簇$F_l^i$联系起来。</p>
<p>之后，为了使source domain和target domain在特征空间的差别更小，作者在每个簇上使用了<strong>CORrelation ALignment</strong> (CORAL) 算法。CORAL是一种领域适应算法 (<strong>Domain Adaption</strong>)，其基本思想是对source domain和target domain进行线性变换使其二阶统计信息（即协方差矩阵）的差别最小化：<br>$$<br>\min_A\parallel A^\top C^i_lA-C^i_u\parallel_F^2<br>$$</p>
<p>在最后一步，作者在每一个sub source domain $\hat{F}_l^i$训练了有监督模型（随机森林或SVM），所以最后我们得到了$K$个基模型。</p>
<h2 id="Active-Learning-Component"><a href="#Active-Learning-Component" class="headerlink" title="Active Learning Component"></a>Active Learning Component</h2><p>由于数据的差异性和复杂性太大，仅仅使用迁移学习的技术不足以达到很好的效果。在ATAD中，作者使用了主动学习技术来用较少的成本标注最有价值的样本来提升性能。本文中使用基于<strong>Uncertainty</strong>和<strong>Context Diversity</strong>的主动学习。</p>
<h3 id="Uncertainty"><a href="#Uncertainty" class="headerlink" title="Uncertainty"></a>Uncertainty</h3><p>大多数主动学习算法使用不确定性 (Uncertainty) 来作为选择要标记的样本的准则。<br>$$<br>Uncertainty=-|Prob(Normal)-Prob(Anomaly)|<br>$$<br>其中的$Prob$由基模型给出。</p>
<h3 id="Context-Diversity"><a href="#Context-Diversity" class="headerlink" title="Context Diversity"></a>Context Diversity</h3><p>多样性 (Diversity) 也是一个选择要标记样本的重要参考。如果有两个相似的样本，那么就没有必要将他们都标记。</p>
<p>时间上相邻的样本往往也是相似的。</p>
<p>具体的来说，我们对所有样本按照<strong>Uncertainty</strong>排序，然后进行一次扫描，如果当前样本在候选集中某个样本的<strong>Context</strong>之中，我们则忽略当前样本，因为这代表当前样本和候选集中的那个样本是相似的。如果不在<strong>Context</strong>之中，我们则将该样本加入候选集中。</p>
<p>判断是否在某个样本的<strong>Context</strong>中，如下图所示，直接判断是否落在区间$[t-\alpha,t+\alpha]$中就是了。</p>
<img src="https://i.loli.net/2020/06/25/nci9PvGDEdjky5R.png" style="zoom:67%;" />

<p>主动学习模块的算法流程图如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/RqonKfQS3IWw6Gb.png" style="zoom: 80%;" />

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>在实验部分，作者试图回答以下问题：</p>
<ol>
<li>ATAD的效果如何？</li>
<li>迁移学习模块的有效性如何？</li>
<li>主动学习模块的有效性如何？</li>
<li>ATAD在基于公开数据时对公司内部数据检测效果如何？</li>
</ol>
<h2 id="Dataset-and-Setup"><a href="#Dataset-and-Setup" class="headerlink" title="Dataset and Setup"></a>Dataset and Setup</h2><p>下表是用到的数据集的一些基本信息：</p>
<img src="https://i.loli.net/2020/06/25/HDNGCrYOxezLwaB.png" style="zoom:67%;" />

<h2 id="Evaluation-Metric"><a href="#Evaluation-Metric" class="headerlink" title="Evaluation Metric"></a>Evaluation Metric</h2><p>评测标准使用的是F1-score：<br>$$<br>F1=\frac{2\cdot P\cdot R}{P+R}, \space P=\frac{TP}{TP+FP}, \space R=\frac{TP}{TP+FN}<br>$$</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="RQ1-How-effective-is-ATAD"><a href="#RQ1-How-effective-is-ATAD" class="headerlink" title="RQ1: How effective is ATAD?"></a>RQ1: How effective is ATAD?</h3><p>Baseline包括孤立森林、K-Sigma、S-H-ESD和随机森林。</p>
<p>最终结果如下表所示：</p>
<img src="https://i.loli.net/2020/06/25/HMneBzlkR7Qg4TN.png" style="zoom:67%;" />

<p>为了评测ATAD利用标签的能力，我们比较了RF在达到和ATAD相似F1 score情况下所需标签的数量，如下表所示：</p>
<img src="https://i.loli.net/2020/06/25/fcoXhCL43yVwYes.png" style="zoom:67%;" />

<h3 id="RQ2-How-effective-is-the-Transfer-Learning-Component"><a href="#RQ2-How-effective-is-the-Transfer-Learning-Component" class="headerlink" title="RQ2:    How  effective  is  the  Transfer  Learning Component?"></a>RQ2:    How  effective  is  the  Transfer  Learning Component?</h3><p>我们从以下两个方面来探究模型迁移知识的能力：</p>
<ul>
<li>使用文中所用到的特征的重要性</li>
<li>本模型迁移知识的能力</li>
</ul>
<p>对于第一点，作者提出传统的方法一般只提取了统计特征，而本文还提取了多种其他特征。作者对提取不同特征进行了比较试验，结果如下表所示：</p>
<img src="https://i.loli.net/2020/06/25/QV2eWzoOxGS6qJd.png" style="zoom:67%;" />

<p>除此之外，作者还展示了不同数据集下前10有效的特征：</p>
<img src="https://i.loli.net/2020/06/25/QXYcP9V3xmJeIq5.png" style="zoom:67%;" />

<p>对于第二点，作者比较了是否使用文中的领域适应算法CORAL，在达到相似F1 score下所需的标签数，如下表所示：</p>
<img src="https://i.loli.net/2020/06/25/JKFf4XngPBdGm3V.png" style="zoom:67%;" />

<h3 id="RQ3-How-effective-is-the-Active-Learning-component"><a href="#RQ3-How-effective-is-the-Active-Learning-component" class="headerlink" title="RQ3:  How effective is the Active Learning component?"></a>RQ3:  How effective is the Active Learning component?</h3><p>为了验证本文所用的主动学习的有效性，作者进行了对比试验。第一个模型 (Supervised model) 使用全部标签但不使用迁移学习训练，第二个 (Naïve) 为只使用主动学习而不使用迁移学习，第三个为本文提出的模型。结果如下图所示，为了达到相似的性能，不同模型需要的标签数。</p>
<img src="https://i.loli.net/2020/06/25/jcXphwBmR8J6SeU.png" style="zoom:67%;" />

<p>下表展示了使用不同主动学习策略 (U - conventional uncertainty method, UCD - 本文使用的方法, random - 随机选择) 进行标记得到的结果：</p>
<img src="https://i.loli.net/2020/06/25/pe24P9gFJfQVrKM.png" style="zoom:67%;" />

<p>同时作者还对不同$\alpha$的选择进行了实验：</p>
<img src="https://i.loli.net/2020/06/25/ifRhv85IqaVw7gx.png" style="zoom:67%;" />

<h3 id="RQ4-How-effective-is-ATAD-in-detecting-anomalies-in-a-company’s-local-dataset-based-on-public-datasets"><a href="#RQ4-How-effective-is-ATAD-in-detecting-anomalies-in-a-company’s-local-dataset-based-on-public-datasets" class="headerlink" title="RQ4: How effective is ATAD in detecting anomalies in a company’s local dataset based on public datasets?"></a>RQ4: How effective is ATAD in detecting anomalies in a company’s local dataset based on public datasets?</h3><p>这里作者对比了不同方法在微软内部数据集上的结果：</p>
<img src="https://i.loli.net/2020/06/25/5oi3rcGugKXmTha.png" style="zoom:67%;" />
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-05-06T03:05:37.000Z" title="2020-5-6 11:05:37 ├F10: AM┤">2020-05-06</time>发表</span><span class="level-item"><time dateTime="2020-06-25T08:15:23.573Z" title="2020-6-25 4:15:23 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/05/06/Learning-Representations-of-Ultrahigh-dimensional-Data-for-Random-Distance-based-Outlier-Detection/">Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文提出了一种针对高维数据异常检测的表示学习方法。文中提出了<strong>RAMODO</strong>框架，一种基于排序的结合表示学习和异常检测的无监督框架。除此之外，基于<strong>RAMODO</strong>，文中还提出了基于此框架的模型<strong>REPEN</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.04808">Paper📰</a></p>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><h2 id="The-Proposed-Framework-RAMODO"><a href="#The-Proposed-Framework-RAMODO" class="headerlink" title="The Proposed Framework: RAMODO"></a>The Proposed Framework: <strong>RAMODO</strong></h2><h3 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h3><p>我们的目的是为高维数据学习低维表示，同时在学到的低维表示中能够更好地进行异常检测。设有数据集$\mathcal{X}={\mathbf x_1,\mathbf x_2,\cdots, \mathbf x_N}$ ($\mathbf x_i\in \mathbb{R}^D$) 和一个基于随机距离的异常检测器$\phi:\mathcal{X}\mapsto \mathbb{R}$，我们的目标是学习一个表示函数$f:\mathcal{X}\mapsto\mathbb{R}^M (M\ll D)$使得对于所有异常样本$\mathbf x_i$和正常样本$\mathbf x_j$都有$\phi(f(\mathbf x_i))&gt;\phi(f(\mathbf x_j))$。</p>
<h3 id="Ranking-Model-based-Representation-Learning-Framework"><a href="#Ranking-Model-based-Representation-Learning-Framework" class="headerlink" title="Ranking Model-based Representation Learning Framework"></a>Ranking Model-based Representation Learning Framework</h3><p><strong>RAMODO</strong>基于<em>pairwise ranking model</em>。第一步是通过一定的预处理算法（原文中称为<em>outlier thresholding</em>）将数据划分为inlier候选集和outlier候选集；第二步通过随机从inlier候选集采样$n$个样本生成query set $(\mathbf x_i,\cdots,\mathbf x_{i+n-1})$，从inlier候选集采样一个样本生成<em>positive example</em> $(\mathbf x^+)$，从outlier候选集采样一个样本生成<em>negative example</em> $(\mathbf x^-)$，将三者组合生成 <em>metatriplet</em> $T=(&lt;\mathbf x_i,\cdots,\mathbf x_{i+n-1}&gt;,\mathbf x^+,\mathbf x^-)$；第三步通过神经网络$f$学习表示；第四步通过<em>outlier score-based ranking loss</em> $L(\phi(f(\mathbf x^+)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;),\phi(f(\mathbf x^-)|&lt;f(\mathbf x_i),\cdots,f(\mathbf x_{i+n-1})&gt;))$来进行优化，其中$\phi(\cdot|\cdot)$为基于距离的异常检测器。</p>
<p><img src="https://i.loli.net/2020/06/25/4I7fx5ZjBhueUDz.png"></p>
<h2 id="A-RAMODO-Instance-REPEN"><a href="#A-RAMODO-Instance-REPEN" class="headerlink" title="A RAMODO Instance: REPEN"></a>A <strong>RAMODO</strong> Instance: <strong>REPEN</strong></h2><p><strong>REPEN</strong>为<strong>RAMODO</strong>的实例模型，使用Sp作为异常检测器。</p>
<h3 id="Outlier-Thresholding-Using-State-of-the-art-Detectors-and-Cantelli’s-Inequality"><a href="#Outlier-Thresholding-Using-State-of-the-art-Detectors-and-Cantelli’s-Inequality" class="headerlink" title="Outlier Thresholding Using State-of-the-art Detectors and Cantelli’s Inequality"></a>Outlier Thresholding Using State-of-the-art Detectors and Cantelli’s Inequality</h3><p>第一步使用Sp作为基础获得初始anomaly score：</p>
<blockquote>
<p><strong>Definition 1</strong> (<em>Sp-based Outlier Scoring</em>). 给定样本$x_i$，Sp 以以下方式定义该样本的异常程度：<br>$$<br>r_i=\frac{1}{m}\sum\limits_{j=1}^m nn_dist(\mathbf x_i|\mathcal{S}_j)<br>$$<br>其中$\mathcal S_j\subset \mathcal X$为数据集随机采样的子集，$m$为集成大小，$nn_dist(\cdot|\cdot)$为$\mathcal S_j$中最近邻居的距离。</p>
</blockquote>
<p>接着通过<em>Cantelli’s Inequality</em>来定义<em>Pseudo Outlier</em>：</p>
<blockquote>
<p>*<em>Definition 2 **(<em>Cantelli’s Inequality-based Outlier Thresholding</em>). 给定异常分数向量$\mathbf r\in\mathbb R^N$，更高异常分数代表更高的可能性为异常，设$\mu$和$\delta^2$分别为均值和方差，</em>Outlier*候选集由以下方式确定：<br>$$<br>\mathcal{O}={\mathbf x_i|r_i \geq \mu + \alpha\delta}, \space\forall \mathbf x_i\in\mathcal X, \space r_i\in\mathbf r<br>$$<br>其中$\alpha\geq 0$为自定义的阈值。</p>
</blockquote>
<p><em>Inlier</em>候选集$\mathcal I=\mathcal X\backslash \mathcal O$。</p>
<h3 id="Triplet-Sampling-Based-on-Outlier-Scores"><a href="#Triplet-Sampling-Based-on-Outlier-Scores" class="headerlink" title="Triplet Sampling Based on Outlier Scores"></a>Triplet Sampling Based on Outlier Scores</h3><p>首先，从$\mathcal I$采样一定数量的样本组成<em>query set</em>，每个样本被采样的概率与其对应的异常分数有关：</p>
<p>$$<br>p(\mathbf x_i)=\frac{\mathbb Z-r_i}{\sum_{t=1}^{|\mathcal I|}[\mathbb Z-r_t]}<br>$$</p>
<p>其中$\mathbb Z=\sum_{t=1}^{|\mathcal I|}r_t$。</p>
<p>之后从<em>inlier set</em>中均匀随机采样一个<em>positive sample</em> $\mathbf x^+$。最后从<em>outlier set</em>中根据以下概率采样一个<em>negative sample</em> $\mathbf x^-$：<br>$$<br>p(\mathbf x_j)=\frac{r_j}{\sum_{t=1}^{|\mathcal O|}r_t}<br>$$</p>
<h3 id="A-Shallow-Data-Representation"><a href="#A-Shallow-Data-Representation" class="headerlink" title="A Shallow Data Representation"></a>A Shallow Data Representation</h3><p>单层神经网络用来获得浅层的表示：</p>
<blockquote>
<p>**Definition 3 **(<em>Single-layer Fully-connected Representations</em>) 给定输入$x$，<br>$$<br>f_\Theta(\mathbf x)={\psi(\mathbf w_1^\top\mathbf x),\psi(\mathbf w_2^\top\mathbf x),\cdots,\psi(\mathbf w_M^\top\mathbf x)}<br>$$<br>其中$\psi(\cdot)$为激活函数，$\mathbf w$为权重矩阵。</p>
</blockquote>
<h3 id="Ranking-Loss-Using-Random-Nearest-Neighbor-Distance-based-Outlier-Scores"><a href="#Ranking-Loss-Using-Random-Nearest-Neighbor-Distance-based-Outlier-Scores" class="headerlink" title="Ranking Loss Using Random Nearest Neighbor Distance-based Outlier Scores"></a>Ranking Loss Using Random Nearest Neighbor Distance-based Outlier Scores</h3><p>设$\mathcal{Q}=&lt;f_\Theta(\mathbf x_i),\cdots,f_\Theta(\mathbf x_{i+n-1})&gt;$为<em>query set</em>，给定样本$\mathbf x$，<strong>REPEN</strong>根据最近邻距离定义了$f_\Theta(\mathbf x)$的异常程度：<br>$$<br>\phi(f_\Theta(\mathbf x)|\mathcal{Q})=nn_dist(f_\Theta(\mathbf x)|\mathcal Q)<br>$$<br>因此，给定三元组$T=(\mathcal Q,f_\Theta(\mathbf x^+),f_\Theta(\mathbf x^-))$，我们的目标是学得表示$f(\cdot)$使得：<br>$$<br>nn_dist(f_\Theta(\mathbf x^+)|\mathcal Q)&lt;nn_dist(f_\Theta(\mathbf x^-)|\mathcal Q)<br>$$<br>损失函数：<br>$$<br>J(\Theta;T)=L(\phi(f_\Theta(\mathbf x^+)|\mathcal Q),\phi(f_\Theta(\mathbf x^-)|\mathcal Q))=\\max{0, c+nn_dist(f_\Theta(\mathbf x^+)|\mathcal Q)-nn_dist(f_\Theta(\mathbf x^-)|\mathcal Q)}<br>$$<br>其中$c$为边界参数。给定一系列三元组，最终优化目标如下：<br>$$<br>\mathop{\text{arg min}}\limits_{\Theta}\frac{1}{|\mathcal{T}|}\sum\limits_{i=1}^{|\mathcal T|}J(\Theta;T_i)<br>$$</p>
<h3 id="The-Algorithm-and-Its-Time-Complexity"><a href="#The-Algorithm-and-Its-Time-Complexity" class="headerlink" title="The Algorithm and Its Time Complexity"></a>The Algorithm and Its Time Complexity</h3><p><img src="https://i.loli.net/2020/06/25/eYtKHBJ7szCgjNa.png"></p>
<h3 id="Leveraging-A-Few-Labeled-Outliers-to-Improve-Triplet-Sampling"><a href="#Leveraging-A-Few-Labeled-Outliers-to-Improve-Triplet-Sampling" class="headerlink" title="Leveraging A Few Labeled Outliers to Improve Triplet Sampling"></a>Leveraging A Few Labeled Outliers to Improve Triplet Sampling</h3><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><ul>
<li>AD：网络广告检测</li>
<li>LC：肺癌疾病监测</li>
<li>p53：异常蛋白质活动检测</li>
<li>R8：文本分类</li>
<li>News20：文本分类</li>
<li>URL：异常网址检测</li>
<li>Webspam：Pascal Large Scale LearningChallenge</li>
</ul>
<h2 id="Effectiveness-in-Real-world-Data-with-Thousands-to-Millions-of-Features"><a href="#Effectiveness-in-Real-world-Data-with-Thousands-to-Millions-of-Features" class="headerlink" title="Effectiveness in Real-world Data with Thousands to Millions of Features"></a>Effectiveness in Real-world Data with Thousands to Millions of Features</h2><p>作者分别使用原始特征和<em>REPEN</em>学到的特征进行异常检测，IMP代表性能提升比例，SU代表加速比例。</p>
<p><img src="https://i.loli.net/2020/06/25/mvUiE1NzyTwOgV8.png"></p>
<h2 id="Comparing-to-State-of-the-art-Representation-Learning-Competitors"><a href="#Comparing-to-State-of-the-art-Representation-Learning-Competitors" class="headerlink" title="Comparing to State-of-the-art Representation Learning Competitors"></a>Comparing to State-of-the-art Representation Learning Competitors</h2><ul>
<li>**AE: **自编码器</li>
<li>**HLLE: ** <em>Hessian Locally Linear Embedding</em></li>
<li>**SRP: ** <em>Sparse Random Projection</em></li>
<li>**CoP: ** <em>Coherent Pursuit</em></li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/yQumCRNrHAheJ34.png"></p>
<h2 id="The-Capability-of-Leveraging-Labeled-Outliers-as-Prior-Knowledge"><a href="#The-Capability-of-Leveraging-Labeled-Outliers-as-Prior-Knowledge" class="headerlink" title="The Capability of Leveraging Labeled Outliers as Prior Knowledge"></a>The Capability of Leveraging Labeled Outliers as Prior Knowledge</h2><p><img src="https://i.loli.net/2020/06/25/NOLfKQd2u1JMPtp.png"></p>
<h2 id="Sensitivity-Test-w-r-t-the-Representation-Dimension"><a href="#Sensitivity-Test-w-r-t-the-Representation-Dimension" class="headerlink" title="Sensitivity Test w.r.t. the Representation Dimension"></a>Sensitivity Test w.r.t. the Representation Dimension</h2><p><img src="https://i.loli.net/2020/06/25/BoGjY5SEu6vrK3X.png"></p>
<p><img src="https://i.loli.net/2020/06/25/Rlx7Df9Hvsjp2Eg.png"></p>
<p>文中提到了对于R8、URL、News20这三个数据集在维度$M=1$的时候表现和其他维度一样好，作者给出的解释是在这几个数据集中异常部分是线性可分的，所以1维就足够了，另一个解释是优化问题。</p>
<h2 id="Scalability-Test"><a href="#Scalability-Test" class="headerlink" title="Scalability Test"></a>Scalability Test</h2><p><img src="https://i.loli.net/2020/06/25/1JfUclWyFYgLdNp.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-03-29T16:17:24.000Z" title="2020-3-30 12:17:24 ├F10: AM┤">2020-03-30</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:33:35.419Z" title="2020-6-25 1:33:35 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/30/Deep-Weakly-supervised-Anomaly-Detection/">Deep Weakly-supervised Anomaly Detection</a></h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>在文献中，因为标注成本的昂贵，无监督方法占据了异常检测的主要位置。然而，在现实生活中，我们可能会有少量标签，如何利用这部分标签信息就成为了一个问题，作者将其称之为<em>anomaly-informed modeling</em>。作者提出了两点挑战：</p>
<ol type="1">
<li>少量标签可能无法提供所有类型异常的信息；</li>
<li>大部分无标签数据为正常样本，但其中包含少部分异常（污染）。</li>
</ol>
<p>作者提出了基于pairwise relation learning的方法来解决这些问题。文章的主要贡献如下：</p>
<ol type="1">
<li>提出了一种基于pairing-based data augmentation和ordinal regression来进行弱监督异常检测的框架</li>
<li>基于该框架提出了PReNet，一种基于双流ordinal regression的网络</li>
<li>从理论和实践角度分析了方法的有效性</li>
<li>在40个真实数据集上进行了完善的实验</li>
</ol>
<h1 id="proposed-method">Proposed Method</h1>
<h2 id="learning-anomaly-scores-by-predicting-pairwise-relation">Learning Anomaly Scores by Predicting Pairwise Relation</h2>
<h3 id="problem-formulation">Problem Formulation</h3>
<p>给定数据集<span class="math inline">\(\mathcal{X}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N,\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，包含两部分，一部分是五标签数据<span class="math inline">\(\mathcal{U}=\{\mathbf{x}_1,\mathbf {x}_2,\cdots,\mathbf{x}_N\}\)</span>，另一部分是有标签异常数据<span class="math inline">\(\mathcal{A}=\{\mathbf{x}_{N+1},\cdots,\mathbf{x}_{N+K}\}\)</span>，其中<span class="math inline">\(K\ll N\)</span>。我们的任务目标是学习一个打分函数<span class="math inline">\(\phi:\mathcal{X}\mapsto \mathbb{R}\)</span>，使得对任任意异常样本的打分高于任意正常样本。</p>
<p>在这个Formulation里，作者将关系学习和异常打分统一了起来。首先，输入的数据集不再是原始样本，而是样本对。样本对包含三种：<em>anomaly-anomaly</em>，<em>anomaly-unlabeled</em>，<em>unlabeled-unlabeled</em>，记为<span class="math inline">\(C_{\{\mathbf{a},\mathbf{a}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>，<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>。每一个样本对包含一个标签<span class="math inline">\(y\)</span>，表示该pair对应的异常分数，整个输入数据集<span class="math inline">\(\mathcal{P}=\{\{\mathbf{x}_i,\mathbf{x}_j,y_{ij}\}|\mathbf{x}_i,\mathbf{x}_j\in\mathcal{X} \space\text{and}\space y_{ij}\in\mathbb{N}\}\)</span>。因为有<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}&gt;y_{\{\mathbf a,\mathbf u\}}&gt;y_{\{\mathbf u,\mathbf u\}}\)</span>，所以对关系的学习也是对异常打分的学习。</p>
<h3 id="the-instantiated-model-prenet">The Instantiated Model: PReNET</h3>
<p>下图为模型示意图，<strong>Data Augmentation</strong>模块负责产生pair数据，<strong>End-to-End Anomaly Score Learner <span class="math inline">\(\phi\)</span></strong> 模块负责关系学习（异常打分）。</p>
<p><img src="https://i.loli.net/2020/06/25/6ZF3w9v1Lux5t4Q.png" style="zoom:67%;" /></p>
<h4 id="data-argumentation-by-pairing">Data Argumentation by Pairing</h4>
<p>数据的产生分为两步：</p>
<ol type="1">
<li>从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>上随机采样，组成pair；</li>
<li>对每个pair打上次序(ordinal class feature) 标签<span class="math inline">\(\mathbf{y}\)</span>。</li>
</ol>
<p>部分<span class="math inline">\(C_{\{\mathbf{a},\mathbf{u}\}}\)</span>和<span class="math inline">\(C_{\{\mathbf{u},\mathbf{u}\}}\)</span>可能包含异常污染，因为在<span class="math inline">\(\mathcal{U}\)</span>中可能会有未标记的异常样本。</p>
<h4 id="end-to-end-anomaly-score-learner">End-to-End Anomaly Score Learner</h4>
<p>令<span class="math inline">\(\mathcal{Z}\in\mathbb{R}^M\)</span>为中间表示空间，那么<strong>Score Learner</strong>可以拆解为特征学习<span class="math inline">\(\psi(\cdot;\Theta_r):\mathcal{X}\mapsto \mathcal{Z}\)</span>和打分函数<span class="math inline">\(\eta((\cdot,\cdot);\Theta_s):(\mathcal{Z},\mathcal{Z})\mapsto\mathbb{R}\)</span>两部分，两部分都由神经网络组成。</p>
<h4 id="ordinal-regression">Ordinal Regression</h4>
<p>损失函数定义为： <span class="math display">\[
L\left(\phi((\mathbf x_i,\mathbf x_j);\Theta),y_{ij}\right)=|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|
\]</span> 采用绝对值而不是均方误差的原因是为了减少异常污染的影响。默认<span class="math inline">\(y_{\{\mathbf a,\mathbf a\}}=8\)</span>，<span class="math inline">\(y_{\{\mathbf a,\mathbf u\}}=4\)</span>，<span class="math inline">\(y_{\{\mathbf u,\mathbf u\}}=0\)</span>。最后的优化函数可以写为： <span class="math display">\[
\mathop{\text{argmin}}\limits_{\Theta}\frac{1}{|\mathcal{B}|}\sum\limits_{\{\mathbf x_i,\mathbf x_j, y_{ij}\}\in\mathcal{B}}|y_{ij}-\phi((\mathbf x_i,\mathbf x_j);\Theta)|+\lambda R(\Theta)
\]</span> <span class="math inline">\(\mathcal{B}\)</span>为一个batch，<span class="math inline">\(R(\Theta)\)</span>为正则项。</p>
<h3 id="anomaly-detection-using-prenet">Anomaly Detection Using PReNet</h3>
<h4 id="training-stage">Training Stage</h4>
<p>训练流程如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/oR6uTL3c7HMpwD4.png" style="zoom: 80%;" /></p>
<p>为了保证训练样本类别的平衡，<span class="math inline">\(\frac{|\mathcal{B}|}{2}\)</span>的样本采样自<span class="math inline">\(C_{\{\mathbf u,\mathbf u\}}\)</span>，采样自<span class="math inline">\(C_{\{\mathbf a,\mathbf u\}}\)</span>和<span class="math display">\[C_{\{\mathbf a,\mathbf a\}}\]</span>的样本都占<span class="math inline">\(\frac{|\mathcal{B}|}{4}\)</span>。</p>
<h4 id="anomaly-scoring-stage">Anomaly Scoring Stage</h4>
<p>在测试阶段，给定测试样本<span class="math inline">\(\mathbf{x}_k\)</span>，先分别从<span class="math inline">\(\mathcal{A}\)</span>和<span class="math inline">\(\mathcal{U}\)</span>采样，然后定义以下<em>anomaly score</em>： <span class="math display">\[
s_{\mathbf{x}_k}=\frac{1}{2E}\left[\sum\limits_{i=1}^E\phi((\mathbf a_i,\mathbf x_k);\Theta^*)+\sum\limits_{j=1}^E\phi((\mathbf x_k,\mathbf u_j);\Theta^*)\right]
\]</span> <span class="math inline">\(\mathbf a_i\)</span>和<span class="math inline">\(\mathbf u_j\)</span>为随机采样得到的异常样本和正常样本，采样大小<span class="math inline">\(E\)</span>默认为30。</p>
<h1 id="experiments">Experiments</h1>
<p>实验部分主要是回答以下四个问题：</p>
<ol type="1">
<li>在有限的标签异常情况下，PReNet能否有效地检测已知和未知的异常；</li>
<li>在不同数量标签异常的情况下，PReNet的表现如何；</li>
<li>PReNet对异常污染的鲁棒性如何；</li>
<li>PReNet不同组件的重要性如何。</li>
</ol>
<h2 id="datasets">Datasets</h2>
<p>实验一共用到了40个数据集，其中12个用来评测算法检测已知的异常的能力（如Table 2所示），28个用来评测算法检测未知的异常的能力（如Table 3所示）。</p>
<h2 id="competing-methods-and-parameter-settings">Competing Methods and Parameter Settings</h2>
<p>用到的baseline有以下几个：</p>
<ul>
<li>DevNet：同一作者在KDD2019提出的异常检测框架</li>
<li>Deep support vector data description (DSVDD)：深度支持向量数据描述</li>
<li>Prototypical network： few-shot classification中的一种模型</li>
<li>iForest：孤立森林</li>
</ul>
<h2 id="performance-evaluation-metrics">Performance Evaluation Metrics</h2>
<p>用到的Metrics为AUC-ROC和AUC-PR。</p>
<h2 id="detection-of-known-anomalies">Detection of Known Anomalies</h2>
<p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）是固定的，下表为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/BfhVE9z8DWipAM6.png" style="zoom: 80%;" /></p>
<h2 id="detection-of-unknown-anomalies">Detection of Unknown Anomalies</h2>
<p>在本实验中，异常污染的比例（2%）和有标记异常样本的数量（60）同样是固定的，下表为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/9GM8XTYiSLUn2Ar.png" style="zoom:80%;" /></p>
<p><img src="https://i.loli.net/2020/06/25/4RxrGZWLqHNnXco.png" style="zoom:80%;" /></p>
<h2 id="availability-of-known-anomalies">Availability of Known Anomalies</h2>
<p>本实验主要是研究不同数量标注异常样本的条件下，算法的性能如何。异常污染的比例固定（2%），标注异常的数量从15到120变化。实验结果如下：</p>
<p><img src="https://i.loli.net/2020/06/25/x4Hf3lU5JO71bvo.png" style="zoom:80%;" /></p>
<h2 id="further-analysis-of-prenet">Further Analysis of PReNet</h2>
<h3 id="tolerance-to-anomaly-contamination-in-unlabeled-data">Tolerance to Anomaly Contamination in Unlabeled Data</h3>
<p>本实验主要研究不同异常污染比例下，算法的性能，即探究算法对异常污染的鲁棒性。标注异常样本的数量恒定（60），异常污染比例在<span class="math inline">\(\{0\%,2\%,5\%,10\%\}\)</span>中变化。实验结果如下所示：</p>
<p><img src="https://i.loli.net/2020/06/25/TGYCJUs8LlmreNV.png" style="zoom:80%;" /></p>
<h3 id="ablation-study">Ablation Study</h3>
<p>这一节是消融实验，分别设置了四个变体：</p>
<ul>
<li><strong>BOR: </strong>损失函数替换成了二值回归<em>Binary Ordinal Regression</em>；</li>
<li><strong>OSNet: </strong>将双流结构简化为单流；</li>
<li><strong>LDM: </strong>将网络中的隐藏层去除；</li>
<li><strong>A2H: </strong>加入了额外的隐藏层，并且加入了<span class="math inline">\(\ell_2\)</span>-norm防止过拟合。</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/oR7qlZWfepFT8jK.png" style="zoom:80%;" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-03-01T14:55:02.000Z" title="2020-3-1 10:55:02 ├F10: PM┤">2020-03-01</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:35:20.215Z" title="2020-6-25 1:35:20 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/">Discovering Physical Concepts with Neural Networks</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>如题目所示，本文的目的是利用神经网络来发掘物理概念。其思路是从实验数据学到表示，然后用学到的表示来回答物理问题，由此物理概念可以从学到的表示来提取出。作者进行了4个实验：</p>
<ol>
<li>在阻尼振动实验中，模型学到了相关的物理参数；</li>
<li>在角动量守恒实验中，模型预测了质点的运动；</li>
<li>给定量子系统的观测数据，模型正确的识别出了量子状态的自由度；</li>
<li>给定从地球观测的太阳和火星的位置时间序列数据，模型发现了日心说模型。</li>
</ol>
<h1 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h1><p>作者在附录中对神经网络的基础知识进行了介绍，这里不再赘述，只截取了一些相对前沿的内容。</p>
<img src="https://i.loli.net/2020/06/25/yh5Wj9AQmd6nsFC.png" style="zoom:67%;" />

<h2 id="Variational-Autoencoders"><a href="#Variational-Autoencoders" class="headerlink" title="Variational Autoencoders"></a>Variational Autoencoders</h2><p>本文用到的模型基础是VAE：</p>
<img src="https://i.loli.net/2020/06/25/zCnYjVEZHdbqAD3.png" style="zoom:67%;" />

<h3 id="Representation-Learning"><a href="#Representation-Learning" class="headerlink" title="Representation Learning"></a>Representation Learning</h3><p><em>Representation learning</em>的主要目标是将数据映射到一个隐向量 (encoder)，为了保证隐向量包含了所有相关信息， 那么应该能够从隐向量还原原数据 (decoder)。传统的Autoencoder是这个思想的最简单实现，而VAE则将AE和<em>Variational Inference</em>结合了起来，是一种经典的生成式模型。现在很多研究关注<em>Disentangled Representation Learning</em>，也就是说我们希望模型能够无监督地学习数据，从中学到有意义的表示。</p>
<h3 id="boldsymbol-beta-VAE"><a href="#boldsymbol-beta-VAE" class="headerlink" title="$\boldsymbol \beta$-VAE"></a>$\boldsymbol \beta$-VAE</h3><p>$\beta$-VAE是一种特殊的VAE，也是一个经典的<em>Disentangled Representation Learning</em>模型，它和VAE主要的区别是对KL散度一项加上了权重$\beta$进行调节：<br>$$<br>C_\beta(x)=-\left[\mathbb{E}_{z\sim p_\phi(z|x)}\log p_\theta(x|z)\right] + \beta D_\text{KL}\left[p_\phi(z|x)\parallel h(z)\right]<br>$$<br>如果假设$p_\phi(z|x)=\mathcal{N}(\mu,\sigma)$，那么损失函数可以进行简化：<br>$$<br>C_\beta(x)=\parallel \hat{x} - x \parallel^2_2-\frac{\beta}{2}\left(\sum\limits_i\log(\sigma_i^2)-\mu_i^2-\sigma_i^2\right)+C<br>$$</p>
<h1 id="Network-Structure"><a href="#Network-Structure" class="headerlink" title="Network Structure"></a>Network Structure</h1><h2 id="Network-Structure-SciNet"><a href="#Network-Structure-SciNet" class="headerlink" title="Network Structure: SciNet"></a>Network Structure: <em>SciNet</em></h2><p>模仿物理学家建模物理问题的过程，作者提出了<em>SciNet</em>，如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/uWd1lOUFxXgQJ7f.png" style="zoom:67%;" />

<p>物理学家在建模物理问题的时候，往往是从一些实验数据出发，根据物理常识提取更加精练的表示，然后用学到的表示来回答物理问题。</p>
<p>对于单纯的输入输出问题，<em>SciNet</em>可以看作是一个映射，$F:\mathcal{O}\times\mathcal{Q}\rightarrow\mathcal{A}$。$\mathcal{O}$是可能的实验数据集合，$\mathcal{Q}$是可能的问题集合，$\mathcal{A}$是可能的答案集合。可以将其分为两个步骤：编码过程$E:\mathcal{O}\rightarrow\mathcal{R}$从实验数据学到表示，解码过程$D:\mathcal{R}\times \mathcal{Q}\rightarrow \mathcal{A}$根据给定的问题从表示来回答问题。由此，$F(o,q)=D(E(o),q)$。在实现方面，<em>SciNet</em>采用的是全连接网络。</p>
<h2 id="Training-and-Testing-SciNet"><a href="#Training-and-Testing-SciNet" class="headerlink" title="Training and Testing SciNet"></a>Training and Testing <em>SciNet</em></h2><p>用来训练的数据形式为$(o,q,a_{cor}(o,q))$，观测$o$和问题$q$分别从观测集$\mathcal{O}$和问题集$\mathcal{Q}$选出，$a_{cor}(o,q)$为对应的正确答案。在训练过程中，我们希望准确度尽量高，并且学到<em>minimal uncorrelated representations</em>。为此，作者采用<em>disentangling variational autoencoder</em>作为模型。</p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>在文中，作者进行了4个实验来验证模型的有效性。</p>
<h2 id="Damped-Pendulum"><a href="#Damped-Pendulum" class="headerlink" title="Damped Pendulum"></a>Damped Pendulum</h2><p>阻尼振动实验：</p>
<ul>
<li><p>任务：预测一维阻尼振动在不同时间的位置。</p>
</li>
<li><p>物理模型：$-kx-b\dot{x}=m\ddot{x}$，$k$为弹性模量，$b$为阻尼系数，通解为$x(t)=A_0e^{-\frac{b}{2m}t}\cos(\omega t+\delta_0), \space \omega=\sqrt{\frac{k}{m}}\sqrt{1-\frac{b^2}{4mk}}$</p>
</li>
<li><p>观测数据：位置时间序列数据$o=[x(t_i)]_{i\in{1,\cdots,50}}\in\mathbb{R}^{50}$，时间间隔相等，质量$m=1\text{kg}$，振幅$A_0=1\text{m}$，相位$\delta_0=0$，弹性模量$k\in[5,10]\text{kg}/\text{s}^2$，阻尼系数$b\in[0.5,1]\text{kg}/\text{s}$。</p>
</li>
<li><p>问题：预测$q=t_\text{pred}\in\mathbb{R}$</p>
</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/yWGzxo4eFKmABul.png"></p>
<p>隐变量大小设置为3，结果如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/Q4PKa3pm2htekqd.png" style="zoom:67%;" />

<p>(b)中的三幅图分别是学到的三个隐变量和我们感兴趣的参数$k$和$b$的关系图。第一幅图中变量$1$与$b$几乎完全线性相关，与$k$基于线性无关，变量$2$只和$k$相关。变量$3$几乎为一个常数，故不提供额外的信息。由此作者认为<em>SciNet</em>学到了我们关心的两个参数的知识。</p>
<h2 id="Conservation-of-Angular-Momentum"><a href="#Conservation-of-Angular-Momentum" class="headerlink" title="Conservation of Angular Momentum"></a>Conservation of Angular Momentum</h2><p>角动量守恒实验：</p>
<ul>
<li>任务：预测一个由长度为$r$的绳子捆绑着的旋转质点在位置$(0,r)$经一个自由质点撞击后的位置</li>
<li>物理模型：给定撞击之前的角动量，自由质点撞击之后的速度，旋转质点在撞击之后在时间$t_\text{pred}^\prime$的位置可以由角动量守恒定律给出：</li>
</ul>
<p>$$<br>J=m_\text{rot}r^2\omega-rm_\text{free}(\mathbf{v}_\text{free})_x=m_\text{rot}r^2\omega^\prime-rm_\text{free}(\mathbf{v}^\prime_\text{free})_x=J^\prime<br>$$</p>
<ul>
<li>观测数据：在撞击之前两个质点的位置数据$o=[(t_i^\text{rot},q_\text{rot}(t_i^\text{rot})),(t_i^\text{free},q_\text{free}(t_i^\text{free}))]_{i\in{1,\cdots,5}}$，质量为固定值，半径$r$也为固定值。数据添加高斯噪声。</li>
<li>问题：预测撞击之后自由质点在时间$t_\text{pred}^\prime$的位置</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/SKfJLxl1QmuzFt9.png"></p>
<p>实验室意图如下：</p>
<img src="https://i.loli.net/2020/06/25/qimk9ZYBe7UPs3z.png" style="zoom:67%;" />

<p>实验结果表明<em>SciNet</em>能够正确预测质点撞击之后的位置，同时对噪音鲁棒。根据(b)，隐变量和角动量存在线性相关关系，作者认为<em>SciNet</em>学到了守恒的动量这一概念。</p>
<h2 id="Representation-of-Qubits"><a href="#Representation-of-Qubits" class="headerlink" title="Representation of Qubits"></a>Representation of Qubits</h2><p>量子比特实验：</p>
<ul>
<li>任务：预测在$n=1,2$的纯$n$量子位状态$\psi\in\mathbb{C}^{2^n}$下任何二进制投影测量$\omega\in\mathbb{C}^{2^n}$的测量概率。</li>
<li>物理模型：在执行测量$\omega\in\mathbb{C}^{2^n}$的状态$\psi\in\mathbb{C}^{2^n}$下测量0的概率$p(\omega,\psi)$由$p(\omega,\psi)=|\left&lt;\omega,\psi\right&gt;|^2$给定</li>
<li>观测数据：状态$\psi: o=[p(\alpha_i,\psi)]_{i\in{i,\cdots,n_1}}$的操作参数化：表示一组固定的随机二元射影测量值$\mathcal{M}_1={\alpha_1,\cdots,\alpha_{n_1}}$（一个量子位$n_1 = 10$，两个量子位$n_1 = 30$）</li>
<li>问题：对于固定的一组随机二元射影测量$\mathcal{M}<em>2={\beta_1,\cdots,\beta_{n_2}}$，测量$\omega:q=[p(\beta_i,\omega)]</em>{i\in{1,\cdots,n_2}}$的Operational参数化（一个量子位$n_2 = 10$，两个量子位$n_2 = 30$）</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/8lY1LBsQCZUwX2I.png"></p>
<p>实验结果如下：</p>
<img src="https://i.loli.net/2020/06/25/ZTRKfzb63Jrvk5C.png" style="zoom:67%;" />

<p>通过实验发现，<em>SciNet</em>可以在不提供先验物理知识的条件下确定表述状态$\psi$最小的参数数量。同时，<em>SciNet</em>还能分辨<em>tomographically complete</em>和<em>tomographically incomplete</em>。</p>
<h2 id="Heliocentric-Model-of-the-Solar-System"><a href="#Heliocentric-Model-of-the-Solar-System" class="headerlink" title="Heliocentric Model of the Solar System"></a>Heliocentric Model of the Solar System</h2><p>日心说模型：</p>
<ul>
<li>问题：在给定初始条件下预测相对与地球的太阳和火星的角度$\theta_M(t)$和$\theta_S(t)$</li>
<li>物理模型：地球和火星围绕太阳以一定角速度做近似圆周运动</li>
<li>观测数据：给定初始角度，随机选择周周期的哥白尼的观测数据</li>
</ul>
<p><img src="https://i.loli.net/2020/06/25/vXl3Ae4RpzibrmY.png"></p>
<p>模型的实现稍有变化，如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/XnsYqcRS6izZGEm.png" style="zoom:67%;" />

<p>这样，对于不同时间都对应一个隐变量$r(t_i)$，而且隐变量是时间依赖的，对于一个隐变量$r(t_i)$有一个解码器来输出答案。</p>
<img src="https://i.loli.net/2020/06/25/az3UmkchyFWevP7.png" style="zoom:67%;" />

<p>实验结果表示，<em>SciNet</em>不仅正确预测了太阳和火星相对地球的角度，同时隐变量揭示了火星和地球相对太阳的角度。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-02-27T12:02:18.000Z" title="2020-2-27 8:02:18 ├F10: PM┤">2020-02-27</time>发表</span><span class="level-item"><time dateTime="2020-06-25T09:01:35.786Z" title="2020-6-25 5:01:35 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/">Transfer Anomaly Detection by Inferring Latent Domain Representations</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>作者提出了一种利用迁移学习提升target domain异常检测性能的算法。文中指出现有的基于迁移学习的异常检测算法需要对每个 target domain 进行单独训练，这样做会带来很大的计算开销。本文通过<em>latent domain vectors</em>来实现无需重新训练的异常检测。<em>latent domain vectors</em>是domain的一种隐含表示，通过该domain中的正常样本得到。在本文中，<em>anomaly score function</em>通过Auto-encoder得到。</p>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>令$\mathbf{X}<em>d^+:={\mathbf{x}^+</em>{dn}}^{N^+<em>d}</em>{n=1}$为第$d$个domain的异常样本集，$\mathbf{x}_{dn}^+\in\mathbb{R}^M$为其中第$n$个样本的$M$维特征向量，$N^+_d$为第$d$个domain异常样本的数量。</p>
<p>类似的，令$\mathbf{X}<em>d^-:={\mathbf{x}^-</em>{dn}}^{N^-<em>d}</em>{n=1}$为第$d$个domain的正常样本集。我们假设对于每个domain都有$N^+_d\ll N^-_d$，且特征向量维度都为$M$。</p>
<p>假设在 source domain $D_S$都有正常样本和异常样本，记为${\mathbf{X}^+<em>d\cup\mathbf{X}_d^-}^{D_S}</em>{d=1}$，在 target domain $D_T$只有正常样本${\mathbf{X}<em>d^-}^{D_S+D_T}</em>{d=D_S+1}$。我们的目标是得到一个对于 target domain 合适的 domain-specific 的异常打分函数。</p>
<img src="https://i.loli.net/2020/06/25/KW2YgScfVZN7Fjz.png" style="zoom:67%;" />

<h2 id="Domain-specific-Anomaly-Score-Function"><a href="#Domain-specific-Anomaly-Score-Function" class="headerlink" title="Domain-specific Anomaly Score Function"></a>Domain-specific Anomaly Score Function</h2><p>我们基于Auto-encoder定义异常打分函数。对于每个domain，我们假设存在一个$K$维的隐变量$\mathbf{z}<em>d\in\mathbb{R}^K$。对于第$d$个 domain，异常打分函数定义如下：<br>$$<br>s_\theta(\mathbf{x}_{dn}|\mathbf{z}<em>d):=\parallel\mathbf{x}</em>{dn}-G</em>{\theta_G}(F_{\theta_F}(\mathbf{x}_{dn},\mathbf{z}_d))\parallel^2<br>$$<br>其中参数$\theta:=(\theta_G,\theta_F)$在所有 domain 之间共享。</p>
<h2 id="Models-for-Latent-Domain-Vectors"><a href="#Models-for-Latent-Domain-Vectors" class="headerlink" title="Models for Latent Domain Vectors"></a>Models for Latent Domain Vectors</h2><p>隐变量$\mathbf{z}_d$是无法观测到的，只能通过数据来估计。首先$\mathbf{z}_d$在$\mathbf{X}_d^-$条件下的条件分布假设为高斯分布：</p>
<p>$$<br>q_\theta(\mathbf{z}<em>d|\mathbf{X}_d^-):=\mathcal{N}(\mathbf{z}_d|\mu_\phi(\mathbf{X}_d^-),\text{diag}(\sigma_\phi^2(\mathbf{X}_d^-)))<br>$$<br>其中均值$\mu_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K$和方差$\sigma^2_\phi(\mathbf{X}_d^-)\in\mathbb{R}^K</em>+$由神经网络建模，且在所有 domain 之间共享。在$\mathbf{X}_d^-$给定的时候，我们便能够推断出该 domain 对应的隐变量，</p>
<p>$q_\phi$的输入为正常样本的集合，故神经网络需要满足<em>permutation invariant</em>。$\tau(\mathbf{X}<em>d^-)=\rho(\sum</em>{n=1}^{N_d^-}\eta(\mathbf{x}_{dn}^-))$，其中$\tau(\mathbf{X}_d^-)$表示$\mu_\phi(\mathbf{X_d^-})$或$\ln\sigma_\phi^2(\mathbf{X}_d^-)$，$\rho$和$\eta$为神经网络，</p>
<h2 id="Objective-Function"><a href="#Objective-Function" class="headerlink" title="Objective Function"></a>Objective Function</h2><p>目标函数由anomaly score函数和隐变量组成。第$d$个domain在对应的隐变量$\mathbf{z}_d$条件下的目标函数为：</p>
<p>$$<br>L_d(\theta|\mathbf{z}<em>d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}</em>{dn}^-|\mathbf{z}<em>d)-\frac{\lambda}{N_d^-N_d^+}\sum\limits</em>{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}<em>{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}</em>{dn}^-|\mathbf{z}_d))<br>$$</p>
<p>其中$\lambda\geq 0$为超参数，$f$为sigmoid函数。公式的第一项表示第$d$个domain正常样本对应的<em>anomaly score</em>。第二项为可微分的AUC。异常样本的<em>anomaly score</em>应当大于正常样本，所以对任何$\mathbf x_{dm}^+\in\mathbf X_d^+, \mathbf x_{dn}^-\in\mathbf X_d^-$有$s_\theta(\mathbf x_{dm}^+|\mathbf z_d)&gt;s_\theta(\mathbf x_{dn}^-|\mathbf z_d)$。第二项$\frac{\lambda}{N_d^-N_d^+}\sum\limits_{n,m=1}^{N_d^-,N_d^+}f(s_\theta(\mathbf{x}<em>{dm}^+|\mathbf{z}_d)-s_\theta(\mathbf{x}</em>{dn}^-|\mathbf{z}<em>d))$的取值范围是$[0,1]$，当所有的$s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}</em>{dm}^-|\mathbf{z}<em>d)$时该项为1，当所有的$s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\ll s_\theta(\mathbf{x}</em>{dm}^-|\mathbf{z}<em>d)$时该项为0，所以最小化该项的相反数相当于鼓励$s_\theta(\mathbf{x}_{dm}^+|\mathbf{z}_d)\gg s_\theta(\mathbf{x}</em>{dm}^-|\mathbf{z}_d)$。</p>
<p>因为隐变量$\mathbf z_d$包含不确定性，我们应该在目标函数里考虑这一点：<br>$$<br>\mathcal{L}<em>d(\theta,\phi):=\mathbb{E}</em>{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))<br>$$</p>
<p>第一项是$L_d(\theta|\mathbf z_d)$关于$q_\phi(\mathbf z_d|\mathbf X_d^-)$的期望，第二项是$q_\phi(\mathbf z_d|\mathbf X_d^-)$和$p(\mathbf z_d):=\mathcal{N}(\boldsymbol 0,\boldsymbol I)$的KL散度。第一项可以用<em>monte carlo</em>估计$\mathbb{E}_{q_\phi(\mathbf{z}_d|\mathbf{X}<em>d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]\approx\frac{1}{L}\sum</em>{\ell=1}^L L_d(\theta|\mathbf z_d^{(\ell)})$，除此之外还需要用到<em>reparametrization trick</em>。</p>
<p>对于第$d$个target domain，因为没有异常样本（假设），所以$L_d(\theta|\mathbf{z}<em>d):=\frac{1}{N_d^-}\sum\limits_{n=1}^{N_d^-}s_\theta(\mathbf{x}</em>{dn}^-|\mathbf{z}<em>d)$，有：<br>$$<br>\mathcal{L}<em>d(\theta,\phi):=\mathbb{E}</em>{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[\frac{1}{N_d^-}\sum\limits</em>{n=1}^{N_d^-}s_\theta(\mathbf{x}_{dn}^-|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z}_d))<br>$$</p>
<p>所以总的损失函数为各domain对应的损失函数之和$\mathcal{L}(\theta,\phi):=\sum_{d=1}^{D_S+D_T}\alpha_d\mathcal{L}_d(\theta,\phi)$。</p>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>训练好之后，domain-specific的<em>anomaly score</em>可以由下式计算出：</p>
<p>$$<br>s(\mathbf{x}<em>{d^\prime}):=\int s</em>{\theta_*}(\mathbf{x_{d^\prime}}|\mathbf{z}<em>{d^\prime})q</em>{\phi_*}(\mathbf{z}<em>{d^\prime}|\mathbf{X}</em>{d^\prime}^-)\mathrm{d}\mathbf{z}<em>{d^\prime}\approx\frac{1}{L}\sum\limits</em>{\ell=1}^L s_{\theta_*}(\mathbf{x}<em>{d^\prime}|\mathbf{z}</em>{d^\prime}^{(\ell)})<br>$$</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>实验包含五个数据集，第一个是合成数据集。如下图(a)所示，围绕$(0,0)$有$8$个圈，每个圈包含了一个内圈作为异常样本，第$7$个圈被选为target domain，其余的为source domain。第二个是MNIST-r，是加入旋转的MNIST，包含6个domain，其中数字“4”被选为异常样本，其余为正常。第三个为Anuran Calls，包含5个domain。第四个是Landmine，主要用在多任务学习中。第五个是IoT，网络流量数据，包含8个domain。</p>
<img src="https://i.loli.net/2020/06/25/6WLAfMwJPuN5Ov9.png" style="zoom:50%;" />

<h2 id="Comparison-Methods"><a href="#Comparison-Methods" class="headerlink" title="Comparison Methods"></a>Comparison Methods</h2><p>对比的baseline包括NN（普通多层神经网络），NNAUC（加入可微分AUC作为损失函数），AE（普通Autoencoer），AEAUC（加入可微分AUC的AE），OSVM（单类支持向量机），CCSA，TOSVM和OTL。</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>4个真实数据集的结果如下：</p>
<img src="https://i.loli.net/2020/06/25/nfkwTVexRNqyFMY.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/06/25/QaMskTZALyeiFI1.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/06/25/F7VTyeHMz8mK2uJ.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/06/25/B32UmgXcwGhZYk1.png" style="zoom:50%;" />

<p>表5为考虑隐变量不确定性的ablation study。将原来的公式$\mathcal{L}<em>d(\theta,\phi):=\mathbb{E}</em>{q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)}\left[L_d(\theta|\mathbf{z}_d)\right]+\beta D_\text{KL}(q_\phi(\mathbf{z}_d|\mathbf{X}_d^-)\parallel p(\mathbf{z_d}))$中$q_\phi(\mathbf z_d|\mathbf X_d^-)$用迪利克雷分布$q_\phi(\mathbf z_d|\mathbf X_d^-)=\delta(\mathbf z_d-\mu_\phi(\mathbf X_d^-))$代替并且去掉KL散度。</p>
<img src="https://i.loli.net/2020/06/25/yUHcTBzixsMlY7f.png" style="zoom: 50%;" />

<p>表6展示了不同异常比例对效果的影响。</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/">上一页</a></div><div class="pagination-next"><a href="/page/3/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link is-current" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li></ul></nav></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>