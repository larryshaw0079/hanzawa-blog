<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="website"><meta property="og:title" content="Hanzawa の 部屋"><meta property="og:url" content="https://larryshaw0079.github.io/hanzawa-blog"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://larryshaw0079.github.io/img/og_image.png"><meta property="article:author" content="Hanzawa"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://larryshaw0079.github.io/hanzawa-blog"},"headline":"Hanzawa の 部屋","image":["https://larryshaw0079.github.io/img/og_image.png"],"author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":null}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-02-24T02:45:08.000Z" title="2020-2-24 10:45:08 ├F10: AM┤">2020-02-24</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:30:55.514Z" title="2020-6-25 1:30:55 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/">Deep Anomaly Detection with Deviation Networks</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文关注<code>Deep Anomaly Detection</code>，也就是用深度学习的方法来进行异常检测。文中提到现有的<code>Deep Anomaly Detection</code>存在两个弊端：一个是采用深度学习方法来进行特征学习，然后通过下游任务得到<code>Anomaly Score</code>，相比文中End-to-End的<code>Anomaly Score</code>学习，存在优化不充分的风险；另一个是现有的方法主要是无监督学习，无法利用已知的信息（如少量标签）。为此，本文提出了一种端到端的异常检测框架，来解决上述问题。</p>
<p>本文的主要贡献如下：</p>
<ul>
<li>提出了一种端到端的异常检测框架，直接学习<code>Anomaly Score</code>并且可以利用已知信息；</li>
<li>基于提出的框架，文中提出了一种实例方法 (DevNet)。</li>
</ul>
<img src="https://i.loli.net/2020/06/25/XT7fqQRWEOuocgy.png" style="zoom:67%;" />

<h1 id="Proposed-Model"><a href="#Proposed-Model" class="headerlink" title="Proposed Model"></a>Proposed Model</h1><h2 id="End-To-End-Anomaly-Score-Learning"><a href="#End-To-End-Anomaly-Score-Learning" class="headerlink" title="End-To-End Anomaly Score Learning"></a>End-To-End Anomaly Score Learning</h2><h3 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h3><p>为了区别于传统的两阶段异常检测（先学习特征表示，然后在学到的特征上定义一个<code>anomaly measure</code>来得到<code>anomaly score</code>），作者对端到端的异常检测问题重新进行形式化。</p>
<p>给定$N+K$个样本$\mathcal{X}={\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N,\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}}$，其中$\boldsymbol x_i\in\mathbb{R}^D$，无标签样本集$\mathcal{U}={\boldsymbol x_1,\boldsymbol x_2,\cdots,\boldsymbol x_N}$，有标签样本集$\mathcal{K}={\boldsymbol x_{N+1},\cdots,\boldsymbol x_{N+K}}$，且$K\ll N$。异常检测的目标是学习一个<code>anomaly scoring function</code>$\phi:\mathcal{X}\mapsto\mathbb{R}$使得$\phi(\boldsymbol x_i)&gt;\phi(\boldsymbol x_j)$，其中$\boldsymbol x_i$为异常样本，$\boldsymbol x_j$为正常样本。</p>
<h3 id="The-Proposed-Framework"><a href="#The-Proposed-Framework" class="headerlink" title="The Proposed Framework"></a>The Proposed Framework</h3><p>为了解决这个问题，文中提出了一种通用异常检测框架，模型框架如下图所示：</p>
<p>模型框架如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/ZuE1mb2Ytv6Jdl7.png" style="zoom:50%;" />

<p>主要包含三个部分：</p>
<ol>
<li><em>anomaly scoring network</em>. 图中左边的部分，一个函数$\phi$，输入样本$\mathbf{x}$，输出<code>anomaly score</code></li>
<li><em>reference score generator</em>. 图中右边的部分。只有一个<em>anomaly scoring network</em>并不能进行训练，需要训练的目标。为此加入<em>reference score generator</em>，输入为随机选择的$l$个正常样本，输出<code>reference score</code>（这$l$个正常样本<code>anomaly score</code>的均值，记为$\mu_\mathcal{R}$）</li>
<li><em>deviation loss</em>. $\phi(\mathbf{x})$，$\mu_\mathcal{R}$及对应的标准差$\sigma_\mathcal{R}$作为<code>deviation loss</code>函数的输入。因为$\mu_\mathcal{R}$和$\sigma_\mathcal{R}$对应正常样本集的均值和方差，那么异常样本的<code>anomaly score</code>应该和$\mu_\mathcal{R}$差别比较大，而正常样本则应该接近$\mu_\mathcal{R}$。</li>
</ol>
<h2 id="Deviation-Networks"><a href="#Deviation-Networks" class="headerlink" title="Deviation Networks"></a>Deviation Networks</h2><p>下面是上述三个部件的具体实现。</p>
<h3 id="End-To-End-Anomaly-Scoring-Network"><a href="#End-To-End-Anomaly-Scoring-Network" class="headerlink" title="End-To-End Anomaly Scoring Network"></a>End-To-End Anomaly Scoring Network</h3><p>记$\mathcal{Q}\in\mathbb{R}^M$为中间表示空间，<code>anomaly scoring network</code>$\phi(\cdot;\Theta):\mathcal{X}\mapsto\mathbb{R}$可以定义为数据表示学习$\psi(\cdot;\Theta_t):\mathcal{X}\mapsto\mathcal{Q}$和异常分数学习$\eta(\cdot;\Theta_s):\mathcal{Q}\mapsto\mathbb{R}$两阶段的组合，其中$\Theta={\Theta_t,\Theta_s}$。</p>
<p>$\psi(\cdot;\Theta_t)$可以用一个$H$层神经网络来实现：<br>$$<br>\mathrm{q}=\psi(\mathbf{x};\Theta_t)<br>$$<br>其中$\mathbf{x}\in\mathcal{X}$，$\mathrm{q}\in\mathcal{Q}$。</p>
<p>$\eta(\cdot;\Theta_s)$可以用一个单层的神经网络来实现：<br>$$<br>\eta(\mathrm q;\Theta_s)=\sum\limits_{i=1}^M w_i^oq_i+w_{M+1}^o<br>$$<br>其中$\mathrm q\in\mathcal Q$，$\Theta_s={\mathbf{w}^o}$。</p>
<p>所以有：<br>$$<br>\phi(\mathbf{x};\Theta)=\eta(\psi(\mathbf{x};\Theta_t);\Theta_s)<br>$$</p>
<h3 id="Gaussian-Prior-based-Reference-Scores"><a href="#Gaussian-Prior-based-Reference-Scores" class="headerlink" title="Gaussian Prior-based Reference Scores"></a>Gaussian Prior-based Reference Scores</h3><p>有两种方法来获得$\mu_\mathcal{R}$，一种是data-driven，一种是prior-driven。如果是data-driven的话则采用另一个神经网络，文中表示为了更好的解释性和计算效率，所以采用的是prior-driven。<br>$$<br>\begin{align}<br>r_1,r_2,\cdots,r_l\sim \mathcal{N}(\mu,\sigma^2),\<br>\mu_\mathcal{R}=\frac{1}{l}\sum\limits_{i=1}^l r_i<br>\end{align}<br>$$<br>在文中，采用的prior是标准高斯分布。</p>
<h2 id="Z-Score-Based-Deviation-Loss"><a href="#Z-Score-Based-Deviation-Loss" class="headerlink" title="Z-Score Based Deviation Loss"></a>Z-Score Based Deviation Loss</h2><p><em>anomaly scoring network</em>的优化目标可以定义为Z-Score的方式：<br>$$<br>dev(\boldsymbol x)=\frac{\phi(\boldsymbol x;\Theta)-\mu_{\mathcal{R}}}{\sigma_{\mathcal{R}}}<br>$$<br>$dev(\boldsymbol x)$可以看作是样本偏离标准的程度，而我们肯定希望异常样本偏离标准越大，正常样本越接近标准。文中采用的损失函数是<code>Contrastive Loss</code>：<br>$$<br>L(\phi(\boldsymbol x;\Theta),\mu_\mathcal{R},\sigma_\mathcal{R})=(1-y)|dev(\boldsymbol x)| + y \max(0, a - dev(\boldsymbol x))<br>$$<br><code>Contrastive Loss</code>的直观解释可以看下图：</p>
<img src="https://i.loli.net/2020/06/25/aPbSipCsk2JwNcD.png" style="zoom: 33%;" />

<p>对于负例（正常），优化过程将他们尽量向原点靠近，对于正例（异常），优化过程将他们拉向边界。</p>
<h2 id="The-DevNet-Algorithm"><a href="#The-DevNet-Algorithm" class="headerlink" title="The DevNet Algorithm"></a>The DevNet Algorithm</h2><p><code>DevNet</code>的算法流程图如下：</p>
<img src="https://i.loli.net/2020/06/25/km9H5DoNRbOQ784.png" style="zoom:67%;" />

<h2 id="Interpretability-of-Anomaly-Scores"><a href="#Interpretability-of-Anomaly-Scores" class="headerlink" title="Interpretability of Anomaly Scores"></a>Interpretability of Anomaly Scores</h2><p>因为<em>reference score generator</em>选择的是确定的高斯分布，于是可以用概率论给出一些解释性。作者给出了一个结论，</p>
<blockquote>
<p><strong>PROPOSITION</strong>： 设$\boldsymbol x\in\mathcal{X}$，$z_p$为$\mathcal{N}(\mu,\sigma^2)$的分位数，那么$\phi(\boldsymbol x)$在区间$\mu\pm z_p\sigma$的概率为$2(1-p)$。</p>
</blockquote>
<p>例如，假设$p=0.95$，那么$z_{0.95}=1.96$，表示异常分数高于1.96的样本将以0.95的置信度为异常。</p>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>实验用到了9个数据集，4个Baseline (REPEN，DSVDD，FSNET，iForest)，以及ROC和PR曲线两种评测标准。</p>
<h2 id="Effectiveness-in-Real-world-Data-Sets"><a href="#Effectiveness-in-Real-world-Data-Sets" class="headerlink" title="Effectiveness in Real-world Data Sets"></a>Effectiveness in Real-world Data Sets</h2><h3 id="Experiment-Settings"><a href="#Experiment-Settings" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p>这一个实验主要是为了验证算法在真实场景下的效果，即大量无标签数据和极少量标签数据。训练集包含两部分，一部分是无标签数据$\mathcal{U}$,包含$2%$的异常样本，另一部分是有标签数据$\mathcal{K}$，由随机采样$0.005%-1%$的训练数据和$0.08%-6%$的异常样本组成。</p>
<h3 id="Findings"><a href="#Findings" class="headerlink" title="Findings"></a>Findings</h3><p>实验结果如下表所示：</p>
<img src="https://i.loli.net/2020/06/25/DKqxJROngML8IS2.png" style="zoom: 50%;" />

<p>从结果上看来，本文提出的方法在所有数据集上都比Baseline好，说明<code>DevNet</code>端到端直接优化<code>Anomaly Score</code>的方式是有效的。</p>
<h2 id="Data-Efficiency"><a href="#Data-Efficiency" class="headerlink" title="Data Efficiency"></a>Data Efficiency</h2><h3 id="Experiment-Settings-1"><a href="#Experiment-Settings-1" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p>这一个实验主要是为了探究基于深度的异常检测方法的<em>data efficiency</em>。和上一个实验一样，无标签数据集包含$2%$的异常，而有标签的异常数量从$5$到$120$不等。本实验试图回答以下两个问题：</p>
<ul>
<li><code>DevNet</code>的<em>data efficiency</em>如何？</li>
<li>基于深度的方法在多大程度上能够利用标签信息？</li>
</ul>
<h3 id="Findings-1"><a href="#Findings-1" class="headerlink" title="Findings"></a>Findings</h3><p>在几个基于深度的Baseline中，<code>DevNet</code>的效果是最好的，同时在有标签异常非常有限的情况下，<code>DevNet</code>也能很好的利用标签信息，达到更好的效果。</p>
<img src="https://i.loli.net/2020/06/25/iIWGBPosKCuxbRF.png" style="zoom:67%;" />

<h2 id="Robustness-w-r-t-Anomaly-Contamination"><a href="#Robustness-w-r-t-Anomaly-Contamination" class="headerlink" title="Robustness w.r.t. Anomaly Contamination"></a>Robustness w.r.t. Anomaly Contamination</h2><h3 id="Experiment-Settings-2"><a href="#Experiment-Settings-2" class="headerlink" title="Experiment Settings"></a>Experiment Settings</h3><p>在第一个实验中，无标签数据集$\mathcal{U}$包含的是固定的异常比例$2%$，而在这个实验中，作者测试了从$0%$到$20%$之间不同异常比例来测试算法的鲁棒性（即使$\mathcal{U}$中包含异常，由于没有标签，在训练的时候仍然假设都为正常来进行训练）。本实验试图回答以下问题：</p>
<ul>
<li>基于深度的异常检测方法的鲁棒性如何？</li>
<li>当训练集中异常污染的比例较高的时候基于深度的方法能否打败无监督的方法？</li>
</ul>
<h3 id="Findings-2"><a href="#Findings-2" class="headerlink" title="Findings"></a>Findings</h3><p>下图为实验结果：</p>
<img src="https://i.loli.net/2020/06/25/JCnIjLOc84RFP2V.png" style="zoom:67%;" />

<p>从结果上来看，<code>DevNet</code>比其他基于深度的方法鲁棒性更好，同时在高异常污染的情况下仍然比纯无监督方法效果要好。</p>
<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><p>本实验设置了<code>DevNet</code>的三个变体（默认的<code>DevNet-Def</code>为单层隐层加上一个输出层）来进行消融实验，分别是：</p>
<ul>
<li><code>DevNet-Rep</code>，去掉了<em>anomaly scoring network</em>网络的输出层，对应<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>；</li>
<li><code>DevNet-Linear</code>，去掉了网络中的非线性层，对应<em>learning of non-linear features</em>；</li>
<li><code>DevNet-3HL</code>，隐层数量为3层。</li>
</ul>
<p>对比结果如下：</p>
<img src="https://i.loli.net/2020/06/25/5LcyAwGMB8gb2UP.png" style="zoom:67%;" />

<p>通过实验可以发现，<code>DevNet-Rep</code>说明了<em>end-to-end learning of anomaly scores</em>和<em>deviation loss</em>的有效性，而<code>DevNet-Linear</code>说明了<em>learning of non-linear features</em>的重要性。<code>DevNet-3HL</code>说明了加深网络并不总能带来性能的提升。</p>
<h2 id="Scalability-Test"><a href="#Scalability-Test" class="headerlink" title="Scalability Test"></a>Scalability Test</h2><p>这一个实验使用合成的数据来测试算法对大规模数据的处理能力，分别从<em>Data Size</em>和<em>Data Dimensionality</em>两方面来测试。结果如下：</p>
<img src="https://i.loli.net/2020/06/25/5gbPdJkB47e3FsV.png" style="zoom:67%;" />

<p>可以看出，<code>DevNet</code>对<em>Data Size</em>并不敏感，同时，面对高维数据，<code>DevNet</code>也没有表现出劣势。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-01-31T03:25:59.000Z" title="2020-1-31 11:25:59 ├F10: AM┤">2020-01-31</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:39:27.592Z" title="2020-6-25 1:39:27 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Technical-Notes/">Technical Notes</a><span> / </span><a class="link-muted" href="/categories/Technical-Notes/Misc/">Misc</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/">Geant4 安装教程与调试环境配置</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Geant4安装的教程很多，版本都很旧了，这里写一个新版本（10.6）基于Ubuntu的安装教程，并且开启CLion IDE调试。</p>
<h1 id="Step-1-Download-Packages"><a href="#Step-1-Download-Packages" class="headerlink" title="Step 1: Download Packages"></a>Step 1: Download Packages</h1><p>首先进入官网(<a target="_blank" rel="noopener" href="http://geant4.web.cern.ch/support/download">http://geant4.web.cern.ch/support/download</a>)下载源代码（推荐tar.gz格式）及数据文件，解压。新建一个文件夹专门用来放<code>Geant4</code>相关文件，新建data，source，build文件夹，将Geant4的文件复制进来并按如下结构组织：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build</span><br><span class="line">├── data</span><br><span class="line">│   ├── G4ABLA3.1</span><br><span class="line">│   ├── G4EMLOW7.9</span><br><span class="line">│   ├── G4ENSDFSTATE2.2</span><br><span class="line">│   ├── G4INCL1.0</span><br><span class="line">│   ├── G4NDL4.6</span><br><span class="line">│   ├── G4PARTICLEXS2.1</span><br><span class="line">│   ├── G4PII1.3</span><br><span class="line">│   ├── G4SAIDDATA2.0</span><br><span class="line">│   ├── G4TENDL1.3.2</span><br><span class="line">│   ├── PhotonEvaporation5.5</span><br><span class="line">│   ├── RadioactiveDecay5.4</span><br><span class="line">│   └── RealSurface2.1.1</span><br><span class="line">└── <span class="built_in">source</span></span><br><span class="line">    └── geant4.10.06</span><br></pre></td></tr></table></figure>



<p><img src="https://i.loli.net/2020/06/25/OuZaAJ3WyEYwsG7.png"></p>
<p><img src="https://i.loli.net/2020/06/25/R5Tmk68AhPbaDHY.png"></p>
<h1 id="Step-2-Install-Dependencies"><a href="#Step-2-Install-Dependencies" class="headerlink" title="Step 2: Install Dependencies"></a>Step 2: Install Dependencies</h1><p>安装编译所需环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential cmake</span><br></pre></td></tr></table></figure>

<p>安装相关依赖：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libgl1-mesa-dev libglu1-mesa-dev libxt-dev libxmu-dev libxi-dev zlib1g-dev libgl2ps-dev libexpat1-dev libxerces-c-dev</span><br></pre></td></tr></table></figure>



<p>如果要用到QT需要单独安装QT。</p>
<h1 id="Step-3-Compile"><a href="#Step-3-Compile" class="headerlink" title="Step 3: Compile"></a>Step 3: Compile</h1><p>进入build文件夹，用cmake命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake ../<span class="built_in">source</span>/geant4.10.06/ -DCMAKE_BUILD_TYPE=DEBUG -DGEANT4_USE_GDML=ON -DGEANT4_USE_OPENGL_X11=ON -DGEANT4_USE_RAYTRACER_X11=ON -DGEANT4_BUILD_MULTITHREADED=ON</span><br></pre></td></tr></table></figure>

<p>其中<code>../source/geant4.10.06/</code>替换成换成（如果版本不一样）你自己的Geant4源代码所在目录，需要QT则加上<code>-DGEANT4_USE_QT=ON</code>。如果不需要调试则把<code>-DCMAKE_BUILD_TYPE=DEBUG</code>改成<code>-DCMAKE_BUILD_TYPE=RELEASE</code>。<code>-DGEANT4_BUILD_MULTITHREADED=ON</code>是多线程，视情况开启。</p>
<p>完成之后开始编译：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make -jX</span><br></pre></td></tr></table></figure>

<p><code>-jX</code>为多线程编译，如<code>-j8</code>。</p>
<p>编译完成之后进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<h1 id="Step-4-Configure"><a href="#Step-4-Configure" class="headerlink" title="Step 4: Configure"></a>Step 4: Configure</h1><p>安装的默认路径在<code>/usr/local/share/Geant4-10.6.0</code>，将下载的数据文件复制到该文件夹：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp -r ./data/ /usr/<span class="built_in">local</span>/share/Geant4-10.6.0/</span><br></pre></td></tr></table></figure>

<p>之后，在<code>~/.bashrc</code>里添加<code>/usr/local/share/Geant4-10.6.0/geant4make/geant4make.sh</code>，如果你的版本和我的不一样，相应修改即可。</p>
<h1 id="Step-5-CLion-Configuration"><a href="#Step-5-CLion-Configuration" class="headerlink" title="Step 5: CLion Configuration"></a>Step 5: CLion Configuration</h1><p>最后我们来配置CLion环境，配好之后可以在IDE中编写<code>Geant4</code>代码，还可以断点调试，非常方便。安装CLion的过程这里省略，打开一个<code>Geant4</code>自带的例子或者自己新建一个项目，打开<code>Edit Configurations</code>。</p>
<p><img src="https://i.loli.net/2020/06/25/W1xXHUqIvofyKQk.png"></p>
<p>随便打开一个终端，输入一下命令获取环境变量：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env | grep G4</span><br></pre></td></tr></table></figure>

<p>在<code>Environment variables</code>填入刚才获取的环境变量（复制之后按一下粘贴就可以了），然后把<code>Working directory</code>设置成当前文件夹。</p>
<p><img src="https://i.loli.net/2020/06/25/HrslFTOau51y8tX.png"></p>
<p><img src="https://i.loli.net/2020/06/25/5UKaCgvnWjYmNTG.png"></p>
<p>现在就大功告成了！</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-01-09T02:15:03.000Z" title="2020-1-9 10:15:03 ├F10: AM┤">2020-01-09</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:25:53.385Z" title="2020-6-25 1:25:53 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/">Complementary Set Variational Autoencoder for Supervised Anomaly Detection</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对于异常检测问题，异常的模式是多种多样的。有监督模型能够较好地处理训练集中出现过的模式，无监督模型能够处理训练集中未出现过的模式，但对于训练集中出现过的异常模型并没有学习。本文提出了一种既能学习训练集中出现过的异常模式，同时能处理未出现过的异常模式的方法。</p>
<h1 id="Proposed-Model"><a href="#Proposed-Model" class="headerlink" title="Proposed Model"></a>Proposed Model</h1><h2 id="Conventional-VAE"><a href="#Conventional-VAE" class="headerlink" title="Conventional VAE"></a>Conventional VAE</h2><p>首先回顾一下原始的VAE。</p>
<p>原始VAE中的损失函数为：<br>$$<br>\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})=\mathbb{E}<em>{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}\parallel p(\boldsymbol{z}))]<br>$$<br>原文中作者证明了$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\leq\log p(\boldsymbol{x};\boldsymbol{\theta})$，所以$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})$可以看作是数据分布$p(\boldsymbol{x})$对数似然的一个下界。$\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})$又被称为证据下界 (ELBO)。$\mathbb{E}</em>{q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})}[\log p(\boldsymbol{x}|\boldsymbol{z};\boldsymbol{\theta})]$中的期望一般用蒙特卡洛来进行估计：<br>$$<br>\begin{align}<br>\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq&amp; \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-\text{KL}[q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi})\parallel p(\boldsymbol{z})],\<br>\boldsymbol{z}^{(l)}&amp;\sim q(\boldsymbol{z}|\boldsymbol{x};\boldsymbol{\phi}), \space l\in{1,2,\cdots,L}<br>\end{align}<br>$$<br>对于隐变量$\boldsymbol{z}$，一般假设先验服从标准高斯分布，后验服从均值为$\mu$，方差为$\sigma^2$的高斯分布，故KL散度能直接写出解析式：<br>$$<br>\mathcal{L}(\boldsymbol{\theta},\boldsymbol{\phi};\boldsymbol{x})\simeq \frac{1}{L}\sum\limits_l\log p(\boldsymbol{x}|\boldsymbol{z}^{(l)};\boldsymbol{\theta})-C(-\frac{1}{2}-\log\sigma+\frac{1}{2}\sigma^2+\frac{1}{2}\mu^2)<br>$$<br>使用VAE来做异常检测通常是在正常数据上进行训练，在检测阶段，如果是异常样本，那么VAE不能很好地重构它，这样会导致较大的重构误差。</p>
<h2 id="Prior-Distribution-for-Anomalies"><a href="#Prior-Distribution-for-Anomalies" class="headerlink" title="Prior Distribution for Anomalies"></a>Prior Distribution for Anomalies</h2><img src="https://i.loli.net/2020/06/25/vrxAzRVCtaE3oLc.png" style="zoom:67%;" />

<p>在原始VAE异常检测中，无论输入样本$\boldsymbol{x}$是否异常，VAE都会使对应编码的后验$p(\boldsymbol{z}|\boldsymbol{x})$服从高斯分布，且施加标准高斯分布的约束。在本文中，作者对异常和正常样本对应的隐变量的先验分布做了不同假设。首先，正常先验依然是标准高斯分布，记为$p_n(\boldsymbol{z})$。而对于异常先验，作者认为异常即为“不正常”，和正常是补集的关系。作者在文中定义异常先验分布$p_a(\boldsymbol{z})$为：<br>$$<br>p_a(\boldsymbol{z})=\frac{1}{Y^\prime}(\max\limits_{\boldsymbol{z}^\prime}p_n(\boldsymbol{z}^\prime)-p_n(\boldsymbol{z}))<br>$$</p>
<p>其中$Y^\prime$为使$p_a(\boldsymbol{z})$成为一个概率分布的调节因子。实际上，$Y^\prime$往往会成为无限大，因为$p(\boldsymbol z)$在整个定义域上都有定义。为了解决这个问题，作者加入了$p_w(\boldsymbol z)$，一个在每个维度都足够宽的辅助分布：</p>
<p>$$<br>p_a(\boldsymbol z)=\frac{1}{Y}p_w(\boldsymbol z)\left(\max\limits_{\boldsymbol z^\prime}p_n(\boldsymbol z^\prime)-p_n(\boldsymbol z)\right)<br>$$</p>
<p>其中$Y$为有限的常数。在文中$p_n(\boldsymbol z)$和$p_w(\boldsymbol z)$都为高斯分布，那么$p_a(\boldsymbol z)$的具体形式为：</p>
<p>$$<br>p_a(\boldsymbol z)=\frac{1}{Y}\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2){\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)-\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol 1)}<br>$$</p>
<p>其中：</p>
<p>$$<br>\max\limits_{\boldsymbol z^\prime}\mathcal N(\boldsymbol z^\prime;\boldsymbol 0,\boldsymbol 1)=\frac{1}{\sqrt{2\pi}}<br>$$</p>
<p>$$<br>Y=\int_{-\infty}^{\infty}p_a(\boldsymbol z)\mathrm{d}\boldsymbol z=\frac{1}{\sqrt{2\pi}}\left{1-\frac{1}{\boldsymbol s^2+1}\right}<br>$$</p>
<p>$\boldsymbol s^2$为超参数，控制分布的宽度。用文中的先验替换VAE原始的KL散度，可写为：</p>
<p>$$<br>\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]=\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\frac{\mathcal N(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)}{\frac{1}{Y}\mathcal N(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\left{\frac{1}{2\pi}-\mathcal N(\boldsymbol z;\boldsymbol0,\boldsymbol 1)\right}}\mathrm{d}\boldsymbol z<br>$$</p>
<p>展开后：</p>
<p>$$<br>\begin{align}<br>\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;=<br>\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol\mu,\boldsymbol\sigma^2)\mathrm{d}\boldsymbol z\<br>&amp;+\log Y\<br>&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\mathcal{N}(\boldsymbol z;\boldsymbol 0,\boldsymbol s^2)\mathrm{d}\boldsymbol z\<br>&amp;-\int_{-\infty}^\infty\mathcal{N}(\boldsymbol z;\boldsymbol \mu,\boldsymbol \sigma^2)\log\left{\frac{1}{\sqrt{2\pi}}-\mathcal{N}(\boldsymbol z;\boldsymbol 0, \boldsymbol 1)\right}\mathrm{d}\boldsymbol z<br>\end{align}<br>$$</p>
<p>使用泰勒展开，$\log (x+\frac{1}{2\pi})\simeq-\log 2\pi+2\pi x$，KL散度可以用下式估计：</p>
<p>$$<br>\begin{align}<br>\text{KL}\left[q(\boldsymbol z|\boldsymbol x;\phi)\parallel p_a(\boldsymbol z)\right]&amp;\simeq\sqrt{\frac{2\pi}{\boldsymbol\sigma^2+1}}\exp\left(\frac{-\boldsymbol\mu^2}{2(\boldsymbol\sigma^2+1)}\right)\<br>&amp;+\frac{\boldsymbol\mu^2+\boldsymbol\sigma^2}{2\boldsymbol s^2}-\log\boldsymbol\sigma+\log\boldsymbol s+\log\left(\sqrt{\boldsymbol s^2+1}-1\right)\<br>&amp;-\frac{\log(\boldsymbol s^2+1)}{2}+\frac{\log(2\pi)-1}{2}<br>\end{align}<br>$$</p>
<p>下图为一维时$p_n(\boldsymbol z)$和$p_a(\boldsymbol z)$的示例：</p>
<img src="https://i.loli.net/2020/06/25/QHXo24cKj9uRwzW.png" style="zoom:67%;" />

<h3 id="Implementation-of-proposed-method"><a href="#Implementation-of-proposed-method" class="headerlink" title="Implementation of proposed method"></a>Implementation of proposed method</h3><p>文中使用编码器输出的分布$\mathcal{N}(\boldsymbol z;\boldsymbol \mu, \boldsymbol \sigma^2)$与标准正态分布之间的KL散度来作为异常分数。在每一轮的训练过程中，加入一轮使用Anomaly Prior的训练。</p>
<img src="https://i.loli.net/2020/06/25/wrmzADXtsyuJ6EZ.png" style="zoom:67%;" />

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><p>作者设计了两个Task：</p>
<ol>
<li>Task 1. $N$ vs. $\bar{N}$. 将手写数字中的一个作为已知异常，其他作为正常，并加入均匀分布作为未知的异常。</li>
<li>Task 2. 手写数字被分为3组：已知异常，正常，未知异常。</li>
</ol>
<p>细节如下表所示：</p>
<img src="https://i.loli.net/2020/06/25/ifcIxr9zOpEhksA.png" style="zoom:67%;" />

<p>在实现上，使用Adam优化器，<code>batch_size</code>为100，<code>epochs</code>为200。<code>Encoder</code>和<code>Decoder</code>都由三层感知机组成，超参数$s^2$设置为400。评测标准使用AUC (area under the receiver characteristic curve)。</p>
<p>下表为实验结果：</p>
<img src="https://i.loli.net/2020/06/25/YTpO98y1ZPmAK3g.png" style="zoom:67%;" /></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-29T03:36:22.000Z" title="2019-10-29 11:36:22 ├F10: AM┤">2019-10-29</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:20:44.374Z" title="2020-6-25 1:20:44 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/">Anomaly Detection in Streams with Extreme Value Theory</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文基于<strong>Extreme Value Theory</strong>提出了一种不需要手动设置阈值也不需要对数据分布作任何假设的时间序列异常检测方法。除此之外，本方法可以用在通用的自动阈值选择的场合中。</p>
<p><a target="_blank" rel="noopener" href="https://www.kdd.org/kdd2017/papers/view/anomaly-detection-in-streams-with-extreme-value-theory">原文</a></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>在很多情况下我们需要进行阈值的选择。阈值的选择可以通过实验的方法或者对数据分布进行假设的方法来得到，不过这样做通常不准确。借助<strong>Extreme Value Theory</strong>我们可以在不需要对原始数据的分布作很强的假设的情况下，推断我们想要的极端事件的分布（在异常检测中就是异常值）。</p>
<p>下面给出一些数学符号，$X$为随机变量，$F$为累积分布函数，即$F(x)=\mathbb{P}(X\leq x)$。记$F$的“末尾”分布$\bar{F}(x)=1-F(x)=\mathbb{P}(X&gt;x)$。对于一个随机变量$X$和给定的概率$q$，记$z_q$为在$1-q$水平的分位数，即$z_q$为满足$\mathbb{P}(X\leq z_q)\geq 1-q$最小的值。</p>
<h2 id="Extreme-Value-Distributions"><a href="#Extreme-Value-Distributions" class="headerlink" title="Extreme Value Distributions"></a>Extreme Value Distributions</h2><p><strong>Extreme Value Theory</strong>主要是为了找出极端事件发生的规律，有学者证明，在很弱的条件下，所有极端事件都服从一个特定的分布，而不管原始分布如何。具体形式如下：</p>
<p>$$<br>G_\gamma:x\mapsto \exp(-(1+\gamma x)^{-\frac{1}{\gamma}}), \space\space\space\space\space\gamma\in\mathbb{R}, \space\space\space\space\space 1+\gamma x&gt;0<br>$$</p>
<p>其中$\gamma$称为<strong>Extreme Value Index</strong>，由原始分布决定。</p>
<p>更严谨的说法是Fisher-Tippett-Gnedenko定理（极值理论第一定理）：</p>
<blockquote>
<p>**THEOREM: **(Fisher-Tippett-Gnedenko). 令$X_1,X_2,\cdots,X_n,\cdots$为独立同分布的随机变量序列，$M_n=\max {X_1,\cdots,X_n}$。如果实数对序列$(a_n,b_n)$存在且满足$a_n&gt;0$和$\lim\limits_{n\rightarrow \infty}P\left(\frac{M_n-b_n}{a_n}\leq x\right)=F(x)$，其中$F$为非退化分布函数，那么$F$属于Gumbel、Fréchet或Weibull分布族（或总称Generalized Extreme Value Distribution）中的一种。</p>
</blockquote>
<p>这是一个反直觉的结论，但是想到当事件发生变得极端时，即$\mathbb{P}(X&gt;x)\rightarrow 0$，$\bar{F}(x)=\mathbb{P}(X&gt;x)$分布的形状其实并没有很多种选择。Table 1展示了几种不同分布对应的$\gamma$：</p>
<p><img src="https://i.loli.net/2020/06/24/jyhoWZGc2gFTrJv.png"></p>
<p>Figure 1展示了几种不同$\gamma$情况下的“末尾”分布：</p>
<p><img src="https://i.loli.net/2020/06/24/4rmZL1AMcBJ2Vzq.png"></p>
<h2 id="Power-of-EVT"><a href="#Power-of-EVT" class="headerlink" title="Power of EVT"></a>Power of EVT</h2><p>根据<strong>Extreme Value Theory</strong>，我们可以在原始分布未知的情况下计算极端事件的概率。但是$\bar{G}_\gamma$分布中参数$\gamma$是未知的，我们需要一种高效的方法来进行估计。<strong>The Peaks-Over-Threshold</strong> (POT) 方法是本文介绍的一种方法。</p>
<p><img src="https://i.loli.net/2020/06/24/hX2T1IkMAqfioZl.png"></p>
<h2 id="Peaks-Over-Threshold-Approach"><a href="#Peaks-Over-Threshold-Approach" class="headerlink" title="Peaks-Over-Threshold Approach"></a>Peaks-Over-Threshold Approach</h2><p>POT方法依赖于Pickands-Balkema-De Haan定理（极值理论第二定理），维基百科版：</p>
<blockquote>
<p>考虑一个未知分布$F$和随机变量$X$，我们的目标是估计$X$在超过确定阈值$u$下的条件分布$F_u$，定义为：<br>$$<br>F_u(y)=P(X-u\leq y|X&gt;u)=\frac{F(u+y)-F(u)}{1-F(u)}<br>$$<br>其中$0\leq y\leq x_F-u$，$x_F$为$F$的右端点。$F_u$描述了超过特征阈值$u$的分布，称为<strong>Conditional Excess Distribution Function</strong>。</p>
<p>**STATEMENT: **(Pickands-Balkema-De Haan). 设$(X_1,X_2,\cdots)$为独立同分布随机变量序列，$F_u$为相应的Conditional Excess Distribution Function。对于一大类的$F$和很大的$u$，$F_u$能够很好的被Generalized Pareto Distribution所拟合：<br>$$<br>F_u(y)\rightarrow G_{k,\sigma}(y),\space\space \text{as } u\rightarrow \infty<br>$$<br>其中：<br>$$<br>G_{k,\sigma}(y)=<br>\begin{cases}<br>1-(1+ky/\sigma)^{-1/k}, &amp;\text{if }k\neq 0\<br>1-e^{-y/\sigma}, &amp;\text{if }k=0<br>\end{cases}<br>$$<br>当$k\geq 0$时$\sigma&gt;0, y\geq 0$，$k&lt;0$时$0\leq y\leq -\sigma/k$。</p>
</blockquote>
<p>论文中给出的定理如下：</p>
<blockquote>
<p>**THEOREM: **(Pickands-Balkema-De Haan). 累积概率密度函数$F\in\mathcal{D}<em>\gamma$当且仅当函数$\sigma$存在时，对所有$x\in\mathbb{R}$在$1+\gamma x&gt;0$的条件下有：<br>$$<br>\frac{\bar{F}(t+\sigma(t)x)}{\bar{F}(t)}\mathop{\rightarrow}\limits</em>{t\rightarrow\tau}(1+\gamma x)^{-\frac{1}{\gamma}}<br>$$</p>
</blockquote>
<p>上式可以写成如下形式：<br>$$<br>\bar{F}<em>t(x)=\mathbb{P}(X-t&gt;x|X&gt;t)\mathop{\sim}\limits</em>{t\rightarrow\tau}\left(1+\frac{\gamma x}{\sigma(t)}\right)^{-\frac{1}{\gamma}}<br>$$<br>该式表明$X$超过阈值$t$的概率（写为$X-t$）服从<strong>Generalized Pareto Distribution</strong> (GPD)，参数为$\gamma$和$\sigma$。POT主要是拟合GPD而不是EVT分布。</p>
<p>如果我们要估计参数$\hat{\gamma}$和$\hat{\sigma}$，分位数可以通过下式计算得到：<br>$$<br>z_q\simeq t+\frac{\hat{\sigma}}{\hat{\gamma}}\left(\left(\frac{qn}{N_t}\right)^{-\hat{\gamma}}-1\right)<br>$$</p>
<p>其中$t$是一个“很高”的阈值，$q$是给定的概率值，$n$是所有观测样本的数量，$N_t$是peaks的数量，即$X_i&gt;t$的数量。为了进行高效的参数估计，文中使用了极大似然估计。</p>
<h2 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h2><p>设$X_1,\cdots,X_n$为独立同分布的随机变量，概率密度函数记为$f_\theta$，$\theta$为分布中的参数，那么似然函数可以写为：</p>
<p>$$<br>\mathcal{L}(X_1,\cdots,X_n;\theta)=\prod\limits_{i=1}^n f_\theta(X_i)<br>$$</p>
<p>在极大似然估计中，我们需要找到合适的参数使得似然函数最大化。在我们的问题中，似然函数如下：<br>$$<br>\log\mathcal{L}(\gamma,\sigma)=-N_t\log\sigma-\left(1+\frac{1}{\gamma}\right)\sum\limits_{i=1}^{N_t}\log\left(1+\frac{\gamma}{\sigma}Y_i\right)<br>$$<br>其中$Y_i&gt;0$表示$X_i$超过阈值$t$的部分。</p>
<p>文中使用了<strong>Grimshaw’s Trick</strong>来将含两个参数的优化问题转换为只含一个参数的优化问题。记$\ell(\gamma,\sigma)=\log\mathcal{L}(\gamma,\sigma)$，对于所有极值来说有$\nabla \ell(\gamma, \sigma)=0$。Grimshaw’s Trick表明对于满足$\nabla \ell(\gamma, \sigma)=0$的一对$(\gamma^*,\sigma^*)$，$x^*=\frac{\gamma^*}{\sigma^*}$为等式$u(X)v(X)=1$的解，其中：<br>$$<br>\begin{align}<br>u(x)&amp;=\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\frac{1}{1+xY_i}\<br>v(x)&amp;=1+\frac{1}{N_t}\sum\limits_{i=1}^{N_t}\log(1+xY_i)<br>\end{align}<br>$$<br>在找到满足该等式的解$x^*$后，我们可以得到$\gamma^*=v(x^*)-1$和$\sigma^*=\gamma^*/x^*$，于是问题就变成了如何寻找方程的所有根。</p>
<p>因为$\log$的存在，所以有$1+xY_i&gt;0$。而$Y_i$是正数，所以$x^*$的范围一定在$\left(-\frac{1}{Y^M},+\infty\right)$，其中$Y^M=\max Y_i$。</p>
<p>Grimshaw（作者参考的一篇<a target="_blank" rel="noopener" href="https://www.tandfonline.com/doi/abs/10.1080/00401706.1993.10485040">论文</a>）还给出了一个上界：<br>$$<br>x^*_{\text{max}}=2\frac{\bar{Y}-Y^m}{(Y^m)^2}<br>$$<br>其中$Y^m=\min Y_i$，$\bar{Y}$为$Y_i$的均值。详细的优化方法会在下文讨论。</p>
<p>背景部分到此结束，接下来的部分就是作者提出的新方法。</p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>Extreme Value Theory给出了在对原始分布未知的情况下估计使得$\mathbb{P}(X&gt;z_q)&lt;q$的$z_q$的方法。</p>
<p>本文据此提出了时间序列流的异常检测方法。首先根据已知的观测值$X_1,\cdots,X_n$得到阈值$z_q$，然后根据数据的特性运用两种不同方法来更新$z_q$。对于平稳时间序列，使用SPOT；对于非平稳时间序列，使用DSPOT。</p>
<h2 id="Initialization-Step"><a href="#Initialization-Step" class="headerlink" title="Initialization Step"></a>Initialization Step</h2><p>在进行异常检测之前，需要根据已有的观测数据进行$z_q$的估计。给定$n$个观测值$X_1,\cdots,X_n$和一个固定的概率值$q$，我们的目标是估计阈值$z_q$使得$\mathbb{P}(X&gt;z_q)&lt;q$。其主要流程是首先设定一个较大的阈值$t$，然后通过拟合GPD分布来计算$z_q$。过程如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/24/fzeC8vuDtA6mEdl.png"></p>
<p>算法流程如下所示：</p>
<p><img src="https://i.loli.net/2020/06/24/AEQpnZPiW3C4mr7.png"></p>
<p>$Y_t$代表大于$t$的观测值的集合，GPD分布的拟合使用了前文提到的Grimshaw’s Trick。</p>
<h2 id="Finding-Anomalies-in-a-Stream"><a href="#Finding-Anomalies-in-a-Stream" class="headerlink" title="Finding Anomalies in a Stream"></a>Finding Anomalies in a Stream</h2><p>通过Initialization Step使用POT算法得到的$z_q$，我们定义其为”Normality Bound”，用于后面的检测。在后面的步骤中，我们会根据新得到的观测值来更新$z_q$。</p>
<h3 id="Stationary-Case"><a href="#Stationary-Case" class="headerlink" title="Stationary Case"></a>Stationary Case</h3><p>我们首先来讨论时间序列没有时间依赖性的情况（$X_1,\cdots,X_n$之间独立同分布）。通过POT算法对所有观测值得到$z_q$之后，Streaming POT (SPOT) 算法会检查$X_n$之后的值（数据流场景，$X_1,\cdots,X_n$是历史数据，还会有新的数据进来），如果大于$z_q$，则将$X_i$加入异常点集合中；如果大于$t$但小于$z_q$，则将$X_i$加入观测值集合中，更新$z_q$；其他情况我们$X_i$是正常情况。算法流程图如下：</p>
<p><img src="https://i.loli.net/2020/06/24/h5yKnlCAYxbHu2R.png"></p>
<h3 id="Drifting-Case"><a href="#Drifting-Case" class="headerlink" title="Drifting Case"></a>Drifting Case</h3><p>SPOT算法只适用于平稳分布的情况，但在现实生活中这样的假设过强了。于是作者提出了能处理时间依赖性的Streaming POT with Drift (DSPOT) 算法。</p>
<p><img src="https://i.loli.net/2020/06/25/O49XwQvVGH7k1ri.png"></p>
<p>在DSPOT中，我们不使用$X_i$的绝对值，而是用相对值$X^\prime_i=X_i-M_i$，其中$M_i$是$i$时刻的局部特征，如Figure 4所示。最简单的实现是使用局部均值，即$M_i=(1/d)\cdot\sum\limits_{k=1}^d X_{i-k}^*$，$X_{i-1}^*,\cdots,X_{i-d}^*$是长度为$d$的窗口。我们假设$X^\prime_i$服从平稳分布的假设。</p>
<p>算法流程图如下所示：</p>
<p><img src="https://i.loli.net/2020/06/25/P6hOsD9dnNIHvUV.png"></p>
<h2 id="Numerical-Optimization"><a href="#Numerical-Optimization" class="headerlink" title="Numerical Optimization"></a>Numerical Optimization</h2><p>现在剩下的问题就是优化了，前文已经提到对GPD的拟合已经被优化成一个参数的优化问题，下面将会详细讨论优化算法。</p>
<h3 id="Reduction-of-the-Optimal-Parameters-Search"><a href="#Reduction-of-the-Optimal-Parameters-Search" class="headerlink" title="Reduction of the Optimal Parameters Search"></a>Reduction of the Optimal Parameters Search</h3><p>前文已经得到了一个初步的$x^*$的Bound，即$x^*&gt;-\frac{1}{Y^M}$和$x^*\leq 2\frac{\bar{Y}-Y^m}{(Y^m)^2}$，下面将给出一个更严格的Bound。</p>
<blockquote>
<p>*<em>PROPOSITION: **如果$x^</em>$是$u(x)v(x)=1$的解，那么：<br>$$<br>x^<em>\leq 0 \text{ or } x^</em>\geq 2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m}<br>$$</p>
</blockquote>
<p>证明见论文原文。</p>
<p>这样$x^*$的范围就进一步缩小了，于是有$u(x)v(X)=1$的解$x^*$在以下范围之内：<br>$$<br>\left(-\frac{1}{Y^M},0\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]<br>$$</p>
<h3 id="How-Can-We-Maximize-the-Likelihood-Function"><a href="#How-Can-We-Maximize-the-Likelihood-Function" class="headerlink" title="How Can We Maximize the Likelihood Function?"></a>How Can We Maximize the Likelihood Function?</h3><p>接下来是优化的具体实现问题。文中首先设定了一个很小的值$\epsilon&gt;0\space(\sim 10^{-8})$，然后在下面的范围内寻找函数$w:x\mapsto u(x)v(x)-1$的根：<br>$$<br>\left[-\frac{1}{Y^M}+\epsilon,-\epsilon\right]\text{ and }\left[2\frac{\bar{Y}-Y^m}{\bar{Y}Y^m},2\frac{\bar{Y}-Y^m}{(Y^m)^2}\right]<br>$$<br>作者没有使用现有的寻找函数根的算法，而是转换为如下优化问题：<br>$$<br>\min\limits_{x_1,\cdots,x_k\in I}\sum\limits_{i=1}^k w(x_k)^2<br>$$<br>其中$I$就是$x^*$的Bound。该问题是一个很典型的优化问题，可以被很多成熟的算法所解决。</p>
<h3 id="Initial-Threshold"><a href="#Initial-Threshold" class="headerlink" title="Initial Threshold"></a>Initial Threshold</h3><p>在算法的Initialization Step，需要事先设定一个阈值$t$，如果设定的太大，那么$Y_t$的数量就会很少。作者给出的建议是保证$t&lt;z_q$，即$t$对应的概率值应该小于$1-q$。</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>在实验部分，作者在合成数据和真实数据上试验了SPOT算法和DSPOT算法的有效性。</p>
<h2 id="D-SPOT-Reliability"><a href="#D-SPOT-Reliability" class="headerlink" title="(D)SPOT Reliability"></a>(D)SPOT Reliability</h2><p>作者首先在合成数据上验证SPOT的有效性。具体做法是使用高斯分布生成数据（高斯分布的分位数能够直接计算），然后将SPOT得出的$z_q$和理论值进行对比。误差定义如下：<br>$$<br>\text{error rate}=\left|\frac{z^{\text{SPOT}}-z^{\text{th}}}{z^{\text{th}}}\right|<br>$$<br>下图是采用不同数量观测值的结果：</p>
<p><img src="https://i.loli.net/2020/06/25/GXlu2MAJaoqxyd4.png"></p>
<h2 id="Finding-Anomalies-with-SPOT"><a href="#Finding-Anomalies-with-SPOT" class="headerlink" title="Finding Anomalies with SPOT"></a>Finding Anomalies with SPOT</h2><p>在这一节作者在真实数据集上进行了实验以验证SPOT算法的有效性，结果如下图：</p>
<p><img src="https://i.loli.net/2020/06/25/wTkZxKarFVDOlp6.png"></p>
<p>在文中作者说算法的True Positive达到了$86%$，False Positive小于$4%$。</p>
<p><img src="https://i.loli.net/2020/06/25/RcUnwtHud7DjNXv.png"></p>
<h2 id="Finding-Anomalies-with-DSPOT"><a href="#Finding-Anomalies-with-DSPOT" class="headerlink" title="Finding Anomalies with DSPOT"></a>Finding Anomalies with DSPOT</h2><p>在这一节作者使用DSPOT在真实数据集上进行了实验。窗口大小$d=450$，预设的风险概率值$q=10^{-3}$。结果如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/lIxqpnGtL7feVKs.png"></p>
<p>在图中可以看出在$8000$ Minutes之后上界显著提高，作者分析了原因，认为是因为超过阈值$t$的点$Y_t$的存储是全局的，在前$8000$ Minutes算法存储了很多较高的$Y_t$值，而在$8000$ Minutes之后，真实数据的趋势开始下降，但算法仍是根据全局的$Y_t$来进行$z_q$的计算（这一段没有特别明白）。作者给出的修正方法是只保存固定数量的Peaks。</p>
<p>下图是作者在股票数据上得到的实验结果：</p>
<p><img src="https://i.loli.net/2020/06/25/VeEo8OPbzyUxXrR.png"></p>
<h2 id="Performances"><a href="#Performances" class="headerlink" title="Performances"></a>Performances</h2><p>作者还验证了算法的时间效率。</p>
<p><img src="https://i.loli.net/2020/06/25/Egh7CxsU2TtL6az.png"></p>
<p>表中T代表的是每个Iteration的时间，M代表的是Peaks的比例，”bi-“前缀代表的是同时计算上界和下界。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-22T07:14:26.000Z" title="2019-10-22 3:14:26 ├F10: PM┤">2019-10-22</time>发表</span><span class="level-item"><time dateTime="2020-06-24T08:16:51.584Z" title="2020-6-24 4:16:51 ├F10: PM┤">2020-06-24</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Tutorial/">Tutorial</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/22/An-Introduction-to-Variational-Autoencoders/">An Introduction to Variational Autoencoders</a></h1><div class="content"><h1 id="Deep-Generative-Models"><a href="#Deep-Generative-Models" class="headerlink" title="Deep Generative Models"></a>Deep Generative Models</h1><p>生成模型是指一系列用于随机生成可观测数据的模型。假设在一个高维空间$\mathcal{X}$中，存在一个随机向量$\mathbf{X}$服从一个未知的分布$p_r(x),x\in \mathcal{X}$。生成模型就是根据一些可观测的样本$x^{(1)},x^{(2)},\cdots,x^{(N)}$来学习一个参数化的模型$p_\theta(x)$来近似未知分布$p_r(x)$。</p>
<p>生成模型主要用于密度估计和样本生成。</p>
<hr>
<p>密度估计即给定一组数据$\mathcal{D}={x^{(i)}},1\leq i\leq N$，假设他们都是从相同的概率密度函数$p_r(x)$独立产生的。密度估计就是根据数据集$\mathcal{D}$来估计其概率密度函数$p_r(x)$。</p>
<p>如果将生成模型用于监督学习，那么就是输出标签的条件概率分布$p(y|x)$，根据贝叶斯公式：</p>
<p>$$p(y|x)=\frac{p(x,y)}{\sum_y p(x,y)}$$</p>
<p>问题就变为了联合概率$p(x,y)$的密度估计问题。</p>
<hr>
<p>样本生成即根据给定的概率分布$p_\theta(x)$生成一些服从这个分布的样本，即采样。在含隐变量的生成模型中，生成$x$的过程一般包含两步：</p>
<ol>
<li>根据隐变量的分布$p_\theta(z)$采样得到$z$；</li>
<li>根据条件分布$p_\theta(x|z;\theta)$进行采样得到$x$。</li>
</ol>
<p>所以在生成模型中的重点是估计条件分布$p(x|z;\theta)$。</p>
<h1 id="Parameter-Estimation-for-Hidden-Variable-with-EM-Algorithm"><a href="#Parameter-Estimation-for-Hidden-Variable-with-EM-Algorithm" class="headerlink" title="Parameter Estimation for Hidden Variable with EM Algorithm"></a>Parameter Estimation for Hidden Variable with EM Algorithm</h1><p>如果图模型中存在隐变量，就需要使用EM算法进行参数估计。</p>
<p>在一个包含隐变量的图模型中，令$\mathbf{X}$为可观测变量集合，$\mathbf{Z}$为隐变量集合，则一个样本$x$的边际似然函数为：</p>
<p>$$p(x;\theta)=\sum_z p(x,z;\theta)$$</p>
<p>给定包含$N$个训练样本的训练集$\mathcal{D}={x^{(n)}},1\leq i\leq N$，则训练集的对数边际似然为：</p>
<p>$$\begin{align}\mathcal{L}(\mathcal{D};\theta)&amp;=\frac{1}{N}\sum_{n=1}^N \log p(x^{(n)};\theta)\&amp;=\frac{1}{N}\sum_{n=1}^N \log \sum_z p(x^{(n)},z;\theta)\end{align}$$</p>
<hr>
<p>这时，只要最大化整个训练集的对数边际似然$\mathcal{L}(\mathcal{D};\theta)$，即可估计出最优的参数$\theta^*$。不过在计算梯度的时候，需要在对数函数内部进行求和或积分计算。为了更好的计算$\log p(x;\theta)$，我们引入一个额外的变分函数$q(z)$，$q(z)$为定义在隐变量$z$上的分布。样本$x$的对数边际似然函数为：</p>
<p>$$\begin{align}\log p(x;\theta)&amp;=\log \sum_z q(z)\frac{p(x,z;\theta)}{q(z)}\&amp;\geq\sum_z q(z)\log \frac{p(x,z;\theta)}{q(z)}\&amp;\triangleq ELBO(q,x;\theta)\end{align}$$</p>
<p>其中$ELBO(q,x;\theta)$为对数边际似然函数$\log p(x;\theta)$的下界，称为证据下界。公式中使用了Jensen不等式(即对于凹函数$g$，有$g(\mathbb{E}[x])\geq\mathbb{E}[g(X)]$)。在这里，$\frac{p(x,z;\theta)}{q(z)}$可视为$q(z)$的函数，记为$f(q(z))$，那么$f(q(z))$的期望即$\mathbb{E}[f(q(z))]=\sum_z q(z)f(q(z))=\sum_z q(z)\frac{p(x,z;\theta)}{q(z)}$。而根据Jensen不等式，有$g(\mathbb{E}[f(q(z))])\geq\mathbb{E}[g(f(q(z)))]\Leftrightarrow g(\sum_z q(z)\frac{p(x,z;\theta)}{q(z)})\geq \sum_z q(z)g(\frac{p(x,z;\theta)}{q(z)})$，在这里$g$就是对数函数。</p>
<hr>
<p>根据Jensen不等式取等的条件：$\frac{p(x,z;\theta)}{q(z)}=c$，$c$为常数，有：</p>
<p>$$\begin{align}\sum_z p(x,z;\theta)&amp;=c\sum_z q(z)\\Leftrightarrow\sum_z p(x,z;\theta)&amp;=c\cdot1\end{align}$$</p>
<p>因此：</p>
<p>$$\begin{align}q(z)&amp;=\frac{p(x,z;\theta)}{\sum_z p(x,z;\theta)}\&amp;=\frac{p(x,z;\theta)}{p(x;\theta)}\&amp;=p(z|x;\theta)\end{align}$$</p>
<p>所以，当且仅当$q(z)=p(z|x;\theta)$时，$\log p(x;\theta)=ELBO(q,x;\theta)$。</p>
<hr>
<p>于是最大化对数边际似然函数$\log p(x;\theta)$的过程可以分解为两个步骤：</p>
<ol>
<li>先找到近似分布$q(z)$使得$\log p(x;\theta)=ELBO(q,x;\theta)$；</li>
<li>再寻找参数$\theta$最大化$ELBO(q,x;\theta)$。</li>
</ol>
<p>这就是期望最大化(Expectation-Maximum,EM)算法。</p>
<hr>
<p>EM算法通过迭代的方法，不断重复直到收敛到某个局部最优解。在第$t$步更新时，E步和M步分别为：</p>
<ol>
<li><p>E步：固定参数$\theta_t$，找到一个分布使$ELBO(q,x;\theta_t)$最大，即等于$\log p(x;\theta_t)$：$q_{t+1}(z)=\text{arg}_q \max ELBO(q,x;\theta_t)$；</p>
</li>
<li><p>M步：固定$q_{t+1}(z)$，找到一组参数使得证据下界最大，即：$\theta_{t+1}=\text{arg}<em>\theta\max ELBO(q</em>{t+1},x;\theta)$。</p>
</li>
</ol>
<hr>
<p>对数边际似然也可以通过信息论的视角来进行分解：</p>
<p>$$\begin{align}\log p(x;\theta)&amp;=\sum_z q(z)\log p(x;\theta)\&amp;=\sum_z q(z)(\log p(x,z;\theta)-\log p(z|x;\theta))\&amp;=\sum_z q(z)\log\frac{p(x,z;\theta)}{q(z)}-\sum_z q(z)\log\frac{p(z|x;\theta)}{q(z)}\&amp;=ELBO(q,x;\theta)+D_{KL}(q(z)\parallel p(z|x;\theta))\end{align}$$</p>
<p>其中$D_{KL}(q(z)\parallel p(z|x;\theta))$</p>
<h1 id="Generative-Model-with-Hidden-Variable"><a href="#Generative-Model-with-Hidden-Variable" class="headerlink" title="Generative Model with Hidden Variable"></a>Generative Model with Hidden Variable</h1><p>假设一个生成模型包含不可观测的隐变量，其中可观测变量$x$为一个高维空间中的随机向量，而不可观测的隐变量$z$为一个相对低维空间中的随机向量。</p>
<p>这个生成模型的联合概率密度函数可以表达为：</p>
<p>$$p(x,z;\theta)=p(x|z;\theta)p(z;\theta)$$</p>
<p>其中$p(z;\theta)$为隐变量$z$的先验概率分布；$p(x|z;\theta)$为已知$z$条件下$x$的概率分布。通常情况下，我们可以假设$p(z;\theta)$和$p(x|z;\theta)$服从某种带参的分布族，其形式已知，而参数可以通过最大似然来进行估计。</p>
<p>给定一个样本$x$，其对数边际似然$\log p(x;\theta)$可以分解为：</p>
<p>$$\log p(x;\theta)=ELBO(q,x;\theta,\phi)+D_{KL}(q(z;\phi)\parallel p(z|x;\theta))$$</p>
<p>其中$q(z;\phi)$为额外引入的变分密度函数，$ELBO(q,x;\theta,\phi)$为证据下界：</p>
<p>$$ELBO(q,x;\theta,\phi)=\mathbb{E}_{z\sim q(z;\phi)}[\log{\frac{p(x,z;\theta)}{q(z;\phi)}}]$$</p>
<p>最大化$\log p(x;\theta)$可以用EM算法来求解：</p>
<ul>
<li><strong>E-step:</strong> 寻找一个密度函数$q(z;\phi)$使其等于或接近于后验密度函数$p(z|x;\theta)$;</li>
<li><strong>M-step:</strong> 保持$q(z;\phi)$固定，寻找$\theta$来最大化$ELBO(q,x;\theta,\phi)$。</li>
</ul>
<p>在EM算法的每次迭代中，理论上最优的$q(z;\phi)$为隐变量的后验概率密度函数$p(z|x;\theta)$：</p>
<p>$$p(z|x;\theta)=\frac{p(x|z;\theta)p(z;\theta)}{\int_z p(x|z;\theta)p(z;\theta)\text{d}z}$$</p>
<p>后验密度函数$p(z|x;\theta)$的计算是一个统计推断的问题，在一般情况下$p(x|z;\theta)$也比较难以计算。</p>
<h1 id="Variational-Autoencoder"><a href="#Variational-Autoencoder" class="headerlink" title="Variational Autoencoder"></a>Variational Autoencoder</h1><p>变分自编码器(Variational Autoencoder, VAE)的主要思想是利用神经网络来分别建模两个复杂的条件概率密度函数：</p>
<ol>
<li><p>用神经网络来产生变分分布$q(z;\phi)$，称为推断网络。推断网络的输入为$x$，输出为变分分布$q(z|x;\phi)$；</p>
</li>
<li><p>用神经网络来产生概率分布$p(x|z;\theta)$，称为生成网络。生成网络的输入为$z$，输出为概率分布$p(x|z;\theta)$。</p>
<p><img src="https://i.loli.net/2020/06/24/B1d9UtTzNfjG6e2.png"></p>
</li>
</ol>
<p>VAE的图模型如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/24/GAhy281seQ3tbZT.png"></p>
<h2 id="Variational-Network"><a href="#Variational-Network" class="headerlink" title="Variational Network"></a>Variational Network</h2><p>假设$q(z|x;\phi)$是服从对角化协方差的高斯分布：</p>
<p>$$q(z|x;\phi)=\mathcal{N}(z;\mu_I,\sigma^2_I I)$$</p>
<p>其中$\mu_I$和$\sigma_I^2$是高斯分布的均值和方差，可以通过推断网络$f_I(x;\phi)$来预测：</p>
<p>$$<br>\left[\begin{matrix}\mu_I\\sigma_I\end{matrix}\right]=f_I(x;\phi)<br>$$<br>推断网络$f_I(x;\phi)$可以是一般的全连接网络或卷积网络，比如一个两层的神经网络：</p>
<p>$$\begin{align}h&amp;=\sigma(W^{(1)}x+b^{(1)})\\mu_I&amp;=W^{(2)}h+b^{(2)}\\sigma_I&amp;=\text{softplus}(W^{(3)}h+b^{(3)})\end{align}$$</p>
<p>其中所有网络参数${W^{(1)},W^{(2)},W^{(3)},b^{(1)},b^{(2)},b^{(3)}}$即对应了变分参数$\phi$。</p>
<hr>
<p>推断网络的目标是使得$q(z|x;\phi)$来尽可能接近真实的后验$p(z|x;\theta)$，需要找到变分参数$\phi^*$来最小化两个分布的KL散度：</p>
<p>$$\phi^*=\text{arg}<em>\phi\min{D</em>{KL}(q(z|x;\phi)\parallel p(z|x;\theta))}$$</p>
<p>由于$p(z|x;\theta)$未知，故KL散度无法直接计算，不过由于$D_{KL}(q(z|x;\phi)\parallel p(z|x;\theta))=\log p(x;\theta)-ELBO(q,x;\theta,\phi)$，所以可以直接最大化证据下界，有：</p>
<p>$$\phi^*=\text{arg}_\phi\max{ELBO(q,x;\theta,\phi)}$$</p>
<h2 id="Generative-Network"><a href="#Generative-Network" class="headerlink" title="Generative Network"></a>Generative Network</h2><p>生成模型的联合分布可以分解为两部分：隐变量$z$的先验分布$p(z;\theta)$和条件概率分布$p(x|z;\theta)$。为简单起见，一般假设隐变量$z$的先验分布为标准正态分布$\mathcal{N}(z|0,I)$，隐变量每一维之间都是独立的。条件概率分布$p(x|z;\theta)$可以通过生成网络来建模，我们同样用参数化的分布族来表示条件概率分布$p(x|z;\theta)$，这些分布族的函数可以用生成网络计算得到。根据变量$x$的类型不同，可以假设$p(x|z;\theta)$服从不同的分布族。如果$x\in{0,1}^d$是$d$维的二值向量，可以假设$\log p(x|z;\theta)$服从多变量的伯努利分布，即：</p>
<p>$$\begin{align}p(x|z;\theta)&amp;=\prod\limits_{i=1}^d p(x_i|z;\theta)\&amp;=\prod\limits_{i=1}^d \gamma_i^{x_i}(1-\gamma_i)^{(1-x_i)}\end{align}$$</p>
<p>如果$x\in\mathbb{R}^d$是$d$维的连续向量，可以假设$p(x|z;\theta)$服从对角化协方差的高斯分布，即：</p>
<p>$$p(x|z;\theta)=\mathcal{N}(x;\mu_G,\sigma_G^2 I)$$</p>
<hr>
<p>生成网络的目标是找到一组$\theta^*$最大化证据下界$ELBO(q,x;\theta,\phi)$：</p>
<p>$$\theta^*=\text{arg}_\theta\max ELBO(q,x;\theta,\phi)$$</p>
<h2 id="Model-Combination"><a href="#Model-Combination" class="headerlink" title="Model Combination"></a>Model Combination</h2><p>推断网络和生成网络的目标都是最大化证据下界因此总的目标函数为：</p>
<p>$$\begin{align}\max_{\theta,\phi}ELBO(q,x;\theta,\phi)&amp;=\max_{\theta,\phi}\mathbb{E}<em>{z\sim q(z;\phi)}[\log\frac{p(x|z;\theta)p(z;\theta)}{q(z;\theta)}]\&amp;=\max</em>{\theta,\phi}\mathbb{E}<em>{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]-D</em>{KL}(q(z|x;\phi)\parallel p(z;\theta))\end{align}$$</p>
<p>其中先验分布$p(z;\theta)=\mathcal{N}(z|0,I)$。</p>
<p>公式中$\mathbb{E}_{z\sim q(z|x;\phi)}[\log p(x|z;\theta)]$一般通过采样的方式进行计算，最后取平均值。</p>
<h2 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h2><p>给定数据集$\mathcal{D}$，包含$N$个从未知数据分布中抽取的独立同分布样本$x^{(1)},x^{(2)},\cdots,x^{(N)}$。变分自编码器的目标函数为：</p>
<p>$$\mathcal{J}(\phi,\theta|\mathcal{D})=\sum\limits_{n=1}^N(\frac{1}{M}\sum\limits_{m=1}^M\log p(x^{(n)}|z^{(n,m)};\theta)-D_{KL}(q(z|x^{(n)};\phi)\parallel\mathcal{N}(z;0,I)))$$</p>
<p>如果采用随机梯度下降法，每次从数据集中采一个样本$x$，然后根据$q(z|x;\phi)$采一个隐变量$z$，则目标函数变为：</p>
<p>$$\mathcal{J}(\phi,\theta|x)=\log p(x|z;\theta)-D_{KL}(q(z|x;\phi)\parallel\mathcal{N}(z;0,I))$$</p>
<p>假设$q(z|x;\phi)$是正态分布，KL散度可直接算出：</p>
<p>$$D_{KL}(\mathcal{N}(\mu_1,\Sigma_1)\parallel\mathcal(\mu_2,\Sigma_2))\=\frac{1}{2}(\text{tr}(\sigma_I^2 I)+\mu_I^T\mu_I-d-\log(|\sigma_I^2 I|))$$</p>
<hr>
<p>再参数化是将一个参数为$u$的函数$f(u)$，通过一个函数$u=g(v)$，转换为参数为$v$的函数$\hat{f}(v)=f(g(v))$。在变分自编码器中，一个问题是如何求随机变量$z$关于$\phi$的导数。但由于是采样的方式，无法直接刻画$z$和$\phi$之间的函数关系，因此也无法计算导数。</p>
<p>如果$z\sim q(z|x;\phi)$的随机性独立于参数$\phi$，我们可以通过再参数化的方法来计算导数。假设$q(z|x;\phi)$为正态分布$\mathcal{N}(\mu_I,\sigma^2_I I)$，其中$\mu_I$和$\sigma_I$是推断网络$f_I(x;\phi)$的输出。我们可以通过下面的方式采样$z$：</p>
<p>$$z=\mu_I+\sigma_I\odot \varepsilon$$</p>
<p>其中$\varepsilon\sim\mathcal{N}(0,I)$。这样$z$和$\mu_I,\sigma_I$的关系从采样关系变为函数关系。</p>
<hr>
<p>如果进一步假设$p(x|z;\theta)$服从高斯分布$\mathcal{N}(x|\mu_G,I)$，其中$\mu_G=f_G(z;\theta)$是生成网络的输出，则目标函数可以简化为：</p>
<p>$$\mathcal{J}(\phi,\theta|x)=-\parallel x-\mu_G\parallel^2+D_{KL}(\mathcal{N}(\mu_I,\sigma_I)\parallel\mathcal{N}(0,I))$$</p>
<p>其中第一项可以近似看作是输入$x$的重构正确性，第二项可以看作是正则化项。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-18T15:17:07.000Z" title="2019-10-18 11:17:07 ├F10: PM┤">2019-10-18</time>发表</span><span class="level-item"><time dateTime="2020-06-25T08:25:06.176Z" title="2020-6-25 4:25:06 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/RNN/">RNN</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/18/Recurrent-Neural-Networks-for-Multivariate-Time-Series-with-Missing-Values/">Recurrent Neural Networks for Multivariate Time Series with Missing Values</a></h1><div class="content"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>文中提出了一种可以处理带缺失值多为时间序列的GRU模型：<strong>GRU-D</strong>。本模型不仅可以捕捉时间序列中的长期依赖模式，并且还能利用时间序列中的缺失模式来达到更好的时间序列预测效果。</p>
<p><a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41598-018-24271-9">原文</a></p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h2><p>记包含$D$个变量的多变量时间序列为$X=(x_1,x_2,\cdots,x_T)^T\in\mathbb{R}^{T\times D}$，其中对于每个$t\in{1,2,\cdots,T},x_t\in\mathbb{R}^D$表示时间序列在时间$t$的观测值，$x_t^d$表示$x_t$的第$d$个成分。记$s_t\in\mathbb{R}$为$t$时刻的时间戳，并假设第一个观测值的时间戳为$0$。对于包含缺失值的时间序列，我们用<strong>Masking Vector</strong> $m_t\in{0,1}$进行标记，同时对每个$x_t^d$维护距离上一个观测值的<strong>Time Interval</strong> $\delta_t^d\in\mathbb{R}$，公式如下：<br>$$<br>m_t^d=\begin{cases}1, &amp;\text{if }x_t^d\text{ is observed}\0, &amp;\text{otherwise}\end{cases}<br>$$</p>
<p>$$<br>\delta_t^d=\begin{cases}s_t-s_{t-1}+\delta_{t-1}^d, &amp;t&gt;1,m_{t-1}^d=0\s_t-s_{t-1}, &amp;t&gt;1, m_{t-1}^d=1\0, &amp;t=1\end{cases}<br>$$</p>
<p>下图是一些示例：</p>
<p><img src="https://i.loli.net/2020/06/25/C4FKQw2AZ9xkalo.png"></p>
<p>在本文中，我们主要关注时间序列的分类问题，即给定数据集$\mathcal{D}={(X_n,s_n,M_n)}_{n=1}^N$，我们要对每个样本的类别进行预测$l_n\in{1,\cdots,L}$。</p>
<h2 id="GRU-RNN-for-Time-Series-Classification"><a href="#GRU-RNN-for-Time-Series-Classification" class="headerlink" title="GRU-RNN for Time Series Classification"></a>GRU-RNN for Time Series Classification</h2><p>GRU是一种改进版本的RNN，其最大不同是加入了门控机制。GRU单元的结构如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/wpKQsxEklizVTtm.png" style="zoom: 33%;" />

<p>GRU包含了重置门和更新门，其中重置门$R_t$负责控制上一时间的隐状态$h_{t-1}$有多少部分需要保留，而更新门则决定由$R_t$计算出来的候选隐状态$\tilde{h}<em>t$有多少部分需要保留。最后当前时间的隐状态由$h</em>{t-1}$和$\tilde{h}<em>t$共同算出。GRU的状态更新公式如下：<br>$$<br>\begin{align}<br>R_t&amp;=\sigma(W_rx_t+U_rh</em>{t-1}+b_r)\<br>Z_t&amp;=\sigma(W_zx_t+U_zh_{t-1}+b_z)\<br>\tilde{h}<em>t&amp;=\text{tanh}(Wx_t+U(R_t\odot h</em>{t-1})+b)\<br>h_t&amp;=(1-Z_t)\odot h_{t-1}+Z_t\odot \tilde{h}_t<br>\end{align}<br>$$<br>文中提出了一些处理缺失值的简单方法：</p>
<ol>
<li>直接用均值替代：$x_t^d\leftarrow m_t^dx_t^d+(1-m_t^d)\tilde{x}^d$，其中$\tilde{x}^d=\frac{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d x_{t,n}^d}{\sum_{n=1}^N\sum_{t=1}^{T_n}m_{t,n}^d\tilde{x}^d}$。这种方法称为<strong>GRU-Mean</strong>；</li>
<li>用上一个观测值替代：$x_t^d\leftarrow m_t^d x_t^d+(1-m_t^d)x_{t^\prime}^d$。这种方法称为<strong>GRU-Forward</strong>；</li>
<li>不填充，将是否缺失，距离上一个观测值的时间作为额外信息输入：$x_t^{(n)}\leftarrow[x_t^{(n)};m_t^{(n)};\delta_t^{(n)}]$。这种方法称为<strong>GRU-Simple</strong>。</li>
</ol>
<h3 id="GRU-D-Model-with-Trainable-Decays"><a href="#GRU-D-Model-with-Trainable-Decays" class="headerlink" title="GRU-D: Model with Trainable Decays"></a>GRU-D: Model with Trainable Decays</h3><p>文中提出了时间序列缺失值的两个性质：一个是在上一个观测值距离很远的情况下缺失值倾向于接近一个默认的值，第二个是缺失值的影响会随着时间减弱。为了体现上述两点，文中提出了GPU-D模型，模型框架如下：</p>
<img src="https://i.loli.net/2020/06/25/aXbS4ADkLfeHPCR.png" style="zoom:67%;" />

<p>在模型中，<strong>Decay Rates</strong>被设定为一个带参数的函数和GRU一起训练：<br>$$<br>\gamma_t=\exp{-\max(0,W_\gamma\delta_t+b_\gamma)}<br>$$</p>
<p>$$<br>\hat{x}<em>t^d=m_t^dx_t^d+(1-m_t^d)(\gamma_{x_t}^dx_{t^\prime}^d+(1-\gamma</em>{x_t}^d)\tilde{x}^d)<br>$$<br>其中$x_{t^\prime}^d$是第$d$个变量的上一个观测值，$\tilde{x}^d$是第$d$个变量的经验均值。这样$\hat{x}_t^d$就代表经过<strong>Input Decay</strong>的输入。</p>
<p>文中提到只用<strong>Input Decay</strong>是不够的，除此之外作者还使用了<strong>Hidden State Decay</strong>，即对$h_{t-1}$进行Decay，公式如下：<br>$$<br>\hat{h}<em>{t-1}=\gamma</em>{h_t}\odot h_{t-1}<br>$$<br>用Decay之后的$\hat{x}<em>t$和$\hat{h}</em>{t-1}$替换原始的GRU公式就得到了GRU-D模型：<br>$$<br>\begin{align}<br>R_t&amp;=\sigma(W_r\hat{x}<em>t-U_r\hat{h}_{t-1}+V_rm_t+b_r)\<br>Z_t&amp;=\sigma(W_z\hat{x}_t+U_z\hat{h}_{t-1}+V_zm_t+b_z)\<br>\tilde{h}_t&amp;=\text{tanh}(W\hat{x}_t+U(R_t\odot \hat{h}</em>{t-1})+Vm_t+b)\<br>h_t&amp;=(1-z_t)\odot \hat{h}_{t-1}+z_t\odot\tilde{h}_t<br>\end{align}<br>$$</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Baseline-Imputation-Methods"><a href="#Baseline-Imputation-Methods" class="headerlink" title="Baseline Imputation Methods"></a>Baseline Imputation Methods</h2><p>下图为文中比较中用到的Baseline：</p>
<p><img src="https://i.loli.net/2020/06/25/BFQdwMOXLc15mnl.png"></p>
<h2 id="Baseline-Prediction-Methods"><a href="#Baseline-Prediction-Methods" class="headerlink" title="Baseline Prediction Methods"></a>Baseline Prediction Methods</h2><p>下图为文中用到的用来预测的Baseline：</p>
<p><img src="https://i.loli.net/2020/06/25/qFxDRBLvNpAIjMs.png"></p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>文中用到的数据集如下：</p>
<ul>
<li><em>Gesture phase segmentation dataset (Gesture)</em>. </li>
<li><em>PhysioNet Challenge 2012 dataset (PhysioNet)</em>.</li>
<li><em>MIMIC-Ⅲ dataset (MIMIC-Ⅲ)</em>. </li>
</ul>
<p>下图展示了不同方法在人工合成数据集上的表现：</p>
<p><img src="https://i.loli.net/2020/06/25/6GVYuxoFP5yHAjf.png"></p>
<p>下表展示了不同模型在预测任务表现的对比：</p>
<img src="https://i.loli.net/2020/06/25/qTZsgtGewh19Y8V.png" style="zoom: 67%;" />

<p>下表展示了不同方法在MIMIC-Ⅲ和PhysioNet数据集上的多任务表现：</p>
<p><img src="https://i.loli.net/2020/06/25/dQiqebwYTCVfm5W.png"></p>
<p>下图分别展示了模型学到的<strong>Input Decay</strong>和<strong>Hidden State Decay</strong>：</p>
<img src="https://i.loli.net/2020/06/25/2F7MrgOXA9nuZqC.png" style="zoom:67%;" />
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-18T15:00:57.000Z" title="2019-10-18 11:00:57 ├F10: PM┤">2019-10-18</time>发表</span><span class="level-item"><time dateTime="2020-09-12T02:54:14.061Z" title="2020-9-12 10:54:14 ├F10: AM┤">2020-09-12</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/18/Robust-Anomaly-Detection-for-Multivariate-Time-Series-through-Stochastic-Recurrent-Neural-Network/">Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</a></h1><div class="content"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出了<em>OmniAnomaly</em>：一种针对多变量时间序列的随机循环神经网络异常检测算法。该模型运用了一系列技术来捕捉多变量时间序列的正常模式，并在检测阶段基于重构误差来检测异常，同时本文还提供了一定的理论解释。</p>
<p><a target="_blank" rel="noopener" href="https://www.kdd.org/kdd2019/accepted-papers/view/robust-anomaly-detection-for-multivariate-time-series-through-stochastic-re">原文</a></p>
<h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ol>
<li>提出了<em>OmniAnomaly</em>，一种基于随机循环神经网络的多变量时间序列异常检测算法；</li>
<li>提出了针对多变量时间序列异常检测的解释方法；</li>
<li>通过实验证明了<em>OmniAnomaly</em>中所用的关键技术的有效性，包括GRU，planar NF, stochastic variable connection和adjusted Peaks-Over-Threshold method；</li>
<li>通过大量的实验我们证明了<em>OmniAnomaly</em>的有效性；</li>
<li>发布了代码和数据集。</li>
</ol>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Linear-Gaussian-State-Space-Model"><a href="#Linear-Gaussian-State-Space-Model" class="headerlink" title="Linear Gaussian State Space Model"></a>Linear Gaussian State Space Model</h2><p>状态空间模型（State Space Model, SSM）的概念来自于控制理论，在这里我们主要讨论其在时间序列中的应用。其大概思想是我们认为时间序列在时刻$t$的观测值$z_t$是一个隐含状态$\boldsymbol{l}<em>t$的条件分布$p(z_t|\boldsymbol{l}_t)$，而这个隐含状态$\boldsymbol{l}_t$刻画了时间序列的内在规律，同时隐含状态会随着时间更新，即服从条件分布$p(\boldsymbol{l}_t|\boldsymbol{l}</em>{t-1})$。</p>
<p>在线性状态空间模型（Linear State Space Model）中我们以如下的方式刻画隐含状态的更新：<br>$$<br>\boldsymbol{l}_t=\boldsymbol{F}<em>t\boldsymbol{l}</em>{t-1}+\boldsymbol{g}_t\varepsilon_t, \space\space\space\varepsilon_t\sim\mathcal{N}(0,1)<br>$$<br>$\boldsymbol{F}_t$为确定的状态转移矩阵，而$\boldsymbol{g}_t\varepsilon_t$则表示了状态转移的随机性。</p>
<p>观测值$z_t$从隐含状态$\boldsymbol{l}<em>t$计算而来：<br>$$<br>\begin{align}<br>z_t&amp;=y_t+\sigma_t\epsilon_t,\<br>y_t&amp;=\boldsymbol{a}_t^\top\boldsymbol{l}</em>{t-1}+b_t,\<br>\epsilon_t&amp;\sim\mathcal{N}(0,1)<br>\end{align}<br>$$<br>其中$\boldsymbol{a}_t\in\mathbb{R}^L,\sigma_t\in \mathbb{R},b_t\in\mathbb{R}$都是额外的参数。初始状态$\boldsymbol{l}_0$则从一个独立的高斯分布得来，即$\boldsymbol{l}_0\sim N(\boldsymbol\mu_0,\text{diag}(\boldsymbol{\sigma}_0^2))$。</p>
<p>令参数集合$\Theta_t=(\boldsymbol{\mu}<em>0,\boldsymbol{\Sigma}<em>0,\boldsymbol{F}_t,\boldsymbol{g}_t,\boldsymbol{a}_t,b_t,\sigma_t),\forall t&gt;0$，一般来说参数集合不会随着时间变化，即每个时刻$t$共享同样的参数$\Theta_t=\Theta,\forall t&gt;0$。对参数的估计可以采用极大似然估计：<br>$$<br>\begin{align}<br>\Theta^*</em>{1:T}&amp;=\arg\max</em>{\Theta_{1:T}}p(z_{1:T}|\Theta_{1:T}),\<br>\end{align}<br>$$<br>其中：<br>$$<br>\begin{align}<br>p(z_{1:T}|\Theta_{1:T})&amp;=p(z_1|\Theta_1)\prod\limits_{t=2}^T p(z_t|z_{1:t-1},\Theta_{1:t})\<br>&amp;=\int p(\boldsymbol{l}<em>0)\left[\prod\limits</em>{t=1}^T p(z_t|\boldsymbol{l}<em>t)p(\boldsymbol{l}<em>t|\boldsymbol{l}</em>{t-1})\right]\mathrm{d}\boldsymbol{l}</em>{0:T}<br>\end{align}<br>$$</p>
<h2 id="Planar-Normalizing-Flow"><a href="#Planar-Normalizing-Flow" class="headerlink" title="Planar Normalizing Flow"></a>Planar Normalizing Flow</h2><h3 id="Normalizing-Flows"><a href="#Normalizing-Flows" class="headerlink" title="Normalizing Flows"></a>Normalizing Flows</h3><p>VAE采用一个变分分布$q_\phi(z|x)$来近似真实的后验分布$p(z|x)$，并推导出$\log p_\theta(x)$的下界（称为ELBO）来作为优化目标函数：<br>$$<br>\begin{align}<br>\log p_\theta(x)&amp;=\log \int p_\theta(x|z)p(z)\mathrm{d}z\<br>&amp;=\log\int\frac{q_\phi(z|x)}{q_\phi(z|x)}p_\theta(x|z)p(z)\mathrm{d}z\<br>&amp;\geq-D_{KL}[q_\phi(z|x)\parallel p(z)]+\mathbb{E}<em>q[\log p_\theta(x|z)]<br>\end{align}<br>$$<br>$\log p_\theta(x)$与ELBO取等的条件是$D</em>{KL}[q_\phi(z|x)\parallel p(z)]$，表明变分分布完全匹配了真实的后验分布。但在实际应用中，真实的后验分布可能会非常复杂，而我们的变分分布通常是一个确定的较为简单的分布，如高斯分布。这样变分分布可能很难对真实后验分布得到一个很好的拟合。</p>
<p>一个解决方案是使用标准化流（Normalizing Flows）。标准化流是从一个相对简单的分布出发，执行一系列可逆的映射，将原始简单的分布转化为一个复杂的分布。</p>
<p>首先考虑一个光滑的、可逆的映射$f:\mathbb{R}^d\mapsto \mathbb{R}^d$，记$g=f^{-1}$，那么$g\circ f(\mathbf{z})=\mathbf{z}$。令$\mathbf{z}^\prime=f(\mathbf{z})$，那么$\mathbf{z}^\prime$的分布为：<br>$$<br>q(\mathbf{z}^\prime)=q(\mathbf{z})\left|\text{det}\frac{\partial f^{-1}}{\partial \mathbf{z}^\prime}\right|=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}<br>$$<br>式中$q(\mathbf{z}^\prime)=q(z)\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|^{-1}$说明了$\mathbf{z}^\prime$的分布等于$\mathbf{z}$的分布乘上$f$的Jacobian矩阵的行列式的倒数。那么对于映射多次的情况：<br>$$<br>\mathbf{z}_K=f_K\circ\cdots\circ f_2\circ f_1(\mathbf{z}_0)<br>$$<br>$\mathbf{z}<em>K$的分布可以通过链式计算得到：<br>$$<br>\ln q_K(\mathbf{z}_K)=\ln q_0(\mathbf{z}_0)-\sum\limits</em>{k=1}^K\ln\left|\text{det}\frac{\partial f_k}{\partial \mathbf{z}_{k-1}}\right|<br>$$</p>
<h3 id="Planar-Flows"><a href="#Planar-Flows" class="headerlink" title="Planar Flows"></a>Planar Flows</h3><p>考虑一个变换族：<br>$$<br>f(\mathbf{z})=\mathbf{z}+\mathbf{u}h(\mathbf{w}^\top\mathbf{z}+b)<br>$$<br>其中$\lambda={\mathbf{w}\in \mathbb{R}^d,\mathbf{u}\in\mathbb{R}^d,b\in\mathbb{R}}$为参数集合，$h(\cdot)$为元素级的非线性函数（如各种激活函数）。令$\psi(\mathbf{z})=h^\prime(\mathbf{w}^\top\mathbf{z}+b)\mathbf{w}$，则$f$的Jacobian矩阵行列式绝对值等于：<br>$$<br>\left|\text{det}\frac{\partial f}{\partial \mathbf{z}}\right|=\left|\text{det}(\mathbf{I}+\mathbf{u}\psi(\mathbf{z})^\top)\right|=\left|1+\mathbf{u}^\top\psi(\mathbf{z})\right|<br>$$<br>但是$f$并不保证总是可逆的，如$h(x)=\tanh(x)$时，$f$可逆的条件是$\mathbf{w}^\top \mathbf{u}\geq-1$。</p>
<p>下面讨论如何保证可逆的条件。考虑将$\mathbf{z}$分解为$\mathbf{z}=\mathbf{z}_\bot+\mathbf{z}_\parallel$，其中$\mathbf{z}_\bot$与$\mathbf{w}$正交，$\mathbf{z}_\parallel$与$\mathbf{w}$平行，那么：<br>$$<br>f(z)=\mathbf{z}_\bot+\mathbf{z}_\parallel+\mathbf{u}h(\mathbf{w}^\top \mathbf{z}_\parallel +b)<br>$$<br>实际上得到$\mathbf{z}_\parallel$之后可以很容易的得到$\mathbf{z}_\bot$，令$\mathbf{y}=f(\mathbf{z})$，有：<br>$$<br>\mathbf{z}_\bot=\mathbf{y}-\mathbf{z}_\parallel-\mathbf{u}h(\mathbf{w}^\top\mathbf{z}_\parallel+b)<br>$$<br>而$\mathbf{z}_\parallel$与$\mathbf{w}$平行，易知$\mathbf{z}_\parallel=\alpha\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}$，其中$\alpha\in\mathbb{R}$。</p>
<p>对式(16)两边同时乘以$\mathbf{w}^\top$可得：<br>$$<br>\mathbf{w}^\top f(\mathbf{z})=\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)<br>$$<br>当$\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)$对于$\alpha$是非递减函数的时候，$f$是可逆的。因为$\alpha+\mathbf{w}^\top\mathbf{u} h(\alpha+b)$是非递减函数时有$1+\mathbf{w}^\top\mathbf{u}h^\prime(\alpha+b)\geq 0\equiv \mathbf{w}^\top \mathbf{u}\geq -\frac{1}{h^\prime(\alpha + b)}$，而$0\leq h^\prime(\alpha + b) \leq 1$（$\tanh$函数的性质），所以总是有$\mathbf{w}^\top \mathbf{u}\geq-1$。</p>
<p>对于任意一个$\mathbf{u}$，我们可以通过特定的方式构造一个$\hat{\mathbf{u}}$使得$\mathbf{w}^\top\hat{\mathbf{u}}&gt;-1$，即令$\hat{\mathbf{u}}(\mathbf{w},\mathbf{u})=\mathbf{u}+[m(\mathbf{w}^\top\mathbf{u})-(\mathbf{w}^\top\mathbf{u})]\frac{\mathbf{w}}{\parallel\mathbf{w}\parallel^2}$，其中$m(x)=-1+\log(1+e^x)$。</p>
<p><img src="https://i.loli.net/2020/06/25/uPyplhWBazROEw4.png"></p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>本文针对的是多变量时间序列$x={x_1,x_2,\cdots,x_N}\in R^{M\times N}$，$N$为时间长度，其中某一时刻的观测值$x_t\in R^M$为一个$M$维的向量。作者使用$x_{t-T:t}\in R^{M\times(T+1)}$来表示$t-T$到$t$之间的时间序列。</p>
<p><img src="https://i.loli.net/2020/06/25/4eHhs82uOzI5tG3.png"></p>
<h2 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h2><p>算法的总体框架如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/wd8maAoVb3Fk9vP.png"></p>
<p>预处理模块主要是对数据进行标准化以及窗口切分。训练模块则根据输入的数据对正常模式进行捕捉，输出异常分数。在线检测模块则会定期执行。</p>
<h2 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h2><p>模型的总体结构如下图所示：</p>
<p><img src="https://i.loli.net/2020/06/25/Lp7D81EvxVsywXQ.png"></p>
<p>在qnet中，首先GRU被用来建模样本的时间依赖关系，之后VAE将样本$\mathbf{x}$映射到隐空间$\mathbf{z}$。文中使用了Linear Gaussian State Space Model来建模隐变量之间的时间依赖关系。除此之外，作者还使用了Planar Normalizing Flow来将隐变量映射到复杂的非高斯分布。在pnet中，隐变量$\mathbf{z}<em>{t-T:t}$被用来重建$\mathbf{x}</em>{t-T:t}$，直观上来说，对样本的好的隐变量表示可以带来更好的重构效果。</p>
<p>从细节上来说，在时间$t$，qnet的输入为$\mathbf{x}<em>t$和$\mathbf{e}</em>{t-1}$，两者经过GRU Cell之后会产生$t$时间的$\mathbf{e_t}$。$\mathbf{e}<em>t$是GRU捕捉时间依赖性的关键，可以认为它包含了$\mathbf{x}</em>{1:t}$的信息。之后$\mathbf{e}<em>t$会和$\mathbf{z}</em>{t-1}$进行拼接，进入标准的VAE变分网络结构，通过网络输出的参数$\mu_{z_t},\sigma_{z_t}$采样得到隐变量$\mathbf{z}_t^0$，此时隐变量可以说捕捉了时间依赖性。</p>
<p>网络中涉及到的公式如下所示：</p>
<p>$$<br>\begin{align}<br>e_t&amp;=(1-c_t^e)\circ\text{tanh}(w^ex_t+u^e(r_t^e\circ e_{t-1})+b^e)+c_t^e\circ e_{t-1}\<br>\mu_{z_t}&amp;=w^{\mu_z}h^\phi([z_{t-1},e_t])+b^{\mu_z}\<br>\sigma_{z_t}&amp;=\text{softplus}(w^{\sigma_z}h^\phi([z_{t-1},e_t])+b^{\sigma_z})+\epsilon^{\sigma_z}<br>\end{align}<br>$$</p>
<p>其中$r_t^e=\text{sigmoid}(\mathbf{w}^{r^e}\mathbf{x}<em>t+\mathbf{u}^{r^e}\mathbf{e}</em>{t-1}+b^{r^e})$是GRU中的重置门，$c_t^e=\text{sigmoid}(\mathbf{w}^{c^e}\mathbf{x}<em>t+\mathbf{u}^{c^e}\mathbf{e}</em>{t-1}+b^{c^e})$是GRU中的更新门。</p>
<p>此时$\mathbf{z}_t^0$服从高斯分布，为了拟合复杂的后验分布，我们使用Planar Normalizing Flow来对$\mathbf{z}_t^0$进行变换，最后得到经$K$次变换后的随机变量$\mathbf{z}_t^K$。</p>
<p>在时间$t$，pnet试图通过$\mathbf{z}<em>t^K$来重构$\mathbf{x}_t$。首先$\mathbf{z}$空间中的变量会根据Linear Gaussian State Space Model来进行“连接“，公式为$\mathbf{z}_t=\mathbf{O}_\theta(\mathbf{T}_\theta\mathbf{z}</em>{t-1}+\mathbf{v}<em>t)+\boldsymbol{\epsilon}_t$，其中$\mathbf{O}_\theta$和$\mathbf{T}_\theta$为状态转移矩阵，$\mathbf{v}_t$和$\boldsymbol{\epsilon}_t$为随机噪声。之后$\mathbf{z}_t$和$\mathbf{d}</em>{t-1}$会作为GRU的输入，产生$\mathbf{d}<em>t$。之后$\mathbf{d}<em>t$会经过标准VAE中的生成网络，通过网络输出的高斯分布参数$\mu</em>{x_t},\sigma_{x_t}$采样得到重构后的样本$\mathbf{x}^\prime_t$。pnet中涉及到的公式如下所示：<br>$$<br>\begin{align}<br>d_t&amp;=(1-c_t^d)\circ\text{tanh}(w^dz_t+u^d(r_t^d\circ d</em>{t-1})+b^d)+c_t^d\circ d_{t-1}\<br>\mu_{x_t}&amp;=w^{\mu_x}h^\theta(d_t)+b^{\mu_x}\<br>\sigma_{x_t}&amp;=\text{softplus}(w^{\sigma_x}h^\theta(d_t)+b^{\sigma_x})+\epsilon^{\sigma_x}<br>\end{align}<br>$$</p>
<p>其中$r_t^d=\text{sigmoid}(\mathbf{w}^{r^d}\mathbf{x}<em>t+\mathbf{u}^{r^d}\mathbf{d}</em>{t-1}+b^{r^d})$是GRU中的重置门，$c_t^d=\text{sigmoid}(\mathbf{w}^{c^d}\mathbf{x}<em>t+\mathbf{u}^{c^d}\mathbf{d}</em>{t-1}+b^{c^d})$是GRU中的更新门。</p>
<h2 id="Offline-Model-Training"><a href="#Offline-Model-Training" class="headerlink" title="Offline Model Training"></a>Offline Model Training</h2><p>和传统VAE类似，模型的训练可以通过优化ELBO来完成。记长度为$T+1$的输入序列为$\mathbf{x}<em>{t-T:t}$，隐空间变量采样次数为$L$，第$l$个隐空间变量为$\mathbf{l}^{(l)}</em>{t-T:t}$，损失函数可以写成如下形式：</p>
<p>$$<br>\tilde{\mathcal{L}}(\mathbf{x}<em>{t-T:t})\approx\frac{1}{L}\sum</em>{t=1}^L[\log(p_\theta(\mathbf{x}<em>{t-T:t}|\mathbf{z}</em>{t-T:t}^{(l)}))+\log(p_\theta(\mathbf{z}<em>{t-T:t}^{(l)}))-\log(q_\phi(\mathbf{z}_{t-T:t}^{(l)}|\mathbf{x}</em>{t-T:t}))]<br>$$</p>
<p>第一项$\log(p_\theta(\mathbf{x}<em>{t-T:t}|\mathbf{z}</em>{t-T:t}^{(l)}))$可以看作是重构误差；第二项$\log(p_\theta(\mathbf{z}<em>{t-T:t}))=\sum</em>{i=t-T}^t \log(p_\theta(\mathbf{z}<em>i|\mathbf{z}</em>{i-1}))$通过Linear Gaussian State Space Model计算；第三项$-\log(q_\phi(\mathbf{z}<em>{t-T:t}|\mathbf{x}</em>{t-T:t}))=-\sum_{i=t-T}^t\log(q_\phi(\mathbf{z}<em>i|\mathbf{z}</em>{i-1},\mathbf{x}_{t-T:i}))$为隐变量$\mathbf{z}$后验分布的估计，同时$\mathbf{z}_i$是经Planar Normalizing Flow转换过的。</p>
<h2 id="Online-Detection"><a href="#Online-Detection" class="headerlink" title="Online Detection"></a>Online Detection</h2><p>在训练好模型之后，就可以进行异常检测了。在时间$t$，我们通过根据长度为$T+1$的序列$\mathbf{x}<em>{t-T:t}$来重构$\mathbf{x}<em>t$，并根据重构概率$\log(p_\theta(\mathbf{x}_t|\mathbf{z}</em>{t-T:t}))$来判定异常。定义$\mathbf{x}_t$对应的异常分数$S_t=\log(p_\theta(\mathbf{x}_t|\mathbf{z}</em>{t-T:t}))$，高异常分数代表样本$\mathbf{x}_t$能够以大概率重构（因为模型是用正常样本训练，可以认为模型建模的是正常样本的分布，重构概率高就代表符合正常分布）。给定阈值之后便可根据异常分数来进行异常的判定。</p>
<h2 id="Automatic-Threshold-Selection"><a href="#Automatic-Threshold-Selection" class="headerlink" title="Automatic Threshold Selection"></a>Automatic Threshold Selection</h2><p>在异常检测阶段，需要根据设定的阈值和每个样本的异常分数来判断该样本是否为异常，所以阈值的选择十分重要。文中用到了一种根据<strong>Extreme Value Theory</strong>自动选择阈值的算法。对于一个分布，其中的极端事件往往位于分布的末尾，而Extreme Value Theory第一定理给出不管原始分布如何，这些极端事件的分布服从一个带参的分布族。因此，可以在对数据分布未知的情况下估计极端事件的分布。</p>
<p>除了Extreme Value Theory第一定理之外，Extreme Value Theory第二定理给出随机变量大于特定阈值$t$的分布可以用Generalized Pareto Distribution来描述。作者使用了基于Extreme Value Theory第二定理的Peaks-Over-Threshold算法来进行阈值的选择。因为Extreme Value Theory第二定理给出随机变量大于特定阈值$t$的分布，而在本文的场景中我们需要刻画的异常点的分布应该是小于一个给定阈值的分布，所以需要修改一下公式。</p>
<p>对于给定的数据，模型会给出对应的异常分数序列${S_1,S_2,\cdots,S_{N^\prime}}$，给定预先设定的阈值$th$，$S_i$极端部分（即小于$th$的部分）的分布符合Generalized Pareto Distribution，公式如下：<br>$$<br>\bar{F}(s)=P(th-S&gt;s|S&lt;th)\sim(1+\frac{\gamma s}{\beta})^{-\frac{1}{\gamma}}<br>$$</p>
<p>其中$\gamma$和$\beta$为分布的形状参数，本文使用极大似然估计来对参数进行估计。设参数的估计值分别为$\hat{\gamma}$和$\hat{\beta}$，最终的阈值$th_F$由拟合得到的分布的分位数确定：</p>
<p>$$<br>th_F\simeq th-\frac{\hat{\beta}}{\hat{\gamma}}((\frac{qN^\prime}{N^\prime_{th}})^{-\hat{\gamma}}-1)<br>$$</p>
<p>其中$q$为期望$S&lt;th$的概率，$N^\prime$为观测值的数量，$N^\prime_{th}$为$S_i&lt;th$的个数。</p>
<h2 id="Anomaly-Interpretation"><a href="#Anomaly-Interpretation" class="headerlink" title="Anomaly Interpretation"></a>Anomaly Interpretation</h2><p>$$<br>\log(p_\theta(\mathbf{x}<em>t|\mathbf{z}</em>{t-T:t}))=\sum_{i=1}^M\log(p_\theta(x_t^i|\mathbf{z}_{t-T:t}))<br>$$</p>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Datasets-and-Metrics"><a href="#Datasets-and-Metrics" class="headerlink" title="Datasets and Metrics"></a>Datasets and Metrics</h2><h2 id="Overall-Performance"><a href="#Overall-Performance" class="headerlink" title="Overall Performance"></a>Overall Performance</h2><p><img src="https://i.loli.net/2020/06/25/yYLiWfXBDQklbPU.png"></p>
<p><img src="https://i.loli.net/2020/06/25/HlORKBEu7ij6Vor.png"></p>
<h2 id="Effects-of-Major-Techniques"><a href="#Effects-of-Major-Techniques" class="headerlink" title="Effects of Major Techniques"></a>Effects of Major Techniques</h2><p><img src="https://i.loli.net/2020/06/25/eKXGJ9ZlSmq8r4Q.png"></p>
<p><img src="https://i.loli.net/2020/06/25/dlibhyOBXNmPrkw.png"></p>
<h2 id="Visualization-on-Z-Space-Representations"><a href="#Visualization-on-Z-Space-Representations" class="headerlink" title="Visualization on Z-Space Representations"></a>Visualization on Z-Space Representations</h2><p><img src="https://i.loli.net/2020/06/25/MQXv5pZAejgVJ9s.png"></p>
<p><img src="https://i.loli.net/2020/06/25/jCkovq2lWrP6KXy.png"></p>
<p><img src="https://i.loli.net/2020/06/25/JSZuUkzNyci41DA.png"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-10-16T12:19:10.000Z" title="2019-10-16 8:19:10 ├F10: PM┤">2019-10-16</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:38:07.244Z" title="2020-6-25 1:38:07 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Time-Series-Imputation/">Time Series Imputation</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/10/16/GAIN-Missing-Data-Imputation-using-Generative-Adversarial-Nets/">GAIN: Missing Data Imputation using Generative Adversarial Nets</a></h1><div class="content"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文基于GAN提出了一种时间序列缺失值填充（Time Series Imputation）的方法。其主要的思路为生成器$G$从隐空间$Z$生成完整的样本，而判别器$D$则输出样本中不同部分为真实的概率。除此之外，作者提出了使用Hint Vector来揭示原始数据中缺失部分的信息，来优化训练过程。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.02920">原文</a></p>
<h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>考虑一个$d$维的空间$\mathcal{X}=\mathcal{X}_1\times \cdots\times \mathcal{X}_d$，设$\mathbf{X}=(X_1,\cdots,X_d)$维空间$\mathcal{X}$上的随机向量（即理想的完整的时间序列），记其分布为$P(\mathbf{X})$。设$\mathbf{M}=(M_1,\cdots,M_d)$为Mask向量表示$\mathbf{X}$中被观察到的部分。（即标识时间序列哪些部分有缺失），取值为${0,1}^d$。</p>
<p>对于每一个$i\in{1,\cdots,d}$，我们定义一个新空间$\tilde{\mathcal{X}}=\mathcal{X}\cup{<em>}$，其中$</em>$表示不属于任意$\mathcal{X}_i$的一个点。令$\tilde{\mathcal{X}}=\tilde{\mathcal{X}<em>1}\times\cdots\times\tilde{\mathcal{X}_d}$，同时定义一个新的随机变量（即我们观测到的含有缺失值的时间序列）$\tilde{\mathbf{X}}=(\tilde{X}_1,\cdots,\tilde{X}_d)\in \tilde{\mathcal{X}}$：<br>$$<br>\tilde{X}_i=\begin{cases}X_i,&amp;\text{if } M_i=1\*,&amp;\text{otherwise}\end{cases}<br>$$<br>假设数据集的形式为$\mathcal{D}={(\tilde{x}^i,m^i)}^n</em>{i=1}$，我们的任务是从$P(\mathbf{X}|\tilde{\mathbf{X}}=\tilde{x}^i)$上采样来对缺失值进行填充。</p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>模型的架构如下图所示：</p>
<img src="https://i.loli.net/2020/06/25/wCK3J8MoTASrj9Y.png" style="zoom:67%;" />

<h3 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h3><p>生成器的输入有三项：$\tilde{\mathbf{X}}$，$\mathbf{M}$和随机噪声$\mathbf{Z}$，输出设为$\bar{\mathbf{X}}$。设生成器为映射$G: \tilde{\mathcal{X}}\times{0,1}^d\times[0,1]^d\rightarrow \mathcal{X}$，而$\mathbf{Z}$为$d$维的高斯噪声。生成器的输出和填充后的时间序列定义为：<br>$$<br>\begin{align}<br>\bar{\mathbf{X}}&amp;=G(\tilde{\mathbf{X}},\mathbf{M},(1-\mathbf{M})\odot\mathbf{Z})\<br>\hat{\mathbf{X}}&amp;=\mathbf{M}\odot\tilde{\mathbf{X}}+(1-\mathbf{M})\odot\bar{\mathbf{X}}<br>\end{align}<br>$$<br>$\bar{\mathbf{X}}$即为生成器的直接输出，因为其实有些部分没有缺失，生成器还是会为每个部分输出值。</p>
<p>$\hat{\mathbf{X}}$为填充后的时间序列，对于缺失的部分采用生成器的输出进行填充。</p>
<h3 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h3><p>和原始的GAN不同的是，我们不需要判断整个样本是真实的或者是生成的，而是需要判断样本的那些部分是真实的或者是生成的，所以判别器为映射$D: \mathcal{X}\rightarrow[0,1]^d$。判别器的具体目标函数将在后面讨论。</p>
<h3 id="Hint"><a href="#Hint" class="headerlink" title="Hint"></a>Hint</h3><p>Hint是一种提示机值，是一个和$\mathbf{X}$相同维度的随机变量$\mathbf{H}$，其分布依赖于$\mathbf{M}$。$\mathbf{H}$是由用户自己定义的，相当于一种不完整的$\mathbf{M}$，用来作为判别器的额外输入。</p>
<h3 id="Objective"><a href="#Objective" class="headerlink" title="Objective"></a>Objective</h3><p>我们训练判别器最大化正确预测$\mathbf{M}$的概率，而生成器最小化判别器正确预测$\mathbf{M}$的概率，目标函数如下：<br>$$<br>\begin{align}<br>V(D,G)=&amp;\mathbb{E}<em>{\hat{X},M,H}[\mathbf{M}^T\log D(\hat{\mathbf{X}},\mathbf{H})\&amp;+(1-\mathbf{M})^T\log(1-D(\hat{\mathbf{X}},\mathbf{H}))]<br>\end{align}<br>$$<br>按照标准的GAN可以将优化函数写成以下的形式：<br>$$<br>\min_G\max_D V(D,G)<br>$$<br>在这里判别器的任务可以看作是一个二分类，而目标函数就是二值交叉熵的定义，因此可以写为：<br>$$<br>\mathcal{L}(a,b)=\sum\limits</em>{i=1}^d[a_i\log(b_i)+(1-a_i)\log(1-b_i)]<br>$$<br>$\mathbf{M}$可以看作Ground Truth，记$\hat{\mathbf{M}}=D(\hat{\mathbf{X},\mathbf{H}})$，即判别器输出的预测，因此优化函数可以简记为：<br>$$<br>\min_G\max_D\mathbb{E}[\mathcal{L}(\mathbf{M},\hat{\mathbf{M}})]<br>$$</p>
<h2 id="GAIN-Algorithm"><a href="#GAIN-Algorithm" class="headerlink" title="GAIN Algorithm"></a>GAIN Algorithm</h2><p>下面讨论GAIN算法的训练流程。</p>
<p>本文通过理论讨论，给出了生成Hint Vector的一个方法，首先定义随机变量$\mathbf{B}=(B_1,\cdots,B_d)\in{0,1}^d$，$\mathbf{B}$通过从${1,\cdots,d}$随机均匀采样一个$k$，然后由下列公式得到：<br>$$<br>B_j=\begin{cases}1, &amp;\text{if }j\neq k\0, &amp;\text{if }j=k\end{cases}<br>$$<br>定义空间$\mathcal{H}={0,0.5,1}^d$，Hint Vector为$\mathbf{H}=\mathbf{B}\odot\mathbf{M}+0.5(1-\mathbf{B})\in\mathcal{H}$。</p>
<p>判别器的训练过程如下：固定生成器$G$，对一个大小为$k_D$的mini-batch，独立同分布采样$k_D$个$z$和$b$，用来计算$\mathbf{Z}$和$\mathbf{B}$。判别器的损失函数定义如下：<br>$$<br>\mathcal{L}<em>D(m,\hat{m},b)=\sum\limits</em>{i:b_i=0}[m_i\log(\hat{m}_i)+(1-m_i)\log(1-\hat{m}_i)]<br>$$<br>判别器的优化函数为：<br>$$<br>\min_D-\sum\limits_{j=1}^{k_D}\mathcal{L}_D(m(j),\hat{m}(j),b(j))<br>$$<br>其中$\hat{m}(j)=D(\hat{x}(j),m(j))$。</p>
<p>在优化了判别器之后，需要优化生成器，对一个大小为$k_G$的mini-batch，生成器的损失函数包含两个部分，一个是在缺失部分的损失：</p>
<p>$$<br>\mathcal{L}<em>G(m,\hat{m},b)=-\sum\limits</em>{i:b_i=0}(1-m_i)\log(\hat{m}<em>i)<br>$$<br>一个是未缺失部分的损失：<br>$$<br>\mathcal{L}<em>M(x,x^\prime)=\sum\limits</em>{i=1}^d m_iL_M(x_i,x_i^\prime)<br>$$<br>其中：<br>$$<br>L_M(x_i,x_i^\prime)=\begin{cases}(x_i^\prime-x_i)^2, &amp;\text{if }x_i\text{ is continuours},\-x_i\log(x_i^\prime), &amp;\text{if }x_i\text{ is binary}.\end{cases}<br>$$<br>最终的优化函数为：<br>$$<br>\min_G\sum\limits</em>{j=1}^{k_G}\mathcal{L}_G(m(j),\hat{m}(j),b(j))+\alpha\mathcal{L}_M(\tilde{x}(j),\hat{x}(j))<br>$$<br>算法流程如下：</p>
<img src="https://i.loli.net/2020/06/25/znABi6x9mJuvDOX.png" style="zoom:67%;" />

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><p>下表为在5个不同数据集上实验，与其他5种方法对比的结果：</p>
<img src="https://i.loli.net/2020/06/25/EenX2YO8aDxQAk3.png" style="zoom:67%;" />

<p>上图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例、样本数量、特征维度下的对比曲线图。</p>
<p>下表为使用不同模型对时间序列进行填充之后，使用逻辑回归进行回归任务的性能：</p>
<img src="https://i.loli.net/2020/06/25/PzeWKdGu4wADshV.png" style="zoom:67%;" />

<p>下图为GAIN、MissForest和Autoencoder三种模型在不同缺失比例下的AUROC曲线图：</p>
<img src="https://i.loli.net/2020/06/25/IKtoTj8xgG1yJDk.png" style="zoom:67%;" />

<p>下表展示的是作者对时间序列填充算法保持特征-标签关系的能力。作者分别用完整的数据和填充后的数据用逻辑回归模型进行训练，将两者的权重求绝对值和均方根的结果。</p>
<img src="https://i.loli.net/2020/06/25/uy2jPcnbtSrC6vI.png" style="zoom:67%;" />

</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-09-22T14:32:18.000Z" title="2019-9-22 10:32:18 ├F10: PM┤">2019-09-22</time>发表</span><span class="level-item"><time dateTime="2020-06-25T05:24:46.844Z" title="2020-6-25 1:24:46 ├F10: PM┤">2020-06-25</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/">Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>这篇文章提出了一个基于GAN的时间序列异常检测模型。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.04758">原文</a></p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ol>
<li>提出了基于GAN的时间序列无监督异常检测模型</li>
<li>我们使用基于LSTM的GAN来对多变量时间序列进行建模</li>
<li>结合使用了Residual Loss和Discrimination Loss来进行异常的判断</li>
</ol>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Generative-Adversarial-Networks"><a href="#Generative-Adversarial-Networks" class="headerlink" title="Generative Adversarial Networks"></a>Generative Adversarial Networks</h3><h4 id="GANs-In-a-Nutshell-an-extremely-simple-explanation"><a href="#GANs-In-a-Nutshell-an-extremely-simple-explanation" class="headerlink" title="GANs In a Nutshell, an extremely simple explanation"></a>GANs In a Nutshell, an extremely simple explanation</h4><ul>
<li>我们想要从一个复杂的、高维的数据分布$p_r(x)$上采样得到我们想要的数据点，然而$p_r(x)$无法直接求得</li>
<li>代替方法：从一个简单的、已知的分布$p_z(z)$上采样，然后学习一个Transformation $G(z): z\rightarrow x$来将$z$映射到$x$</li>
</ul>
<img src="https://i.loli.net/2020/06/25/frIYtuao9mexQUT.png" style="zoom:67%;" />

<h4 id="Training-Two-player-Game"><a href="#Training-Two-player-Game" class="headerlink" title="Training: Two-player Game"></a>Training: Two-player Game</h4><ul>
<li>**Generator Network: ** 从随机分布$p_z(z)$采样$z$，通过映射生成样本$x$，这个生成的样本要尽量“真实”。怎么“真实”？优化生成器参数$\theta_G$最大化判别器对生成样本的评分即可</li>
<li>**Discriminator Network: **接受一个样本$x$，判断其是生成的样本还是真实的样本。在训练阶段，我们是知道一个样本$x$到底是生成的还是真实的，所以优化判别器参数$\theta_D$最小化判别器对生成样本的评分，最大化对真实样本的评分（即最大化分辨真实样本的能力）</li>
</ul>
<img src="https://i.loli.net/2020/06/25/ECP2Dkpq6FrSoef.png" style="zoom:67%;" />

<p>形式化的来讲，优化函数如下：</p>
<p>$$\min\limits_{\theta_G}\max\limits_{\theta_D}V(G,D)=\mathbb{E}<em>{x\sim p</em>{data}(x)\log(\underbrace{D_{\theta{D}}(x)}<em>{判别器对真实样本的评分})}+\mathbb{E}</em>{z\sim p_z(z)}\log(1-\underbrace{D_{\theta_d}(G_{\theta_G}(z))}_{判别器对生成样本的评分})$$</p>
<p>训练过程如下：</p>
<img src="https://i.loli.net/2020/06/25/57N4yUrfoS1cBWd.png" style="zoom:67%;" />

<h3 id="Long-Short-Time-Memory-Networks"><a href="#Long-Short-Time-Memory-Networks" class="headerlink" title="Long Short Time Memory Networks"></a>Long Short Time Memory Networks</h3><h4 id="Vanilla-Recurrent-Neural-Networks"><a href="#Vanilla-Recurrent-Neural-Networks" class="headerlink" title="Vanilla Recurrent Neural Networks"></a>Vanilla Recurrent Neural Networks</h4><p>普通的神经网络：</p>
<img src="https://i.loli.net/2020/06/25/U5rxdYR4jKoqQOX.png" style="zoom:50%;" />

<p>概括的来讲，可以涵盖为一个公式$\hat{\mathbf{y}}=f(\mathbf{x})$。对于一个样本$\mathbf{x}$，通过多层神经网络映射，输出$\mathbf{y}$。</p>
<p>对于RNN，我们处理的是序列数据，也就是说所有样本之间并不是相互独立的。对于一个序列中的一个样本$x_t\in{x_1,x_2,\cdots,x_n}$，将其输入到神经网络的时候，为了建模$x_t$之前的子序列对$x_t$的影响关系，需要将这个子序列的信息也输入到神经网络中，怎么做呢？为每一个样本点保存一个State。即定义$h_t=g(\hat{y_t})=g(f(x_t))$，对于当前样本点，$\hat{y_t}=f(x_t,h_{t-1})$。也就是说神经网络的输入不仅包含了当前样本点的特征，也包含了上一个样本点的“状态”(上一个样本点的“状态”又隐含了上上个样本点的“状态”…)，就像是为网络加上了短期记忆。</p>
<img src="https://i.loli.net/2020/06/25/cxBk6SQTydsOVYt.png" style="zoom: 67%;" />

<img src="https://i.loli.net/2020/06/25/ODKWYBI83tJXurM.png" style="zoom: 33%;" />

<img src="https://i.loli.net/2020/06/25/jq1LAytRKCub3kX.png" style="zoom:33%;" />

<h4 id="Gradient-Flow-of-Vanilla-RNN"><a href="#Gradient-Flow-of-Vanilla-RNN" class="headerlink" title="Gradient Flow of Vanilla RNN"></a>Gradient Flow of Vanilla RNN</h4><p>下面来进行一些形式化的定义，假设在时刻$t$网络输入特征为$x_t$，输出隐含状态为$h_{t}$，其不仅和当前输入$x_t$有关，还和上一个隐含状态$h_{t-1}$有关：</p>
<ul>
<li>当前时刻总的净输入$z_t=Uh_{t-1}+Wx_t+b$</li>
<li>当前时刻输出隐含状态$h_t=f(z_t)$</li>
<li>当前时刻输出$\hat{y}_t=Vh_t$</li>
</ul>
<p>RNN的梯度更新公式(推导过程比较复杂)：</p>
<p>$$\frac{\partial{\mathcal{L}}}{\partial U}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t \delta_{t,k}\mathbf{h}_{k-1}^T$$</p>
<p>$$\frac{\partial{\mathcal{L}}}{\partial{W}}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t \delta_{t,k}x_k^T$$</p>
<p>$$\frac{\partial\mathcal{L}}{\partial{b}}=\sum\limits_{t=1}^T\sum\limits_{k=1}^t\delta_{t,k}$$</p>
<p>其中$\delta_{t,k}=\frac{\partial{\mathcal{L}}}{\partial{z_k}}=\text{diag}(f^\prime(z_k))U^T\delta_{t,k+1}$定义为第$t$时刻的损失对第$k$时刻隐藏神经层的净输入$z_k$的导数，且$z_k=Uh_{k-1}+Wx_k+b,1\leq k&lt;t$。</p>
<p>RNN的梯度流向如下图红箭头所示：</p>
<img src="https://i.loli.net/2020/06/25/F5xvo9kCiNl8ehZ.png" style="zoom: 50%;" />

<p>RNN会遇到梯度消失和梯度爆炸的问题。根据前面的公式，$\delta_{t,k}$实际上是递归定义的，展开得到：</p>
<p>$$\delta_{t,k}=\prod\limits_{\tau=k}^{t-1}(\text{diag}(f^\prime(z_\tau))U^T)\delta_{t,t}$$</p>
<p>如果定义$\gamma\cong\parallel\text{diag}(f^\prime(z_\tau))U^T\parallel$，那么$\delta_{t,k}\cong\gamma^{t-k}\delta_{t,t}$。在$t-k$很大时，$\gamma&lt;1$会导致梯度消失，$\gamma&gt;1$时会导致梯度爆炸。</p>
<img src="https://i.loli.net/2020/06/25/RGW4oVtQ7KEFUCA.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/06/25/i4O9kJQpnZ5GYeq.png" style="zoom:50%;" />



<h4 id="Long-Short-Time-Memory"><a href="#Long-Short-Time-Memory" class="headerlink" title="Long Short Time Memory"></a>Long Short Time Memory</h4><p>LSTM是一种解决RNN梯度消失问题的改进版本：</p>
<img src="https://i.loli.net/2020/06/25/B4NXzb6fSdgGowL.png" style="zoom:50%;" />

<p>在LSTM中，维护了两个State，$c_t$和$h_t$。其中$c_t$由遗忘门$f$与上一个$c_{t-1}$相乘(代表继承上一个Cell的信息并加以一定程度的遗忘)，加上输出门$i$与Gate Gate $g$相乘(Gate Gate代表当前的候选状态，输出门$i$控制当前候选状态有多少信息需要保存)。最后，输出门$o$控制当前时刻的Cell State $c_t$有多少信息需要输出给外部状态$h_t$。</p>
<p>三个门的计算方式为： </p>
<p>$$i_t=\sigma(W_ix_t+U_ih_{t-1}+b_i)$$</p>
<p>$$f_t=\sigma(W_fx_t+U_fh_{t-1}+b_f)$$</p>
<p>$$o_t=\sigma(W_ox_t+U_oh_{t-1}+b_o)$$</p>
<img src="https://i.loli.net/2020/06/25/PXQMb9vih1yEKrf.png" style="zoom:50%;" />

<img src="https://i.loli.net/2020/06/25/1zZQqlI6r9Yjp47.png" style="zoom:50%;" />

<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>总体框架图如Fig 1所示：</p>
<img src="https://i.loli.net/2020/06/25/scEA9Ou1Yi7nThG.png" style="zoom: 50%;" />

<h3 id="GAN-with-LSTM-RNN"><a href="#GAN-with-LSTM-RNN" class="headerlink" title="GAN with LSTM-RNN"></a>GAN with LSTM-RNN</h3><p>网络结构上生成器和判别器都是LSTM，优化函数和普通GAN一样：</p>
<p>$$\min\limits_G\max\limits_D V(D,G)=\mathbb{E}<em>{x\sim p</em>{data}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)}[\log (1-D(G(z)))]$$</p>
<h3 id="GAN-based-Anomaly-Score"><a href="#GAN-based-Anomaly-Score" class="headerlink" title="GAN-based Anomaly Score"></a>GAN-based Anomaly Score</h3><p>在测试阶段，需要使用梯度优化寻找一个使得$G_{rnn}(z)$最接近$X^{test}$的$z^k$：</p>
<p>$$\min\limits_{Z^k}Error(X^{test},G_{rnn}(Z^k))=1-Similarity(X^{test},G_{rnn}(Z^k))$$</p>
<p>本文定义了两种Anomaly Score，一种是Residual Loss：</p>
<p>$$Res(X^{test}<em>t)=\sum\limits</em>{i=1}^n|x^{test,i}<em>t-G</em>{rnn}(Z^{k,i}_t)|$$</p>
<p>一种是Discrimination Loss，即判别器的输出$D_{rnn}(x_t^{test})$。</p>
<p>总的Anomaly Score：</p>
<p>$$S^{test}_t=\lambda Res(X^{test}<em>t)+(1-\lambda)D</em>{rnn}(x^{test}_t)$$</p>
<h3 id="Anomaly-Detection-Framework"><a href="#Anomaly-Detection-Framework" class="headerlink" title="Anomaly Detection Framework"></a>Anomaly Detection Framework</h3><p>模型的算法流程如下：</p>
<img src="https://i.loli.net/2020/06/25/84ZCT1JOEru53Ue.png" style="zoom:67%;" />

<p>由于本文是多变量时间序列预测，而且时间序列的长度有可能比较长，作者使用了滑动窗口和PCA来进行预处理。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><img src="https://i.loli.net/2020/06/25/LGsiMw6IjYUtx8T.png" style="zoom:67%;" /></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2019-09-22T14:29:18.000Z" title="2019-9-22 10:29:18 ├F10: PM┤">2019-09-22</time>发表</span><span class="level-item"><time dateTime="2020-06-24T07:48:38.792Z" title="2020-6-24 3:48:38 ├F10: PM┤">2020-06-24</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/">ALSR: An Adaptive Label Screening and Relearning Approach for Interval-Oriented Anomaly Detection</a></h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文针对面向区间的KPI异常检测提出了Label Screening方法和Relearning Algorithm.</p>
<p><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0957417419304282">原文</a></p>
<h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ol>
<li>提出了一种Label Screening方法来对区间内不同重要性进行过滤</li>
<li>提出了一种Relearning Algorithm来对FP和TP进行Relearning，在不减少Recall的条件下增大Precision</li>
</ol>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Overall-Structure"><a href="#Overall-Structure" class="headerlink" title="Overall Structure"></a>Overall Structure</h3><p>算法的整体框架如下：</p>
<p><img src="https://i.loli.net/2020/06/24/yGMWzdgTf43qImp.png"></p>
<h3 id="Label-Screening-Model"><a href="#Label-Screening-Model" class="headerlink" title="Label Screening Model"></a>Label Screening Model</h3><p>预训练的结果被分为$TP_{po},FP_{po},TN_{po},FN_{po}$四类，$TP_{po}$和$FN_{po}$可以被细分如下：<br>$$<br>\begin{align}TP_{po}&amp;=TP_{po,withinT}+TP_{po,afterT}\&amp;=TP_{po,withinT}+TP_{po,afterT,tpl}+TP_{po,after,fnl}\end{align}<br>$$</p>
<p>$$<br>\begin{align}FN_{po}&amp;=FN_{po,withinT}+FN_{po,afterT}\&amp;=FN_{po,withinT,tpl}+FN_{po,,withinT,fnl}+FN_{po,afterT,tpl}+FN_{po,afterT,fnl}\end{align}<br>$$</p>
<p>其中下标${}<em>{withinT}$代表在异常片段第一个点$T$距离内的所有点，下标${}</em>{afterT}$代表$T$距离之后。下标${}<em>{tpl}$和${}</em>{fnl}$分别代表在异常片段中，包含和不包含$TP_{po,withinT}$的点。</p>
<p>以TP为例，Point-based的TP包含了在T范围之内的（即在Interval-based的标准中也会被认为是TP的点）和T范围之外的点（即在Interval-based的标准中不认为是TP的点）。而在T范围之外的点又可以细分为该异常片段是否包含$TP_{po,withinT}$的点（即该点在Interval-based的标准中不会被判定为TP，但该异常片段有其点会被判定为TP）。</p>
<p>类似的，$TP_{io}$和$FN_{io}$可以被分解为：<br>$$<br>\begin{align}TP_{io}&amp;=TP_{po,withinT}+TP_{po,afterT,tpl}+FN_{po,withinT,tpl}+FN_{po,afterT,tpl}\&amp;=TP_{po}+FN_{po,withinT,tpl}+FN_{po,afterT,tpl}-TP_{po,afterT,fnl}\end{align}<br>$$</p>
<p>$$<br>\begin{align}FN_{io}&amp;=FN_{po,withinT,fnl}+FN_{po,afterT,fnl}+TP_{po,afterT,fnl}\&amp;=FN_{po}+TP_{po,afterT,fnl}-FN_{po,withinT,tpl}-FN_{po,afterT,tpl}\end{align}<br>$$</p>
<p>文中对该部分的分析可以分为以下几点：</p>
<ol>
<li>在Interval-oriented的标准中，$FN_{po,tpl}$的点仍会被认为是$TP_{io}$，而$TP_{po,afterT}$（不带${}<em>{tpl}$）不会被认为是$TP</em>{io}$，所以最终$TP_{io}$由所有$TP_{po}$加上那些会被认为是$TP_{io}$的$FN_{po,tpl}$再去掉不带${}<em>{tpl}$的$TP</em>{po,afterT}$组成，即公式(6)</li>
<li>同时，根据公式(6)，如果$TP_{po}$变为$FN_{po,tpl}$，也不会对最终结果造成影响。但是根据公式(5)和公式(7)，$TP_{po,withinT}$变成$FN_{po,withinT,fnl}$会减小$TP_{io}$同时增大$FN_{io}$</li>
<li>文章指出，虽然$FN_{po,withinT,tpl}$和$FN_{po,afterT,tpl}$最后都会被认为是$TP_{io}$，但作者假设$FN_{po,withinT,tpl}$更难检测，所以应该保留，而$FN_{po,afterT,tpl}$应该削减</li>
<li>Label Screening方法去除了$FN_{po,afterT}$的点</li>
<li>Screened之后的训练集被用来训练DNN主模型，但Label Screening的预测结果也会被保留，和DNN主模型的结果进行组合</li>
</ol>
<p>算法流程如下：</p>
<p><img src="https://i.loli.net/2020/06/24/ZDT5fQNojsXm84q.png"></p>
<h3 id="Relearning-Algorithm"><a href="#Relearning-Algorithm" class="headerlink" title="Relearning Algorithm"></a>Relearning Algorithm</h3><p>Relearning Model的输入是DNN主模型预测出来的异常，其中包括TP和FP。Relearning Model采用的是随机森林，其输入的样本通过采样得到：<br>$$<br>\begin{align}<br>\text{relearning}\space&amp;\text{training set}=\&amp; shuffle{4C\ast\text{randomof}(TP_{po})\&amp;+C\cdot\text{randomof}(FP_{po})+C\cdot\text{randomof}(TN_{po})}<br>\end{align}<br>$$<br>其中$C$为常数。TN和FP都看作是负例(正常样本)，TP看作是正例。</p>
<p><img src="https://i.loli.net/2020/06/24/qgvINaFu9JLfM4j.png"></p>
<h3 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a>Detection</h3><p>对于一个滑动窗口$x_t={x_{t-w+1},\cdots,x_t}$，异常检测算法的目标是输出检测结果$y_t\in{0,1}$来表示时间$t$是否发生异常。实际上算法输出的是$p_{y_t}\in[0,1]$概率值来表示在时间$t$发生异常的概率。文中三个模型会得到三个输出：$y_{t,ls},y_{t,main},y_{t,re}$。最终结果为：<br>$$<br>y_t=y_{t,ls}\space&amp;\space y_{t,main}\space&amp; \space y_{t,re}<br>$$<br>在绘制PR曲线时，采用的公式为：<br>$$<br>\begin{align}<br>p_{y_t}(th)=&amp;(1-sig(p_{y_t,ls},th))\cdot(p_{y_t,ls})\<br>&amp;+sig(p_{y_t,ls},th)\cdot(1-sig(p_{y_t,main},th))\cdot p_{y_t,main}\<br>&amp;+sig(p_{y_t,ls},th)\cdot sig(p_{y_t,main},th)\cdot p_{y_t,re}\<br>\end{align}<br>$$</p>
<p>$$<br>y_t(th)=sig(p_{y_t}(th),th)<br>$$</p>
<p>算法流程如下：</p>
<p><img src="https://i.loli.net/2020/06/24/LBT59eugKEPymO8.png"></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>清华AIOps数据集，选取了25条KPI。</p>
<h3 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h3><ol>
<li><strong>Missing Data.</strong> 去除。</li>
<li><strong>Standardization.</strong> Minmax Standardization，Feature Extraction使用的是Standardization后的数据。</li>
<li><strong>Feature Extraction.</strong> 使用了12种特征。</li>
</ol>
<table>
<thead>
<tr>
<th>Group</th>
<th>Feature Name</th>
</tr>
</thead>
<tbody><tr>
<td>Values</td>
<td>The original values standardized</td>
</tr>
<tr>
<td>Statistical Features</td>
<td>Mean,  Standard Deviation, Range, Difference…</td>
</tr>
<tr>
<td>Fitting Features</td>
<td>EWMA, AR</td>
</tr>
<tr>
<td>Wavelet Features</td>
<td>Db2 wavelet decomposition</td>
</tr>
</tbody></table>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="AUCPR"><a href="#AUCPR" class="headerlink" title="AUCPR"></a>AUCPR</h3><p><img src="https://i.loli.net/2020/06/24/LAqNMW1zvs8eP7X.png"></p>
<p><img src="https://i.loli.net/2020/06/24/YgRwavDIn7izLPX.png"></p>
<h3 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h3><p><img src="https://i.loli.net/2020/06/24/FM3BUhQXtpo7aiu.png"></p>
<h2 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h2><ol>
<li>这篇文章的Label Screening方法实际上是在处理样本分类难易度的问题，将异常区间内容易的样本去除了</li>
<li>对于时间序列的异常检测问题，我们的目标一般是Point-based的异常标签，一个时间点的特征是有限的。如果用窗口的方式，以${x_{t-w+1},\cdots,x_t}$作为时间$t$的输入（当然每个$x_t$可以有多个Channel），然后把预测结果作为时间$t$的输出</li>
</ol>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/2/">上一页</a></div><div class="pagination-next"><a href="/page/4/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link is-current" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li></ul></nav></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>