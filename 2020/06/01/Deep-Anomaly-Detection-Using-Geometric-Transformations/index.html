<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Deep Anomaly Detection Using Geometric Transformations - Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Introduction本文考虑图像数据的异常检测问题。与基于重构的方法不同，本文提出的方法通过对正常图片施加不同的几何变换之后，训练一个多分类器将无监督异常检测问题转化为一个有监督问题。本方法背后的直觉是在训练能够分辨不同变换后的图片之后，分类器一定学得了一些显著的几何特征，这些几何特征是正常类别独有的。 Proposed MethodProblem Statement本文考虑针对图像的异常检测"><meta property="og:type" content="article"><meta property="og:title" content="Deep Anomaly Detection Using Geometric Transformations"><meta property="og:url" content="https://larryshaw0079.github.io/hanzawa-blog/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:description" content="Introduction本文考虑图像数据的异常检测问题。与基于重构的方法不同，本文提出的方法通过对正常图片施加不同的几何变换之后，训练一个多分类器将无监督异常检测问题转化为一个有监督问题。本方法背后的直觉是在训练能够分辨不同变换后的图片之后，分类器一定学得了一些显著的几何特征，这些几何特征是正常类别独有的。 Proposed MethodProblem Statement本文考虑针对图像的异常检测"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/06/24/XBFKcPio64u3U1C.png"><meta property="og:image" content="https://i.loli.net/2020/06/23/fLkst4i7Hu6PhQl.png"><meta property="og:image" content="https://i.loli.net/2020/06/23/z8MpdeoD6ZGavlN.png"><meta property="og:image" content="https://i.loli.net/2020/06/24/gHZohrvz9MGYmxi.png"><meta property="og:image" content="https://i.loli.net/2020/06/24/8Pi4EKCRuBXrYgp.png"><meta property="article:published_time" content="2020-06-01T15:17:03.000Z"><meta property="article:modified_time" content="2020-06-26T16:25:25.079Z"><meta property="article:author" content="Hanzawa"><meta property="article:tag" content="Anomaly Detection"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.loli.net/2020/06/24/XBFKcPio64u3U1C.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://larryshaw0079.github.io/hanzawa-blog/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/"},"headline":"Deep Anomaly Detection Using Geometric Transformations","image":["https://i.loli.net/2020/06/24/XBFKcPio64u3U1C.png","https://i.loli.net/2020/06/23/fLkst4i7Hu6PhQl.png","https://i.loli.net/2020/06/23/z8MpdeoD6ZGavlN.png","https://i.loli.net/2020/06/24/gHZohrvz9MGYmxi.png","https://i.loli.net/2020/06/24/8Pi4EKCRuBXrYgp.png"],"datePublished":"2020-06-01T15:17:03.000Z","dateModified":"2020-06-26T16:25:25.079Z","author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":"Introduction本文考虑图像数据的异常检测问题。与基于重构的方法不同，本文提出的方法通过对正常图片施加不同的几何变换之后，训练一个多分类器将无监督异常检测问题转化为一个有监督问题。本方法背后的直觉是在训练能够分辨不同变换后的图片之后，分类器一定学得了一些显著的几何特征，这些几何特征是正常类别独有的。 Proposed MethodProblem Statement本文考虑针对图像的异常检测"}</script><link rel="canonical" href="https://larryshaw0079.github.io/hanzawa-blog/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-06-01T15:17:03.000Z" title="2020-6-1 11:17:03 ├F10: PM┤">2020-06-01</time>发表</span><span class="level-item"><time dateTime="2020-06-26T16:25:25.079Z" title="2020-6-27 12:25:25 ├F10: AM┤">2020-06-27</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Deep Anomaly Detection Using Geometric Transformations</h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文考虑图像数据的异常检测问题。与基于重构的方法不同，本文提出的方法通过对正常图片施加不同的几何变换之后，训练一个多分类器将无监督异常检测问题转化为一个有监督问题。本方法背后的直觉是在训练能够分辨不同变换后的图片之后，分类器一定学得了一些显著的几何特征，这些几何特征是正常类别独有的。</p>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>本文考虑针对图像的异常检测。记$\mathcal X$为所有自然图像的空间，$X\subseteq\mathcal X$为正常图像集合。给定数据集$S\subseteq X$，异常检测的目的是学习一个分类器$h_S(x):\mathcal X\rightarrow{0,1}$，其中$h_S(x)=1\Leftrightarrow x\in X$。</p>
<p>为了兼顾查准率和查全率，常用的设置是学习一个打分函数$n_S(x):\mathcal X\rightarrow\mathbb R$，分数越高代表样本属于$X$的概率越大。之后，通过设定阈值，便可以构建异常分类器：<br>$$<br>\begin{align}<br>h_S^\lambda(x)=<br>\begin{cases}<br>1 &amp; n_S(x)\leq\lambda\<br>0 &amp; n_S(x)&lt;\lambda<br>\end{cases}<br>\end{align}<br>$$</p>
<h2 id="Discriminative-Learning-of-an-Anomaly-Scoring-Function-Using-Geometric-Transformations"><a href="#Discriminative-Learning-of-an-Anomaly-Scoring-Function-Using-Geometric-Transformations" class="headerlink" title="Discriminative Learning of an Anomaly Scoring Function Using Geometric Transformations"></a>Discriminative Learning of an Anomaly Scoring Function Using Geometric Transformations</h2><p>有初始数据集$S$，几何变换集合$\mathcal T$，通过对$S$中每个样本施加这$|\mathcal T|$个几何变换得到新数据集记为$S_\mathcal{T}$，且$S_\mathcal{T}$中每个样本的标签为变换的序号。之后，在$S_\mathcal{T}$上训练一个$|\mathcal T|$分类器。在测试阶段，对测试样本同样施加$|\mathcal T|$个几何变换，分类器会给出经过$\mathrm{softmax}$的输出向量，最终的异常分数由经过输出的向量构造的分布对数似然得来。</p>
<h3 id="Creating-and-Learning-the-Self-Labeled-Dataset"><a href="#Creating-and-Learning-the-Self-Labeled-Dataset" class="headerlink" title="Creating and Learning the Self-Labeled Dataset"></a>Creating and Learning the Self-Labeled Dataset</h3><p>设$\mathcal T={T_0,T_1,\cdots,T_{k-1}}$为几何变换集合，$1\leq i\leq k-1,\space T_i:\mathcal X\rightarrow \mathcal X$，且$T_0(x)=x$。$S_\mathcal{T}$定义为：</p>
<p>$$<br>S_\mathcal T={(T_j(x),j):x\in S,T_j\in\mathcal T}<br>$$<br>对于每个$x\in S$，$j$为$T_j(x)$的标签。我们直接学习一个$K$类分类器$f_\theta$，来预测输入样本对应的几何变换种类，这相当于是一个图像分类问题。</p>
<img src="https://i.loli.net/2020/06/24/XBFKcPio64u3U1C.png" style="zoom:67%;" />

<h3 id="Dirichlet-Normality-Score"><a href="#Dirichlet-Normality-Score" class="headerlink" title="Dirichlet Normality Score"></a>Dirichlet Normality Score</h3><p>接下来要做的是如何定义异常分数，记为$n_S(x)$，这是文中的一个重要的部分。设几何变换集合$\mathcal T={T_0,T_1,\cdots,T_{k-1}}$，且$k$分类器$f_\theta$在$S_\mathcal{T}$上完成训练。对于任意一个样本$x$，令$\mathbf y(x)=\text{softmax}(f_{\theta}(x))$，即分类器$f_\theta$输出的$\text{softmax}$之后的向量。异常分数$n_S(x)$定义为：</p>
<p>$$<br>n_S(x)=\sum\limits_{i=0}^{k-1}\log p(\mathbf y(T_i(x))|T_i)<br>$$</p>
<p>该异常分数定义为每个类别上，在几何变换$T_i$的条件下，输出的$\mathbf y$的对数似然之和。在文中，作者假设$\mathbf y(T_i(x)|T_i$服从迪利克雷分布：$\mathbf y(T_i(x))|T_i\sim\text{Dir}(\boldsymbol \alpha_i)$，其中$\boldsymbol \alpha_i\in\mathbb R^k_+$，$x\sim p_X(x)$，$i\sim\text{Uni}(0,k-1)$，而$p_X(x)$代表正常样本的真实数据分布。于是：</p>
<p>$$<br>n_S(x)=\sum_{i=0}^{k-1}\left[\log\Gamma(\sum_{j=0}^{k-1}[\tilde{\boldsymbol\alpha}<em>i]<em>j)-\sum</em>{j=0}^{k-1}\log\Gamma([\tilde{\boldsymbol\alpha}_i]_j)+\sum</em>{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]_j-1)\log\mathbf y(T_i(x))_j\right]<br>$$</p>
<p>因为$\tilde{\alpha}<em>i$相对于$x$来说是常数，所以可以直接忽略，于是式子简化为：<br>$$<br>n_S(x)=\sum_{i=0}^{k-1}\sum</em>{j=0}^{k-1}([\tilde{\boldsymbol\alpha}_i]<em>j-1)\log\mathbf y(T_i(x))_j=\sum</em>{i=0}^{k-1}(\tilde{\boldsymbol \alpha}_i-1)\cdot\log\mathbf y(T_i(x))<br>$$</p>
<p>注意这里的每个$\boldsymbol \alpha_i$都是一个向量，即对于每个变换$i$，都对应一个迪利克雷分布，其参数为$\boldsymbol\alpha_i$；在对训练集进行第$i$个几何变换之后，我们得到了${T_i(x)}$，然后分类器$f_\theta(\cdot)$的输出$\mathbf y(T_i(x))$相当于迪利克雷分布的观测值，我们需要根据观测值来估计参数$\boldsymbol \alpha_i$，然后根据这个参数来计算$n_S(x)$。对于$\boldsymbol\alpha_i$，可以知道其第$i$个分量应该是相对比较大的，下面是运行官方代码得到的$\boldsymbol\alpha_i$的结果（$i=69$，$i$从$0$开始，总共为$72$维），可以看到第$69$个分量是最大的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[INFO] value of mle_alpha_t:</span><br><span class="line"> [ 0.10228925  0.08997199  0.13083569  0.10862965  0.09811163  0.08527119</span><br><span class="line">  0.17637901  0.27628416  0.12873376  0.19197053  0.11587154  0.09873095</span><br><span class="line">  0.12700618  0.07688542  0.10488203  0.12499191  0.11637607  0.07739511</span><br><span class="line">  0.13049147  0.51031647  0.20546597  0.15558449  0.09288609  0.12134945</span><br><span class="line">  0.09324992  0.14650162  0.16281216  0.11827823  0.08214853  0.15618336</span><br><span class="line">  0.28129761  0.45293697  0.11485838  1.78598954  0.16556983  0.1141158</span><br><span class="line">  0.10909459  0.13916602  0.11563799  0.07309986  0.11049714  0.12974086</span><br><span class="line">  0.15930642  0.13714361  0.13938356  0.70619553  0.11174039  0.07201538</span><br><span class="line">  0.16626109  0.12153727  0.09548811  0.07940956  0.15832209  0.11035474</span><br><span class="line">  0.12487912  0.16937875  0.23212662  0.37041831  0.08557451  0.0839439</span><br><span class="line">  0.09924258  0.39766872  0.14917286  0.08704662  0.09554555  0.31047109</span><br><span class="line">  0.24504759  0.16812463  0.11508187 63.98878807  0.12971073  0.07972932]</span><br></pre></td></tr></table></figure>

<p>下图也展示了对于每个变换$i$，$\mathbf y(T_i(x)|T_i$分布的情况：</p>
<img src="https://i.loli.net/2020/06/23/fLkst4i7Hu6PhQl.png" style="zoom:67%;" />

<p>作者还给出了一种简化的形式，$\hat{n}<em>S(x)=\frac{1}{k}\sum^{k-1}</em>{j=0}[\mathbf y(T_j(x))]_j$。相当于说，对于每个变换$T_i$分类器都会给出一个$\text{softmax}$向量，取其第$i$个分量$[\mathbf y(T_j(x))]_j$，然后把每个变换对应的$[\mathbf y(T_j(x))]_j$加起来。</p>
<p>整个算法的流程如下：</p>
<img src="https://i.loli.net/2020/06/23/z8MpdeoD6ZGavlN.png" style="zoom:67%;" />

<p>这里结合作者的源代码简单说一下检测阶段的流程。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t_ind <span class="keyword">in</span> <span class="built_in">range</span>(transformer.n_transforms):</span><br><span class="line">        observed_dirichlet = mdl.predict(transformer.transform_batch(observed_data, [t_ind] * <span class="built_in">len</span>(observed_data)), batch_size=<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>

<p>在训练好模型之后，对于训练集的所有样本，对其进行$K$个几何变换之后，得到$K$个样本${T_i(x)}$，对于所有第$i$个几何变换对应的样本${T_i(x)}$，通过分类器$f_\theta$会给出输出$\mathbf y(T_i(x))$。这里对应算法中的第$7-8$行，这个<code>observed_dirichlet</code>就是$S_i$。</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">log_p_hat_train = np.log(observed_dirichlet).mean(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">alpha_sum_approx = calc_approx_alpha_sum(observed_dirichlet)</span><br><span class="line">alpha_0 = observed_dirichlet.mean(axis=<span class="number">0</span>) * alpha_sum_approx</span><br></pre></td></tr></table></figure>

<p>之后这部分主要对应算法中的$9-11$行。作者把所有的第$i$个变换，分类器的输出的集合（也就是变量<code>observed_dirichlet</code>）记为$S_i$，$\bar s$为$S_i$的平均，$\bar l$为$S_i$对数的平均（变量<code>log_p_hat_train</code>），初始值$\tilde{\alpha}_i$由$\bar s\frac{(k-1)(-\Psi(1))}{\bar s\cdot\log\bar s-\bar s\cdot\bar l}$给出（变量<code>alpha_0</code>）。函数<code>calc_approx_alpha_sum</code>实现的是算法中第$11$行的$\frac{(k-1)(-\Psi(1))}{\bar s\cdot\log\bar s-\bar s\cdot\bar l}$，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_approx_alpha_sum</span>(<span class="params">observations</span>):</span></span><br><span class="line">    N = <span class="built_in">len</span>(observations)</span><br><span class="line">    f = np.mean(observations, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (N * (<span class="built_in">len</span>(f) - <span class="number">1</span>) * (-psi(<span class="number">1</span>))) / (</span><br><span class="line">        N * np.<span class="built_in">sum</span>(f * np.log(f)) - np.<span class="built_in">sum</span>(f * np.<span class="built_in">sum</span>(np.log(observations), axis=<span class="number">0</span>)))</span><br></pre></td></tr></table></figure>

<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mle_alpha_t = fixed_point_dirichlet_mle(alpha_0, log_p_hat_train)</span><br></pre></td></tr></table></figure>

<p>这里对应算法中的$12-14$行，即重复$\tilde\alpha_i\leftarrow\Psi^{-1}\left(\Psi(\sum_j[\alpha_i]_j)+\bar l\right)$来估计$\alpha$，函数<code>fixed_point_dirichlet_mle</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fixed_point_dirichlet_mle</span>(<span class="params">alpha_init, log_p_hat, max_iter=<span class="number">1000</span></span>):</span></span><br><span class="line">    alpha_new = alpha_old = alpha_init</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_iter):</span><br><span class="line">        alpha_new = inv_psi(psi(np.<span class="built_in">sum</span>(alpha_old)) + log_p_hat)</span><br><span class="line">        <span class="keyword">if</span> np.sqrt(np.<span class="built_in">sum</span>((alpha_old - alpha_new) ** <span class="number">2</span>)) &lt; <span class="number">1e-9</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        alpha_old = alpha_new</span><br><span class="line">    <span class="keyword">return</span> alpha_new</span><br></pre></td></tr></table></figure>

<p>$\Psi^{-1}(\cdot)$是通过数值方法来估计的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inv_psi</span>(<span class="params">y, iters=<span class="number">5</span></span>):</span></span><br><span class="line">    <span class="comment"># initial estimate</span></span><br><span class="line">    cond = y &gt;= <span class="number">-2.22</span></span><br><span class="line">    x = cond * (np.exp(y) + <span class="number">0.5</span>) + (<span class="number">1</span> - cond) * <span class="number">-1</span> / (y - psi(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        x = x - (psi(x) - y) / polygamma(<span class="number">1</span>, x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<hr>
<p>最后，在得到对$\alpha$的估计之后，可以来计算测试样本的分数了。这里对应的是算法中的第$16$行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_test_p = mdl.predict(transformer.transform_batch(x_test, [t_ind] * <span class="built_in">len</span>(x_test)), batch_size=<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">scores += dirichlet_normality_score(mle_alpha_t, x_test_p)</span><br></pre></td></tr></table></figure>

<p>函数<code>dirichlet_normality_score</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dirichlet_normality_score</span>(<span class="params">alpha, p</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>((alpha - <span class="number">1</span>) * np.log(p), axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h2><p>文中用到了如下的Baseline：</p>
<ul>
<li><strong>One-class SVM. **单类支持向量机，作者使用了三个变体，分别为</strong>RAW-OC-SVM<strong>——使用原始数据作为输入，</strong>CAE-OC-SVM<strong>——使用一个卷积自编码器来获得低维表示作为输入和</strong>E2E-OC-SVM<strong>——全名为</strong>One-Class Deep Support Vector Data Description**；</li>
<li>**Deep structured energy-based models. **</li>
<li>**Deep Autoencoding Gaussian Mixture Model. **</li>
<li>**Generative Adversarial Networks. **</li>
</ul>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>文中用到了一下几个数据集：</p>
<ul>
<li><strong>CIFAR-10</strong></li>
<li><strong>CIFAR-100</strong></li>
<li><strong>Fashion-MNIST</strong></li>
<li><strong>CatsVsDogs</strong></li>
</ul>
<p>在实验中所有图片都被归一化到$[-1,1]$的范围。</p>
<h2 id="Experimental-Protocol"><a href="#Experimental-Protocol" class="headerlink" title="Experimental Protocol"></a>Experimental Protocol</h2><p>设数据集有$C$个类，我们会进行$C$次实验，在第$c$次实验 ($1\leq c \leq C$)中我们会将第$c$个类作为正常样本，而其他类作为异常样本。在训练阶段，训练集只包含正常样本，而在测试阶段则会有正常样本和异常样本。在获得异常分数之后，阈值$\lambda$则根据ROC曲线下面积选择。</p>
<p>实验中使用的几何变换基于以下三种基变换：</p>
<ul>
<li>**Horizontal flip: ** 记为$T_b^{flip}(x)$，$b\in{T,F}$代表是否翻转；</li>
<li>**Translation: ** 记为$T_{s_h,s_w}^{trans}(x)$，其中$s_h,s_w\in{-1,0,1}$。在长宽两个维度上位移分别为$0.25$高度和$0.25$宽度，这两个维度发生位移的方向由$s_h$和$s_w$决定，当$s_h=s_w=0$时代表不移动；</li>
<li>**Rotation by multiples 90 degrees: ** 记为$T_k^{rot}(x)$，$k\in{0,1,2,3}$。旋转$k\times90$度。</li>
</ul>
<p>将三种基变换叠加有：<br>$$<br>\mathcal T=\left{<br> T_k^{rot}\circ T_{s_h,s_w}^{trans}\circ T_b^{flip} : \begin{matrix}<br> b &amp;\in {T,F}\<br> s_h,s_w&amp;\in{-1,0,1}\<br> k&amp;\in{0,1,2,3}<br> \end{matrix}<br>\right}<br>$$<br>最终几何变换种数为$2\times3\times3\times4=72$种。</p>
<p>分类器模型使用的是<strong>Wide Residual Network</strong>，优化器为Adam，Batch size为128，训练轮数为200。</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>下面是不同方法在不同数据集上的实验结果：</p>
<p><img src="https://i.loli.net/2020/06/24/gHZohrvz9MGYmxi.png"></p>
<p>评测标准使用的是AUROC。作者关于结果的分析主要有以下几点：</p>
<ol>
<li>在绝大多数情况下，我们的算法都比Baseline要好，而且是越大的数据集效果越好。CatsVsDogs数据集每张图片的大小比其他几个数据集都要大，而Baseline在这个数据集上的结果都在$50%$或不到$50%$，这基本等同于瞎猜；</li>
<li>在CIFAR-100数据集里，由于将这100类聚合为了20类，所以存在类内样本差异大的问题。比如在类$5$、类$7$和类$13$上，模型表现就不够好；</li>
<li>在Fashion-MNIST数据集上几乎所有方法（除了DAGMM）都表现很好。</li>
</ol>
<h2 id="On-the-Intuition-for-Using-Geometric-Transformations"><a href="#On-the-Intuition-for-Using-Geometric-Transformations" class="headerlink" title="On the Intuition for Using Geometric Transformations"></a>On the Intuition for Using Geometric Transformations</h2><p>这里作者对所选用的几何变换做了一些解释。实验中选用的三种基本几何变换都是可逆的线性几何变换（且为双射），作者也试过一些复杂的非线性变换，如高斯模糊、锐化、伽马校正等等，但是效果并不好。</p>
<p>作者认为分类器能够分辨不同变换的能力与最终性能成正比，为了验证这一点，进行了$3$个实验。从MNIST数据集选择一个数字作为正常样本，几何变换只采用两个，然后选择另一个数字作为异常样本，结果如下：</p>
<ul>
<li>**Normal digit: 8，Anomaly: 3，Transformations: Identity and horizontal flip. **由于数字$8$是对称的，所以要让分类器分辨原始的$8$和翻转之后的$8$是很难的，AUROC只有$0.646$；</li>
<li>**Normal digit: 3，Anomaly: 8，Transformations: Identity and horizontal flip. **这里把$3$作为正常样本，由于$3$不是对称的，所以两种变换是可以分辨的，AUROC达到了$0.957$；</li>
<li>**Normal digit: 8，Anomaly: 3，Transformations: Identity and translation by 7 pixels. **同样是把$8$作为正常样本，但变换用的是平移，AUROC达到了$0.919$。</li>
</ul>
<p>除此之外，作者还设计了一个实验，目的是测试什么样的图像会获得较高的分数$n_S(x)$。在给定训练好的分类器的情况下，优化输入的图像，目标函数是最大化分数$n_S(x)$。下图为实验结果：</p>
<p><img src="https://i.loli.net/2020/06/24/8Pi4EKCRuBXrYgp.png"></p>
<p>在左图中，将数字$3$作为正常样本训练的分类器、原始输入为数字$0$的图片时，随着优化的进行，图片慢慢地变得像数字$3$。在右图中，同样是将数字$3$作为正常样本训练的分类器，不过原始输入也是数字$3$，这时图像却没有怎么变化。</p>
<h1 id="Remark"><a href="#Remark" class="headerlink" title="Remark"></a>Remark</h1><ul>
<li>文中提到的在CIFAR100数据的实验上，由于类间差异比较大导致效果较差，那么很自然地，不同的变换样本对应的集簇实际上应当足够分开，集簇内的样本要足够进，这样对于分类器来说才能比较好的分类。不过采用的几何变换并没有针对这一点进行特别设计；</li>
<li>文中强调了所使用的变换为几何变换，其实除此之外，所使用的变换还都是可以用矩阵表示的可逆的变换。</li>
</ul>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Anomaly-Detection/">Anomaly Detection</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA11-cuDNN8-Tensorflow-Pytorch/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Ubuntu20.04LTS 深度学习环境配置 CUDA11 + cuDNN8 + Tensorflow + Pytorch</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/06/01/Cross-dataset-Time-Series-Anomaly-Detection-for-Cloud-Systems/"><span class="level-item">Cross-dataset Time Series Anomaly Detection for Cloud Systems</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>