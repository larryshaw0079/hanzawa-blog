<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Effective End-to-end Unsupervised Outlier Detection via Linear Priority of Discriminative Network - Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Introduction本文针对无监督异常检测提出了$E^3\space{Outlier}$。作者使用自监督学习的方法，通过构建有监督任务在没有标签的情况下学习高层语义特征。PS：这篇文章的方法和NIPS18上的Deep Anomaly Detection Using Geometric Transformations（后面简称GEOM）颇为相似，但是不知为啥没有在实验中进行比较。后面我会分析一些"><meta property="og:type" content="article"><meta property="og:title" content="Effective End-to-end Unsupervised Outlier Detection via Linear Priority of Discriminative Network"><meta property="og:url" content="https://larryshaw0079.github.io/hanzawa-blog/2020/07/14/Effective-End-to-end-Unsupervised-Outlier-Detection-via-Linear-Priority-of-Discriminative-Network/"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:description" content="Introduction本文针对无监督异常检测提出了$E^3\space{Outlier}$。作者使用自监督学习的方法，通过构建有监督任务在没有标签的情况下学习高层语义特征。PS：这篇文章的方法和NIPS18上的Deep Anomaly Detection Using Geometric Transformations（后面简称GEOM）颇为相似，但是不知为啥没有在实验中进行比较。后面我会分析一些"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/07/14/ULAdYpzsoGfFwtD.png"><meta property="og:image" content="https://i.loli.net/2020/07/16/iPa7h9HWqrnZvgx.png"><meta property="og:image" content="https://i.loli.net/2020/07/16/U5fVk8YOEGPx3Q7.png"><meta property="og:image" content="https://i.loli.net/2020/07/16/PgzqXIdJ67s3BtG.png"><meta property="og:image" content="https://i.loli.net/2020/07/16/G3agKPuJBowFmIW.png"><meta property="og:image" content="https://i.loli.net/2020/07/16/h6iYjBkrwQdFvMJ.png"><meta property="og:image" content="https://i.loli.net/2020/07/16/waWAi7zI3QpOcf6.png"><meta property="article:published_time" content="2020-07-14T10:46:13.000Z"><meta property="article:modified_time" content="2020-07-21T12:35:42.220Z"><meta property="article:author" content="Hanzawa"><meta property="article:tag" content="Self-Supervised Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.loli.net/2020/07/14/ULAdYpzsoGfFwtD.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://larryshaw0079.github.io/hanzawa-blog/2020/07/14/Effective-End-to-end-Unsupervised-Outlier-Detection-via-Linear-Priority-of-Discriminative-Network/"},"headline":"Effective End-to-end Unsupervised Outlier Detection via Linear Priority of Discriminative Network","image":["https://i.loli.net/2020/07/14/ULAdYpzsoGfFwtD.png","https://i.loli.net/2020/07/16/iPa7h9HWqrnZvgx.png","https://i.loli.net/2020/07/16/U5fVk8YOEGPx3Q7.png","https://i.loli.net/2020/07/16/PgzqXIdJ67s3BtG.png","https://i.loli.net/2020/07/16/G3agKPuJBowFmIW.png","https://i.loli.net/2020/07/16/h6iYjBkrwQdFvMJ.png","https://i.loli.net/2020/07/16/waWAi7zI3QpOcf6.png"],"datePublished":"2020-07-14T10:46:13.000Z","dateModified":"2020-07-21T12:35:42.220Z","author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":"Introduction本文针对无监督异常检测提出了$E^3\\space{Outlier}$。作者使用自监督学习的方法，通过构建有监督任务在没有标签的情况下学习高层语义特征。PS：这篇文章的方法和NIPS18上的Deep Anomaly Detection Using Geometric Transformations（后面简称GEOM）颇为相似，但是不知为啥没有在实验中进行比较。后面我会分析一些"}</script><link rel="canonical" href="https://larryshaw0079.github.io/hanzawa-blog/2020/07/14/Effective-End-to-end-Unsupervised-Outlier-Detection-via-Linear-Priority-of-Discriminative-Network/"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-07-14T10:46:13.000Z" title="2020-7-14 6:46:13 ├F10: PM┤">2020-07-14</time>发表</span><span class="level-item"><time dateTime="2020-07-21T12:35:42.220Z" title="2020-7-21 8:35:42 ├F10: PM┤">2020-07-21</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Anomaly-Detection/">Anomaly Detection</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Effective End-to-end Unsupervised Outlier Detection via Linear Priority of Discriminative Network</h1><div class="content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文针对无监督异常检测提出了$E^3\space{Outlier}$。作者使用自监督学习的方法，通过构建有监督任务在没有标签的情况下学习高层语义特征。PS：这篇文章的方法和NIPS18上的<em>Deep Anomaly Detection Using Geometric Transformations</em>（后面简称GEOM）颇为相似，但是不知为啥没有在实验中进行比较。后面我会分析一些两篇文章方法上的异同。</p>
<h1 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h1><h2 id="Surrogate-Supervision-Based-Effective-Representation-Learning-for-UOD"><a href="#Surrogate-Supervision-Based-Effective-Representation-Learning-for-UOD" class="headerlink" title="Surrogate Supervision Based Effective Representation Learning for UOD"></a>Surrogate Supervision Based Effective Representation Learning for UOD</h2><p>这里作者提到了使用重构的模型来进行异常检测的不足：重构模型采用像素级别的损失函数（如mean square error），而这太过于严格和细节，并不能学到高层语义特征。</p>
<p>为此，作者提出了<em>surrogate supervision based discriminative network</em> (SSD)。具体操作和GEOM类似，首先预定义大小为$K$的几何变换集合$\mathcal O={O(\cdot|y)}<em>{y=1}^K$。对每一个样本$\mathbf x$，在经过$K$个集合变换之后会得到$K$个变换后的样本（第$y$个变换产生的样本即记为$\mathbf x^{(y)}=O(\mathbf x|y)$），每个样本对应的pseudo label即为变换的序号或者说种类。之后在新的数据集上（大小为原来的$K$倍）训练$K$分类网络。网络的输出为$P(\mathbf x^{(y^\prime)}|\boldsymbol\theta)=[P^{(y)}(\mathbf x^{(y^\prime)}|\boldsymbol\theta)]</em>{y=1}^K$，每个维度代表输入样本对应的变换的概率。总的损失函数为：<br>$$<br>\min_\theta\frac{1}{N}\sum_{i=1}^{N}\mathcal L_{SS}(\mathbf x_i|\theta)<br>$$</p>
<p>其中$\mathcal L_{SS}(\mathbf x_i|\theta)$代表每个样本对应的Loss，这个Loss可以由分类器在$K$个变换上的交叉熵损失来确定：</p>
<p>$$<br>\mathcal L_{SS}(\mathbf x_i|\boldsymbol\theta)=-\frac{1}{K}\sum_{y=1}^K\log(P^{(y)}(\mathbf x_i^{(y)}|\boldsymbol\theta))=-\frac{1}{K}\sum_{y=1}^K\log(P^{(y)}(O(\mathbf x_i|y)|\boldsymbol\theta))<br>$$<br><img src="https://i.loli.net/2020/07/14/ULAdYpzsoGfFwtD.png"></p>
<p>变换集合$\mathcal O$由一系列基本变换的组合确定。作者将这些基本变换分为了：1) 旋转 2) 翻转 3) 平移，包括横向和纵向 4) Patch置换（参考图1(a)中的Patch Re-arranging）。最终的变换集合$\mathcal O$由三个子集组成，分别是$\mathcal O_{RA}$（代表Regular Affine，其中每个变换为旋转$90°$的倍数、翻转、横向平移和纵向平移这四个基本变换的叠加），$\mathcal O_{IA}$（代表Irregular Affine，其中每个变换为进行$30°$的倍数且不为$90°$的倍数角度的旋转、翻转这两个基本变换的叠加）和$\mathcal O_{PR}$（只包含Patch Re-arranging）。</p>
<p>为了验证SSD学到的特征的有效性，作者将CAE提取的特征和SSD提取的特征分别用孤立森林进行异常检测，发现SSD效果更好（见图1(b)）。</p>
<p>到这里为止本文和GEOM基本没有大的区别。值得注意的是在所采用的几何变换中，采用了非线性变换（进行$30°$的倍数且不为$90°$的倍数角度的旋转）。而在GEOM中，提到过使用非线性变换的话效果会比较差，至于具体的影响如何，可能需要实验来确定。</p>
<h2 id="Inlier-Priority-The-Foundation-of-End-to-end-UOD"><a href="#Inlier-Priority-The-Foundation-of-End-to-end-UOD" class="headerlink" title="Inlier Priority: The Foundation of End-to-end UOD"></a>Inlier Priority: The Foundation of End-to-end UOD</h2><p>在这里作者主要对在训练集包含少量异常的情况下做出的理论分析，作者将其称为<em>Inlier Priority</em>，原句如下：</p>
<blockquote>
<p><em>Inlier Priority</em>: Despite that inliers/outliersare indiscriminately fed into SSD for training, SSD will prioritize the minimization of inliers’ loss.</p>
</blockquote>
<h3 id="Priority-by-Gradient-Magnitude"><a href="#Priority-by-Gradient-Magnitude" class="headerlink" title="Priority by Gradient Magnitude"></a>Priority by Gradient Magnitude</h3><p>对于第$c$个类来说，设<code>softmax</code>层和倒数第二层之间的权重矩阵为$\mathbf w_c=[w_{s,c}]^{(L+1)}<em>{s=1}$，损失函数记为$\mathcal L$，梯度记为$\nabla</em>{\mathbf w_c}\mathcal L=[\nabla_{w_{s,c}}\mathcal L]^{(L+1)}<em>{s=1}$。设训练集$X^{(c)}$包含$N</em>{in}$个正常样本，$N_{out}$个异常样本。记正常样本和异常样本对应的梯度分别为$\parallel\nabla^{(in)}_{\mathbf w_c}\mathcal L\parallel$和$\parallel\nabla^{(out)}_{\mathbf w_c}\mathcal L\parallel$，在网络只有一个隐层且采用<code>Sigmoid</code>作为激活函数时，两者梯度的期望之比有如下关系：</p>
<p>$$<br>\frac{E(\parallel\nabla^{(in)}<em>{\mathbf w_c}\mathcal L\parallel^2)}{E(\parallel\nabla^{(out)}_{\mathbf w_c}\mathcal L\parallel^2)}\approx\frac{N^2_{in}}{N^2</em>{out}}<br>$$</p>
<p>在训练集中，正常样本和异常样本的数量是极不均衡的，$N_{in}\gg N_{out}$，所以有$E(\parallel\nabla^{(in)}_{\mathbf w_c}\mathcal L\parallel^2)\gg E(\parallel\nabla^{(out)}_{\mathbf w_c}\mathcal L\parallel^2)$。</p>
<p>在使用更复杂的网络时，作者通过实验展示了正常样本和异常样本对应的梯度大小的比较：</p>
<p><img src="https://i.loli.net/2020/07/16/iPa7h9HWqrnZvgx.png"></p>
<h3 id="Priority-by-Network-Updating-Direction"><a href="#Priority-by-Network-Updating-Direction" class="headerlink" title="Priority by Network Updating Direction"></a>Priority by Network Updating Direction</h3><p>这里作者通过梯度更新的方向来进行了理论上的解释。对于一个Batch的数据$X$，梯度为$-\nabla_\theta\mathcal L(X)=-\frac{1}{N}\sum_i\nabla_\theta\mathcal L(\mathbf x_i)$，如果将该梯度在Batch中某一样本$\mathbf x_i$对应的梯度的方向上进行分解$-\nabla_\theta\mathcal L(\mathbf x_i):d_i=-\nabla_\theta\mathcal L(X)\cdot\frac{-\nabla_\theta\mathcal L(\mathbf x_i)}{\parallel -\nabla_\theta\mathcal L(\mathbf x_i)\parallel}$，这代表了总的Loss在多大程度上减小样本$\mathbf x_i$对应的Loss，由于一个Batch即包含正常样本，也可能包含异常样本，所以作者将两者对应的梯度方向贡献进行了可视化：</p>
<p><img src="https://i.loli.net/2020/07/16/U5fVk8YOEGPx3Q7.png"></p>
<p>可以看到随着训练的进行，正常样本对应的贡献更高。</p>
<p>PS: 我以为作者会对基于几何变换的异常检测为什么有效做一些理论上的解释，不过却没有。这里只是对在训练集包含少量异常的情况下做出的理论分析，而这个实际上直觉上就很显然了。</p>
<h2 id="Scoring-Strategies-for-UOD"><a href="#Scoring-Strategies-for-UOD" class="headerlink" title="Scoring Strategies for UOD"></a>Scoring Strategies for UOD</h2><p>作者采用了三种方法来计算异常分数：</p>
<h3 id="Pseudo-Label-based-Score-PL"><a href="#Pseudo-Label-based-Score-PL" class="headerlink" title="Pseudo Label based Score (PL)"></a>Pseudo Label based Score (PL)</h3><p>对于一个测试样本$\mathbf x$，对其进行$K$个几何变换，通过分类器会得到$K$个输出，对于第$k$个输出，我们只取其第$k$个分量，最后把他们加起来除以$K$：</p>
<p>$$<br>S_{pl}(\mathbf x)=\frac{1}{K}\sum_{y=1}^K P^{(y)}(\mathbf x^{(y)}|\boldsymbol\theta)<br>$$</p>
<h3 id="Maximum-Probability-based-Score-MP"><a href="#Maximum-Probability-based-Score-MP" class="headerlink" title="Maximum Probability based Score (MP)"></a>Maximum Probability based Score (MP)</h3><p>这里稍有不同，对于第$k$个输出，我们取其值最大的分量，而不是第$k$个分量：</p>
<p>$$<br>S_{mp}(\mathbf x)=\frac{1}{K}\sum_{y=1}^K\max_t P^{(t)}(\mathbf x^{(y)}|\boldsymbol\theta)<br>$$</p>
<h3 id="Negative-Entropy-based-Score-NE"><a href="#Negative-Entropy-based-Score-NE" class="headerlink" title="Negative Entropy based Score (NE)"></a>Negative Entropy based Score (NE)</h3><p>作者认为，标签为One-Hot向量，分类器的输出分布越“尖峰”就越接近于正常样本，而越“平均”就越接近于异常样本，所以作者提出使用熵来描述分类器输出的“尖锐度”：<br>$$<br>S_{ne}(\mathbf x)=-\frac{1}{K}\sum_{y=1}^K H(P(\mathbf x^{(y)}|\boldsymbol\theta))=\frac{1}{K}\sum_{y=1}^K\sum_{t=1}^K P^{(t)}(\mathbf x^{(y)}|\boldsymbol\theta)\log(P^{(t)}(\mathbf x^{(y)}|\boldsymbol\theta))<br>$$<br>这里作者对第一种方法得到的结果进行了可视化：</p>
<p><img src="https://i.loli.net/2020/07/16/PgzqXIdJ67s3BtG.png"></p>
<p>PS：对比NIPS18 的Dirichlet Normality Score</p>
<ol>
<li>也用到了全部$K$个维度的信息</li>
<li>相当于对分类器的输出做了迪利克雷分布的先验假设，然后通过训练集的输出估计分布参数。因为直觉上对于正常分布来说，分类器的输出分布形状上都类似一个尖峰，但对于不同的数据集来说具体形状还是会有所差异</li>
</ol>
<h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Experiment-Setup"><a href="#Experiment-Setup" class="headerlink" title="Experiment Setup"></a>Experiment Setup</h2><p> 数据集用到了MNIST, Fashion-MNIST (F-MNIST) , CIFAR10, SVHN和CIFAR100。为了模拟无监督异常检测的环境，人为在训练集中加入异常样本，异常的比例$\rho$从$5%$到$25%$以$5%$的步长递增。评测标准采用AUPR和AUROC。</p>
<h2 id="UOD-Performance-Comparison-and-Discussion"><a href="#UOD-Performance-Comparison-and-Discussion" class="headerlink" title="UOD Performance Comparison and Discussion"></a>UOD Performance Comparison and Discussion</h2><p>下表展示了模型性能对比结果：</p>
<p><img src="https://i.loli.net/2020/07/16/G3agKPuJBowFmIW.png"></p>
<p>下图展示了在不同的Outlier Ratio下的性能对比：</p>
<p><img src="https://i.loli.net/2020/07/16/h6iYjBkrwQdFvMJ.png"></p>
<p>下图展示了在不同的变换集合，网络结构，异常分数的条件下的性能：</p>
<p><img src="https://i.loli.net/2020/07/16/waWAi7zI3QpOcf6.png"></p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Self-Supervised-Learning/">Self-Supervised Learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/08/24/Unsupervised-Representation-Learning-by-Predicting-Random-Distances/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Unsupervised Representation Learning by Predicting Random Distances</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/06/22/Probability-Distributions-Binary-and-Multinomial-Variables/"><span class="level-item">Probability Distributions - Binary and Multinomial Variables</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>