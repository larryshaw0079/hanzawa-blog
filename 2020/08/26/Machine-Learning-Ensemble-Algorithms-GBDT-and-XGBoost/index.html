<!DOCTYPE html>
<html lang="zh-CN">
    <!-- title -->




<!-- keywords -->




<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="Hanzawa">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Hanzawa">
    
    <meta name="keywords" content="hexo,hexo-theme,hexo-blog">
    
    <meta name="description" content="">
    <meta name="description" content="Introduction 本文主要介绍GBDT和XGBoost，在学习本文内容之前建议先学习决策树相关内容。 下面是一些有用的参考链接： XGBoost Documentation AdaBoost blog GBDT blog slide 陈天奇slide blog blog Preliminaries 实际上，GBDT和梯度下降、XGBoost和牛顿法之间是存在密切关系的">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Ensemble Algorithms: GBDT and XGBoost">
<meta property="og:url" content="http://qfxiao.me/2020/08/26/Machine-Learning-Ensemble-Algorithms-GBDT-and-XGBoost/index.html">
<meta property="og:site_name" content="Hanzawa の 部屋">
<meta property="og:description" content="Introduction 本文主要介绍GBDT和XGBoost，在学习本文内容之前建议先学习决策树相关内容。 下面是一些有用的参考链接： XGBoost Documentation AdaBoost blog GBDT blog slide 陈天奇slide blog blog Preliminaries 实际上，GBDT和梯度下降、XGBoost和牛顿法之间是存在密切关系的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/09/10/HoPR9XlpZSm1Gju.png">
<meta property="og:image" content="https://i.loli.net/2020/09/15/nxloKNmTHwJRqI9.png">
<meta property="article:published_time" content="2020-08-25T16:28:26.000Z">
<meta property="article:modified_time" content="2021-02-21T17:02:14.307Z">
<meta property="article:author" content="Hanzawa">
<meta property="article:tag" content="GBDT">
<meta property="article:tag" content="XGBoost">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/09/10/HoPR9XlpZSm1Gju.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    
    <title>Machine Learning Ensemble Algorithms: GBDT and XGBoost · fi3ework&#39;s Studio</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= "/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= "/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/assets/favicon.ico" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >fi3ework&#39;s Studio.</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Machine Learning Ensemble Algorithms: GBDT and XGBoost</a>
            </div>
    </div>
    
    <a class="home-link" href=/>fi3ework's Studio.</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Machine Learning Ensemble Algorithms: GBDT and XGBoost
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "GBDT">GBDT</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "XGBoost">XGBoost</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>字数统计: <span class="post-count word-count">3k</span>阅读时长: <span class="post-count reading-time">12 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2020/08/26</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="introduction">Introduction</h1>
<p>本文主要介绍GBDT和XGBoost，在学习本文内容之前建议先学习<a href="http://qfxiao.me/2020/09/02/Decision-Tree-Machine-Learning-Classification-Algorithms-3/">决策树相关内容</a>。</p>
<p>下面是一些有用的参考链接：</p>
<p><a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">XGBoost Documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6133937.html">AdaBoost blog</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6140514.html">GBDT blog</a></p>
<p><a target="_blank" rel="noopener" href="http://wepon.me/files/gbdt.pdf">slide</a></p>
<p><a target="_blank" rel="noopener" href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf">陈天奇slide</a></p>
<p><a target="_blank" rel="noopener" href="https://snaildove.github.io/2018/10/01/8.Booting-Methods_LiHang-Statistical-Learning-Methods/">blog</a></p>
<p><a target="_blank" rel="noopener" href="https://snaildove.github.io/2018/10/02/get-started-XGBoost/">blog</a></p>
<h1 id="preliminaries">Preliminaries</h1>
<p>实际上，GBDT和梯度下降、XGBoost和牛顿法之间是存在密切关系的，这里我们先回顾一下梯度下降算法和牛顿法的基础知识。</p>
<h2 id="taylor-formulation">Taylor Formulation</h2>
<p>函数<span class="math inline">\(f(x)\)</span>在点<span class="math inline">\(x_0\)</span>处的泰勒展开为： <span class="math display">\[
f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n
\]</span> 特别的，一阶展开为： <span class="math display">\[
f(x)\approx f(x_0)+f^\prime(x_0)(x-x_0)
\]</span> 二阶展开为： <span class="math display">\[
f(x)\approx f(x_0)+f^\prime(x_0)(x-x_0) + f^{\prime\prime}(x_0)\frac{(x-x_0)^2}{2}
\]</span> 迭代形式：假设<span class="math inline">\(x^t=x^{t-1}+\Delta x\)</span>，将<span class="math inline">\(f(x)\)</span>在<span class="math inline">\(x^{t-1}\)</span>处进行泰勒展开 <span class="math display">\[
\begin{align}
f(x^t) &amp;= f(x^{t-1}+\Delta x)\\
&amp;\approx f(x^{t-1})+f^\prime(x^{t-1})\Delta x + f^{\prime\prime}(x^{t-1})\frac{\Delta x^2}{2}
\end{align}
\]</span></p>
<h2 id="gradient-descend-method">Gradient Descend Method</h2>
<p>设参数<span class="math inline">\(\theta\)</span>，那么参数对应的损失函数为<span class="math inline">\(L(\theta)\)</span>，</p>
<p>设当前步数为<span class="math inline">\(t\)</span>，那么<span class="math inline">\(t-1\)</span>步时的参数为<span class="math inline">\(\theta^{t-1}\)</span>，将<span class="math inline">\(L(\theta^t)\)</span>在<span class="math inline">\(\theta^{t-1}\)</span>处展开得到：</p>
<p><span class="math display">\[
L(\theta^t) \approx L(\theta^{t-1})+ L^\prime(\theta^{t-1})\Delta\theta
\]</span> 我们想求的<span class="math inline">\(\theta^t=\theta^{t-1}+\Delta \theta\)</span></p>
<h2 id="newtons-method">Newton's Method</h2>
<p>将<span class="math inline">\(L(\theta^t)\)</span>在<span class="math inline">\(\theta^{t-1}\)</span>处进行二阶泰勒展开 <span class="math display">\[
\begin{align}
L(\theta^t) &amp;= L(\theta^{t-1}+\Delta \theta)\\
&amp;\approx L(\theta^{t-1})+L^\prime(\theta^{t-1})\Delta \theta + L^{\prime\prime}(\theta^{t-1})\frac{\Delta \theta^2}{2}
\end{align}
\]</span> 记一阶导数和二阶导数分别为<span class="math inline">\(g\)</span>和<span class="math inline">\(H\)</span>，那么 <span class="math display">\[
L(\theta^t)=L(\theta^{t-1})+g\Delta \theta + H\frac{\Delta \theta^2}{2}
\]</span> 要使得迭代后的结果尽量小，即<span class="math inline">\(g\Delta \theta + H\frac{\Delta \theta^2}{2}\)</span>尽量小，那么有<span class="math inline">\(\frac{\left(g\Delta \theta + H\frac{\Delta \theta^2}{2}\right)}{\partial\Delta\theta}=0\)</span></p>
<p>求得<span class="math inline">\(\Delta \theta=H^{-1}g\)</span>，故<span class="math inline">\(\theta^{t}=\theta^{t-1}+\Delta \theta=\theta^{t-1}-\frac{g}{h}\)</span>。如果<span class="math inline">\(\theta\)</span>是一个向量，那么<span class="math inline">\(\theta^{t}=\theta^{t-1}-H^{-1}g\)</span>，这里<span class="math inline">\(H\)</span>为海森矩阵。</p>
<h1 id="gradient-boosting-decision-tree-gbdt">Gradient Boosting Decision Tree (GBDT)</h1>
<p>我们首先来看基于树的Boosting模型中，非常经典的梯度提升树 (Gradient Boosting Decision Tree)。</p>
<h2 id="the-additive-model">The Additive Model</h2>
<p>首先GBDT是一个加法模型，即最终模型由一系列树模型乘以对应权重相加得来： <span class="math display">\[
F_T(x;w)=\sum_{t=0}^T\alpha_t h_t(x;w_t)=\sum_{t=0}^T f_t(x;w_t)
\]</span> 我们的目标是使得<span class="math inline">\(F\)</span>的损失函数最小化： <span class="math display">\[
F_T^*=\mathop{\arg\min}\limits_{F}\sum_{i=1}^N L(y_i, F_T(x_i;w))
\]</span></p>
<p>直接优化这个损失函数复杂度是很高的，GBDT实际上运用了一种类似贪心的策略来优化这个函数，将优化过程分解成了迭代的步骤。</p>
<p>回想梯度下降算法进行优化的步骤，我们有参数<span class="math inline">\(\theta\)</span>，损失函数<span class="math inline">\(L(\theta)\)</span>是<span class="math inline">\(\theta\)</span>的函数，我们希望找到最优的<span class="math inline">\(\theta^*\)</span>使得<span class="math inline">\(L(\theta^*)\)</span>最小，于是我们使用了迭代优化的步骤。假设迭代执行到第<span class="math inline">\(t\)</span>步，也就是说我们现在的参数<span class="math inline">\(\theta^{t-1}\)</span>为前面<span class="math inline">\(t-1\)</span>步增量之和：<span class="math inline">\(\theta^{t-1}=\sum_{j=1}^{t-1}\Delta \theta_j\)</span>，每一步的增量记为<span class="math inline">\(\Delta \theta_t\)</span>。当前的增量<span class="math inline">\(\Delta \theta_{t}\)</span>是怎么计算得到的呢？大家都知道是采用的损失函数在<span class="math inline">\(\theta^{t-1}\)</span>的负梯度乘以一个步长，即<span class="math inline">\(\Delta \theta_t=-\alpha_t \frac{\partial L(\theta)}{\partial \theta^{t-1}}\)</span>。</p>
<p>梯度下降相当于是在参数空间<span class="math inline">\(\theta\)</span>找到最合适的参数<span class="math inline">\(\theta^*\)</span>使得损失函数<span class="math inline">\(L(\theta)\)</span>最小化，如果我们把模型<span class="math inline">\(F_T\)</span>看作是函数空间，我们的目的是在函数空间中找到最优的<span class="math inline">\(F_T^*\)</span>使得损失函数最小化，在这一个角度上GBDT和梯度下降就统一起来了。每一步的基模型<span class="math inline">\(f_t\)</span>就相当于梯度下降中的增量<span class="math inline">\(\Delta \theta\)</span>，所以我们就得到了GBDT每一的优化目标，即损失函数<span class="math inline">\(L\)</span>对于<span class="math inline">\(F_{t-1}\)</span>的负梯度。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>梯度下降</th>
<th>GBDT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>损失函数</td>
<td><span class="math inline">\(L(\theta)\)</span></td>
<td><span class="math inline">\(L(F_t)\)</span></td>
</tr>
<tr class="even">
<td>参数</td>
<td><span class="math inline">\(\theta^t\)</span></td>
<td><span class="math inline">\(F_t\)</span></td>
</tr>
<tr class="odd">
<td>增量</td>
<td><span class="math inline">\(\Delta \theta_t=-\alpha_t g_t\)</span></td>
<td><span class="math inline">\(f_t=-\alpha_t g_t\)</span></td>
</tr>
<tr class="even">
<td>步长</td>
<td><span class="math inline">\(\alpha_t\)</span></td>
<td><span class="math inline">\(\alpha_t\)</span></td>
</tr>
<tr class="odd">
<td>初始值</td>
<td><span class="math inline">\(\theta_0\)</span></td>
<td><span class="math inline">\(f_0\)</span></td>
</tr>
</tbody>
</table>
<h2 id="gradient-boosting-tree-for-regression">Gradient Boosting Tree for Regression</h2>
<p>我们先来讨论GBDT解决回归问题的算法。前面我们已经讨论过，在每一步GBDT的优化目标是损失函数的负梯度，那么现在的问题就是如何求得每一步最优的基模型（GBDT的基模型选用的是CART）。GBDT的算法步骤如下：</p>
<blockquote>
<p><strong>Gradient Boosting Tree Algorithm</strong></p>
<p>INPUT: 训练样本<span class="math inline">\(\{(x_1,y_1),\cdots,(x_m,y_m)\}\)</span>，迭代轮数<span class="math inline">\(T\)</span>，损失函数<span class="math inline">\(L\)</span></p>
<p>OUTPUT: 强模型<span class="math inline">\(F_T\)</span></p>
<ol type="1">
<li>初始化弱学习器<span class="math inline">\(f_0\)</span>，直接使用一个基模型在训练集上进行训练</li>
<li>在步骤<span class="math inline">\(t=1...T\)</span>，对于每个样本计算负梯度<span class="math inline">\(r_{ti}=\left[\frac{\partial L(y_i,F_{t-1}(x_i))}{\partial F_{t-1}}\right]\)</span></li>
<li>在<span class="math inline">\((x_i,r_{ti})\)</span>上训练得到一个CART回归树，确定树的结构</li>
<li>假设一共有<span class="math inline">\(J\)</span>个叶子节点，那么对每个叶子节点计算最佳输出值<span class="math inline">\(c_{tj}=\mathop{\arg\min}\limits_{c_{tj}}\sum_{x_i\in R_{tj}} L(y_i,F_{t-1}(x_i)+c_{tj})\)</span>（其中<span class="math inline">\(c_{tj}\)</span>代表第<span class="math inline">\(j\)</span>个叶子的输出，<span class="math inline">\(R_{tj}\)</span>代表第<span class="math inline">\(j\)</span>个叶子对应的样本集合），确定每个叶子节点的输出</li>
<li>更新强学习器<span class="math inline">\(F_t=F_{t-1}+f_t\)</span>，回到步骤2直到达到迭代轮数</li>
<li>最终得到强学习器的表达式：<span class="math inline">\(f(x)=f_0(x)+\sum\limits_{t=1}^T\sum\limits_{j=1}^J c_{tj}\mathrm I(x\in R_{tj})\)</span></li>
</ol>
</blockquote>
<p>于是我们就得到了最终模型<span class="math inline">\(F_T\)</span>。</p>
<h2 id="gradient-boosting-tree-for-classification">Gradient Boosting Tree for Classification</h2>
<p>在处理分类任务时，由于输出是离散的值</p>
<p>一种方法是使用指数损失函数，此时GBDT退化为AdaBoost；另一种方法是借鉴逻辑回归的方法，去建模真实值的概率</p>
<h3 id="binary-classification">Binary Classification</h3>
<h3 id="multi-class-classfication">Multi-class Classfication</h3>
<h2 id="gbdt-sumarry">GBDT Sumarry</h2>
<p>优点：</p>
<ol type="1">
<li>可以灵活处理</li>
<li>相对SVM，调参较少</li>
<li>使用某些损失函数对异常值的鲁棒性高</li>
</ol>
<p>缺点：</p>
<ol type="1">
<li>难以并行训练</li>
</ol>
<h1 id="xgboost">XGBoost</h1>
<p>前面我们讲了梯度下降和牛顿法，刚才又讨论了GBDT和梯度下降的关系，那么XGBoost是否和牛顿法有什么关系呢？答案是肯定的。GBDT利用了损失函数在<span class="math inline">\(F_{t-1}\)</span>的一阶展开（即一阶导数信息），而XGBoost则利用了损失函数在<span class="math inline">\(F_{t-1}\)</span>的二阶展开，这也是XGBoost和GBDT最根本的区别。下面我们将详细讲解XGBoost算法。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>牛顿法</th>
<th>XGBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>损失函数</td>
<td><span class="math inline">\(L(\theta)\)</span></td>
<td><span class="math inline">\(L(F_t)\)</span></td>
</tr>
<tr class="even">
<td>参数</td>
<td><span class="math inline">\(\theta^t\)</span></td>
<td><span class="math inline">\(F_t\)</span></td>
</tr>
<tr class="odd">
<td>增量</td>
<td><span class="math inline">\(\Delta \theta_t=-\alpha_t H^{-1}_tg_t\)</span></td>
<td><span class="math inline">\(f_t=-\alpha_t H^{-1}_tg_t\)</span></td>
</tr>
<tr class="even">
<td>步长</td>
<td><span class="math inline">\(\alpha_t\)</span></td>
<td><span class="math inline">\(\alpha_t\)</span></td>
</tr>
<tr class="odd">
<td>初始值</td>
<td><span class="math inline">\(\theta_0\)</span></td>
<td><span class="math inline">\(f_0\)</span></td>
</tr>
</tbody>
</table>
<h2 id="regularization">Regularization</h2>
<p>XGBoost相比GBDT的另一大改进是加入了正则化项，即控制每个树的复杂度。衡量树的复杂度的度量有很多，XGBoost采用的是每棵树叶子节点的个数<span class="math inline">\(T\)</span>和每个叶子节点输出<span class="math inline">\(w\)</span>的平方和： <span class="math display">\[
\Omega(f)=\gamma T+\frac{1}{2}\lambda\parallel w\parallel^2
\]</span></p>
<p>这一步主要是为了进一步降低每个弱学习器的方差。</p>
<h2 id="objective-function">Objective Function</h2>
<p>加上正则项之后总的损失函数变为： <span class="math display">\[
L=\sum_{i=1}^N \ell(y_i, F_T(x_i))+\Omega(F_T)
\]</span> 和GBDT类似，我们来推导第<span class="math inline">\(t\)</span>步的优化公式，对于第<span class="math inline">\(t\)</span>步，我们的损失函数为： <span class="math display">\[
\begin{align}
L_t&amp;=\sum_{i=1}^N \ell(y_i,F_t(x_i))+\Omega(F_t)\\
&amp;=\sum_{i=1}^N \ell(y_i, F_{t-1}(x_i) + f_t(x_i))+\Omega(F_t)
\end{align}
\]</span> 将损失函数在<span class="math inline">\(F_{t-1}\)</span>处进行二阶泰勒展开，得到 <span class="math display">\[
L_t \approx \left[\sum_{i=1}^N \ell(y_i, F_{t-1}) + g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i) \right] + \Omega(F_t)
\]</span> 其中<span class="math inline">\(g_i=\frac{\partial \ell(y_i, F_{t-1})}{\partial F_{t-1}}\)</span>，<span class="math inline">\(h_i=\frac{\partial \ell(y_i, F_{t-1}) ^2}{\partial^2 F_{t-1}}\)</span>，分别代表损失函数对<span class="math inline">\(F_{t-1}\)</span>的一阶导和二阶导。</p>
<p>由于我们要优化的是本轮的基模型<span class="math inline">\(f_t\)</span>，<span class="math inline">\(\ell(y_i, F_{t-1})\)</span>已经是固定的了，相当于常数，把常数项去掉，得到： <span class="math display">\[
\begin{align}
\tilde L_t &amp;= \left[\sum_{i=1}^N  g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i) \right] + \Omega(f_t)\\
&amp;=\left[\sum_{i=1}^N  g_i f_t(x_i) + \frac{1}{2}h_i f_t^2(x_i) \right] + \gamma T + \frac{1}{2}\lambda \parallel w \parallel^2
\end{align}
\]</span> 我们都知道样本<span class="math inline">\(x_i\)</span>在树<span class="math inline">\(f_t\)</span>上的输出取决于<span class="math inline">\(x_i\)</span>在哪个叶子节点。假设树<span class="math inline">\(f_t\)</span>一共有<span class="math inline">\(J\)</span>个叶节点，记<span class="math inline">\(q(x_i)=j\)</span>代表样本<span class="math inline">\(x_i\)</span>经过决策树对应的叶节点是<span class="math inline">\(j\)</span>，<span class="math inline">\(I_j\)</span>代表叶子节点<span class="math inline">\(j\)</span>的所有样本下标集合，<span class="math inline">\(w_j\)</span>代表叶子节点<span class="math inline">\(j\)</span>的输出，我们可以将损失函数改写为： <span class="math display">\[
\begin{align}
\tilde L_t &amp;= \sum_{j=1}^J\left[\sum_{i\in I_j}g_i w_j+\frac{1}{2}(\sum_{i\in I_j}h_i +\lambda)w_j^2\right]+\gamma T\\
&amp;= \sum_{j=1}^J\left[G_j w_j + \frac{1}{2}(H_j+\lambda)w_j^2 \right] + \gamma T
\end{align}
\]</span> 其中<span class="math inline">\(G_j=\sum_{i\in I_j}g_i\)</span>和<span class="math inline">\(H_j=\sum_{i\in I_j}h_i\)</span>为简记，分别代表损失函数在叶子节点<span class="math inline">\(j\)</span>对应的所有样本上的一阶导之和与二阶导之和。</p>
<p>到现在，我们还剩两个问题需要解决，一个是确定树<span class="math inline">\(f_t\)</span>的最优结构，也就是怎么去分裂节点，另一个是确定每个叶子节点的最优输出。我们可以先确定下一个问题，找到另一个问题的最优答案，再来确定剩下的问题。</p>
<p>这里先去寻找树<span class="math inline">\(f_t\)</span>每一个叶子节点对应的最优输出。和牛顿法的推导类似，为了使损失函数下降的最快，我们令<span class="math inline">\(G_j w_j + \frac{1}{2}(H_j+\lambda)w_j^2\)</span>的导数为<span class="math inline">\(0\)</span>，得到： <span class="math display">\[
w_j^*=-\frac{G_j}{H_j + \lambda}
\]</span> 加上正则项有： <span class="math display">\[
\tilde L_t^*=-\frac{1}{2}\sum_{j=1}^J\frac{G_j^2}{H_j+\lambda}+\gamma T
\]</span></p>
<h2 id="splitting-strategy">Splitting Strategy</h2>
<p>现在来确定树<span class="math inline">\(f_t\)</span>的最优结构。最优结构的确定实际上使用了一种类似贪心的策略，和决策树类似，我们从一个只有根节点的树出发（所有样本都在根节点这一叶子节点上），不断分裂节点来降低<span class="math inline">\(\tilde L_t^*\)</span>。在每一步的分裂中，我们会希望<span class="math inline">\(\frac{G_j^2}{H_j+\lambda}\)</span>越大越好，于是： <span class="math display">\[
Gain = \frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda} - \gamma
\]</span></p>
<p>我们希望挑选能使得<span class="math inline">\(Gain\)</span>最大的特征和特征分裂点，而选择的策略又有很多种，下面介绍三种。</p>
<h3 id="exact-greedy-algorithm-for-split-finding">Exact Greedy Algorithm for Split Finding</h3>
<p>最简单的方法是枚举所有特征，然后对于这个特征下的所有可能取值进行排序，然后遍历分裂点，找到使得<span class="math inline">\(gain\)</span>最高的那个。这样做的好处是找到的分裂点确定是最好的，不过坏处是时间复杂度过高。</p>
<h3 id="approximate-algorithm-for-split-finding">Approximate Algorithm for Split Finding</h3>
<p>一个比较容易想到的优化方案是不去遍历所有可能的分裂点，而是只考察其中的分位数，如下图展示了三分位数方法：</p>
<p><img src="https://i.loli.net/2020/09/10/HoPR9XlpZSm1Gju.png" /></p>
<p>这样需要考察的点就大大减少。</p>
<p>同时分位数的选择由有global和local之分，global是指在训练之前我们就可以提前对每个特征的分位数进行预处理，local是指每次分裂前计算分位数点。直观上来说global需要更多的分位点数，而local则需要更多的计算量。</p>
<p>实际上，XGBoost还会使用二阶导信息<span class="math inline">\(h_i\)</span>对样本进行夹权，如下图所示：</p>
<p><img src="https://i.loli.net/2020/09/15/nxloKNmTHwJRqI9.png" /></p>
<h3 id="sparsity-aware-split-finding">Sparsity-aware Split Finding</h3>
<p>稀疏感知分裂算法 (Sparsity-aware Split Finding)</p>
<h2 id="other-features">Other Features</h2>
<p>除了上面提到的之外，XGBoost还有很多工程优化。</p>
<h3 id="block-structure-and-parallelism">Block Structure and Parallelism</h3>
<p>XGBoost预先对特征进行了排序，</p>
<p>每个特征的增益的计算可以并行进行</p>
<h3 id="column-sample">Column Sample</h3>
<p>借鉴随机森林，即每次只用一部分特征进行特征选择，进一步降低过拟合</p>
<h3 id="shrinkage">Shrinkage</h3>
<p>在每次迭代会对叶子节点的权总乘以一个系数，让后面的树有更大的学习空间。</p>
<h3 id="custom-loss-function">Custom Loss Function</h3>
<h3 id="missing-values">Missing Values</h3>
<h1 id="lightgbm">LightGBM</h1>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/227782064">parameter tuning</a></p>

    </article>
    <!-- license  -->
    
        <div class="license-wrapper">
            <p>原文作者：<a href="http://qfxiao.me">Hanzawa</a>
            <p>原文链接：<a href="http://qfxiao.me/2020/08/26/Machine-Learning-Ensemble-Algorithms-GBDT-and-XGBoost/">http://qfxiao.me/2020/08/26/Machine-Learning-Ensemble-Algorithms-GBDT-and-XGBoost/</a>
            <p>发表日期：<a href="http://qfxiao.me/2020/08/26/Machine-Learning-Ensemble-Algorithms-GBDT-and-XGBoost/">August 26th 2020, 12:28:26 am</a>
            <p>更新日期：<a href="http://qfxiao.me/2020/08/26/Machine-Learning-Ensemble-Algorithms-GBDT-and-XGBoost/">February 22nd 2021, 1:02:14 am</a>
            <p>版权声明：本文采用<a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
        </div>
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href= "/2020/08/26/Boosting-and-AdaBoost-Machine-Learning-Ensemble-Algorithms-1/" title= "Boosting and AdaBoost: Machine Learning Ensemble Algorithms 1">
                    <div class="nextTitle">Boosting and AdaBoost: Machine Learning Ensemble Algorithms 1</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2020/08/24/Unsupervised-Representation-Learning-by-Predicting-Random-Distances/" title= "Unsupervised Representation Learning by Predicting Random Distances">
                    <div class="prevTitle">Unsupervised Representation Learning by Predicting Random Distances</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!-- gitalk评论 -->

    <!-- utteranc评论 -->

    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:12345@qq.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/fi3ework" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
            
                <span class="iconfont-archer wechat" title=wechat>
                  
                  <img class="profile-qr" src="/assets/example_qr.png" />
                </span>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#preliminaries"><span class="toc-number">2.</span> <span class="toc-text">Preliminaries</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#taylor-formulation"><span class="toc-number">2.1.</span> <span class="toc-text">Taylor Formulation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gradient-descend-method"><span class="toc-number">2.2.</span> <span class="toc-text">Gradient Descend Method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#newtons-method"><span class="toc-number">2.3.</span> <span class="toc-text">Newton&#39;s Method</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#gradient-boosting-decision-tree-gbdt"><span class="toc-number">3.</span> <span class="toc-text">Gradient Boosting Decision Tree (GBDT)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#the-additive-model"><span class="toc-number">3.1.</span> <span class="toc-text">The Additive Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gradient-boosting-tree-for-regression"><span class="toc-number">3.2.</span> <span class="toc-text">Gradient Boosting Tree for Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gradient-boosting-tree-for-classification"><span class="toc-number">3.3.</span> <span class="toc-text">Gradient Boosting Tree for Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#binary-classification"><span class="toc-number">3.3.1.</span> <span class="toc-text">Binary Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-class-classfication"><span class="toc-number">3.3.2.</span> <span class="toc-text">Multi-class Classfication</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gbdt-sumarry"><span class="toc-number">3.4.</span> <span class="toc-text">GBDT Sumarry</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#xgboost"><span class="toc-number">4.</span> <span class="toc-text">XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#regularization"><span class="toc-number">4.1.</span> <span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#objective-function"><span class="toc-number">4.2.</span> <span class="toc-text">Objective Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#splitting-strategy"><span class="toc-number">4.3.</span> <span class="toc-text">Splitting Strategy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#exact-greedy-algorithm-for-split-finding"><span class="toc-number">4.3.1.</span> <span class="toc-text">Exact Greedy Algorithm for Split Finding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#approximate-algorithm-for-split-finding"><span class="toc-number">4.3.2.</span> <span class="toc-text">Approximate Algorithm for Split Finding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sparsity-aware-split-finding"><span class="toc-number">4.3.3.</span> <span class="toc-text">Sparsity-aware Split Finding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#other-features"><span class="toc-number">4.4.</span> <span class="toc-text">Other Features</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#block-structure-and-parallelism"><span class="toc-number">4.4.1.</span> <span class="toc-text">Block Structure and Parallelism</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#column-sample"><span class="toc-number">4.4.2.</span> <span class="toc-text">Column Sample</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shrinkage"><span class="toc-number">4.4.3.</span> <span class="toc-text">Shrinkage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#custom-loss-function"><span class="toc-number">4.4.4.</span> <span class="toc-text">Custom Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#missing-values"><span class="toc-number">4.4.5.</span> <span class="toc-text">Missing Values</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lightgbm"><span class="toc-number">5.</span> <span class="toc-text">LightGBM</span></a></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 33
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2020 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/23</span><a class="archive-post-title" href= "/2020/09/23/Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination/" >Unsupervised Feature Learning via Non-Parametric Instance Discrimination</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/17</span><a class="archive-post-title" href= "/2020/09/17/Representation-Learning-with-Contrastive-Predictive-Coding/" >Representation Learning with Contrastive Predictive Coding</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/09</span><a class="archive-post-title" href= "/2020/09/09/Model-Selection-and-Evaluation-Machine-Learning-Basics/" >Model Selection and Evaluation: Machine Learning Basics</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/2020/09/02/Machine-Learning-Classification-Algorithms-Decision-Trees/" >Machine Learning Classification Algorithms: Decision Trees</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/26</span><a class="archive-post-title" href= "/2020/08/26/Machine-Learning-Classification-Algorithms-Support-Vector-Machine/" >Machine Learning Classification Algorithms: Support Vector Machine</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/26</span><a class="archive-post-title" href= "/2020/08/26/Boosting-and-AdaBoost-Machine-Learning-Ensemble-Algorithms-1/" >Boosting and AdaBoost: Machine Learning Ensemble Algorithms 1</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/26</span><a class="archive-post-title" href= "/2020/08/26/Machine-Learning-Ensemble-Algorithms-GBDT-and-XGBoost/" >Machine Learning Ensemble Algorithms: GBDT and XGBoost</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/24</span><a class="archive-post-title" href= "/2020/08/24/Unsupervised-Representation-Learning-by-Predicting-Random-Distances/" >Unsupervised Representation Learning by Predicting Random Distances</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/14</span><a class="archive-post-title" href= "/2020/07/14/Effective-End-to-end-Unsupervised-Outlier-Detection-via-Linear-Priority-of-Discriminative-Network/" >Effective End-to-end Unsupervised Outlier Detection via Linear Priority of Discriminative Network</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/22</span><a class="archive-post-title" href= "/2020/06/22/Probability-Distributions-Binary-and-Multinomial-Variables/" >Probability Distributions - Binary and Multinomial Variables</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/13</span><a class="archive-post-title" href= "/2020/06/13/Time2Graph-Revisiting-Time-Series-Modeling-with-Dynamic-Shapelets/" >Time2Graph: Revisiting Time Series Modeling with Dynamic Shapelets</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/06</span><a class="archive-post-title" href= "/2020/06/06/Generative-Probabilistic-Novelty-Detection-with-Adversarial-Autoencoders/" >Generative Probabilistic Novelty Detection with Adversarial Autoencoders</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2020/06/02/Classification-based-Anomaly-Detection-for-General-Data/" >Classification-based Anomaly Detection for General Data</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2020/06/02/%E9%9D%A2%E5%90%91OpenPAI%E7%9A%84Docker%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E5%8F%8AOpenPAI%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/" >面向OpenPAI的Docker镜像配置及OpenPAI基本使用方法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2020/06/02/Ubuntu20-4LTS-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-CUDA11-cuDNN8-Tensorflow-Pytorch/" >Ubuntu20.04LTS 深度学习环境配置 CUDA11 + cuDNN8 + Tensorflow + Pytorch</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2020/06/01/Deep-Anomaly-Detection-Using-Geometric-Transformations/" >Deep Anomaly Detection Using Geometric Transformations</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/01</span><a class="archive-post-title" href= "/2020/06/01/Cross-dataset-Time-Series-Anomaly-Detection-for-Cloud-Systems/" >Cross-dataset Time Series Anomaly Detection for Cloud Systems</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/06</span><a class="archive-post-title" href= "/2020/05/06/Learning-Representations-of-Ultrahigh-dimensional-Data-for-Random-Distance-based-Outlier-Detection/" >Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/30</span><a class="archive-post-title" href= "/2020/03/30/Deep-Weakly-supervised-Anomaly-Detection/" >Deep Weakly-supervised Anomaly Detection</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/01</span><a class="archive-post-title" href= "/2020/03/01/Discovering-Physical-Concepts-with-Neural-Networks/" >Discovering Physical Concepts with Neural Networks</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/27</span><a class="archive-post-title" href= "/2020/02/27/Transfer-Anomaly-Detection-by-Inferring-Latent-Domain-Representations/" >Transfer Anomaly Detection by Inferring Latent Domain Representations</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/24</span><a class="archive-post-title" href= "/2020/02/24/Deep-Anomaly-Detection-with-Deviation-Networks/" >Deep Anomaly Detection with Deviation Networks</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/31</span><a class="archive-post-title" href= "/2020/01/31/Geant4-%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B/" >Geant4 安装教程与调试环境配置</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/09</span><a class="archive-post-title" href= "/2020/01/09/Complementary-Set-Variational-Autoencoder-for-Supervised-Anomaly-Detection/" >Complementary Set Variational Autoencoder for Supervised Anomaly Detection</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/29</span><a class="archive-post-title" href= "/2019/10/29/Anomaly-Detection-in-Streams-with-Extreme-Value-Theory/" >Anomaly Detection in Streams with Extreme Value Theory</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/22</span><a class="archive-post-title" href= "/2019/10/22/An-Introduction-to-Variational-Autoencoders/" >An Introduction to Variational Autoencoders</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/18</span><a class="archive-post-title" href= "/2019/10/18/Recurrent-Neural-Networks-for-Multivariate-Time-Series-with-Missing-Values/" >Recurrent Neural Networks for Multivariate Time Series with Missing Values</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/18</span><a class="archive-post-title" href= "/2019/10/18/Robust-Anomaly-Detection-for-Multivariate-Time-Series-through-Stochastic-Recurrent-Neural-Network/" >Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/16</span><a class="archive-post-title" href= "/2019/10/16/GAIN-Missing-Data-Imputation-using-Generative-Adversarial-Nets/" >GAIN: Missing Data Imputation using Generative Adversarial Nets</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2019/09/22/Anomaly-Detection-with-Generative-Adversarial-Networks-for-Multivariate-Time-Series/" >Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2019/09/22/ALSR-An-adaptive-label-screening-and-relearning-approach-for-interval-oriented-anomaly-detection/" >ALSR: An Adaptive Label Screening and Relearning Approach for Interval-Oriented Anomaly Detection</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2019/09/22/Time-Series-Anomaly-Detection-Service-at-Microsoft/" >Time-Series Anomaly Detection Service at Microsoft</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/22</span><a class="archive-post-title" href= "/2019/09/22/Unsupervised-Anomaly-Detection-via-Variational-Auto-Encoder-for-Seasonal-KPIs-in-Web-Applications/" >Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="Variational Inference"><span class="iconfont-archer">&#xe606;</span>Variational Inference</span>
    
        <span class="sidebar-tag-name" data-tags="VAE"><span class="iconfont-archer">&#xe606;</span>VAE</span>
    
        <span class="sidebar-tag-name" data-tags="Deep Learning"><span class="iconfont-archer">&#xe606;</span>Deep Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Time Series"><span class="iconfont-archer">&#xe606;</span>Time Series</span>
    
        <span class="sidebar-tag-name" data-tags="Statistics"><span class="iconfont-archer">&#xe606;</span>Statistics</span>
    
        <span class="sidebar-tag-name" data-tags="Machine Learning"><span class="iconfont-archer">&#xe606;</span>Machine Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Anomaly Detection"><span class="iconfont-archer">&#xe606;</span>Anomaly Detection</span>
    
        <span class="sidebar-tag-name" data-tags="Boosting"><span class="iconfont-archer">&#xe606;</span>Boosting</span>
    
        <span class="sidebar-tag-name" data-tags="Adaboost"><span class="iconfont-archer">&#xe606;</span>Adaboost</span>
    
        <span class="sidebar-tag-name" data-tags="Transfer Learning"><span class="iconfont-archer">&#xe606;</span>Transfer Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Self-Supervised Learning"><span class="iconfont-archer">&#xe606;</span>Self-Supervised Learning</span>
    
        <span class="sidebar-tag-name" data-tags="GAN"><span class="iconfont-archer">&#xe606;</span>GAN</span>
    
        <span class="sidebar-tag-name" data-tags="Novelty Detection"><span class="iconfont-archer">&#xe606;</span>Novelty Detection</span>
    
        <span class="sidebar-tag-name" data-tags="Geant4"><span class="iconfont-archer">&#xe606;</span>Geant4</span>
    
        <span class="sidebar-tag-name" data-tags="Representation Learning"><span class="iconfont-archer">&#xe606;</span>Representation Learning</span>
    
        <span class="sidebar-tag-name" data-tags="ID3"><span class="iconfont-archer">&#xe606;</span>ID3</span>
    
        <span class="sidebar-tag-name" data-tags="C4.5"><span class="iconfont-archer">&#xe606;</span>C4.5</span>
    
        <span class="sidebar-tag-name" data-tags="CART"><span class="iconfont-archer">&#xe606;</span>CART</span>
    
        <span class="sidebar-tag-name" data-tags="SVM"><span class="iconfont-archer">&#xe606;</span>SVM</span>
    
        <span class="sidebar-tag-name" data-tags="Probability"><span class="iconfont-archer">&#xe606;</span>Probability</span>
    
        <span class="sidebar-tag-name" data-tags="GBDT"><span class="iconfont-archer">&#xe606;</span>GBDT</span>
    
        <span class="sidebar-tag-name" data-tags="XGBoost"><span class="iconfont-archer">&#xe606;</span>XGBoost</span>
    
        <span class="sidebar-tag-name" data-tags="Overfitting"><span class="iconfont-archer">&#xe606;</span>Overfitting</span>
    
        <span class="sidebar-tag-name" data-tags="Bias"><span class="iconfont-archer">&#xe606;</span>Bias</span>
    
        <span class="sidebar-tag-name" data-tags="Variance"><span class="iconfont-archer">&#xe606;</span>Variance</span>
    
        <span class="sidebar-tag-name" data-tags="RNN"><span class="iconfont-archer">&#xe606;</span>RNN</span>
    
        <span class="sidebar-tag-name" data-tags="Contrastive Learning"><span class="iconfont-archer">&#xe606;</span>Contrastive Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Self-supervised Learning"><span class="iconfont-archer">&#xe606;</span>Self-supervised Learning</span>
    
        <span class="sidebar-tag-name" data-tags="Flow-based Model"><span class="iconfont-archer">&#xe606;</span>Flow-based Model</span>
    
        <span class="sidebar-tag-name" data-tags="Spectral"><span class="iconfont-archer">&#xe606;</span>Spectral</span>
    
        <span class="sidebar-tag-name" data-tags="Shapelet"><span class="iconfont-archer">&#xe606;</span>Shapelet</span>
    
        <span class="sidebar-tag-name" data-tags="Pytorch"><span class="iconfont-archer">&#xe606;</span>Pytorch</span>
    
        <span class="sidebar-tag-name" data-tags="Tensorflow"><span class="iconfont-archer">&#xe606;</span>Tensorflow</span>
    
        <span class="sidebar-tag-name" data-tags="Docker"><span class="iconfont-archer">&#xe606;</span>Docker</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="Research"><span class="iconfont-archer">&#xe60a;</span>Research</span>
    
        <span class="sidebar-category-name" data-categories="Research/Anomaly-Detection"><span class="iconfont-archer">&#xe60a;</span>Research/Anomaly-Detection</span>
    
        <span class="sidebar-category-name" data-categories="Research/Tutorial"><span class="iconfont-archer">&#xe60a;</span>Research/Tutorial</span>
    
        <span class="sidebar-category-name" data-categories="Technical-Notes"><span class="iconfont-archer">&#xe60a;</span>Technical-Notes</span>
    
        <span class="sidebar-category-name" data-categories="Research/Misc"><span class="iconfont-archer">&#xe60a;</span>Research/Misc</span>
    
        <span class="sidebar-category-name" data-categories="Research/Time-Series-Imputation"><span class="iconfont-archer">&#xe60a;</span>Research/Time-Series-Imputation</span>
    
        <span class="sidebar-category-name" data-categories="Technical-Notes/Misc"><span class="iconfont-archer">&#xe60a;</span>Technical-Notes/Misc</span>
    
        <span class="sidebar-category-name" data-categories="Technical-Notes/Machine-Learning"><span class="iconfont-archer">&#xe60a;</span>Technical-Notes/Machine-Learning</span>
    
        <span class="sidebar-category-name" data-categories="Research/Notes"><span class="iconfont-archer">&#xe60a;</span>Research/Notes</span>
    
        <span class="sidebar-category-name" data-categories="Research/RNN"><span class="iconfont-archer">&#xe60a;</span>Research/RNN</span>
    
        <span class="sidebar-category-name" data-categories="Research/Self-supervised-Learning"><span class="iconfont-archer">&#xe60a;</span>Research/Self-supervised-Learning</span>
    
        <span class="sidebar-category-name" data-categories="Research/Time-Series-Modeling"><span class="iconfont-archer">&#xe60a;</span>Research/Time-Series-Modeling</span>
    
        <span class="sidebar-category-name" data-categories="Research/Representation-Learning"><span class="iconfont-archer">&#xe60a;</span>Research/Representation-Learning</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "Hanzawa"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    </body>
</html>


