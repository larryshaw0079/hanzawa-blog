<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Unsupervised Feature Learning via Non-Parametric Instance Discrimination - Hanzawa の 部屋</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Hanzawa の 部屋"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Introduction 本文基于样本分类和噪声对比估计提出了一个无监督表示学习算法。下图展示了一个Intuition Example：  对于一个有监督的分类器，输入一张图片，作者观察到分类器的Softmax Response中较高的那些类都是在视觉上看起来比较接近的（美洲豹Leopard，美洲虎Jaguar，印度豹Cheetah），也就是说网络捕捉到了类间的视觉相似性，不过这是在有标签的"><meta property="og:type" content="article"><meta property="og:title" content="Unsupervised Feature Learning via Non-Parametric Instance Discrimination"><meta property="og:url" content="https://larryshaw0079.github.io/hanzawa-blog/2020/09/23/Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination/"><meta property="og:site_name" content="Hanzawa の 部屋"><meta property="og:description" content="Introduction 本文基于样本分类和噪声对比估计提出了一个无监督表示学习算法。下图展示了一个Intuition Example：  对于一个有监督的分类器，输入一张图片，作者观察到分类器的Softmax Response中较高的那些类都是在视觉上看起来比较接近的（美洲豹Leopard，美洲虎Jaguar，印度豹Cheetah），也就是说网络捕捉到了类间的视觉相似性，不过这是在有标签的"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://i.loli.net/2020/07/28/AimfJM7gtuDsPGQ.png"><meta property="og:image" content="https://i.loli.net/2020/07/26/WICKVkhrBu6Mci5.png"><meta property="og:image" content="https://i.loli.net/2020/08/06/nvS3Z7jEldVcCep.png"><meta property="og:image" content="https://i.loli.net/2020/07/29/Cl8xHeFZzXpvosL.png"><meta property="og:image" content="https://i.loli.net/2020/08/07/Zj628R7WYixJGog.png"><meta property="og:image" content="https://i.loli.net/2020/08/07/k6rx1LoaZiFYGpQ.png"><meta property="og:image" content="https://i.loli.net/2020/08/07/a3tNMQ7I2xmGdYA.png"><meta property="og:image" content="https://i.loli.net/2020/08/07/CK7s3wHbmgnv2j4.png"><meta property="og:image" content="https://i.loli.net/2020/08/07/rM7n3jhOiBbvJXf.png"><meta property="og:image" content="https://i.loli.net/2020/08/07/PVL4nlGFtqOdyRh.png"><meta property="og:image" content="https://i.loli.net/2020/08/07/F3miMXyOqg1DUtK.png"><meta property="article:published_time" content="2020-09-23T12:36:57.000Z"><meta property="article:modified_time" content="2021-02-28T04:58:08.309Z"><meta property="article:author" content="Hanzawa"><meta property="article:tag" content="Representation Learning"><meta property="article:tag" content="Self-supervised Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://i.loli.net/2020/07/28/AimfJM7gtuDsPGQ.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://larryshaw0079.github.io/hanzawa-blog/2020/09/23/Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination/"},"headline":"Unsupervised Feature Learning via Non-Parametric Instance Discrimination","image":["https://i.loli.net/2020/07/28/AimfJM7gtuDsPGQ.png","https://i.loli.net/2020/07/26/WICKVkhrBu6Mci5.png","https://i.loli.net/2020/08/06/nvS3Z7jEldVcCep.png","https://i.loli.net/2020/07/29/Cl8xHeFZzXpvosL.png","https://i.loli.net/2020/08/07/Zj628R7WYixJGog.png","https://i.loli.net/2020/08/07/k6rx1LoaZiFYGpQ.png","https://i.loli.net/2020/08/07/a3tNMQ7I2xmGdYA.png","https://i.loli.net/2020/08/07/CK7s3wHbmgnv2j4.png","https://i.loli.net/2020/08/07/rM7n3jhOiBbvJXf.png","https://i.loli.net/2020/08/07/PVL4nlGFtqOdyRh.png","https://i.loli.net/2020/08/07/F3miMXyOqg1DUtK.png"],"datePublished":"2020-09-23T12:36:57.000Z","dateModified":"2021-02-28T04:58:08.309Z","author":{"@type":"Person","name":"Hanzawa"},"publisher":{"@type":"Organization","name":"Hanzawa の 部屋","logo":{"@type":"ImageObject"}},"description":"Introduction\r 本文基于样本分类和噪声对比估计提出了一个无监督表示学习算法。下图展示了一个Intuition Example：\r \r 对于一个有监督的分类器，输入一张图片，作者观察到分类器的Softmax Response中较高的那些类都是在视觉上看起来比较接近的（美洲豹Leopard，美洲虎Jaguar，印度豹Cheetah），也就是说网络捕捉到了类间的视觉相似性，不过这是在有标签的"}</script><link rel="canonical" href="https://larryshaw0079.github.io/hanzawa-blog/2020/09/23/Unsupervised-Feature-Learning-via-Non-Parametric-Instance-Discrimination/"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Hanzawa の 部屋" type="application/atom+xml">
</head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Hanzawa の 部屋</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2020-09-23T12:36:57.000Z" title="2020-9-23 8:36:57 ├F10: PM┤">2020-09-23</time>发表</span><span class="level-item"><time dateTime="2021-02-28T04:58:08.309Z" title="2021-2-28 12:58:08 ├F10: PM┤">2021-02-28</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Research/">Research</a><span> / </span><a class="link-muted" href="/categories/Research/Self-supervised-Learning/">Self-supervised Learning</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</h1><div class="content"><h1 id="introduction">Introduction</h1>
<p>本文基于样本分类和噪声对比估计提出了一个无监督表示学习算法。下图展示了一个Intuition Example：</p>
<p><img src="https://i.loli.net/2020/07/28/AimfJM7gtuDsPGQ.png"  /></p>
<p>对于一个有监督的分类器，输入一张图片，作者观察到分类器的Softmax Response中较高的那些类都是在视觉上看起来比较接近的（美洲豹Leopard，美洲虎Jaguar，印度豹Cheetah），也就是说网络捕捉到了类间的视觉相似性，不过这是在有标签的情况下。对于无监督表示学习任务，作者将这个观察推广到了一个极端情况，就是把每一个样本都视作不同的类，然后让分类器来学习样本（类）间的视觉相似性。不过直接这么做会有严重的效率问题，所以作者还利用了Memory Bank机制和噪声对比估计来提高效率。</p>
<h1 id="proposed-method">Proposed Method</h1>
<p>学习一个嵌入表示函数<span class="math inline">\(\mathbf v=f_\theta(x)\)</span>。在表示空间中<span class="math inline">\(d_\theta(x,y)=\parallel f_\theta(x)-f_\theta(y)\parallel\)</span></p>
<p><img src="https://i.loli.net/2020/07/26/WICKVkhrBu6Mci5.png" /></p>
<h2 id="non-parametric-softmax-classifier">Non-Parametric Softmax Classifier</h2>
<h3 id="parametric-classifier">Parametric Classifier</h3>
<p>在经过嵌入表示函数之后，得到表示向量<span class="math inline">\(\mathbf v_i=f_\theta(\mathbf x_i)\)</span>。要基于这个向量进行分类， <span class="math display">\[
P(i|\mathbf v)=\frac{\exp(\mathbf w_i^\top\mathbf v)}{\sum_j\exp(\mathbf w_j^\top\mathbf v)}
\]</span> ### Non-Parametric Classifier</p>
<p><span class="math display">\[
P(i|\mathbf v)=\frac{\exp(\mathbf v_i^\top\mathbf v/\tau)}{\sum_j\exp(\mathbf v_j^\top\mathbf v/\tau)}
\]</span></p>
<p>同时约束<span class="math inline">\(\parallel \mathbf v\parallel=1\)</span></p>
<p>最后的损失函数为负对数似然损失（negative log-likelihood）： <span class="math display">\[
J(\theta)=-\sum_{i=1}^n\log P(i|f_\theta(x_i))
\]</span></p>
<p>到这里，算法的大框架就确定下来了，剩下的就是解决两个效率上的问题。一个是损失函数的计算每次都需要计算整个训练集的表示，同时Softmax函数由于分母对应的项目很多（等于训练集大小）在效率上也有问题。</p>
<h3 id="learning-with-a-memory-bank">Learning with A Memory Bank</h3>
<p>这里解决第一个效率问题。要计算损失函数，需要遍历整个训练集获得对应的表示，而在训练的时候是一批一批的数据，每次重新计算表示效率很低。为了解决这个问题，作者引入了缓存机制，即加入一个memory bank <span class="math inline">\(V\)</span>，用来保存计算好的表示<span class="math inline">\(\mathbf f_i=f_\theta(x_i)\)</span>。一开始<span class="math inline">\(V\)</span>采用单位随机向量初始化，之后在训练的时候不断更新<span class="math inline">\(\mathbf f_i\rightarrow \mathbf v_i\)</span>。</p>
<h2 id="noise-contrastive-estimation">Noise Contrastive Estimation</h2>
<p>第二个效率问题很容易想到使用噪声对比估计（Noise Contrastive Estimation, NCE）来做。NCE主要是将计算复杂的分母作为一个参数来进行优化： <span class="math display">\[
P(i|\mathbf v)=\frac{\exp(\mathbf v^\top\mathbf f_i/\tau)}{Z_i}
\]</span></p>
<p>其中<span class="math inline">\(Z_i=\sum_{j=1}^n\exp(\mathbf v^\top_j\mathbf f_i/\tau)\)</span>，噪声分布<span class="math inline">\(P_n=1/n\)</span>，如果噪声样本数量是真实数据的<span class="math inline">\(m\)</span>倍，那么随意给定一个样本，其属于真实样本的后验概率为： <span class="math display">\[
h(i,\mathbf v)=P(D=1|i,\mathbf v)=\frac{P(i|\mathbf v)}{P(i|\mathbf v)+mP_n(i)}=\sigma\left(s(\mathbf v)-\log \{m P_n(i)\}\right)
\]</span> 其中<span class="math inline">\(\Delta s=s(\mathbf v)-\log [m P_n(i)]\)</span>。这里的真实数据分布<span class="math inline">\(P_d\)</span>为。NCE的损失函数就是要最大化<span class="math inline">\(h(i,\mathbf v)\)</span>，最小化<span class="math inline">\(h(i,\mathbf v^\prime)\)</span> <span class="math display">\[
J_{NCE}(\theta)=-E_{P_d}[\log h(i,\mathbf v)]-m\cdot E_{P_n}[\log(1-h(i,\mathbf v^\prime))]
\]</span> 为了计算<span class="math inline">\(Z_i\)</span> <span class="math display">\[
Z\simeq Z_i\simeq nE_j[\exp(\mathbf v_j^\top\mathbf f_i/\tau)]=\frac{n}{m}\sum_{k=1}^m\exp(\mathbf v_{j_k}^\top\mathbf f_i/\tau)
\]</span></p>
<h2 id="proximal-regularization">Proximal Regularization</h2>
<p>每个类别只有一个样本 <span class="math display">\[
-\log h(i,\mathbf v_i^{(t-1)})+\lambda\parallel\mathbf v_i^{(i)}-\mathbf v_i^{(i-1)}\parallel^2_2
\]</span></p>
<p>最终的损失函数：</p>
<p><span class="math display">\[
J_{NCE}(\theta)=-E_{P_d}\left[\log h(i,\mathbf v_i^{(t-1)})-\lambda\parallel\mathbf v_i^{(t)}-\mathbf v_i^{(t-1)}\parallel^2_2\right]\\
-m\cdot E_{P_n}\left[\log(1-h(i,\mathbf v^{\prime(t-1)}))\right]
\]</span></p>
<p><img src="https://i.loli.net/2020/08/06/nvS3Z7jEldVcCep.png" style="zoom:67%;" /></p>
<h2 id="weighted-k-nearest-neighbor-classifier">Weighted k-Nearest Neighbor Classifier</h2>
<p><span class="math inline">\(s_i=\cos(\mathbf v_i,\hat{\mathbf f})\)</span>。记<span class="math inline">\(\mathcal N_k\)</span>。<span class="math inline">\(w_c=\sum_{i\in\mathcal N_k}\alpha_i\cdot 1(c_i=c)\)</span>。</p>
<p><img src="https://i.loli.net/2020/07/29/Cl8xHeFZzXpvosL.png" style="zoom:67%;" /></p>
<h1 id="experiments">Experiments</h1>
<p><img src="https://i.loli.net/2020/08/07/Zj628R7WYixJGog.png" style="zoom:67%;" /></p>
<p><img src="https://i.loli.net/2020/08/07/k6rx1LoaZiFYGpQ.png" style="zoom:67%;" /></p>
<p><img src="https://i.loli.net/2020/08/07/a3tNMQ7I2xmGdYA.png" /></p>
<p><img src="https://i.loli.net/2020/08/07/CK7s3wHbmgnv2j4.png" style="zoom:80%;" /></p>
<p><img src="https://i.loli.net/2020/08/07/rM7n3jhOiBbvJXf.png" style="zoom:67%;" /></p>
<p><img src="https://i.loli.net/2020/08/07/PVL4nlGFtqOdyRh.png" style="zoom:67%;" /></p>
<p><img src="https://i.loli.net/2020/08/07/F3miMXyOqg1DUtK.png" style="zoom:67%;" /></p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Representation-Learning/">Representation Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Self-supervised-Learning/">Self-supervised Learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/09/17/Representation-Learning-with-Contrastive-Predictive-Coding/"><span class="level-item">Representation Learning with Contrastive Predictive Coding</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Hanzawa の 部屋</a><p class="is-size-7"><span>&copy; 2021 Hanzawa</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>